"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
GloVe model loaded
Pretrained Embedding: GloVe
Italian: False
Loading data...
4681
11566
Max Document length: 36
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:77: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 7231
Train/Dev split: 4681/4681
2019-03-09 12:02:34.575010: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
34
33
32
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:485: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552154555

2019-03-09T12:02:38.749350: step 1, loss 1.45406, accuracy 0.496094, precision 0.7582417582417582, recall 0.39204545454545453
2019-03-09T12:02:39.513310: step 2, loss 1.30832, accuracy 0.519531, precision 0.3218390804597701, recall 0.30434782608695654

Evaluation:
[[ 131 1562]
 [ 294 2694]]
2019-03-09T12:02:42.213130: step 2, loss 0.868523, accuracy 0.603504, precision 0.07737743650324867, recall 0.30823529411764705

2019-03-09T12:02:43.019931: step 3, loss 1.25343, accuracy 0.605469, precision 0.21428571428571427, recall 0.33962264150943394
2019-03-09T12:02:43.668194: step 4, loss 1.42459, accuracy 0.597656, precision 0.12941176470588237, recall 0.275

Evaluation:
[[  42 1651]
 [ 142 2846]]
2019-03-09T12:02:45.390589: step 4, loss 0.976545, accuracy 0.616962, precision 0.024808033077377438, recall 0.22826086956521738

2019-03-09T12:02:45.861329: step 5, loss 1.32674, accuracy 0.605469, precision 0.1511627906976744, recall 0.3170731707317073
2019-03-09T12:02:46.324124: step 6, loss 1.42668, accuracy 0.597656, precision 0.17894736842105263, recall 0.40476190476190477

Evaluation:
[[ 158 1535]
 [ 399 2589]]
2019-03-09T12:02:47.743296: step 6, loss 0.856524, accuracy 0.58684, precision 0.09332545776727702, recall 0.2836624775583483

2019-03-09T12:02:48.214072: step 7, loss 1.19399, accuracy 0.582031, precision 0.2916666666666667, recall 0.417910447761194
2019-03-09T12:02:48.666826: step 8, loss 1.34595, accuracy 0.476562, precision 0.29591836734693877, recall 0.30851063829787234

Evaluation:
[[ 758  935]
 [1771 1217]]
2019-03-09T12:02:50.109967: step 8, loss 0.80637, accuracy 0.421918, precision 0.4477259303012404, recall 0.2997232107552392

2019-03-09T12:02:50.565785: step 9, loss 1.21301, accuracy 0.535156, precision 0.4946236559139785, recall 0.3898305084745763
2019-03-09T12:02:51.019535: step 10, loss 1.51154, accuracy 0.46875, precision 0.4074074074074074, recall 0.3793103448275862

Evaluation:
[[1168  525]
 [2292  696]]
2019-03-09T12:02:52.474644: step 10, loss 0.851426, accuracy 0.398206, precision 0.689899586532782, recall 0.3375722543352601

2019-03-09T12:02:52.944390: step 11, loss 1.40095, accuracy 0.496094, precision 0.5588235294117647, recall 0.40425531914893614
2019-03-09T12:02:53.420117: step 12, loss 1.1837, accuracy 0.523438, precision 0.5789473684210527, recall 0.40145985401459855

Evaluation:
[[ 876  817]
 [1967 1021]]
2019-03-09T12:02:54.848329: step 12, loss 0.819123, accuracy 0.405255, precision 0.5174246898995866, recall 0.30812521983819907

2019-03-09T12:02:55.314051: step 13, loss 1.37089, accuracy 0.472656, precision 0.4777777777777778, recall 0.3282442748091603
2019-03-09T12:02:55.762851: step 14, loss 1.31009, accuracy 0.53125, precision 0.3409090909090909, recall 0.32608695652173914

Evaluation:
[[ 303 1390]
 [ 783 2205]]
2019-03-09T12:02:57.211974: step 14, loss 0.817231, accuracy 0.535783, precision 0.1789722386296515, recall 0.27900552486187846

2019-03-09T12:02:57.666758: step 15, loss 1.09097, accuracy 0.605469, precision 0.4186046511627907, recall 0.41379310344827586
2019-03-09T12:02:58.115558: step 16, loss 1.35458, accuracy 0.542969, precision 0.3020833333333333, recall 0.3670886075949367

Evaluation:
[[  69 1624]
 [ 235 2753]]
2019-03-09T12:02:59.540746: step 16, loss 0.899591, accuracy 0.602863, precision 0.040756054341405785, recall 0.22697368421052633

2019-03-09T12:02:59.994532: step 17, loss 1.17746, accuracy 0.609375, precision 0.20512820512820512, recall 0.2962962962962963
2019-03-09T12:03:00.449317: step 18, loss 1.27292, accuracy 0.585938, precision 0.26732673267326734, recall 0.4576271186440678

Evaluation:
[[  38 1655]
 [ 166 2822]]
2019-03-09T12:03:01.877497: step 18, loss 0.928787, accuracy 0.610981, precision 0.02244536326048435, recall 0.18627450980392157

2019-03-09T12:03:02.132815: step 19, loss 1.46703, accuracy 0.410959, precision 0.14705882352941177, recall 0.2631578947368421
2019-03-09T12:03:02.591589: step 20, loss 1.01103, accuracy 0.644531, precision 0.3617021276595745, recall 0.5230769230769231

Evaluation:
[[ 138 1555]
 [ 413 2575]]
2019-03-09T12:03:04.073625: step 20, loss 0.845193, accuracy 0.579577, precision 0.08151210868281157, recall 0.25045372050816694

2019-03-09T12:03:04.538380: step 21, loss 0.970543, accuracy 0.640625, precision 0.39603960396039606, recall 0.5633802816901409
2019-03-09T12:03:04.993166: step 22, loss 0.930774, accuracy 0.671875, precision 0.4731182795698925, recall 0.5569620253164557

Evaluation:
[[ 500 1193]
 [1211 1777]]
2019-03-09T12:03:06.430322: step 22, loss 0.796477, accuracy 0.486435, precision 0.29533372711163614, recall 0.29222676797194624

2019-03-09T12:03:06.892086: step 23, loss 1.15444, accuracy 0.585938, precision 0.5252525252525253, recall 0.46846846846846846
2019-03-09T12:03:07.341885: step 24, loss 1.04589, accuracy 0.5625, precision 0.48936170212765956, recall 0.41818181818181815

Evaluation:
[[ 817  876]
 [1772 1216]]
2019-03-09T12:03:08.788016: step 24, loss 0.807143, accuracy 0.434309, precision 0.4825753101004135, recall 0.3155658555426806

2019-03-09T12:03:09.252773: step 25, loss 0.900288, accuracy 0.578125, precision 0.4946236559139785, recall 0.42990654205607476
2019-03-09T12:03:09.705596: step 26, loss 0.807454, accuracy 0.632812, precision 0.6931818181818182, recall 0.4765625

Evaluation:
[[ 768  925]
 [1672 1316]]
2019-03-09T12:03:12.112129: step 26, loss 0.807772, accuracy 0.445204, precision 0.4536326048434731, recall 0.31475409836065577

2019-03-09T12:03:12.770365: step 27, loss 1.12318, accuracy 0.566406, precision 0.46875, recall 0.42857142857142855
2019-03-09T12:03:13.344831: step 28, loss 0.941838, accuracy 0.605469, precision 0.5340909090909091, recall 0.4392523364485981

Evaluation:
[[ 449 1244]
 [1130 1858]]
2019-03-09T12:03:15.029324: step 28, loss 0.806427, accuracy 0.492843, precision 0.26520968694624925, recall 0.28435718809373023

2019-03-09T12:03:15.491089: step 29, loss 0.883194, accuracy 0.609375, precision 0.42857142857142855, recall 0.4482758620689655
2019-03-09T12:03:15.946871: step 30, loss 1.0186, accuracy 0.621094, precision 0.4742268041237113, recall 0.5

Evaluation:
[[ 207 1486]
 [ 641 2347]]
2019-03-09T12:03:17.370065: step 30, loss 0.834956, accuracy 0.54561, precision 0.12226816302421736, recall 0.24410377358490565

2019-03-09T12:03:17.823897: step 31, loss 0.970014, accuracy 0.625, precision 0.3516483516483517, recall 0.463768115942029
2019-03-09T12:03:18.274645: step 32, loss 0.764534, accuracy 0.683594, precision 0.4318181818181818, recall 0.5507246376811594

Evaluation:
[[ 134 1559]
 [ 433 2555]]
2019-03-09T12:03:19.698839: step 32, loss 0.856004, accuracy 0.57445, precision 0.07914943886591849, recall 0.23633156966490299

2019-03-09T12:03:20.164591: step 33, loss 0.882686, accuracy 0.675781, precision 0.358974358974359, recall 0.45901639344262296
2019-03-09T12:03:20.613393: step 34, loss 0.938958, accuracy 0.628906, precision 0.23958333333333334, recall 0.5111111111111111

Evaluation:
[[ 129 1564]
 [ 424 2564]]
2019-03-09T12:03:22.046560: step 34, loss 0.849505, accuracy 0.575304, precision 0.07619610159480213, recall 0.2332730560578662

2019-03-09T12:03:22.498352: step 35, loss 0.930492, accuracy 0.652344, precision 0.40860215053763443, recall 0.5277777777777778
2019-03-09T12:03:22.951140: step 36, loss 0.830675, accuracy 0.632812, precision 0.33707865168539325, recall 0.46153846153846156

Evaluation:
[[ 243 1450]
 [ 731 2257]]
2019-03-09T12:03:24.386301: step 36, loss 0.812037, accuracy 0.534074, precision 0.14353219137625517, recall 0.24948665297741274

2019-03-09T12:03:24.855049: step 37, loss 0.93348, accuracy 0.628906, precision 0.48, recall 0.5274725274725275
2019-03-09T12:03:25.105377: step 38, loss 0.864826, accuracy 0.616438, precision 0.375, recall 0.4090909090909091

Evaluation:
[[ 453 1240]
 [1145 1843]]
2019-03-09T12:03:26.534555: step 38, loss 0.79083, accuracy 0.490493, precision 0.26757235676314234, recall 0.2834793491864831

2019-03-09T12:03:27.001308: step 39, loss 0.86873, accuracy 0.636719, precision 0.5287356321839081, recall 0.46938775510204084
2019-03-09T12:03:27.455094: step 40, loss 0.919379, accuracy 0.625, precision 0.5588235294117647, recall 0.5277777777777778

Evaluation:
[[ 577 1116]
 [1354 1634]]
2019-03-09T12:03:28.876293: step 40, loss 0.789766, accuracy 0.472335, precision 0.3408151210868281, recall 0.2988089073019161

2019-03-09T12:03:29.329082: step 41, loss 0.813565, accuracy 0.664062, precision 0.5697674418604651, recall 0.5
2019-03-09T12:03:29.790847: step 42, loss 0.899007, accuracy 0.628906, precision 0.5268817204301075, recall 0.49

Evaluation:
[[ 527 1166]
 [1258 1730]]
2019-03-09T12:03:31.207098: step 42, loss 0.789488, accuracy 0.482162, precision 0.3112817483756645, recall 0.29523809523809524

2019-03-09T12:03:31.653864: step 43, loss 0.711169, accuracy 0.675781, precision 0.5875, recall 0.4845360824742268
2019-03-09T12:03:32.105659: step 44, loss 0.837349, accuracy 0.621094, precision 0.45569620253164556, recall 0.4

Evaluation:
[[ 286 1407]
 [ 793 2195]]
2019-03-09T12:03:33.555780: step 44, loss 0.802531, accuracy 0.530015, precision 0.16893089190785587, recall 0.26506024096385544

2019-03-09T12:03:34.004577: step 45, loss 0.879386, accuracy 0.65625, precision 0.5365853658536586, recall 0.46808510638297873
2019-03-09T12:03:34.457369: step 46, loss 0.877991, accuracy 0.648438, precision 0.4722222222222222, recall 0.6071428571428571

Evaluation:
[[ 127 1566]
 [ 467 2521]]
2019-03-09T12:03:35.946386: step 46, loss 0.841043, accuracy 0.565691, precision 0.07501476668635558, recall 0.2138047138047138

2019-03-09T12:03:36.398178: step 47, loss 0.777135, accuracy 0.644531, precision 0.34065934065934067, recall 0.5
2019-03-09T12:03:36.846013: step 48, loss 0.702218, accuracy 0.699219, precision 0.3978494623655914, recall 0.6379310344827587

Evaluation:
[[ 127 1566]
 [ 455 2533]]
2019-03-09T12:03:38.299095: step 48, loss 0.843971, accuracy 0.568255, precision 0.07501476668635558, recall 0.218213058419244

2019-03-09T12:03:38.764847: step 49, loss 0.748082, accuracy 0.703125, precision 0.4318181818181818, recall 0.59375
2019-03-09T12:03:39.206666: step 50, loss 0.768911, accuracy 0.648438, precision 0.41935483870967744, recall 0.52

Evaluation:
[[ 207 1486]
 [ 679 2309]]
2019-03-09T12:03:40.635843: step 50, loss 0.818432, accuracy 0.537492, precision 0.12226816302421736, recall 0.23363431151241534

2019-03-09T12:03:41.093619: step 51, loss 0.769632, accuracy 0.667969, precision 0.44660194174757284, recall 0.6216216216216216
2019-03-09T12:03:41.553389: step 52, loss 0.763195, accuracy 0.679688, precision 0.5157894736842106, recall 0.5764705882352941

Evaluation:
[[ 452 1241]
 [1147 1841]]
2019-03-09T12:03:43.009499: step 52, loss 0.796595, accuracy 0.489853, precision 0.2669816893089191, recall 0.28267667292057536

2019-03-09T12:03:43.467271: step 53, loss 0.671086, accuracy 0.679688, precision 0.5360824742268041, recall 0.5842696629213483
2019-03-09T12:03:43.945992: step 54, loss 0.678434, accuracy 0.65625, precision 0.5306122448979592, recall 0.5531914893617021

Evaluation:
[[ 757  936]
 [1649 1339]]
2019-03-09T12:03:45.384146: step 54, loss 0.803002, accuracy 0.447768, precision 0.44713526284701716, recall 0.31463009143807147

2019-03-09T12:03:45.883810: step 55, loss 0.778405, accuracy 0.664062, precision 0.6161616161616161, recall 0.5596330275229358
2019-03-09T12:03:46.340587: step 56, loss 0.811727, accuracy 0.652344, precision 0.5581395348837209, recall 0.48484848484848486

Evaluation:
[[ 824  869]
 [1729 1259]]
2019-03-09T12:03:47.759792: step 56, loss 0.806572, accuracy 0.44499, precision 0.48670998227997636, recall 0.3227575401488445

2019-03-09T12:03:48.013114: step 57, loss 0.724845, accuracy 0.69863, precision 0.6666666666666666, recall 0.6666666666666666
2019-03-09T12:03:48.512779: step 58, loss 0.786045, accuracy 0.667969, precision 0.6354166666666666, recall 0.5495495495495496

Evaluation:
[[ 710  983]
 [1567 1421]]
2019-03-09T12:03:49.939961: step 58, loss 0.797631, accuracy 0.455245, precision 0.4193738924985233, recall 0.31181379007465965

2019-03-09T12:03:50.395781: step 59, loss 0.648762, accuracy 0.671875, precision 0.574468085106383, recall 0.5510204081632653
2019-03-09T12:03:50.849531: step 60, loss 0.64149, accuracy 0.707031, precision 0.5473684210526316, recall 0.6190476190476191

Evaluation:
[[ 465 1228]
 [1168 1820]]
2019-03-09T12:03:52.310622: step 60, loss 0.792204, accuracy 0.488144, precision 0.2746603662138216, recall 0.2847519902020821

2019-03-09T12:03:52.756429: step 61, loss 0.735893, accuracy 0.667969, precision 0.4897959183673469, recall 0.5783132530120482
2019-03-09T12:03:53.215202: step 62, loss 0.641176, accuracy 0.703125, precision 0.49473684210526314, recall 0.6266666666666667

Evaluation:
[[ 344 1349]
 [ 940 2048]]
2019-03-09T12:03:54.655351: step 62, loss 0.794968, accuracy 0.511002, precision 0.20318960425280566, recall 0.26791277258566976

2019-03-09T12:03:55.110135: step 63, loss 0.625717, accuracy 0.722656, precision 0.5698924731182796, recall 0.6309523809523809
2019-03-09T12:03:55.558937: step 64, loss 0.575698, accuracy 0.726562, precision 0.4772727272727273, recall 0.6363636363636364

Evaluation:
[[ 294 1399]
 [ 861 2127]]
2019-03-09T12:03:57.056928: step 64, loss 0.796729, accuracy 0.517197, precision 0.17365623154164206, recall 0.2545454545454545

2019-03-09T12:03:57.511746: step 65, loss 0.610494, accuracy 0.71875, precision 0.5353535353535354, recall 0.6708860759493671
2019-03-09T12:03:57.956522: step 66, loss 0.748681, accuracy 0.652344, precision 0.46846846846846846, recall 0.6341463414634146

Evaluation:
[[ 387 1306]
 [1026 1962]]
2019-03-09T12:03:59.378719: step 66, loss 0.787968, accuracy 0.501816, precision 0.22858830478440637, recall 0.27388535031847133

2019-03-09T12:03:59.830545: step 67, loss 0.584872, accuracy 0.734375, precision 0.6483516483516484, recall 0.6210526315789474
2019-03-09T12:04:00.290315: step 68, loss 0.684381, accuracy 0.707031, precision 0.5411764705882353, recall 0.5609756097560976

Evaluation:
[[ 413 1280]
 [1074 1914]]
2019-03-09T12:04:01.714475: step 68, loss 0.78613, accuracy 0.497116, precision 0.24394565859421147, recall 0.2777404169468729

2019-03-09T12:04:02.168261: step 69, loss 0.625355, accuracy 0.742188, precision 0.5825242718446602, recall 0.7228915662650602
2019-03-09T12:04:02.617061: step 70, loss 0.691294, accuracy 0.703125, precision 0.575, recall 0.5227272727272727

Evaluation:
[[ 411 1282]
 [1083 1905]]
2019-03-09T12:04:04.060235: step 70, loss 0.787289, accuracy 0.494766, precision 0.24276432368576492, recall 0.2751004016064257

2019-03-09T12:04:04.512024: step 71, loss 0.48424, accuracy 0.785156, precision 0.6309523809523809, recall 0.6883116883116883
2019-03-09T12:04:04.956801: step 72, loss 0.731192, accuracy 0.683594, precision 0.449438202247191, recall 0.5555555555555556

Evaluation:
[[ 375 1318]
 [1016 1972]]
2019-03-09T12:04:06.384983: step 72, loss 0.792082, accuracy 0.501389, precision 0.22150029533372712, recall 0.2695902228612509

2019-03-09T12:04:06.889635: step 73, loss 0.654156, accuracy 0.648438, precision 0.4731182795698925, recall 0.5176470588235295
2019-03-09T12:04:07.336437: step 74, loss 0.639327, accuracy 0.683594, precision 0.46987951807228917, recall 0.5131578947368421

Evaluation:
[[ 318 1375]
 [ 967 2021]]
2019-03-09T12:04:08.764620: step 74, loss 0.799888, accuracy 0.49968, precision 0.1878322504430006, recall 0.24747081712062258

2019-03-09T12:04:09.226416: step 75, loss 0.631733, accuracy 0.710938, precision 0.5053763440860215, recall 0.6266666666666667
2019-03-09T12:04:09.484730: step 76, loss 0.610005, accuracy 0.69863, precision 0.43478260869565216, recall 0.5263157894736842

Evaluation:
[[ 303 1390]
 [ 947 2041]]
2019-03-09T12:04:10.893923: step 76, loss 0.803631, accuracy 0.500748, precision 0.1789722386296515, recall 0.2424

2019-03-09T12:04:11.348707: step 77, loss 0.49871, accuracy 0.769531, precision 0.5294117647058824, recall 0.703125
2019-03-09T12:04:11.798504: step 78, loss 0.548715, accuracy 0.742188, precision 0.6091954022988506, recall 0.6235294117647059

Evaluation:
[[ 333 1360]
 [ 998 1990]]
2019-03-09T12:04:13.248626: step 78, loss 0.802727, accuracy 0.496261, precision 0.19669226225634967, recall 0.2501878287002254

2019-03-09T12:04:13.712385: step 79, loss 0.574898, accuracy 0.753906, precision 0.5757575757575758, recall 0.7307692307692307
2019-03-09T12:04:14.174152: step 80, loss 0.495005, accuracy 0.765625, precision 0.6555555555555556, recall 0.6704545454545454

Evaluation:
[[ 395 1298]
 [1115 1873]]
2019-03-09T12:04:15.597344: step 80, loss 0.798813, accuracy 0.484512, precision 0.23331364441819255, recall 0.26158940397350994

2019-03-09T12:04:16.048139: step 81, loss 0.595763, accuracy 0.75, precision 0.5057471264367817, recall 0.676923076923077
2019-03-09T12:04:16.508906: step 82, loss 0.567035, accuracy 0.730469, precision 0.6145833333333334, recall 0.6483516483516484

Evaluation:
[[ 474 1219]
 [1228 1760]]
2019-03-09T12:04:17.974987: step 82, loss 0.796041, accuracy 0.477248, precision 0.27997637330183106, recall 0.2784958871915394

2019-03-09T12:04:18.433760: step 83, loss 0.491457, accuracy 0.789062, precision 0.5925925925925926, recall 0.6956521739130435
2019-03-09T12:04:18.896521: step 84, loss 0.619139, accuracy 0.714844, precision 0.5760869565217391, recall 0.6091954022988506

Evaluation:
[[ 488 1205]
 [1265 1723]]
2019-03-09T12:04:20.312736: step 84, loss 0.795928, accuracy 0.472335, precision 0.28824571766095686, recall 0.27837992013690815

2019-03-09T12:04:20.776528: step 85, loss 0.534447, accuracy 0.738281, precision 0.6043956043956044, recall 0.6395348837209303
2019-03-09T12:04:21.224296: step 86, loss 0.517333, accuracy 0.765625, precision 0.6333333333333333, recall 0.6785714285714286

Evaluation:
[[ 446 1247]
 [1200 1788]]
2019-03-09T12:04:22.657498: step 86, loss 0.795129, accuracy 0.477248, precision 0.26343768458357947, recall 0.2709599027946537

2019-03-09T12:04:23.109257: step 87, loss 0.521284, accuracy 0.78125, precision 0.6262626262626263, recall 0.7654320987654321
2019-03-09T12:04:23.568028: step 88, loss 0.579411, accuracy 0.695312, precision 0.5652173913043478, recall 0.5777777777777777

Evaluation:
[[ 398 1295]
 [1129 1859]]
2019-03-09T12:04:25.005185: step 88, loss 0.794965, accuracy 0.482162, precision 0.23508564678086238, recall 0.26064178127046495

2019-03-09T12:04:25.483905: step 89, loss 0.519055, accuracy 0.75, precision 0.6421052631578947, recall 0.6703296703296703
2019-03-09T12:04:25.947698: step 90, loss 0.573686, accuracy 0.746094, precision 0.5523809523809524, recall 0.7631578947368421

Evaluation:
[[ 393 1300]
 [1127 1861]]
2019-03-09T12:04:27.354903: step 90, loss 0.795042, accuracy 0.481521, precision 0.232132309509746, recall 0.25855263157894737

2019-03-09T12:04:27.837610: step 91, loss 0.555639, accuracy 0.730469, precision 0.5638297872340425, recall 0.654320987654321
2019-03-09T12:04:28.288404: step 92, loss 0.666831, accuracy 0.710938, precision 0.5955056179775281, recall 0.5824175824175825

Evaluation:
[[ 399 1294]
 [1157 1831]]
2019-03-09T12:04:29.719578: step 92, loss 0.794802, accuracy 0.476394, precision 0.23567631423508564, recall 0.256426735218509

2019-03-09T12:04:30.185333: step 93, loss 0.568369, accuracy 0.753906, precision 0.6237623762376238, recall 0.7159090909090909
2019-03-09T12:04:30.646100: step 94, loss 0.517583, accuracy 0.777344, precision 0.6304347826086957, recall 0.7160493827160493

Evaluation:
[[ 405 1288]
 [1175 1813]]
2019-03-09T12:04:32.065304: step 94, loss 0.795514, accuracy 0.47383, precision 0.23922031896042528, recall 0.2563291139240506

2019-03-09T12:04:32.320621: step 95, loss 0.495842, accuracy 0.739726, precision 0.6785714285714286, recall 0.6551724137931034
2019-03-09T12:04:32.781390: step 96, loss 0.511517, accuracy 0.769531, precision 0.6105263157894737, recall 0.725

Evaluation:
[[ 398 1295]
 [1144 1844]]
2019-03-09T12:04:34.217550: step 96, loss 0.797575, accuracy 0.478957, precision 0.23508564678086238, recall 0.25810635538262

2019-03-09T12:04:34.675325: step 97, loss 0.551653, accuracy 0.769531, precision 0.6451612903225806, recall 0.6976744186046512
2019-03-09T12:04:35.132104: step 98, loss 0.537129, accuracy 0.761719, precision 0.5816326530612245, recall 0.7402597402597403

Evaluation:
[[ 431 1262]
 [1196 1792]]
2019-03-09T12:04:36.547317: step 98, loss 0.795523, accuracy 0.474899, precision 0.25457767277023036, recall 0.26490473263675474

2019-03-09T12:04:37.021050: step 99, loss 0.490426, accuracy 0.734375, precision 0.6363636363636364, recall 0.6631578947368421
2019-03-09T12:04:37.478827: step 100, loss 0.546235, accuracy 0.734375, precision 0.6344086021505376, recall 0.6344086021505376

Evaluation:
[[ 428 1265]
 [1185 1803]]
2019-03-09T12:04:38.958868: step 100, loss 0.794362, accuracy 0.476608, precision 0.2528056704075605, recall 0.2653440793552387

2019-03-09T12:04:39.411696: step 101, loss 0.55954, accuracy 0.714844, precision 0.6125, recall 0.5384615384615384
2019-03-09T12:04:39.867440: step 102, loss 0.522548, accuracy 0.761719, precision 0.6391752577319587, recall 0.7045454545454546

Evaluation:
[[ 349 1344]
 [1045 1943]]
2019-03-09T12:04:41.286642: step 102, loss 0.798214, accuracy 0.489639, precision 0.20614294152392204, recall 0.2503586800573888

2019-03-09T12:04:41.746447: step 103, loss 0.429256, accuracy 0.820312, precision 0.6875, recall 0.7236842105263158
2019-03-09T12:04:42.193217: step 104, loss 0.568707, accuracy 0.722656, precision 0.5632183908045977, recall 0.5975609756097561

Evaluation:
[[ 265 1428]
 [ 899 2089]]
2019-03-09T12:04:43.617409: step 104, loss 0.80545, accuracy 0.502884, precision 0.15652687536916715, recall 0.22766323024054982

2019-03-09T12:04:44.082203: step 105, loss 0.403561, accuracy 0.8125, precision 0.5777777777777777, recall 0.8387096774193549
2019-03-09T12:04:44.538950: step 106, loss 0.503224, accuracy 0.78125, precision 0.6111111111111112, recall 0.7236842105263158

Evaluation:
[[ 290 1403]
 [ 963 2025]]
2019-03-09T12:04:45.954160: step 106, loss 0.801693, accuracy 0.494552, precision 0.17129356172474897, recall 0.23144453312051078

2019-03-09T12:04:46.418916: step 107, loss 0.507803, accuracy 0.777344, precision 0.5333333333333333, recall 0.7619047619047619
2019-03-09T12:04:46.875695: step 108, loss 0.44431, accuracy 0.785156, precision 0.6382978723404256, recall 0.7407407407407407

Evaluation:
[[ 379 1314]
 [1121 1867]]
2019-03-09T12:04:48.337787: step 108, loss 0.796319, accuracy 0.479812, precision 0.2238629651506202, recall 0.25266666666666665

2019-03-09T12:04:48.804537: step 109, loss 0.472699, accuracy 0.765625, precision 0.616822429906542, recall 0.7764705882352941
2019-03-09T12:04:49.262312: step 110, loss 0.562594, accuracy 0.753906, precision 0.6372549019607843, recall 0.7142857142857143

Evaluation:
[[ 532 1161]
 [1372 1616]]
2019-03-09T12:04:50.678525: step 110, loss 0.794771, accuracy 0.458876, precision 0.3142350856467809, recall 0.27941176470588236

2019-03-09T12:04:51.134306: step 111, loss 0.470156, accuracy 0.796875, precision 0.6947368421052632, recall 0.7415730337078652
2019-03-09T12:04:51.583106: step 112, loss 0.488982, accuracy 0.734375, precision 0.6551724137931034, recall 0.6

Evaluation:
[[ 585 1108]
 [1457 1531]]
2019-03-09T12:04:53.006302: step 112, loss 0.796812, accuracy 0.45204, precision 0.3455404607206143, recall 0.2864838393731636

2019-03-09T12:04:53.470061: step 113, loss 0.511139, accuracy 0.773438, precision 0.6888888888888889, recall 0.6739130434782609
2019-03-09T12:04:53.722387: step 114, loss 0.481292, accuracy 0.753425, precision 0.6538461538461539, recall 0.6538461538461539

Evaluation:
[[ 479 1214]
 [1290 1698]]
2019-03-09T12:04:55.155553: step 114, loss 0.798078, accuracy 0.465072, precision 0.2829297105729474, recall 0.2707744488411532

2019-03-09T12:04:55.609341: step 115, loss 0.380833, accuracy 0.839844, precision 0.7252747252747253, recall 0.8048780487804879
2019-03-09T12:04:56.058139: step 116, loss 0.46075, accuracy 0.78125, precision 0.6091954022988506, recall 0.7066666666666667

Evaluation:
[[ 366 1327]
 [1122 1866]]
2019-03-09T12:04:57.468369: step 116, loss 0.804581, accuracy 0.476821, precision 0.21618428824571767, recall 0.24596774193548387

2019-03-09T12:04:57.923184: step 117, loss 0.432041, accuracy 0.804688, precision 0.6633663366336634, recall 0.8072289156626506
2019-03-09T12:04:58.391897: step 118, loss 0.422418, accuracy 0.816406, precision 0.6477272727272727, recall 0.7808219178082192

Evaluation:
[[ 344 1349]
 [1100 1888]]
2019-03-09T12:04:59.847006: step 118, loss 0.807118, accuracy 0.476821, precision 0.20318960425280566, recall 0.23822714681440443

2019-03-09T12:05:00.334703: step 119, loss 0.466558, accuracy 0.792969, precision 0.6, recall 0.7916666666666666
2019-03-09T12:05:00.792477: step 120, loss 0.450674, accuracy 0.804688, precision 0.6504854368932039, recall 0.8271604938271605

Evaluation:
[[ 476 1217]
 [1303 1685]]
2019-03-09T12:05:02.221655: step 120, loss 0.802118, accuracy 0.461654, precision 0.28115770821027763, recall 0.26756604834176506

2019-03-09T12:05:02.687411: step 121, loss 0.513241, accuracy 0.75, precision 0.5544554455445545, recall 0.7466666666666667
2019-03-09T12:05:03.145187: step 122, loss 0.385769, accuracy 0.796875, precision 0.6868686868686869, recall 0.7640449438202247

Evaluation:
[[ 698  995]
 [1639 1349]]
2019-03-09T12:05:04.587329: step 122, loss 0.806942, accuracy 0.4373, precision 0.4122858830478441, recall 0.29867351305092

2019-03-09T12:05:05.068043: step 123, loss 0.425308, accuracy 0.800781, precision 0.6951219512195121, recall 0.6867469879518072
2019-03-09T12:05:05.529807: step 124, loss 0.380999, accuracy 0.828125, precision 0.7555555555555555, recall 0.7555555555555555

Evaluation:
[[ 683 1010]
 [1615 1373]]
2019-03-09T12:05:06.979930: step 124, loss 0.808021, accuracy 0.439222, precision 0.40342587123449497, recall 0.29721496953872933

2019-03-09T12:05:07.467626: step 125, loss 0.484653, accuracy 0.773438, precision 0.6938775510204082, recall 0.7083333333333334
2019-03-09T12:05:07.951332: step 126, loss 0.478218, accuracy 0.757812, precision 0.6597938144329897, recall 0.6881720430107527

Evaluation:
[[ 575 1118]
 [1478 1510]]
2019-03-09T12:05:09.436361: step 126, loss 0.806349, accuracy 0.445418, precision 0.3396337861783816, recall 0.2800779347296639

2019-03-09T12:05:09.933032: step 127, loss 0.452923, accuracy 0.828125, precision 0.7654320987654321, recall 0.7126436781609196
2019-03-09T12:05:10.414743: step 128, loss 0.362197, accuracy 0.832031, precision 0.6989247311827957, recall 0.8125

Evaluation:
[[ 369 1324]
 [1143 1845]]
2019-03-09T12:05:11.852898: step 128, loss 0.812594, accuracy 0.472976, precision 0.21795629060838748, recall 0.24404761904761904

2019-03-09T12:05:12.328627: step 129, loss 0.427734, accuracy 0.792969, precision 0.631578947368421, recall 0.7692307692307693
2019-03-09T12:05:12.808343: step 130, loss 0.441073, accuracy 0.808594, precision 0.6565656565656566, recall 0.8125

Evaluation:
[[ 295 1398]
 [ 999 1989]]
2019-03-09T12:05:14.256471: step 130, loss 0.824518, accuracy 0.48793, precision 0.17424689899586532, recall 0.22797527047913446

2019-03-09T12:05:14.732198: step 131, loss 0.361654, accuracy 0.824219, precision 0.6835443037974683, recall 0.7297297297297297
2019-03-09T12:05:15.205930: step 132, loss 0.451149, accuracy 0.789062, precision 0.5238095238095238, recall 0.7586206896551724

Evaluation:
[[ 256 1437]
 [ 934 2054]]
2019-03-09T12:05:16.631120: step 132, loss 0.833405, accuracy 0.493484, precision 0.1512108682811577, recall 0.21512605042016808

2019-03-09T12:05:16.913365: step 133, loss 0.397894, accuracy 0.821918, precision 0.7333333333333333, recall 0.8148148148148148
2019-03-09T12:05:17.374132: step 134, loss 0.470693, accuracy 0.777344, precision 0.5909090909090909, recall 0.7123287671232876

Evaluation:
[[ 241 1452]
 [ 906 2082]]
2019-03-09T12:05:18.857166: step 134, loss 0.837814, accuracy 0.496261, precision 0.14235085646780862, recall 0.2101133391455972

2019-03-09T12:05:19.342867: step 135, loss 0.409091, accuracy 0.820312, precision 0.6774193548387096, recall 0.7974683544303798
2019-03-09T12:05:19.861481: step 136, loss 0.460349, accuracy 0.75, precision 0.5825242718446602, recall 0.7407407407407407

Evaluation:
[[ 316 1377]
 [1012 1976]]
2019-03-09T12:05:21.291655: step 136, loss 0.829678, accuracy 0.489639, precision 0.18665091553455404, recall 0.23795180722891565

2019-03-09T12:05:21.772369: step 137, loss 0.374205, accuracy 0.828125, precision 0.6526315789473685, recall 0.8493150684931506
2019-03-09T12:05:22.238127: step 138, loss 0.359913, accuracy 0.871094, precision 0.78125, recall 0.8620689655172413

Evaluation:
[[ 436 1257]
 [1279 1709]]
2019-03-09T12:05:23.675281: step 138, loss 0.820622, accuracy 0.458235, precision 0.2575310100413467, recall 0.2542274052478134

2019-03-09T12:05:24.126107: step 139, loss 0.332227, accuracy 0.84375, precision 0.7473684210526316, recall 0.8160919540229885
2019-03-09T12:05:24.584848: step 140, loss 0.401442, accuracy 0.804688, precision 0.7083333333333334, recall 0.7555555555555555

Evaluation:
[[ 538 1155]
 [1437 1551]]
2019-03-09T12:05:26.015023: step 140, loss 0.820984, accuracy 0.446272, precision 0.3177790903721205, recall 0.2724050632911392

2019-03-09T12:05:26.463860: step 141, loss 0.338603, accuracy 0.863281, precision 0.8604651162790697, recall 0.7628865979381443
2019-03-09T12:05:26.911625: step 142, loss 0.362108, accuracy 0.855469, precision 0.8617021276595744, recall 0.7714285714285715

Evaluation:
[[ 452 1241]
 [1301 1687]]
2019-03-09T12:05:28.321888: step 142, loss 0.824331, accuracy 0.456954, precision 0.2669816893089191, recall 0.257843696520251

2019-03-09T12:05:28.778633: step 143, loss 0.363354, accuracy 0.820312, precision 0.7227722772277227, recall 0.8021978021978022
2019-03-09T12:05:29.234413: step 144, loss 0.377826, accuracy 0.820312, precision 0.7215189873417721, recall 0.7037037037037037

Evaluation:
[[ 384 1309]
 [1169 1819]]
2019-03-09T12:05:30.716452: step 144, loss 0.830781, accuracy 0.470626, precision 0.22681630242173656, recall 0.24726336123631681

2019-03-09T12:05:31.199159: step 145, loss 0.33892, accuracy 0.835938, precision 0.8235294117647058, recall 0.7216494845360825
2019-03-09T12:05:31.661924: step 146, loss 0.367418, accuracy 0.835938, precision 0.7303370786516854, recall 0.7831325301204819

Evaluation:
[[ 230 1463]
 [ 857 2131]]
2019-03-09T12:05:33.085115: step 146, loss 0.855362, accuracy 0.504379, precision 0.13585351447135263, recall 0.21159153633854647

2019-03-09T12:05:33.544886: step 147, loss 0.390605, accuracy 0.816406, precision 0.6808510638297872, recall 0.7901234567901234
2019-03-09T12:05:34.002661: step 148, loss 0.393902, accuracy 0.84375, precision 0.6276595744680851, recall 0.921875

Evaluation:
[[ 171 1522]
 [ 758 2230]]
2019-03-09T12:05:35.426853: step 148, loss 0.873276, accuracy 0.512925, precision 0.10100413467217956, recall 0.18406889128094725

2019-03-09T12:05:35.883631: step 149, loss 0.412017, accuracy 0.824219, precision 0.5569620253164557, recall 0.8148148148148148
2019-03-09T12:05:36.337417: step 150, loss 0.36489, accuracy 0.816406, precision 0.651685393258427, recall 0.7837837837837838

Evaluation:
[[ 228 1465]
 [ 869 2119]]
2019-03-09T12:05:37.768591: step 150, loss 0.859323, accuracy 0.501389, precision 0.13467217956290609, recall 0.20783956244302643

2019-03-09T12:05:38.230355: step 151, loss 0.416006, accuracy 0.804688, precision 0.6545454545454545, recall 0.8571428571428571
2019-03-09T12:05:38.475704: step 152, loss 0.39784, accuracy 0.821918, precision 0.6666666666666666, recall 0.8181818181818182

Evaluation:
[[ 391 1302]
 [1222 1766]]
2019-03-09T12:05:39.901885: step 152, loss 0.836157, accuracy 0.460799, precision 0.23095097460129946, recall 0.24240545567265964

2019-03-09T12:05:40.357669: step 153, loss 0.281418, accuracy 0.882812, precision 0.75, recall 0.8918918918918919
2019-03-09T12:05:40.845363: step 154, loss 0.368345, accuracy 0.84375, precision 0.7303370786516854, recall 0.8024691358024691

Evaluation:
[[ 598 1095]
 [1565 1423]]
2019-03-09T12:05:42.280524: step 154, loss 0.836142, accuracy 0.431745, precision 0.3532191376255168, recall 0.2764678687008784

2019-03-09T12:05:42.739296: step 155, loss 0.326809, accuracy 0.867188, precision 0.8125, recall 0.8297872340425532
2019-03-09T12:05:43.189128: step 156, loss 0.293019, accuracy 0.890625, precision 0.8488372093023255, recall 0.8295454545454546

Evaluation:
[[ 638 1055]
 [1638 1350]]
2019-03-09T12:05:44.634229: step 156, loss 0.839169, accuracy 0.424696, precision 0.37684583579444775, recall 0.28031634446397186

2019-03-09T12:05:45.091009: step 157, loss 0.349908, accuracy 0.863281, precision 0.8620689655172413, recall 0.7653061224489796
2019-03-09T12:05:45.543831: step 158, loss 0.298729, accuracy 0.894531, precision 0.8877551020408163, recall 0.8446601941747572

Evaluation:
[[ 511 1182]
 [1445 1543]]
2019-03-09T12:05:46.969984: step 158, loss 0.837672, accuracy 0.438795, precision 0.30183106910809215, recall 0.2612474437627812

2019-03-09T12:05:47.434740: step 159, loss 0.385631, accuracy 0.832031, precision 0.8023255813953488, recall 0.7263157894736842
2019-03-09T12:05:47.894510: step 160, loss 0.345789, accuracy 0.839844, precision 0.7373737373737373, recall 0.8295454545454546

Evaluation:
[[ 295 1398]
 [1066 1922]]
2019-03-09T12:05:49.324685: step 160, loss 0.851538, accuracy 0.473617, precision 0.17424689899586532, recall 0.21675238795003673

2019-03-09T12:05:49.777475: step 161, loss 0.307921, accuracy 0.878906, precision 0.7692307692307693, recall 0.821917808219178
2019-03-09T12:05:50.227275: step 162, loss 0.363028, accuracy 0.863281, precision 0.6853932584269663, recall 0.8970588235294118

Evaluation:
[[ 182 1511]
 [ 821 2167]]
2019-03-09T12:05:51.705321: step 162, loss 0.880208, accuracy 0.501816, precision 0.10750147666863556, recall 0.1814556331006979

2019-03-09T12:05:52.164094: step 163, loss 0.449534, accuracy 0.800781, precision 0.6288659793814433, recall 0.8026315789473685
2019-03-09T12:05:52.620872: step 164, loss 0.32484, accuracy 0.867188, precision 0.7, recall 0.9

Evaluation:
[[ 193 1500]
 [ 873 2115]]
2019-03-09T12:05:54.042072: step 164, loss 0.877746, accuracy 0.493057, precision 0.11399881866509155, recall 0.18105065666041276

2019-03-09T12:05:54.516800: step 165, loss 0.370673, accuracy 0.851562, precision 0.6808510638297872, recall 0.8888888888888888
2019-03-09T12:05:54.978565: step 166, loss 0.360389, accuracy 0.820312, precision 0.6448598130841121, recall 0.8961038961038961

Evaluation:
[[ 330 1363]
 [1148 1840]]
2019-03-09T12:05:56.398768: step 166, loss 0.855675, accuracy 0.463576, precision 0.19492025989367986, recall 0.2232746955345061

2019-03-09T12:05:56.870508: step 167, loss 0.316725, accuracy 0.871094, precision 0.75, recall 0.8888888888888888
2019-03-09T12:05:57.315315: step 168, loss 0.344885, accuracy 0.847656, precision 0.8426966292134831, recall 0.75

Evaluation:
[[ 492 1201]
 [1447 1541]]
2019-03-09T12:05:58.744493: step 168, loss 0.849879, accuracy 0.434309, precision 0.29060838747784995, recall 0.25373904074265086

2019-03-09T12:05:59.197282: step 169, loss 0.301364, accuracy 0.875, precision 0.8543689320388349, recall 0.8380952380952381
2019-03-09T12:05:59.647079: step 170, loss 0.314507, accuracy 0.847656, precision 0.7934782608695652, recall 0.7849462365591398

Evaluation:
[[ 546 1147]
 [1528 1460]]
2019-03-09T12:06:01.077255: step 170, loss 0.852088, accuracy 0.428541, precision 0.3225044300059067, recall 0.26325940212150434

2019-03-09T12:06:01.330579: step 171, loss 0.386724, accuracy 0.876712, precision 0.8275862068965517, recall 0.8571428571428571
2019-03-09T12:06:01.825255: step 172, loss 0.329086, accuracy 0.867188, precision 0.8478260869565217, recall 0.7959183673469388

Evaluation:
[[ 553 1140]
 [1538 1450]]
2019-03-09T12:06:03.250443: step 172, loss 0.853856, accuracy 0.4279, precision 0.3266391021854696, recall 0.26446676231468197

2019-03-09T12:06:03.710213: step 173, loss 0.341261, accuracy 0.859375, precision 0.8085106382978723, recall 0.8085106382978723
2019-03-09T12:06:04.159047: step 174, loss 0.266572, accuracy 0.902344, precision 0.8679245283018868, recall 0.8932038834951457

Evaluation:
[[ 479 1214]
 [1416 1572]]
2019-03-09T12:06:05.596172: step 174, loss 0.854421, accuracy 0.438154, precision 0.2829297105729474, recall 0.25277044854881264

2019-03-09T12:06:06.063918: step 175, loss 0.368837, accuracy 0.851562, precision 0.7647058823529411, recall 0.7831325301204819
2019-03-09T12:06:06.525686: step 176, loss 0.268211, accuracy 0.890625, precision 0.8421052631578947, recall 0.8602150537634409

Evaluation:
[[ 352 1341]
 [1191 1797]]
2019-03-09T12:06:07.939901: step 176, loss 0.860631, accuracy 0.45909, precision 0.20791494388659185, recall 0.22812702527543746

2019-03-09T12:06:08.405658: step 177, loss 0.289581, accuracy 0.886719, precision 0.8415841584158416, recall 0.8673469387755102
2019-03-09T12:06:08.859442: step 178, loss 0.312896, accuracy 0.859375, precision 0.6987951807228916, recall 0.8405797101449275

Evaluation:
[[ 264 1429]
 [1029 1959]]
2019-03-09T12:06:10.284631: step 178, loss 0.873686, accuracy 0.474899, precision 0.1559362079149439, recall 0.20417633410672853

2019-03-09T12:06:10.740446: step 179, loss 0.290259, accuracy 0.875, precision 0.7472527472527473, recall 0.8831168831168831
2019-03-09T12:06:11.189211: step 180, loss 0.304062, accuracy 0.867188, precision 0.6794871794871795, recall 0.8548387096774194

Evaluation:
[[ 248 1445]
 [ 998 1990]]
2019-03-09T12:06:12.655291: step 180, loss 0.876973, accuracy 0.478103, precision 0.14648552864737152, recall 0.19903691813804172

2019-03-09T12:06:13.112071: step 181, loss 0.336197, accuracy 0.859375, precision 0.6914893617021277, recall 0.9027777777777778
2019-03-09T12:06:13.560869: step 182, loss 0.283644, accuracy 0.875, precision 0.7444444444444445, recall 0.881578947368421

Evaluation:
[[ 300 1393]
 [1086 1902]]
2019-03-09T12:06:15.007036: step 182, loss 0.865485, accuracy 0.470412, precision 0.1772002362669817, recall 0.21645021645021645

2019-03-09T12:06:15.456801: step 183, loss 0.299259, accuracy 0.875, precision 0.7788461538461539, recall 0.9
2019-03-09T12:06:15.918565: step 184, loss 0.272213, accuracy 0.898438, precision 0.8602150537634409, recall 0.8602150537634409

Evaluation:
[[ 379 1314]
 [1236 1752]]
2019-03-09T12:06:17.336773: step 184, loss 0.855976, accuracy 0.455245, precision 0.2238629651506202, recall 0.2346749226006192

2019-03-09T12:06:17.797573: step 185, loss 0.293478, accuracy 0.882812, precision 0.8333333333333334, recall 0.8333333333333334
2019-03-09T12:06:18.248367: step 186, loss 0.345556, accuracy 0.839844, precision 0.75, recall 0.8089887640449438

Evaluation:
[[ 398 1295]
 [1277 1711]]
2019-03-09T12:06:19.655572: step 186, loss 0.856322, accuracy 0.450545, precision 0.23508564678086238, recall 0.23761194029850746

2019-03-09T12:06:20.109359: step 187, loss 0.317822, accuracy 0.863281, precision 0.8068181818181818, recall 0.797752808988764
2019-03-09T12:06:20.561148: step 188, loss 0.286381, accuracy 0.867188, precision 0.7558139534883721, recall 0.8333333333333334

Evaluation:
[[ 366 1327]
 [1204 1784]]
2019-03-09T12:06:21.978357: step 188, loss 0.861148, accuracy 0.459304, precision 0.21618428824571767, recall 0.23312101910828026

2019-03-09T12:06:22.471040: step 189, loss 0.372479, accuracy 0.832031, precision 0.7227722772277227, recall 0.8295454545454546
2019-03-09T12:06:22.710402: step 190, loss 0.317901, accuracy 0.90411, precision 0.8461538461538461, recall 0.88

Evaluation:
[[ 341 1352]
 [1167 1821]]
2019-03-09T12:06:24.122624: step 190, loss 0.866957, accuracy 0.461867, precision 0.20141760189013586, recall 0.22612732095490717

2019-03-09T12:06:24.576410: step 191, loss 0.292105, accuracy 0.878906, precision 0.8181818181818182, recall 0.8275862068965517
2019-03-09T12:06:25.023215: step 192, loss 0.280334, accuracy 0.886719, precision 0.8021978021978022, recall 0.8690476190476191

Evaluation:
[[ 299 1394]
 [1087 1901]]
2019-03-09T12:06:26.463363: step 192, loss 0.878971, accuracy 0.469985, precision 0.1766095688127584, recall 0.21572871572871574

2019-03-09T12:06:26.924131: step 193, loss 0.261038, accuracy 0.90625, precision 0.7954545454545454, recall 0.9210526315789473
2019-03-09T12:06:27.381907: step 194, loss 0.267962, accuracy 0.878906, precision 0.7472527472527473, recall 0.8947368421052632

Evaluation:
[[ 280 1413]
 [1047 1941]]
2019-03-09T12:06:28.796128: step 194, loss 0.889375, accuracy 0.474471, precision 0.16538688718251623, recall 0.21100226073850792

2019-03-09T12:06:29.257924: step 195, loss 0.349702, accuracy 0.867188, precision 0.7551020408163265, recall 0.8809523809523809
2019-03-09T12:06:29.703698: step 196, loss 0.299758, accuracy 0.867188, precision 0.7303370786516854, recall 0.8666666666666667

Evaluation:
[[ 313 1380]
 [1118 1870]]
2019-03-09T12:06:31.122904: step 196, loss 0.886642, accuracy 0.466353, precision 0.18487891317188423, recall 0.21872816212438853

2019-03-09T12:06:31.589656: step 197, loss 0.309792, accuracy 0.859375, precision 0.7052631578947368, recall 0.8933333333333333
2019-03-09T12:06:32.032470: step 198, loss 0.287024, accuracy 0.902344, precision 0.8111111111111111, recall 0.9012345679012346

Evaluation:
[[ 387 1306]
 [1282 1706]]
2019-03-09T12:06:33.502538: step 198, loss 0.880087, accuracy 0.447127, precision 0.22858830478440637, recall 0.23187537447573398

2019-03-09T12:06:33.955327: step 199, loss 0.321837, accuracy 0.855469, precision 0.7608695652173914, recall 0.8235294117647058
2019-03-09T12:06:34.402134: step 200, loss 0.301482, accuracy 0.886719, precision 0.8350515463917526, recall 0.8617021276595744

Evaluation:
[[ 443 1250]
 [1379 1609]]
2019-03-09T12:06:35.829318: step 200, loss 0.875417, accuracy 0.438368, precision 0.26166568222090963, recall 0.24313940724478594

2019-03-09T12:06:36.293077: step 201, loss 0.300212, accuracy 0.890625, precision 0.8921568627450981, recall 0.8425925925925926
2019-03-09T12:06:36.755838: step 202, loss 0.287602, accuracy 0.886719, precision 0.8202247191011236, recall 0.8488372093023255

Evaluation:
[[ 406 1287]
 [1317 1671]]
2019-03-09T12:06:38.207957: step 202, loss 0.876296, accuracy 0.443709, precision 0.23981098641464854, recall 0.23563551944283226

2019-03-09T12:06:38.666730: step 203, loss 0.304221, accuracy 0.871094, precision 0.8152173913043478, recall 0.8241758241758241
2019-03-09T12:06:39.137470: step 204, loss 0.305617, accuracy 0.886719, precision 0.7956989247311828, recall 0.8809523809523809

Evaluation:
[[ 348 1345]
 [1190 1798]]
2019-03-09T12:06:40.550690: step 204, loss 0.881188, accuracy 0.458449, precision 0.20555227406969875, recall 0.22626788036410922

2019-03-09T12:06:41.007469: step 205, loss 0.292401, accuracy 0.886719, precision 0.7586206896551724, recall 0.8918918918918919
2019-03-09T12:06:41.469233: step 206, loss 0.264716, accuracy 0.890625, precision 0.8315789473684211, recall 0.8681318681318682

Evaluation:
[[ 324 1369]
 [1148 1840]]
2019-03-09T12:06:42.907386: step 206, loss 0.887869, accuracy 0.462294, precision 0.19137625516834023, recall 0.22010869565217392

2019-03-09T12:06:43.390132: step 207, loss 0.285069, accuracy 0.894531, precision 0.8222222222222222, recall 0.8705882352941177
2019-03-09T12:06:43.853858: step 208, loss 0.270385, accuracy 0.898438, precision 0.797979797979798, recall 0.9294117647058824

Evaluation:
[[ 319 1374]
 [1138 1850]]
2019-03-09T12:06:45.266079: step 208, loss 0.890261, accuracy 0.463363, precision 0.18842291789722387, recall 0.2189430336307481

2019-03-09T12:06:45.513417: step 209, loss 0.23238, accuracy 0.917808, precision 0.8518518518518519, recall 0.92
2019-03-09T12:06:45.978174: step 210, loss 0.249394, accuracy 0.925781, precision 0.868421052631579, recall 0.88

Evaluation:
[[ 337 1356]
 [1179 1809]]
2019-03-09T12:06:47.384415: step 210, loss 0.88948, accuracy 0.458449, precision 0.19905493207324276, recall 0.22229551451187335

2019-03-09T12:06:47.848175: step 211, loss 0.243216, accuracy 0.90625, precision 0.8367346938775511, recall 0.9111111111111111
2019-03-09T12:06:48.294978: step 212, loss 0.222041, accuracy 0.941406, precision 0.8936170212765957, recall 0.9438202247191011

Evaluation:
[[ 351 1342]
 [1213 1775]]
2019-03-09T12:06:49.722165: step 212, loss 0.89104, accuracy 0.454176, precision 0.2073242764323686, recall 0.22442455242966752

2019-03-09T12:06:50.174950: step 213, loss 0.230255, accuracy 0.917969, precision 0.8913043478260869, recall 0.8817204301075269
2019-03-09T12:06:50.622753: step 214, loss 0.239006, accuracy 0.902344, precision 0.8202247191011236, recall 0.8902439024390244

Evaluation:
[[ 323 1370]
 [1168 1820]]
2019-03-09T12:06:52.045981: step 214, loss 0.897927, accuracy 0.457808, precision 0.19078558771411694, recall 0.21663313212608987

2019-03-09T12:06:52.509707: step 215, loss 0.281314, accuracy 0.878906, precision 0.7685185185185185, recall 0.9325842696629213
2019-03-09T12:06:52.960501: step 216, loss 0.21106, accuracy 0.902344, precision 0.8354430379746836, recall 0.8461538461538461

Evaluation:
[[ 365 1328]
 [1257 1731]]
2019-03-09T12:06:54.448522: step 216, loss 0.896295, accuracy 0.447768, precision 0.21559362079149438, recall 0.22503082614056721

2019-03-09T12:06:54.916271: step 217, loss 0.247751, accuracy 0.914062, precision 0.8865979381443299, recall 0.8865979381443299
2019-03-09T12:06:55.362080: step 218, loss 0.245729, accuracy 0.90625, precision 0.8260869565217391, recall 0.9047619047619048

Evaluation:
[[ 373 1320]
 [1278 1710]]
2019-03-09T12:06:56.792255: step 218, loss 0.898305, accuracy 0.44499, precision 0.22031896042528057, recall 0.225923682616596

2019-03-09T12:06:57.256015: step 219, loss 0.324456, accuracy 0.855469, precision 0.7692307692307693, recall 0.8602150537634409
2019-03-09T12:06:57.705810: step 220, loss 0.252589, accuracy 0.890625, precision 0.8061224489795918, recall 0.8977272727272727

Evaluation:
[[ 427 1266]
 [1379 1609]]
2019-03-09T12:06:59.126013: step 220, loss 0.895556, accuracy 0.43495, precision 0.25221500295333726, recall 0.2364341085271318

2019-03-09T12:06:59.584785: step 221, loss 0.29649, accuracy 0.90625, precision 0.8383838383838383, recall 0.9120879120879121
2019-03-09T12:07:00.037574: step 222, loss 0.216686, accuracy 0.917969, precision 0.8888888888888888, recall 0.8979591836734694

Evaluation:
[[ 495 1198]
 [1492 1496]]
2019-03-09T12:07:01.475729: step 222, loss 0.894062, accuracy 0.425336, precision 0.2923803898405198, recall 0.24911927528938096

2019-03-09T12:07:01.925527: step 223, loss 0.21198, accuracy 0.921875, precision 0.8803418803418803, recall 0.944954128440367
2019-03-09T12:07:02.377319: step 224, loss 0.262516, accuracy 0.914062, precision 0.9012345679012346, recall 0.8390804597701149

Evaluation:
[[ 522 1171]
 [1513 1475]]
2019-03-09T12:07:03.827441: step 224, loss 0.893097, accuracy 0.426618, precision 0.3083284111045481, recall 0.2565110565110565

2019-03-09T12:07:04.290203: step 225, loss 0.304414, accuracy 0.875, precision 0.8554216867469879, recall 0.7802197802197802
2019-03-09T12:07:04.741993: step 226, loss 0.330451, accuracy 0.863281, precision 0.825, recall 0.7586206896551724

Evaluation:
[[ 382 1311]
 [1274 1714]]
2019-03-09T12:07:06.181144: step 226, loss 0.897847, accuracy 0.447768, precision 0.22563496751329, recall 0.23067632850241546

2019-03-09T12:07:06.634933: step 227, loss 0.226061, accuracy 0.933594, precision 0.9090909090909091, recall 0.875
2019-03-09T12:07:06.887258: step 228, loss 0.359675, accuracy 0.876712, precision 0.7666666666666667, recall 0.92

Evaluation:
[[ 274 1419]
 [1064 1924]]
2019-03-09T12:07:08.327405: step 228, loss 0.918822, accuracy 0.469558, precision 0.16184288245717662, recall 0.20478325859491778

2019-03-09T12:07:08.780194: step 229, loss 0.250416, accuracy 0.902344, precision 0.7934782608695652, recall 0.9240506329113924
2019-03-09T12:07:09.239965: step 230, loss 0.223373, accuracy 0.898438, precision 0.7623762376237624, recall 0.9746835443037974

Evaluation:
[[ 276 1417]
 [1064 1924]]
2019-03-09T12:07:10.673131: step 230, loss 0.919589, accuracy 0.469985, precision 0.16302421736562314, recall 0.20597014925373133

2019-03-09T12:07:11.124924: step 231, loss 0.233363, accuracy 0.890625, precision 0.7835051546391752, recall 0.9156626506024096
2019-03-09T12:07:11.583696: step 232, loss 0.260179, accuracy 0.871094, precision 0.7474747474747475, recall 0.9024390243902439

Evaluation:
[[ 363 1330]
 [1252 1736]]
2019-03-09T12:07:13.014869: step 232, loss 0.90459, accuracy 0.448408, precision 0.21441228588304784, recall 0.22476780185758513

2019-03-09T12:07:13.473642: step 233, loss 0.205443, accuracy 0.925781, precision 0.8297872340425532, recall 0.9629629629629629
2019-03-09T12:07:13.925433: step 234, loss 0.237178, accuracy 0.910156, precision 0.8117647058823529, recall 0.9078947368421053

Evaluation:
[[ 540 1153]
 [1513 1475]]
2019-03-09T12:07:15.386527: step 234, loss 0.899216, accuracy 0.430464, precision 0.31896042528056706, recall 0.26302971261568436

2019-03-09T12:07:15.838318: step 235, loss 0.21277, accuracy 0.925781, precision 0.8846153846153846, recall 0.9292929292929293
2019-03-09T12:07:16.283128: step 236, loss 0.272111, accuracy 0.902344, precision 0.8645833333333334, recall 0.8736842105263158

Evaluation:
[[ 665 1028]
 [1669 1319]]
2019-03-09T12:07:17.711308: step 236, loss 0.90598, accuracy 0.423841, precision 0.3927938570584761, recall 0.28491859468723224

2019-03-09T12:07:18.175070: step 237, loss 0.232416, accuracy 0.929688, precision 0.9047619047619048, recall 0.8837209302325582
2019-03-09T12:07:18.626860: step 238, loss 0.246267, accuracy 0.917969, precision 0.9252336448598131, recall 0.8839285714285714

Evaluation:
[[ 606 1087]
 [1600 1388]]
2019-03-09T12:07:20.053047: step 238, loss 0.905549, accuracy 0.425977, precision 0.357944477259303, recall 0.2747053490480508

2019-03-09T12:07:20.504839: step 239, loss 0.293097, accuracy 0.894531, precision 0.8518518518518519, recall 0.8214285714285714
2019-03-09T12:07:20.953637: step 240, loss 0.241305, accuracy 0.914062, precision 0.8829787234042553, recall 0.8829787234042553

Evaluation:
[[ 435 1258]
 [1359 1629]]
2019-03-09T12:07:22.370848: step 240, loss 0.907277, accuracy 0.440931, precision 0.25694034258712345, recall 0.24247491638795987

2019-03-09T12:07:22.832616: step 241, loss 0.206034, accuracy 0.917969, precision 0.8333333333333334, recall 0.9411764705882353
2019-03-09T12:07:23.275430: step 242, loss 0.192937, accuracy 0.933594, precision 0.9036144578313253, recall 0.8928571428571429

Evaluation:
[[ 331 1362]
 [1166 1822]]
2019-03-09T12:07:24.720567: step 242, loss 0.921165, accuracy 0.459944, precision 0.19551092734790312, recall 0.22110888443553775

2019-03-09T12:07:25.194296: step 243, loss 0.238747, accuracy 0.921875, precision 0.8426966292134831, recall 0.9259259259259259
2019-03-09T12:07:25.653070: step 244, loss 0.259345, accuracy 0.894531, precision 0.7727272727272727, recall 0.9066666666666666

Evaluation:
[[ 276 1417]
 [1025 1963]]
2019-03-09T12:07:27.072276: step 244, loss 0.9369, accuracy 0.478317, precision 0.16302421736562314, recall 0.2121445042275173

2019-03-09T12:07:27.527058: step 245, loss 0.235195, accuracy 0.910156, precision 0.7857142857142857, recall 0.9295774647887324
2019-03-09T12:07:27.982839: step 246, loss 0.261043, accuracy 0.910156, precision 0.7777777777777778, recall 0.958904109589041

Evaluation:
[[ 304 1389]
 [1120 1868]]
2019-03-09T12:07:29.391073: step 246, loss 0.934187, accuracy 0.464003, precision 0.1795629060838748, recall 0.21348314606741572

2019-03-09T12:07:29.640406: step 247, loss 0.238245, accuracy 0.945205, precision 0.896551724137931, recall 0.9629629629629629
2019-03-09T12:07:30.089240: step 248, loss 0.209006, accuracy 0.921875, precision 0.8144329896907216, recall 0.9753086419753086

Evaluation:
[[ 415 1278]
 [1314 1674]]
2019-03-09T12:07:31.512400: step 248, loss 0.922613, accuracy 0.446272, precision 0.245126993502658, recall 0.24002313475997686

2019-03-09T12:07:31.974165: step 249, loss 0.191945, accuracy 0.945312, precision 0.91, recall 0.9479166666666666
2019-03-09T12:07:32.431941: step 250, loss 0.19669, accuracy 0.9375, precision 0.9130434782608695, recall 0.9130434782608695

Evaluation:
[[ 544 1149]
 [1523 1465]]
2019-03-09T12:07:33.893033: step 250, loss 0.923747, accuracy 0.429182, precision 0.32132309509746015, recall 0.26318335752298017

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552154555\checkpoints\model-250

2019-03-09T12:07:35.013042: step 251, loss 0.207837, accuracy 0.921875, precision 0.8571428571428571, recall 0.9
2019-03-09T12:07:35.823870: step 252, loss 0.187141, accuracy 0.941406, precision 0.9313725490196079, recall 0.9223300970873787

Evaluation:
[[ 591 1102]
 [1594 1394]]
2019-03-09T12:07:37.877380: step 252, loss 0.930216, accuracy 0.424055, precision 0.34908446544595395, recall 0.27048054919908465

2019-03-09T12:07:38.416935: step 253, loss 0.216363, accuracy 0.925781, precision 0.9431818181818182, recall 0.8556701030927835
2019-03-09T12:07:38.938540: step 254, loss 0.195961, accuracy 0.925781, precision 0.8804347826086957, recall 0.9101123595505618

Evaluation:
[[ 547 1146]
 [1528 1460]]
2019-03-09T12:07:41.042912: step 254, loss 0.934649, accuracy 0.428755, precision 0.32309509746012993, recall 0.2636144578313253

2019-03-09T12:07:41.621365: step 255, loss 0.206429, accuracy 0.933594, precision 0.945054945054945, recall 0.8775510204081632
2019-03-09T12:07:42.126048: step 256, loss 0.228539, accuracy 0.910156, precision 0.8791208791208791, recall 0.8695652173913043

Evaluation:
[[ 442 1251]
 [1344 1644]]
2019-03-09T12:07:44.067822: step 256, loss 0.941743, accuracy 0.445631, precision 0.2610750147666864, recall 0.24748040313549832

2019-03-09T12:07:44.607380: step 257, loss 0.238771, accuracy 0.910156, precision 0.8555555555555555, recall 0.8850574712643678
2019-03-09T12:07:45.119052: step 258, loss 0.210003, accuracy 0.917969, precision 0.8446601941747572, recall 0.9456521739130435

Evaluation:
[[ 365 1328]
 [1234 1754]]
2019-03-09T12:07:46.612018: step 258, loss 0.952183, accuracy 0.452681, precision 0.21559362079149438, recall 0.22826766729205752

2019-03-09T12:07:47.080764: step 259, loss 0.22487, accuracy 0.890625, precision 0.7708333333333334, recall 0.925
2019-03-09T12:07:47.532559: step 260, loss 0.221679, accuracy 0.90625, precision 0.8131868131868132, recall 0.9135802469135802

Evaluation:
[[ 365 1328]
 [1234 1754]]
2019-03-09T12:07:48.967718: step 260, loss 0.954989, accuracy 0.452681, precision 0.21559362079149438, recall 0.22826766729205752

2019-03-09T12:07:49.426493: step 261, loss 0.172343, accuracy 0.933594, precision 0.8478260869565217, recall 0.9629629629629629
2019-03-09T12:07:49.882273: step 262, loss 0.214217, accuracy 0.898438, precision 0.7586206896551724, recall 0.9295774647887324

Evaluation:
[[ 392 1301]
 [1267 1721]]
2019-03-09T12:07:51.303474: step 262, loss 0.952803, accuracy 0.451399, precision 0.23154164205552275, recall 0.23628691983122363

2019-03-09T12:07:51.763244: step 263, loss 0.212185, accuracy 0.914062, precision 0.8095238095238095, recall 0.918918918918919
2019-03-09T12:07:52.216031: step 264, loss 0.241193, accuracy 0.910156, precision 0.8404255319148937, recall 0.9080459770114943

Evaluation:
[[ 448 1245]
 [1340 1648]]
2019-03-09T12:07:53.646210: step 264, loss 0.947346, accuracy 0.447768, precision 0.264619019492026, recall 0.2505592841163311

2019-03-09T12:07:54.098032: step 265, loss 0.250341, accuracy 0.917969, precision 0.8478260869565217, recall 0.9176470588235294
2019-03-09T12:07:54.347334: step 266, loss 0.208527, accuracy 0.917808, precision 0.8518518518518519, recall 0.92

Evaluation:
[[ 490 1203]
 [1434 1554]]
2019-03-09T12:07:55.761553: step 266, loss 0.945305, accuracy 0.436659, precision 0.28942705256940343, recall 0.25467775467775466

2019-03-09T12:07:56.254234: step 267, loss 0.178524, accuracy 0.941406, precision 0.9361702127659575, recall 0.9072164948453608
2019-03-09T12:07:56.713005: step 268, loss 0.17457, accuracy 0.957031, precision 0.9285714285714286, recall 0.9397590361445783

Evaluation:
[[ 489 1204]
 [1436 1552]]
2019-03-09T12:07:58.156146: step 268, loss 0.949642, accuracy 0.436018, precision 0.28883638511518017, recall 0.25402597402597404

2019-03-09T12:07:58.609931: step 269, loss 0.219058, accuracy 0.941406, precision 0.926829268292683, recall 0.8941176470588236
2019-03-09T12:07:59.058766: step 270, loss 0.197927, accuracy 0.941406, precision 0.8823529411764706, recall 0.967741935483871

Evaluation:
[[ 424 1269]
 [1319 1669]]
2019-03-09T12:08:00.496886: step 270, loss 0.956888, accuracy 0.447127, precision 0.25044300059066743, recall 0.24325874928284566

2019-03-09T12:08:00.978597: step 271, loss 0.191429, accuracy 0.929688, precision 0.8785046728971962, recall 0.9494949494949495
2019-03-09T12:08:01.434415: step 272, loss 0.21482, accuracy 0.925781, precision 0.8829787234042553, recall 0.9120879120879121

Evaluation:
[[ 406 1287]
 [1300 1688]]
2019-03-09T12:08:02.890484: step 272, loss 0.963469, accuracy 0.44734, precision 0.23981098641464854, recall 0.23798358733880423

2019-03-09T12:08:03.343273: step 273, loss 0.18819, accuracy 0.945312, precision 0.872093023255814, recall 0.9615384615384616
2019-03-09T12:08:03.801051: step 274, loss 0.19859, accuracy 0.921875, precision 0.8588235294117647, recall 0.9012345679012346

Evaluation:
[[ 389 1304]
 [1275 1713]]
2019-03-09T12:08:05.213272: step 274, loss 0.970641, accuracy 0.449049, precision 0.2297696396928529, recall 0.23377403846153846

2019-03-09T12:08:05.681022: step 275, loss 0.172805, accuracy 0.9375, precision 0.8383838383838383, recall 1.0
2019-03-09T12:08:06.125866: step 276, loss 0.229037, accuracy 0.921875, precision 0.8828828828828829, recall 0.9333333333333333

Evaluation:
[[ 430 1263]
 [1341 1647]]
2019-03-09T12:08:07.602882: step 276, loss 0.972614, accuracy 0.443709, precision 0.2539870053160071, recall 0.24280067758328627

2019-03-09T12:08:08.056668: step 277, loss 0.172998, accuracy 0.941406, precision 0.8777777777777778, recall 0.9518072289156626
2019-03-09T12:08:08.507463: step 278, loss 0.169235, accuracy 0.921875, precision 0.8875, recall 0.8658536585365854

Evaluation:
[[ 443 1250]
 [1375 1613]]
2019-03-09T12:08:09.921680: step 278, loss 0.977045, accuracy 0.439222, precision 0.26166568222090963, recall 0.24367436743674367

2019-03-09T12:08:10.375469: step 279, loss 0.214263, accuracy 0.914062, precision 0.8837209302325582, recall 0.8636363636363636
2019-03-09T12:08:10.822306: step 280, loss 0.189641, accuracy 0.933594, precision 0.8645833333333334, recall 0.9540229885057471

Evaluation:
[[ 427 1266]
 [1341 1647]]
2019-03-09T12:08:12.254476: step 280, loss 0.984132, accuracy 0.443068, precision 0.25221500295333726, recall 0.2415158371040724

2019-03-09T12:08:12.717205: step 281, loss 0.187847, accuracy 0.945312, precision 0.9021739130434783, recall 0.9431818181818182
2019-03-09T12:08:13.168996: step 282, loss 0.215638, accuracy 0.929688, precision 0.875, recall 0.9166666666666666

Evaluation:
[[ 416 1277]
 [1332 1656]]
2019-03-09T12:08:14.587203: step 282, loss 0.987911, accuracy 0.44264, precision 0.24571766095688127, recall 0.2379862700228833

2019-03-09T12:08:15.052957: step 283, loss 0.171182, accuracy 0.953125, precision 0.9230769230769231, recall 0.9438202247191011
2019-03-09T12:08:15.507741: step 284, loss 0.145632, accuracy 0.957031, precision 0.9393939393939394, recall 0.9489795918367347

Evaluation:
[[ 401 1292]
 [1309 1679]]
2019-03-09T12:08:16.984791: step 284, loss 0.992632, accuracy 0.444349, precision 0.2368576491435322, recall 0.23450292397660819

2019-03-09T12:08:17.228140: step 285, loss 0.132535, accuracy 0.972603, precision 0.9259259259259259, recall 1.0
2019-03-09T12:08:17.676942: step 286, loss 0.190129, accuracy 0.933594, precision 0.9042553191489362, recall 0.9139784946236559

Evaluation:
[[ 387 1306]
 [1287 1701]]
2019-03-09T12:08:19.108113: step 286, loss 0.995763, accuracy 0.446059, precision 0.22858830478440637, recall 0.23118279569892472

2019-03-09T12:08:19.561901: step 287, loss 0.15569, accuracy 0.96875, precision 0.9333333333333333, recall 0.958904109589041
2019-03-09T12:08:20.020671: step 288, loss 0.151874, accuracy 0.960938, precision 0.9080459770114943, recall 0.9753086419753086

Evaluation:
[[ 353 1340]
 [1226 1762]]
2019-03-09T12:08:21.436886: step 288, loss 1.00221, accuracy 0.451827, precision 0.20850561134081513, recall 0.22355921469284357

2019-03-09T12:08:21.888677: step 289, loss 0.162711, accuracy 0.941406, precision 0.8921568627450981, recall 0.9578947368421052
2019-03-09T12:08:22.338473: step 290, loss 0.195501, accuracy 0.9375, precision 0.8541666666666666, recall 0.9761904761904762

Evaluation:
[[ 381 1312]
 [1266 1722]]
2019-03-09T12:08:23.766654: step 290, loss 1.00068, accuracy 0.449263, precision 0.22504430005906675, recall 0.23132969034608378

2019-03-09T12:08:24.234403: step 291, loss 0.183122, accuracy 0.945312, precision 0.9117647058823529, recall 0.9489795918367347
2019-03-09T12:08:24.683203: step 292, loss 0.171508, accuracy 0.941406, precision 0.9047619047619048, recall 0.9156626506024096

Evaluation:
[[ 424 1269]
 [1335 1653]]
2019-03-09T12:08:26.116370: step 292, loss 0.998636, accuracy 0.443709, precision 0.25044300059066743, recall 0.24104604889141557

2019-03-09T12:08:26.566170: step 293, loss 0.166764, accuracy 0.945312, precision 0.8863636363636364, recall 0.9512195121951219
2019-03-09T12:08:27.037906: step 294, loss 0.15585, accuracy 0.945312, precision 0.9130434782608695, recall 0.9333333333333333

Evaluation:
[[ 457 1236]
 [1388 1600]]
2019-03-09T12:08:28.533905: step 294, loss 0.999368, accuracy 0.439436, precision 0.26993502658003543, recall 0.24769647696476965

2019-03-09T12:08:29.015617: step 295, loss 0.147655, accuracy 0.9375, precision 0.9042553191489362, recall 0.9239130434782609
2019-03-09T12:08:29.476418: step 296, loss 0.194339, accuracy 0.9375, precision 0.9130434782608695, recall 0.9130434782608695

Evaluation:
[[ 451 1242]
 [1375 1613]]
2019-03-09T12:08:30.898581: step 296, loss 1.00091, accuracy 0.440931, precision 0.2663910218546958, recall 0.2469879518072289

2019-03-09T12:08:31.348378: step 297, loss 0.173895, accuracy 0.945312, precision 0.9072164948453608, recall 0.946236559139785
2019-03-09T12:08:31.798209: step 298, loss 0.164958, accuracy 0.941406, precision 0.9270833333333334, recall 0.9175257731958762

Evaluation:
[[ 453 1240]
 [1378 1610]]
2019-03-09T12:08:33.222367: step 298, loss 1.0019, accuracy 0.440718, precision 0.26757235676314234, recall 0.24740578918623704

2019-03-09T12:08:33.676153: step 299, loss 0.18521, accuracy 0.929688, precision 0.8695652173913043, recall 0.9302325581395349
2019-03-09T12:08:34.121963: step 300, loss 0.145997, accuracy 0.949219, precision 0.9080459770114943, recall 0.9404761904761905

Evaluation:
[[ 429 1264]
 [1334 1654]]
2019-03-09T12:08:35.536181: step 300, loss 1.00563, accuracy 0.44499, precision 0.25339633786178384, recall 0.24333522404991492

2019-03-09T12:08:35.993954: step 301, loss 0.213737, accuracy 0.917969, precision 0.8365384615384616, recall 0.9560439560439561
2019-03-09T12:08:36.446745: step 302, loss 0.148923, accuracy 0.953125, precision 0.9306930693069307, recall 0.9494949494949495

Evaluation:
[[ 444 1249]
 [1374 1614]]
2019-03-09T12:08:37.890883: step 302, loss 1.00569, accuracy 0.43965, precision 0.2622563496751329, recall 0.24422442244224424

2019-03-09T12:08:38.356672: step 303, loss 0.134844, accuracy 0.957031, precision 0.9411764705882353, recall 0.9302325581395349
2019-03-09T12:08:38.605968: step 304, loss 0.105666, accuracy 0.986301, precision 0.96, recall 1.0

Evaluation:
[[ 435 1258]
 [1360 1628]]
2019-03-09T12:08:40.045119: step 304, loss 1.0079, accuracy 0.440718, precision 0.25694034258712345, recall 0.24233983286908078

2019-03-09T12:08:40.508879: step 305, loss 0.186382, accuracy 0.933594, precision 0.8932038834951457, recall 0.9387755102040817
2019-03-09T12:08:40.967654: step 306, loss 0.162732, accuracy 0.953125, precision 0.9191919191919192, recall 0.9578947368421052

Evaluation:
[[ 443 1250]
 [1389 1599]]
2019-03-09T12:08:42.402816: step 306, loss 1.00939, accuracy 0.436232, precision 0.26166568222090963, recall 0.2418122270742358

2019-03-09T12:08:42.867572: step 307, loss 0.114367, accuracy 0.984375, precision 0.9659090909090909, recall 0.9883720930232558
2019-03-09T12:08:43.326344: step 308, loss 0.187557, accuracy 0.945312, precision 0.8705882352941177, recall 0.961038961038961

Evaluation:
[[ 458 1235]
 [1419 1569]]
2019-03-09T12:08:44.758516: step 308, loss 1.00995, accuracy 0.433027, precision 0.2705256940342587, recall 0.24400639318060735

2019-03-09T12:08:45.214295: step 309, loss 0.177226, accuracy 0.945312, precision 0.9029126213592233, recall 0.9587628865979382
2019-03-09T12:08:45.669081: step 310, loss 0.153985, accuracy 0.941406, precision 0.898989898989899, recall 0.9468085106382979

Evaluation:
[[ 482 1211]
 [1446 1542]]
2019-03-09T12:08:47.222924: step 310, loss 1.01225, accuracy 0.432386, precision 0.28470171293561725, recall 0.25

2019-03-09T12:08:47.679702: step 311, loss 0.162051, accuracy 0.949219, precision 0.9361702127659575, recall 0.9263157894736842
2019-03-09T12:08:48.134487: step 312, loss 0.192362, accuracy 0.925781, precision 0.9042553191489362, recall 0.8947368421052632

Evaluation:
[[ 466 1227]
 [1415 1573]]
2019-03-09T12:08:49.614528: step 312, loss 1.01661, accuracy 0.435591, precision 0.2752510336680449, recall 0.2477405635300372

2019-03-09T12:08:50.066322: step 313, loss 0.155006, accuracy 0.945312, precision 0.9367088607594937, recall 0.891566265060241
2019-03-09T12:08:50.518111: step 314, loss 0.134953, accuracy 0.972656, precision 0.9431818181818182, recall 0.9764705882352941

Evaluation:
[[ 405 1288]
 [1284 1704]]
2019-03-09T12:08:51.934326: step 314, loss 1.02607, accuracy 0.450545, precision 0.23922031896042528, recall 0.23978685612788633

2019-03-09T12:08:52.389108: step 315, loss 0.171526, accuracy 0.929688, precision 0.88, recall 0.9361702127659575
2019-03-09T12:08:52.833918: step 316, loss 0.169561, accuracy 0.921875, precision 0.8444444444444444, recall 0.926829268292683

Evaluation:
[[ 380 1313]
 [1235 1753]]
2019-03-09T12:08:54.247139: step 316, loss 1.03005, accuracy 0.455672, precision 0.22445363260484347, recall 0.23529411764705882

2019-03-09T12:08:54.719916: step 317, loss 0.176704, accuracy 0.917969, precision 0.8271604938271605, recall 0.9054054054054054
2019-03-09T12:08:55.166714: step 318, loss 0.150005, accuracy 0.953125, precision 0.9411764705882353, recall 0.9411764705882353

Evaluation:
[[ 366 1327]
 [1205 1783]]
2019-03-09T12:08:56.579900: step 318, loss 1.03219, accuracy 0.45909, precision 0.21618428824571767, recall 0.23297262889879058

2019-03-09T12:08:57.038674: step 319, loss 0.135859, accuracy 0.949219, precision 0.8850574712643678, recall 0.9625
2019-03-09T12:08:57.501470: step 320, loss 0.173304, accuracy 0.929688, precision 0.8472222222222222, recall 0.8970588235294118

Evaluation:
[[ 362 1331]
 [1198 1790]]
2019-03-09T12:08:58.944576: step 320, loss 1.03422, accuracy 0.459731, precision 0.21382161842882458, recall 0.23205128205128206

2019-03-09T12:08:59.412325: step 321, loss 0.168428, accuracy 0.941406, precision 0.8932038834951457, recall 0.9583333333333334
2019-03-09T12:08:59.866111: step 322, loss 0.151566, accuracy 0.949219, precision 0.8888888888888888, recall 0.9777777777777777

Evaluation:
[[ 396 1297]
 [1253 1735]]
2019-03-09T12:09:01.286314: step 322, loss 1.02924, accuracy 0.455245, precision 0.23390431187241584, recall 0.24014554275318375

2019-03-09T12:09:01.543626: step 323, loss 0.191377, accuracy 0.90411, precision 0.8148148148148148, recall 0.9166666666666666
2019-03-09T12:09:02.000404: step 324, loss 0.147394, accuracy 0.941406, precision 0.912621359223301, recall 0.94

Evaluation:
[[ 447 1246]
 [1344 1644]]
2019-03-09T12:09:03.421604: step 324, loss 1.02208, accuracy 0.446699, precision 0.2640283520378027, recall 0.24958123953098826

2019-03-09T12:09:03.872401: step 325, loss 0.168146, accuracy 0.957031, precision 0.9069767441860465, recall 0.9629629629629629
2019-03-09T12:09:04.336158: step 326, loss 0.125069, accuracy 0.972656, precision 0.9578947368421052, recall 0.9680851063829787

Evaluation:
[[ 498 1195]
 [1435 1553]]
2019-03-09T12:09:05.765335: step 326, loss 1.02097, accuracy 0.438154, precision 0.2941523922031896, recall 0.2576306259699948

2019-03-09T12:09:06.230091: step 327, loss 0.13427, accuracy 0.957031, precision 0.9560439560439561, recall 0.925531914893617
2019-03-09T12:09:06.693854: step 328, loss 0.127908, accuracy 0.964844, precision 0.9204545454545454, recall 0.9759036144578314

Evaluation:
[[ 507 1186]
 [1446 1542]]
2019-03-09T12:09:08.112062: step 328, loss 1.02535, accuracy 0.437727, precision 0.29946839929119906, recall 0.25960061443932414

2019-03-09T12:09:08.581802: step 329, loss 0.134548, accuracy 0.953125, precision 0.9405940594059405, recall 0.9405940594059405
2019-03-09T12:09:09.040576: step 330, loss 0.152922, accuracy 0.945312, precision 0.925, recall 0.9024390243902439

Evaluation:
[[ 456 1237]
 [1341 1647]]
2019-03-09T12:09:10.528598: step 330, loss 1.02872, accuracy 0.449263, precision 0.26934435912581217, recall 0.25375626043405675

2019-03-09T12:09:10.980388: step 331, loss 0.140358, accuracy 0.949219, precision 0.9175257731958762, recall 0.9468085106382979
2019-03-09T12:09:11.438163: step 332, loss 0.143722, accuracy 0.945312, precision 0.9010989010989011, recall 0.9425287356321839

Evaluation:
[[ 400 1293]
 [1244 1744]]
2019-03-09T12:09:12.886294: step 332, loss 1.03607, accuracy 0.458022, precision 0.23626698168930893, recall 0.24330900243309003

2019-03-09T12:09:13.342106: step 333, loss 0.152921, accuracy 0.957031, precision 0.9512195121951219, recall 0.9176470588235294
2019-03-09T12:09:13.791870: step 334, loss 0.153434, accuracy 0.929688, precision 0.8314606741573034, recall 0.961038961038961

Evaluation:
[[ 360 1333]
 [1168 1820]]
2019-03-09T12:09:15.209080: step 334, loss 1.04617, accuracy 0.465712, precision 0.21264028352037803, recall 0.2356020942408377

2019-03-09T12:09:15.680817: step 335, loss 0.132392, accuracy 0.957031, precision 0.8962264150943396, recall 1.0
2019-03-09T12:09:16.145574: step 336, loss 0.150703, accuracy 0.945312, precision 0.8631578947368421, recall 0.9879518072289156

Evaluation:
[[ 394 1299]
 [1242 1746]]
2019-03-09T12:09:17.577744: step 336, loss 1.03989, accuracy 0.457167, precision 0.2327229769639693, recall 0.24083129584352078

2019-03-09T12:09:18.025546: step 337, loss 0.119541, accuracy 0.964844, precision 0.9166666666666666, recall 0.9887640449438202
2019-03-09T12:09:18.471356: step 338, loss 0.196059, accuracy 0.921875, precision 0.8653846153846154, recall 0.9375

Evaluation:
[[ 484 1209]
 [1387 1601]]
2019-03-09T12:09:19.919483: step 338, loss 1.03064, accuracy 0.445418, precision 0.28588304784406376, recall 0.2586851950828434

2019-03-09T12:09:20.396207: step 339, loss 0.184995, accuracy 0.921875, precision 0.9111111111111111, recall 0.8723404255319149
2019-03-09T12:09:20.840020: step 340, loss 0.120392, accuracy 0.960938, precision 0.9578947368421052, recall 0.9381443298969072

Evaluation:
[[ 499 1194]
 [1414 1574]]
2019-03-09T12:09:22.254272: step 340, loss 1.03079, accuracy 0.442854, precision 0.2947430596574129, recall 0.26084683742812337

2019-03-09T12:09:22.728968: step 341, loss 0.128322, accuracy 0.964844, precision 0.9753086419753086, recall 0.9186046511627907
2019-03-09T12:09:22.977304: step 342, loss 0.174087, accuracy 0.945205, precision 0.8695652173913043, recall 0.9523809523809523

Evaluation:
[[ 428 1265]
 [1289 1699]]
2019-03-09T12:09:24.383543: step 342, loss 1.03833, accuracy 0.45439, precision 0.2528056704075605, recall 0.24927198602213163

2019-03-09T12:09:24.843317: step 343, loss 0.13673, accuracy 0.960938, precision 0.9072164948453608, recall 0.9887640449438202
2019-03-09T12:09:25.296137: step 344, loss 0.159483, accuracy 0.949219, precision 0.8977272727272727, recall 0.9518072289156626

Evaluation:
[[ 359 1334]
 [1156 1832]]
2019-03-09T12:09:27.650809: step 344, loss 1.05335, accuracy 0.468062, precision 0.21204961606615474, recall 0.23696369636963696

2019-03-09T12:09:28.294086: step 345, loss 0.200121, accuracy 0.921875, precision 0.8, recall 0.972972972972973
2019-03-09T12:09:29.067019: step 346, loss 0.128198, accuracy 0.949219, precision 0.9285714285714286, recall 0.9381443298969072

Evaluation:
[[ 350 1343]
 [1142 1846]]
2019-03-09T12:09:31.531461: step 346, loss 1.06002, accuracy 0.469131, precision 0.2067336089781453, recall 0.23458445040214476

2019-03-09T12:09:32.180694: step 347, loss 0.140616, accuracy 0.953125, precision 0.912621359223301, recall 0.9690721649484536
2019-03-09T12:09:32.797073: step 348, loss 0.10649, accuracy 0.976562, precision 0.9487179487179487, recall 0.9736842105263158

Evaluation:
[[ 364 1329]
 [1161 1827]]
2019-03-09T12:09:34.412724: step 348, loss 1.05936, accuracy 0.468062, precision 0.21500295333727112, recall 0.23868852459016393

2019-03-09T12:09:34.873492: step 349, loss 0.15669, accuracy 0.9375, precision 0.8764044943820225, recall 0.9397590361445783
2019-03-09T12:09:35.361186: step 350, loss 0.13593, accuracy 0.945312, precision 0.8854166666666666, recall 0.9659090909090909

Evaluation:
[[ 398 1295]
 [1221 1767]]
2019-03-09T12:09:36.812304: step 350, loss 1.05504, accuracy 0.462508, precision 0.23508564678086238, recall 0.2458307597282273

2019-03-09T12:09:37.277062: step 351, loss 0.13185, accuracy 0.96875, precision 0.9263157894736842, recall 0.9887640449438202
2019-03-09T12:09:37.730850: step 352, loss 0.144434, accuracy 0.953125, precision 0.9108910891089109, recall 0.968421052631579

Evaluation:
[[ 456 1237]
 [1312 1676]]
2019-03-09T12:09:39.195930: step 352, loss 1.04786, accuracy 0.455458, precision 0.26934435912581217, recall 0.2579185520361991

2019-03-09T12:09:39.659690: step 353, loss 0.167586, accuracy 0.9375, precision 0.9148936170212766, recall 0.9148936170212766
2019-03-09T12:09:40.102539: step 354, loss 0.142479, accuracy 0.953125, precision 0.9318181818181818, recall 0.9318181818181818

Evaluation:
[[ 470 1223]
 [1334 1654]]
2019-03-09T12:09:41.571577: step 354, loss 1.04978, accuracy 0.453749, precision 0.27761370348493797, recall 0.26053215077605324

2019-03-09T12:09:42.032345: step 355, loss 0.140095, accuracy 0.953125, precision 0.9659090909090909, recall 0.9042553191489362
2019-03-09T12:09:42.500094: step 356, loss 0.124483, accuracy 0.96875, precision 0.9425287356321839, recall 0.9647058823529412

Evaluation:
[[ 405 1288]
 [1230 1758]]
2019-03-09T12:09:43.940243: step 356, loss 1.0627, accuracy 0.462081, precision 0.23922031896042528, recall 0.24770642201834864

2019-03-09T12:09:44.398018: step 357, loss 0.166761, accuracy 0.9375, precision 0.8850574712643678, recall 0.927710843373494
2019-03-09T12:09:44.867796: step 358, loss 0.161158, accuracy 0.945312, precision 0.900990099009901, recall 0.9578947368421052

Evaluation:
[[ 366 1327]
 [1162 1826]]
2019-03-09T12:09:46.287001: step 358, loss 1.07576, accuracy 0.468276, precision 0.21618428824571767, recall 0.2395287958115183

2019-03-09T12:09:46.774661: step 359, loss 0.12444, accuracy 0.964844, precision 0.9375, recall 0.967741935483871
2019-03-09T12:09:47.236428: step 360, loss 0.139799, accuracy 0.960938, precision 0.9333333333333333, recall 0.9545454545454546

Evaluation:
[[ 353 1340]
 [1136 1852]]
2019-03-09T12:09:48.648651: step 360, loss 1.08433, accuracy 0.471053, precision 0.20850561134081513, recall 0.23707186030893218

2019-03-09T12:09:48.909951: step 361, loss 0.124109, accuracy 0.958904, precision 0.9259259259259259, recall 0.9615384615384616
2019-03-09T12:09:49.392660: step 362, loss 0.151462, accuracy 0.945312, precision 0.898989898989899, recall 0.956989247311828

Evaluation:
[[ 364 1329]
 [1169 1819]]
2019-03-09T12:09:50.810870: step 362, loss 1.08384, accuracy 0.466353, precision 0.21500295333727112, recall 0.2374429223744292

2019-03-09T12:09:51.262659: step 363, loss 0.150009, accuracy 0.953125, precision 0.9120879120879121, recall 0.9540229885057471
2019-03-09T12:09:51.748363: step 364, loss 0.133597, accuracy 0.964844, precision 0.9130434782608695, recall 0.9882352941176471

Evaluation:
[[ 395 1298]
 [1215 1773]]
2019-03-09T12:09:53.188512: step 364, loss 1.08075, accuracy 0.463149, precision 0.23331364441819255, recall 0.2453416149068323

2019-03-09T12:09:53.649279: step 365, loss 0.112013, accuracy 0.960938, precision 0.9157894736842105, recall 0.9775280898876404
2019-03-09T12:09:54.098110: step 366, loss 0.131021, accuracy 0.957031, precision 0.925531914893617, recall 0.9560439560439561

Evaluation:
[[ 423 1270]
 [1271 1717]]
2019-03-09T12:09:55.505314: step 366, loss 1.07769, accuracy 0.457167, precision 0.24985233313644417, recall 0.24970484061393153

2019-03-09T12:09:55.958104: step 367, loss 0.124563, accuracy 0.964844, precision 0.9479166666666666, recall 0.9578947368421052
2019-03-09T12:09:56.405905: step 368, loss 0.132842, accuracy 0.957031, precision 0.9130434782608695, recall 0.9655172413793104

Evaluation:
[[ 434 1259]
 [1290 1698]]
2019-03-09T12:09:57.845058: step 368, loss 1.07695, accuracy 0.455458, precision 0.2563496751329002, recall 0.2517401392111369

2019-03-09T12:09:58.300872: step 369, loss 0.140217, accuracy 0.945312, precision 0.9146341463414634, recall 0.9146341463414634
2019-03-09T12:09:58.754626: step 370, loss 0.109609, accuracy 0.972656, precision 0.9891304347826086, recall 0.9381443298969072

Evaluation:
[[ 405 1288]
 [1252 1736]]
2019-03-09T12:10:00.233668: step 370, loss 1.08145, accuracy 0.457381, precision 0.23922031896042528, recall 0.2444176222088111

2019-03-09T12:10:00.680507: step 371, loss 0.105822, accuracy 0.964844, precision 0.9529411764705882, recall 0.9418604651162791
2019-03-09T12:10:01.127278: step 372, loss 0.123667, accuracy 0.96875, precision 0.9626168224299065, recall 0.9626168224299065

Evaluation:
[[ 378 1315]
 [1206 1782]]
2019-03-09T12:10:02.621283: step 372, loss 1.08655, accuracy 0.46144, precision 0.22327229769639692, recall 0.23863636363636365

2019-03-09T12:10:03.083048: step 373, loss 0.165141, accuracy 0.949219, precision 0.9333333333333333, recall 0.9230769230769231
2019-03-09T12:10:03.526860: step 374, loss 0.111081, accuracy 0.96875, precision 0.9310344827586207, recall 0.9759036144578314

Evaluation:
[[ 355 1338]
 [1083 1905]]
2019-03-09T12:10:04.948060: step 374, loss 1.09379, accuracy 0.482803, precision 0.20968694624926165, recall 0.24687065368567454

2019-03-09T12:10:05.410822: step 375, loss 0.112936, accuracy 0.964844, precision 0.93, recall 0.9789473684210527
2019-03-09T12:10:05.851644: step 376, loss 0.158662, accuracy 0.9375, precision 0.8809523809523809, recall 0.925

Evaluation:
[[ 360 1333]
 [1094 1894]]
2019-03-09T12:10:07.498240: step 376, loss 1.09335, accuracy 0.481521, precision 0.21264028352037803, recall 0.24759284731774414

2019-03-09T12:10:07.966986: step 377, loss 0.157007, accuracy 0.945312, precision 0.9072164948453608, recall 0.946236559139785
2019-03-09T12:10:08.412794: step 378, loss 0.131643, accuracy 0.964844, precision 0.9390243902439024, recall 0.9506172839506173

Evaluation:
[[ 365 1328]
 [1110 1878]]
2019-03-09T12:10:09.832997: step 378, loss 1.09387, accuracy 0.479171, precision 0.21559362079149438, recall 0.24745762711864408

2019-03-09T12:10:10.310719: step 379, loss 0.129576, accuracy 0.964844, precision 0.9603960396039604, recall 0.9509803921568627
2019-03-09T12:10:10.564042: step 380, loss 0.0899741, accuracy 0.972603, precision 0.9259259259259259, recall 1.0

Evaluation:
[[ 387 1306]
 [1211 1777]]
2019-03-09T12:10:12.009177: step 380, loss 1.09221, accuracy 0.462294, precision 0.22858830478440637, recall 0.24217772215269087

2019-03-09T12:10:12.478921: step 381, loss 0.108492, accuracy 0.960938, precision 0.9369369369369369, recall 0.9719626168224299
2019-03-09T12:10:12.957668: step 382, loss 0.0941819, accuracy 0.980469, precision 0.9534883720930233, recall 0.9879518072289156

Evaluation:
[[ 420 1273]
 [1261 1727]]
2019-03-09T12:10:14.381865: step 382, loss 1.09126, accuracy 0.458663, precision 0.24808033077377437, recall 0.2498512790005949

2019-03-09T12:10:14.831630: step 383, loss 0.0832569, accuracy 0.992188, precision 0.9807692307692307, recall 1.0
2019-03-09T12:10:15.277436: step 384, loss 0.0841027, accuracy 0.988281, precision 0.9866666666666667, recall 0.9736842105263158

Evaluation:
[[ 433 1260]
 [1277 1711]]
2019-03-09T12:10:16.690658: step 384, loss 1.09484, accuracy 0.458022, precision 0.25575900767867693, recall 0.25321637426900584

2019-03-09T12:10:17.139456: step 385, loss 0.1108, accuracy 0.972656, precision 0.9787234042553191, recall 0.9484536082474226
2019-03-09T12:10:17.587258: step 386, loss 0.110919, accuracy 0.964844, precision 0.945054945054945, recall 0.9555555555555556

Evaluation:
[[ 413 1280]
 [1262 1726]]
2019-03-09T12:10:19.000479: step 386, loss 1.10276, accuracy 0.456954, precision 0.24394565859421147, recall 0.2465671641791045

2019-03-09T12:10:19.468230: step 387, loss 0.141694, accuracy 0.949219, precision 0.9120879120879121, recall 0.9431818181818182
2019-03-09T12:10:19.908051: step 388, loss 0.116383, accuracy 0.972656, precision 0.963855421686747, recall 0.9523809523809523

Evaluation:
[[ 388 1305]
 [1217 1771]]
2019-03-09T12:10:21.423003: step 388, loss 1.11392, accuracy 0.461226, precision 0.22917897223862965, recall 0.24174454828660435

2019-03-09T12:10:21.926655: step 389, loss 0.118061, accuracy 0.957031, precision 0.9157894736842105, recall 0.9666666666666667
2019-03-09T12:10:22.416380: step 390, loss 0.137711, accuracy 0.941406, precision 0.8857142857142857, recall 0.96875

Evaluation:
[[ 376 1317]
 [1185 1803]]
2019-03-09T12:10:23.990135: step 390, loss 1.12319, accuracy 0.465499, precision 0.22209096278795037, recall 0.24087123638693145

2019-03-09T12:10:24.443955: step 391, loss 0.11901, accuracy 0.960938, precision 0.9186046511627907, recall 0.9634146341463414
2019-03-09T12:10:24.894717: step 392, loss 0.125588, accuracy 0.957031, precision 0.9148936170212766, recall 0.9662921348314607

Evaluation:
[[ 381 1312]
 [1198 1790]]
2019-03-09T12:10:26.318907: step 392, loss 1.1282, accuracy 0.46379, precision 0.22504430005906675, recall 0.24129195693476885

2019-03-09T12:10:26.772693: step 393, loss 0.134181, accuracy 0.953125, precision 0.8977272727272727, recall 0.9634146341463414
2019-03-09T12:10:27.219530: step 394, loss 0.116989, accuracy 0.957031, precision 0.9042553191489362, recall 0.9770114942528736

Evaluation:
[[ 393 1300]
 [1231 1757]]
2019-03-09T12:10:28.649674: step 394, loss 1.13123, accuracy 0.459304, precision 0.232132309509746, recall 0.2419950738916256

2019-03-09T12:10:29.131386: step 395, loss 0.109598, accuracy 0.964844, precision 0.9222222222222223, recall 0.9764705882352941
2019-03-09T12:10:29.586172: step 396, loss 0.0800065, accuracy 0.984375, precision 0.9777777777777777, recall 0.9777777777777777

Evaluation:
[[ 417 1276]
 [1292 1696]]
2019-03-09T12:10:31.029309: step 396, loss 1.13227, accuracy 0.451399, precision 0.24630832841110456, recall 0.24400234055002926

2019-03-09T12:10:31.487085: step 397, loss 0.128022, accuracy 0.9375, precision 0.8554216867469879, recall 0.9466666666666667
2019-03-09T12:10:31.927906: step 398, loss 0.10666, accuracy 0.960938, precision 0.9223300970873787, recall 0.979381443298969

Evaluation:
[[ 444 1249]
 [1340 1648]]
2019-03-09T12:10:33.357086: step 398, loss 1.13518, accuracy 0.446913, precision 0.2622563496751329, recall 0.24887892376681614

2019-03-09T12:10:33.628360: step 399, loss 0.133794, accuracy 0.945205, precision 0.9333333333333333, recall 0.9333333333333333
2019-03-09T12:10:34.102092: step 400, loss 0.0946545, accuracy 0.980469, precision 0.978494623655914, recall 0.9680851063829787

Evaluation:
[[ 471 1222]
 [1377 1611]]
2019-03-09T12:10:35.527281: step 400, loss 1.13781, accuracy 0.444777, precision 0.2782043709391612, recall 0.25487012987012986

2019-03-09T12:10:35.988050: step 401, loss 0.119295, accuracy 0.957031, precision 0.9247311827956989, recall 0.9555555555555556
2019-03-09T12:10:36.443829: step 402, loss 0.0961412, accuracy 0.96875, precision 0.9509803921568627, recall 0.97

Evaluation:
[[ 491 1202]
 [1405 1583]]
2019-03-09T12:10:38.359742: step 402, loss 1.14033, accuracy 0.443068, precision 0.2900177200236267, recall 0.25896624472573837

2019-03-09T12:10:38.892316: step 403, loss 0.102761, accuracy 0.960938, precision 0.9270833333333334, recall 0.967391304347826
2019-03-09T12:10:39.394938: step 404, loss 0.15012, accuracy 0.945312, precision 0.898876404494382, recall 0.9411764705882353

Evaluation:
[[ 495 1198]
 [1413 1575]]
2019-03-09T12:10:40.827107: step 404, loss 1.14187, accuracy 0.442213, precision 0.2923803898405198, recall 0.25943396226415094

2019-03-09T12:10:41.305829: step 405, loss 0.118701, accuracy 0.980469, precision 0.9787234042553191, recall 0.968421052631579
2019-03-09T12:10:41.757621: step 406, loss 0.107471, accuracy 0.980469, precision 0.9662921348314607, recall 0.9772727272727273

Evaluation:
[[ 486 1207]
 [1392 1596]]
2019-03-09T12:10:43.188791: step 406, loss 1.14397, accuracy 0.444777, precision 0.28706438275251034, recall 0.25878594249201275

2019-03-09T12:10:43.664518: step 407, loss 0.121162, accuracy 0.953125, precision 0.95, recall 0.9313725490196079
2019-03-09T12:10:44.132268: step 408, loss 0.0961252, accuracy 0.976562, precision 0.9882352941176471, recall 0.9438202247191011

Evaluation:
[[ 434 1259]
 [1320 1668]]
2019-03-09T12:10:45.541533: step 408, loss 1.15292, accuracy 0.449049, precision 0.2563496751329002, recall 0.24743443557582667

2019-03-09T12:10:45.992294: step 409, loss 0.0659544, accuracy 0.988281, precision 0.9770114942528736, recall 0.9883720930232558
2019-03-09T12:10:46.446080: step 410, loss 0.089753, accuracy 0.972656, precision 0.9340659340659341, recall 0.9883720930232558

Evaluation:
[[ 385 1308]
 [1215 1773]]
2019-03-09T12:10:47.844340: step 410, loss 1.16653, accuracy 0.461013, precision 0.22740696987595985, recall 0.240625

2019-03-09T12:10:48.323099: step 411, loss 0.106444, accuracy 0.964844, precision 0.9239130434782609, recall 0.9770114942528736
2019-03-09T12:10:48.778841: step 412, loss 0.105542, accuracy 0.957031, precision 0.9191919191919192, recall 0.9680851063829787

Evaluation:
[[ 363 1330]
 [1176 1812]]
2019-03-09T12:10:50.198046: step 412, loss 1.17535, accuracy 0.464644, precision 0.21441228588304784, recall 0.23586744639376217

2019-03-09T12:10:50.653827: step 413, loss 0.106483, accuracy 0.953125, precision 0.9195402298850575, recall 0.9411764705882353
2019-03-09T12:10:51.159474: step 414, loss 0.118188, accuracy 0.964844, precision 0.9514563106796117, recall 0.9607843137254902

Evaluation:
[[ 356 1337]
 [1159 1829]]
2019-03-09T12:10:52.679410: step 414, loss 1.18173, accuracy 0.466781, precision 0.21027761370348494, recall 0.23498349834983498

2019-03-09T12:10:53.127213: step 415, loss 0.11675, accuracy 0.960938, precision 0.9318181818181818, recall 0.9534883720930233
2019-03-09T12:10:53.572057: step 416, loss 0.101504, accuracy 0.976562, precision 0.9484536082474226, recall 0.989247311827957

Evaluation:
[[ 369 1324]
 [1188 1800]]
2019-03-09T12:10:55.041094: step 416, loss 1.18347, accuracy 0.463363, precision 0.21795629060838748, recall 0.23699421965317918

2019-03-09T12:10:55.506886: step 417, loss 0.101606, accuracy 0.976562, precision 0.9230769230769231, recall 1.0
2019-03-09T12:10:55.747205: step 418, loss 0.0594694, accuracy 0.986301, precision 1.0, recall 0.967741935483871

Evaluation:
[[ 402 1291]
 [1271 1717]]
2019-03-09T12:10:57.156437: step 418, loss 1.18409, accuracy 0.452681, precision 0.23744831659775548, recall 0.2402869097429767

2019-03-09T12:10:57.622191: step 419, loss 0.105804, accuracy 0.960938, precision 0.9484536082474226, recall 0.9484536082474226
2019-03-09T12:10:58.085951: step 420, loss 0.0828239, accuracy 0.972656, precision 0.9375, recall 0.974025974025974

Evaluation:
[[ 413 1280]
 [1296 1692]]
2019-03-09T12:10:59.547045: step 420, loss 1.19079, accuracy 0.44969, precision 0.24394565859421147, recall 0.24166179052077238

2019-03-09T12:10:59.999835: step 421, loss 0.0879231, accuracy 0.980469, precision 0.9519230769230769, recall 1.0
2019-03-09T12:11:00.454618: step 422, loss 0.0909277, accuracy 0.972656, precision 0.9528301886792453, recall 0.9805825242718447

Evaluation:
[[ 446 1247]
 [1357 1631]]
2019-03-09T12:11:01.886786: step 422, loss 1.19517, accuracy 0.443709, precision 0.26343768458357947, recall 0.2473655019412091

2019-03-09T12:11:02.365538: step 423, loss 0.10961, accuracy 0.96875, precision 0.9574468085106383, recall 0.9574468085106383
2019-03-09T12:11:02.807324: step 424, loss 0.106873, accuracy 0.957031, precision 0.9325842696629213, recall 0.9431818181818182

Evaluation:
[[ 474 1219]
 [1431 1557]]
2019-03-09T12:11:04.219547: step 424, loss 1.19845, accuracy 0.433882, precision 0.27997637330183106, recall 0.24881889763779527

2019-03-09T12:11:04.708258: step 425, loss 0.100879, accuracy 0.96875, precision 0.9454545454545454, recall 0.9811320754716981
2019-03-09T12:11:05.196934: step 426, loss 0.0816187, accuracy 0.984375, precision 0.9753086419753086, recall 0.9753086419753086

Evaluation:
[[ 475 1218]
 [1440 1548]]
2019-03-09T12:11:06.606165: step 426, loss 1.20421, accuracy 0.432173, precision 0.2805670407560543, recall 0.24804177545691905

2019-03-09T12:11:07.053004: step 427, loss 0.105395, accuracy 0.964844, precision 0.925531914893617, recall 0.9775280898876404
2019-03-09T12:11:07.505762: step 428, loss 0.0860832, accuracy 0.980469, precision 0.9540229885057471, recall 0.9880952380952381

Evaluation:
[[ 479 1214]
 [1435 1553]]
2019-03-09T12:11:08.939924: step 428, loss 1.2086, accuracy 0.434095, precision 0.2829297105729474, recall 0.2502612330198537

2019-03-09T12:11:09.409669: step 429, loss 0.0867501, accuracy 0.976562, precision 0.9512195121951219, recall 0.975
2019-03-09T12:11:09.851485: step 430, loss 0.113326, accuracy 0.960938, precision 0.9130434782608695, recall 0.9767441860465116

Evaluation:
[[ 472 1221]
 [1422 1566]]
2019-03-09T12:11:11.305598: step 430, loss 1.21137, accuracy 0.435377, precision 0.27879503839338454, recall 0.249208025343189

2019-03-09T12:11:11.754398: step 431, loss 0.0917754, accuracy 0.972656, precision 0.9504950495049505, recall 0.9795918367346939
2019-03-09T12:11:12.198210: step 432, loss 0.114154, accuracy 0.960938, precision 0.9120879120879121, recall 0.9764705882352941

Evaluation:
[[ 478 1215]
 [1422 1566]]
2019-03-09T12:11:13.634369: step 432, loss 1.21206, accuracy 0.436659, precision 0.28233904311872415, recall 0.25157894736842107

2019-03-09T12:11:14.096136: step 433, loss 0.116723, accuracy 0.957031, precision 0.9054054054054054, recall 0.9436619718309859
2019-03-09T12:11:14.540978: step 434, loss 0.117752, accuracy 0.964844, precision 0.9583333333333334, recall 0.9484536082474226

Evaluation:
[[ 477 1216]
 [1418 1570]]
2019-03-09T12:11:15.995058: step 434, loss 1.2174, accuracy 0.4373, precision 0.2817483756645009, recall 0.2517150395778364

2019-03-09T12:11:16.451834: step 435, loss 0.118668, accuracy 0.964844, precision 0.9574468085106383, recall 0.9473684210526315
2019-03-09T12:11:17.052229: step 436, loss 0.114668, accuracy 0.964844, precision 0.9387755102040817, recall 0.968421052631579

Evaluation:
[[ 453 1240]
 [1383 1605]]
2019-03-09T12:11:18.713786: step 436, loss 1.22614, accuracy 0.43965, precision 0.26757235676314234, recall 0.24673202614379086

2019-03-09T12:11:18.997062: step 437, loss 0.0775234, accuracy 0.986301, precision 1.0, recall 0.9583333333333334
2019-03-09T12:11:19.477777: step 438, loss 0.073499, accuracy 0.980469, precision 0.968421052631579, recall 0.9787234042553191

Evaluation:
[[ 439 1254]
 [1362 1626]]
2019-03-09T12:11:20.882984: step 438, loss 1.2326, accuracy 0.441145, precision 0.25930301240401654, recall 0.24375347029428096

2019-03-09T12:11:21.338765: step 439, loss 0.085446, accuracy 0.988281, precision 0.9787234042553191, recall 0.989247311827957
2019-03-09T12:11:21.790559: step 440, loss 0.0913497, accuracy 0.972656, precision 0.9540229885057471, recall 0.9651162790697675

Evaluation:
[[ 413 1280]
 [1318 1670]]
2019-03-09T12:11:23.214749: step 440, loss 1.24403, accuracy 0.44499, precision 0.24394565859421147, recall 0.2385904101675332

2019-03-09T12:11:23.668536: step 441, loss 0.0849849, accuracy 0.96875, precision 0.9387755102040817, recall 0.9787234042553191
2019-03-09T12:11:24.121325: step 442, loss 0.0954226, accuracy 0.972656, precision 0.9313725490196079, recall 1.0

Evaluation:
[[ 405 1288]
 [1307 1681]]
2019-03-09T12:11:25.539531: step 442, loss 1.25313, accuracy 0.445631, precision 0.23922031896042528, recall 0.23656542056074767

2019-03-09T12:11:26.030218: step 443, loss 0.0765664, accuracy 0.988281, precision 0.9722222222222222, recall 1.0
2019-03-09T12:11:26.484007: step 444, loss 0.104694, accuracy 0.964844, precision 0.9418604651162791, recall 0.9529411764705882

Evaluation:
[[ 416 1277]
 [1340 1648]]
2019-03-09T12:11:27.915177: step 444, loss 1.2517, accuracy 0.440931, precision 0.24571766095688127, recall 0.23690205011389523

2019-03-09T12:11:28.402873: step 445, loss 0.0854051, accuracy 0.980469, precision 0.9382716049382716, recall 1.0
2019-03-09T12:11:28.896587: step 446, loss 0.115804, accuracy 0.972656, precision 0.9578947368421052, recall 0.9680851063829787

Evaluation:
[[ 441 1252]
 [1410 1578]]
2019-03-09T12:11:30.321742: step 446, loss 1.25105, accuracy 0.431318, precision 0.26048434731246306, recall 0.23824959481361427

2019-03-09T12:11:30.768547: step 447, loss 0.106326, accuracy 0.976562, precision 0.9489795918367347, recall 0.9893617021276596
2019-03-09T12:11:31.216348: step 448, loss 0.0720178, accuracy 0.984375, precision 0.979381443298969, recall 0.979381443298969

Evaluation:
[[ 479 1214]
 [1472 1516]]
2019-03-09T12:11:32.632561: step 448, loss 1.25191, accuracy 0.426191, precision 0.2829297105729474, recall 0.24551512045105076

2019-03-09T12:11:33.088343: step 449, loss 0.0956854, accuracy 0.96875, precision 0.9484536082474226, recall 0.968421052631579
2019-03-09T12:11:33.535147: step 450, loss 0.0933032, accuracy 0.976562, precision 0.963855421686747, recall 0.963855421686747

Evaluation:
[[ 500 1193]
 [1511 1477]]
2019-03-09T12:11:34.957345: step 450, loss 1.2543, accuracy 0.422346, precision 0.29533372711163614, recall 0.2486325211337643

2019-03-09T12:11:35.411131: step 451, loss 0.0825768, accuracy 0.980469, precision 0.963855421686747, recall 0.975609756097561
2019-03-09T12:11:35.867908: step 452, loss 0.122744, accuracy 0.964844, precision 0.9494949494949495, recall 0.9591836734693877

Evaluation:
[[ 490 1203]
 [1488 1500]]
2019-03-09T12:11:37.341967: step 452, loss 1.25925, accuracy 0.425123, precision 0.28942705256940343, recall 0.24772497472194135

2019-03-09T12:11:37.789771: step 453, loss 0.0852988, accuracy 0.976562, precision 0.9294117647058824, recall 1.0
2019-03-09T12:11:38.240563: step 454, loss 0.107789, accuracy 0.957031, precision 0.9431818181818182, recall 0.9325842696629213

Evaluation:
[[ 458 1235]
 [1434 1554]]
2019-03-09T12:11:39.677721: step 454, loss 1.26665, accuracy 0.429823, precision 0.2705256940342587, recall 0.24207188160676532

2019-03-09T12:11:40.130509: step 455, loss 0.0957533, accuracy 0.976562, precision 0.9431818181818182, recall 0.9880952380952381
2019-03-09T12:11:40.385826: step 456, loss 0.0819745, accuracy 0.972603, precision 0.9655172413793104, recall 0.9655172413793104

Evaluation:
[[ 430 1263]
 [1381 1607]]
2019-03-09T12:11:41.804035: step 456, loss 1.27525, accuracy 0.435163, precision 0.2539870053160071, recall 0.23743787962451685

2019-03-09T12:11:42.256857: step 457, loss 0.0959766, accuracy 0.964844, precision 0.9058823529411765, recall 0.9871794871794872
2019-03-09T12:11:42.722580: step 458, loss 0.100577, accuracy 0.953125, precision 0.8865979381443299, recall 0.9885057471264368

Evaluation:
[[ 428 1265]
 [1363 1625]]
2019-03-09T12:11:44.160731: step 458, loss 1.2791, accuracy 0.438581, precision 0.2528056704075605, recall 0.23897264098269122

2019-03-09T12:11:44.616512: step 459, loss 0.0909138, accuracy 0.976562, precision 0.961038961038961, recall 0.961038961038961
2019-03-09T12:11:45.063319: step 460, loss 0.0905747, accuracy 0.976562, precision 0.9659090909090909, recall 0.9659090909090909

Evaluation:
[[ 433 1260]
 [1368 1620]]
2019-03-09T12:11:46.509451: step 460, loss 1.28382, accuracy 0.438581, precision 0.25575900767867693, recall 0.24042198778456414

2019-03-09T12:11:46.987206: step 461, loss 0.0928906, accuracy 0.960938, precision 0.9651162790697675, recall 0.9222222222222223
2019-03-09T12:11:47.430985: step 462, loss 0.0697551, accuracy 0.972656, precision 0.9294117647058824, recall 0.9875

Evaluation:
[[ 420 1273]
 [1346 1642]]
2019-03-09T12:11:48.858169: step 462, loss 1.29494, accuracy 0.440504, precision 0.24808033077377437, recall 0.23782559456398641

2019-03-09T12:11:49.317941: step 463, loss 0.121207, accuracy 0.957031, precision 0.9072164948453608, recall 0.9777777777777777
2019-03-09T12:11:49.766739: step 464, loss 0.0847942, accuracy 0.984375, precision 0.978494623655914, recall 0.978494623655914

Evaluation:
[[ 428 1265]
 [1377 1611]]
2019-03-09T12:11:51.176969: step 464, loss 1.30127, accuracy 0.435591, precision 0.2528056704075605, recall 0.2371191135734072

2019-03-09T12:11:51.623772: step 465, loss 0.0635729, accuracy 0.984375, precision 0.9655172413793104, recall 0.9882352941176471
2019-03-09T12:11:52.088531: step 466, loss 0.0883934, accuracy 0.984375, precision 0.9693877551020408, recall 0.9895833333333334

Evaluation:
[[ 455 1238]
 [1425 1563]]
2019-03-09T12:11:53.509763: step 466, loss 1.30477, accuracy 0.431104, precision 0.2687536916715889, recall 0.24202127659574468

2019-03-09T12:11:53.957533: step 467, loss 0.0945773, accuracy 0.964844, precision 0.9120879120879121, recall 0.9880952380952381
2019-03-09T12:11:54.404339: step 468, loss 0.083891, accuracy 0.984375, precision 0.9887640449438202, recall 0.967032967032967

Evaluation:
[[ 492 1201]
 [1496 1492]]
2019-03-09T12:11:55.832518: step 468, loss 1.30687, accuracy 0.423841, precision 0.29060838747784995, recall 0.24748490945674045

2019-03-09T12:11:56.297275: step 469, loss 0.0900464, accuracy 0.976562, precision 0.9583333333333334, recall 0.9787234042553191
2019-03-09T12:11:56.756083: step 470, loss 0.109451, accuracy 0.957031, precision 0.9245283018867925, recall 0.9702970297029703

Evaluation:
[[ 532 1161]
 [1549 1439]]
2019-03-09T12:11:58.233131: step 470, loss 1.3098, accuracy 0.421064, precision 0.3142350856467809, recall 0.25564632388274866

2019-03-09T12:11:58.680899: step 471, loss 0.0766585, accuracy 0.980469, precision 0.9770114942528736, recall 0.9659090909090909
2019-03-09T12:11:59.145657: step 472, loss 0.0975606, accuracy 0.960938, precision 0.9504950495049505, recall 0.9504950495049505

Evaluation:
[[ 530 1163]
 [1547 1441]]
2019-03-09T12:12:00.566856: step 472, loss 1.31232, accuracy 0.421064, precision 0.3130537507383343, recall 0.25517573423206547

2019-03-09T12:12:01.036599: step 473, loss 0.113055, accuracy 0.96875, precision 0.9619047619047619, recall 0.9619047619047619
2019-03-09T12:12:01.479416: step 474, loss 0.0768583, accuracy 0.980469, precision 0.96875, recall 0.9789473684210527

Evaluation:
[[ 488 1205]
 [1471 1517]]
2019-03-09T12:12:02.897623: step 474, loss 1.31746, accuracy 0.428327, precision 0.28824571766095686, recall 0.24910668708524758

2019-03-09T12:12:03.144961: step 475, loss 0.0513747, accuracy 1, precision 1.0, recall 1.0
2019-03-09T12:12:03.589771: step 476, loss 0.104409, accuracy 0.957031, precision 0.9263157894736842, recall 0.9565217391304348

Evaluation:
[[ 443 1250]
 [1419 1569]]
2019-03-09T12:12:05.027925: step 476, loss 1.32485, accuracy 0.429823, precision 0.26166568222090963, recall 0.23791621911922664

2019-03-09T12:12:05.489692: step 477, loss 0.0811216, accuracy 0.992188, precision 0.9879518072289156, recall 0.9879518072289156
2019-03-09T12:12:05.932507: step 478, loss 0.0810293, accuracy 0.96875, precision 0.9444444444444444, recall 0.9659090909090909

Evaluation:
[[ 422 1271]
 [1354 1634]]
2019-03-09T12:12:07.343732: step 478, loss 1.3365, accuracy 0.439222, precision 0.2492616656822209, recall 0.2376126126126126

2019-03-09T12:12:07.830462: step 479, loss 0.0799934, accuracy 0.972656, precision 0.9484536082474226, recall 0.9787234042553191
2019-03-09T12:12:08.274243: step 480, loss 0.0821521, accuracy 0.980469, precision 0.967032967032967, recall 0.9777777777777777

Evaluation:
[[ 401 1292]
 [1320 1668]]
2019-03-09T12:12:09.703421: step 480, loss 1.34811, accuracy 0.442, precision 0.2368576491435322, recall 0.23300406740267288

2019-03-09T12:12:10.155246: step 481, loss 0.0882125, accuracy 0.976562, precision 0.9489795918367347, recall 0.9893617021276596
2019-03-09T12:12:10.613985: step 482, loss 0.0433394, accuracy 0.992188, precision 1.0, recall 0.978021978021978

Evaluation:
[[ 409 1284]
 [1329 1659]]
2019-03-09T12:12:12.033191: step 482, loss 1.35115, accuracy 0.441786, precision 0.24158298877731837, recall 0.23532796317606444

2019-03-09T12:12:12.484982: step 483, loss 0.103495, accuracy 0.976562, precision 0.9574468085106383, recall 0.9782608695652174
2019-03-09T12:12:12.940763: step 484, loss 0.0759669, accuracy 0.984375, precision 0.9693877551020408, recall 0.9895833333333334

Evaluation:
[[ 424 1269]
 [1371 1617]]
2019-03-09T12:12:14.362961: step 484, loss 1.35255, accuracy 0.436018, precision 0.25044300059066743, recall 0.2362116991643454

2019-03-09T12:12:14.809765: step 485, loss 0.0877998, accuracy 0.972656, precision 0.9651162790697675, recall 0.9540229885057471
2019-03-09T12:12:15.265548: step 486, loss 0.104025, accuracy 0.96875, precision 0.9354838709677419, recall 0.9775280898876404

Evaluation:
[[ 433 1260]
 [1380 1608]]
2019-03-09T12:12:16.817396: step 486, loss 1.35419, accuracy 0.436018, precision 0.25575900767867693, recall 0.23883066740209596

2019-03-09T12:12:17.284148: step 487, loss 0.0878059, accuracy 0.960938, precision 0.9375, recall 0.9574468085106383
2019-03-09T12:12:18.059079: step 488, loss 0.0831402, accuracy 0.980469, precision 0.9813084112149533, recall 0.9722222222222222

Evaluation:
[[ 449 1244]
 [1419 1569]]
2019-03-09T12:12:20.279138: step 488, loss 1.35354, accuracy 0.431104, precision 0.26520968694624925, recall 0.2403640256959315

2019-03-09T12:12:20.853601: step 489, loss 0.0848921, accuracy 0.976562, precision 0.9647058823529412, recall 0.9647058823529412
2019-03-09T12:12:21.528796: step 490, loss 0.0794587, accuracy 0.984375, precision 0.9770114942528736, recall 0.9770114942528736

Evaluation:
[[ 462 1231]
 [1438 1550]]
2019-03-09T12:12:23.999190: step 490, loss 1.35553, accuracy 0.429823, precision 0.2728883638511518, recall 0.2431578947368421

2019-03-09T12:12:24.771128: step 491, loss 0.0745303, accuracy 0.972656, precision 0.9629629629629629, recall 0.9512195121951219
2019-03-09T12:12:25.513141: step 492, loss 0.101039, accuracy 0.953125, precision 0.91, recall 0.9680851063829787

Evaluation:
[[ 462 1231]
 [1451 1537]]
2019-03-09T12:12:27.981539: step 492, loss 1.36243, accuracy 0.427045, precision 0.2728883638511518, recall 0.2415054887611082

2019-03-09T12:12:28.703608: step 493, loss 0.0830011, accuracy 0.972656, precision 0.9387755102040817, recall 0.989247311827957
2019-03-09T12:12:29.140443: step 494, loss 0.0427192, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[ 477 1216]
 [1479 1509]]
2019-03-09T12:12:31.603852: step 494, loss 1.36523, accuracy 0.424268, precision 0.2817483756645009, recall 0.24386503067484663

2019-03-09T12:12:32.357839: step 495, loss 0.069214, accuracy 0.976562, precision 0.9438202247191011, recall 0.9882352941176471
2019-03-09T12:12:33.011089: step 496, loss 0.0776482, accuracy 0.972656, precision 0.979381443298969, recall 0.95

Evaluation:
[[ 478 1215]
 [1486 1502]]
2019-03-09T12:12:34.741463: step 496, loss 1.36919, accuracy 0.422987, precision 0.28233904311872415, recall 0.24338085539714868

2019-03-09T12:12:35.229159: step 497, loss 0.0857888, accuracy 0.96875, precision 0.941747572815534, recall 0.9797979797979798
2019-03-09T12:12:35.677956: step 498, loss 0.0887438, accuracy 0.980469, precision 0.9634146341463414, recall 0.9753086419753086

Evaluation:
[[ 467 1226]
 [1470 1518]]
2019-03-09T12:12:37.143039: step 498, loss 1.37305, accuracy 0.424055, precision 0.2758417011222682, recall 0.24109447599380485

2019-03-09T12:12:37.605802: step 499, loss 0.0737348, accuracy 0.972656, precision 0.9345794392523364, recall 1.0
2019-03-09T12:12:38.055600: step 500, loss 0.0609008, accuracy 0.984375, precision 0.9583333333333334, recall 1.0

Evaluation:
[[ 469 1224]
 [1478 1510]]
2019-03-09T12:12:39.485775: step 500, loss 1.37655, accuracy 0.422773, precision 0.2770230360307147, recall 0.2408834103749358

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552154555\checkpoints\model-500


Process finished with exit code 0
