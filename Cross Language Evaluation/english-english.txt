"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
English English
GloVe model loaded
Pretrained Embedding: GloVe
Italian: False
Loading data...
11566
Max Document length: 36
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:98: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 1
Train/Dev split: 9253/2313
2019-03-18 23:06:00.857867: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
34
33
32
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:485: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968365

2019-03-18T23:06:14.865703: step 1, loss 2.45652, accuracy 0.488281, precision 0.5, recall 0.6717557251908397
2019-03-18T23:06:16.963099: step 2, loss 4.02203, accuracy 0.640625, precision 0.9818181818181818, recall 0.6454183266932271
2019-03-18T23:06:18.757305: step 3, loss 2.5112, accuracy 0.640625, precision 0.9444444444444444, recall 0.6483050847457628
2019-03-18T23:06:20.480698: step 4, loss 2.03653, accuracy 0.496094, precision 0.47701149425287354, recall 0.6859504132231405
2019-03-18T23:06:22.180159: step 5, loss 2.81316, accuracy 0.4375, precision 0.2916666666666667, recall 0.6621621621621622
2019-03-18T23:06:23.696107: step 6, loss 2.11317, accuracy 0.515625, precision 0.4659090909090909, recall 0.7321428571428571
2019-03-18T23:06:25.153213: step 7, loss 1.7121, accuracy 0.597656, precision 0.6909090909090909, recall 0.6867469879518072
2019-03-18T23:06:26.536518: step 8, loss 1.91828, accuracy 0.621094, precision 0.8218390804597702, recall 0.6842105263157895
2019-03-18T23:06:27.880924: step 9, loss 2.38752, accuracy 0.621094, precision 0.9354838709677419, recall 0.625
2019-03-18T23:06:29.101663: step 10, loss 2.14356, accuracy 0.648438, precision 0.910828025477707, recall 0.6529680365296804
2019-03-18T23:06:30.192748: step 11, loss 1.41171, accuracy 0.664062, precision 0.8265895953757225, recall 0.7185929648241206
2019-03-18T23:06:31.235960: step 12, loss 1.71451, accuracy 0.578125, precision 0.7005988023952096, recall 0.6685714285714286
2019-03-18T23:06:32.408827: step 13, loss 1.42976, accuracy 0.578125, precision 0.5722543352601156, recall 0.7443609022556391
2019-03-18T23:06:33.458024: step 14, loss 1.71588, accuracy 0.582031, precision 0.4936708860759494, recall 0.7428571428571429
2019-03-18T23:06:34.654824: step 15, loss 1.58644, accuracy 0.554688, precision 0.461038961038961, recall 0.696078431372549
2019-03-18T23:06:35.858606: step 16, loss 1.38462, accuracy 0.582031, precision 0.6273291925465838, recall 0.6824324324324325
2019-03-18T23:06:37.052418: step 17, loss 1.2916, accuracy 0.625, precision 0.6845238095238095, recall 0.7278481012658228
2019-03-18T23:06:38.136520: step 18, loss 1.48041, accuracy 0.648438, precision 0.8280254777070064, recall 0.6735751295336787
2019-03-18T23:06:39.291435: step 19, loss 1.66416, accuracy 0.605469, precision 0.8380281690140845, recall 0.6040609137055838
2019-03-18T23:06:40.418423: step 20, loss 1.42242, accuracy 0.628906, precision 0.868421052631579, recall 0.6376811594202898
2019-03-18T23:06:41.523469: step 21, loss 1.28805, accuracy 0.640625, precision 0.7777777777777778, recall 0.7112299465240641
2019-03-18T23:06:42.607572: step 22, loss 1.13073, accuracy 0.671875, precision 0.7590361445783133, recall 0.7411764705882353
2019-03-18T23:06:43.680705: step 23, loss 0.921511, accuracy 0.703125, precision 0.7559523809523809, recall 0.7839506172839507
2019-03-18T23:06:44.724915: step 24, loss 1.25407, accuracy 0.613281, precision 0.5705882352941176, recall 0.7886178861788617
2019-03-18T23:06:45.779098: step 25, loss 1.11583, accuracy 0.65625, precision 0.6428571428571429, recall 0.7941176470588235
2019-03-18T23:06:46.819318: step 26, loss 1.29976, accuracy 0.625, precision 0.6123595505617978, recall 0.8014705882352942
2019-03-18T23:06:47.748834: step 27, loss 0.992859, accuracy 0.65625, precision 0.7528735632183908, recall 0.7443181818181818
2019-03-18T23:06:48.641452: step 28, loss 1.43848, accuracy 0.625, precision 0.7988165680473372, recall 0.6852791878172588
2019-03-18T23:06:49.660725: step 29, loss 1.18409, accuracy 0.6875, precision 0.8452380952380952, recall 0.7244897959183674
2019-03-18T23:06:50.539378: step 30, loss 0.96646, accuracy 0.71875, precision 0.9195402298850575, recall 0.7339449541284404
2019-03-18T23:06:51.613508: step 31, loss 1.25425, accuracy 0.6875, precision 0.891566265060241, recall 0.7047619047619048
2019-03-18T23:06:52.665701: step 32, loss 0.900225, accuracy 0.722656, precision 0.88, recall 0.7549019607843137
2019-03-18T23:06:53.806647: step 33, loss 0.765337, accuracy 0.722656, precision 0.8089887640449438, recall 0.7955801104972375
2019-03-18T23:06:54.822931: step 34, loss 1.04254, accuracy 0.691406, precision 0.7041420118343196, recall 0.8040540540540541
2019-03-18T23:06:55.918005: step 35, loss 1.18345, accuracy 0.601562, precision 0.65, recall 0.6933333333333334
2019-03-18T23:06:56.968200: step 36, loss 0.977365, accuracy 0.660156, precision 0.651685393258427, recall 0.8226950354609929
2019-03-18T23:06:57.517731: step 37, loss 1.12342, accuracy 0.648649, precision 0.6956521739130435, recall 0.7272727272727273
2019-03-18T23:06:58.609813: step 38, loss 0.83521, accuracy 0.671875, precision 0.6629213483146067, recall 0.8309859154929577
2019-03-18T23:06:59.590193: step 39, loss 0.828047, accuracy 0.714844, precision 0.7650602409638554, recall 0.7888198757763976
2019-03-18T23:07:00.590520: step 40, loss 0.88161, accuracy 0.726562, precision 0.7894736842105263, recall 0.7988165680473372
2019-03-18T23:07:01.566910: step 41, loss 0.806553, accuracy 0.699219, precision 0.8109756097560976, recall 0.7430167597765364
2019-03-18T23:07:02.627077: step 42, loss 0.974744, accuracy 0.699219, precision 0.8011695906432749, recall 0.7611111111111111
2019-03-18T23:07:03.663309: step 43, loss 0.722537, accuracy 0.777344, precision 0.9053254437869822, recall 0.788659793814433
2019-03-18T23:07:04.727465: step 44, loss 0.813895, accuracy 0.769531, precision 0.9075144508670521, recall 0.785
2019-03-18T23:07:05.829520: step 45, loss 0.795502, accuracy 0.71875, precision 0.84472049689441, recall 0.7431693989071039
2019-03-18T23:07:06.891682: step 46, loss 0.758234, accuracy 0.695312, precision 0.8012048192771084, recall 0.7471910112359551
2019-03-18T23:07:07.935892: step 47, loss 0.635855, accuracy 0.757812, precision 0.8070175438596491, recall 0.8263473053892215
2019-03-18T23:07:08.945194: step 48, loss 0.665393, accuracy 0.777344, precision 0.8021978021978022, recall 0.874251497005988
2019-03-18T23:07:09.965468: step 49, loss 0.7061, accuracy 0.722656, precision 0.7810650887573964, recall 0.7951807228915663
2019-03-18T23:07:11.042590: step 50, loss 0.651966, accuracy 0.75, precision 0.7692307692307693, recall 0.8387096774193549
2019-03-18T23:07:12.119712: step 51, loss 0.634979, accuracy 0.75, precision 0.8035714285714286, recall 0.8132530120481928
2019-03-18T23:07:13.144972: step 52, loss 0.780398, accuracy 0.75, precision 0.8372093023255814, recall 0.8
2019-03-18T23:07:14.232068: step 53, loss 0.543963, accuracy 0.789062, precision 0.8385093167701864, recall 0.8282208588957055
2019-03-18T23:07:15.182529: step 54, loss 0.969764, accuracy 0.714844, precision 0.8433734939759037, recall 0.7486631016042781
2019-03-18T23:07:16.070156: step 55, loss 0.819775, accuracy 0.722656, precision 0.76875, recall 0.7834394904458599
2019-03-18T23:07:17.041560: step 56, loss 0.685218, accuracy 0.765625, precision 0.8202247191011236, recall 0.8390804597701149
2019-03-18T23:07:18.048868: step 57, loss 0.607254, accuracy 0.742188, precision 0.7870967741935484, recall 0.7870967741935484
2019-03-18T23:07:19.135963: step 58, loss 0.751138, accuracy 0.746094, precision 0.8571428571428571, recall 0.7666666666666667
2019-03-18T23:07:20.203111: step 59, loss 0.621983, accuracy 0.789062, precision 0.8496732026143791, recall 0.8074534161490683
2019-03-18T23:07:21.307161: step 60, loss 0.575189, accuracy 0.765625, precision 0.8421052631578947, recall 0.8135593220338984
2019-03-18T23:07:22.327434: step 61, loss 0.603764, accuracy 0.785156, precision 0.8520710059171598, recall 0.8275862068965517
2019-03-18T23:07:23.144253: step 62, loss 0.642431, accuracy 0.769531, precision 0.8129032258064516, recall 0.8076923076923077
2019-03-18T23:07:24.122639: step 63, loss 0.606228, accuracy 0.773438, precision 0.834319526627219, recall 0.8245614035087719
2019-03-18T23:07:25.146902: step 64, loss 0.555731, accuracy 0.769531, precision 0.8170731707317073, recall 0.8220858895705522
2019-03-18T23:07:26.203079: step 65, loss 0.525551, accuracy 0.769531, precision 0.8146067415730337, recall 0.847953216374269
2019-03-18T23:07:27.259256: step 66, loss 0.660613, accuracy 0.738281, precision 0.8353658536585366, recall 0.7740112994350282
2019-03-18T23:07:28.351338: step 67, loss 0.578257, accuracy 0.757812, precision 0.8571428571428571, recall 0.7796610169491526
2019-03-18T23:07:29.511239: step 68, loss 0.517621, accuracy 0.796875, precision 0.8481012658227848, recall 0.8271604938271605
2019-03-18T23:07:30.917481: step 69, loss 0.677081, accuracy 0.757812, precision 0.8311688311688312, recall 0.7804878048780488
2019-03-18T23:07:32.335692: step 70, loss 0.683504, accuracy 0.738281, precision 0.7771084337349398, recall 0.8113207547169812
2019-03-18T23:07:33.656162: step 71, loss 0.779717, accuracy 0.730469, precision 0.8402366863905325, recall 0.7717391304347826
2019-03-18T23:07:35.047446: step 72, loss 0.764667, accuracy 0.730469, precision 0.75625, recall 0.8013245033112583
2019-03-18T23:07:36.431749: step 73, loss 0.512031, accuracy 0.808594, precision 0.8333333333333334, recall 0.8959537572254336
2019-03-18T23:07:37.025161: step 74, loss 0.632675, accuracy 0.837838, precision 0.75, recall 1.0
2019-03-18T23:07:38.539116: step 75, loss 0.585512, accuracy 0.753906, precision 0.879746835443038, recall 0.7595628415300546
2019-03-18T23:07:39.894492: step 76, loss 0.538415, accuracy 0.78125, precision 0.8606060606060606, recall 0.8114285714285714
2019-03-18T23:07:41.292757: step 77, loss 0.547334, accuracy 0.753906, precision 0.8611111111111112, recall 0.8031088082901554
2019-03-18T23:07:42.633176: step 78, loss 0.594362, accuracy 0.78125, precision 0.8841463414634146, recall 0.7967032967032966
2019-03-18T23:07:44.037423: step 79, loss 0.588608, accuracy 0.789062, precision 0.8947368421052632, recall 0.8095238095238095
2019-03-18T23:07:45.322987: step 80, loss 0.60166, accuracy 0.792969, precision 0.9328859060402684, recall 0.7637362637362637
2019-03-18T23:07:46.625507: step 81, loss 0.49451, accuracy 0.800781, precision 0.8947368421052632, recall 0.8225806451612904
2019-03-18T23:07:47.871178: step 82, loss 0.502953, accuracy 0.769531, precision 0.806060606060606, recall 0.83125
2019-03-18T23:07:49.201623: step 83, loss 0.586071, accuracy 0.742188, precision 0.7559523809523809, recall 0.8355263157894737
2019-03-18T23:07:50.693636: step 84, loss 0.629819, accuracy 0.742188, precision 0.7586206896551724, recall 0.8461538461538461
2019-03-18T23:07:52.359186: step 85, loss 0.549083, accuracy 0.808594, precision 0.8068181818181818, recall 0.9044585987261147
2019-03-18T23:07:53.717557: step 86, loss 0.456051, accuracy 0.773438, precision 0.8258064516129032, recall 0.8050314465408805
2019-03-18T23:07:55.143745: step 87, loss 0.430118, accuracy 0.8125, precision 0.8614457831325302, recall 0.8511904761904762
2019-03-18T23:07:56.536049: step 88, loss 0.440376, accuracy 0.816406, precision 0.8975903614457831, recall 0.8324022346368715
2019-03-18T23:07:57.982161: step 89, loss 0.520585, accuracy 0.761719, precision 0.8819875776397516, recall 0.7717391304347826
2019-03-18T23:07:59.328562: step 90, loss 0.504795, accuracy 0.785156, precision 0.8963414634146342, recall 0.7945945945945946
2019-03-18T23:08:00.795642: step 91, loss 0.434622, accuracy 0.835938, precision 0.9047619047619048, recall 0.8539325842696629
2019-03-18T23:08:02.049292: step 92, loss 0.473369, accuracy 0.804688, precision 0.8846153846153846, recall 0.8117647058823529
2019-03-18T23:08:03.272024: step 93, loss 0.620254, accuracy 0.757812, precision 0.8072289156626506, recall 0.8170731707317073
2019-03-18T23:08:04.604465: step 94, loss 0.586799, accuracy 0.738281, precision 0.7951807228915663, recall 0.8
2019-03-18T23:08:05.819220: step 95, loss 0.460528, accuracy 0.792969, precision 0.8439306358381503, recall 0.8488372093023255
2019-03-18T23:08:07.123733: step 96, loss 0.515041, accuracy 0.777344, precision 0.8187134502923976, recall 0.8433734939759037
2019-03-18T23:08:08.481106: step 97, loss 0.507301, accuracy 0.796875, precision 0.8404907975460123, recall 0.8404907975460123
2019-03-18T23:08:09.891337: step 98, loss 0.427842, accuracy 0.8125, precision 0.847953216374269, recall 0.8682634730538922
2019-03-18T23:08:11.213803: step 99, loss 0.440611, accuracy 0.785156, precision 0.8304093567251462, recall 0.8452380952380952
2019-03-18T23:08:12.465459: step 100, loss 0.441796, accuracy 0.816406, precision 0.9005847953216374, recall 0.8369565217391305
2019-03-18T23:08:13.876688: step 101, loss 0.485283, accuracy 0.832031, precision 0.89171974522293, recall 0.8433734939759037
2019-03-18T23:08:15.273954: step 102, loss 0.51787, accuracy 0.800781, precision 0.9038461538461539, recall 0.7966101694915254
2019-03-18T23:08:16.655263: step 103, loss 0.418266, accuracy 0.828125, precision 0.9171597633136095, recall 0.8378378378378378
2019-03-18T23:08:18.053527: step 104, loss 0.462843, accuracy 0.800781, precision 0.8538011695906432, recall 0.8488372093023255
2019-03-18T23:08:19.460766: step 105, loss 0.503349, accuracy 0.824219, precision 0.8982035928143712, recall 0.8426966292134831
2019-03-18T23:08:20.882966: step 106, loss 0.308944, accuracy 0.855469, precision 0.8562874251497006, recall 0.9166666666666666
2019-03-18T23:08:22.494659: step 107, loss 0.493691, accuracy 0.804688, precision 0.8285714285714286, recall 0.8787878787878788
2019-03-18T23:08:23.903894: step 108, loss 0.49817, accuracy 0.792969, precision 0.8154761904761905, recall 0.8616352201257862
2019-03-18T23:08:25.308144: step 109, loss 0.499367, accuracy 0.789062, precision 0.8502994011976048, recall 0.8304093567251462
2019-03-18T23:08:26.732335: step 110, loss 0.394886, accuracy 0.820312, precision 0.864406779661017, recall 0.8742857142857143
2019-03-18T23:08:27.352678: step 111, loss 0.30813, accuracy 0.864865, precision 0.896551724137931, recall 0.9285714285714286
2019-03-18T23:08:28.944427: step 112, loss 0.465247, accuracy 0.824219, precision 0.8982035928143712, recall 0.8426966292134831
2019-03-18T23:08:30.430453: step 113, loss 0.323943, accuracy 0.863281, precision 0.9230769230769231, recall 0.8622754491017964
2019-03-18T23:08:31.965352: step 114, loss 0.347236, accuracy 0.855469, precision 0.8876404494382022, recall 0.9028571428571428
2019-03-18T23:08:33.487285: step 115, loss 0.376309, accuracy 0.839844, precision 0.8895348837209303, recall 0.8742857142857143
2019-03-18T23:08:35.101971: step 116, loss 0.435414, accuracy 0.824219, precision 0.9053254437869822, recall 0.8406593406593407
2019-03-18T23:08:36.490262: step 117, loss 0.456159, accuracy 0.804688, precision 0.8666666666666667, recall 0.8125
2019-03-18T23:08:37.861597: step 118, loss 0.486231, accuracy 0.773438, precision 0.8294117647058824, recall 0.8294117647058824
2019-03-18T23:08:39.296762: step 119, loss 0.439921, accuracy 0.835938, precision 0.8787878787878788, recall 0.8682634730538922
2019-03-18T23:08:40.788775: step 120, loss 0.431846, accuracy 0.804688, precision 0.8588957055214724, recall 0.8383233532934131
2019-03-18T23:08:42.086314: step 121, loss 0.325787, accuracy 0.847656, precision 0.8659217877094972, recall 0.9117647058823529
2019-03-18T23:08:43.662096: step 122, loss 0.453674, accuracy 0.824219, precision 0.8662790697674418, recall 0.8713450292397661
2019-03-18T23:08:45.199987: step 123, loss 0.357486, accuracy 0.839844, precision 0.896551724137931, recall 0.8715083798882681
2019-03-18T23:08:46.744860: step 124, loss 0.419085, accuracy 0.820312, precision 0.8627450980392157, recall 0.8407643312101911
2019-03-18T23:08:48.200969: step 125, loss 0.32193, accuracy 0.867188, precision 0.9132947976878613, recall 0.8926553672316384
2019-03-18T23:08:49.370843: step 126, loss 0.320204, accuracy 0.878906, precision 0.9, recall 0.9056603773584906
2019-03-18T23:08:50.733202: step 127, loss 0.425982, accuracy 0.804688, precision 0.8820224719101124, recall 0.8440860215053764
2019-03-18T23:08:52.118500: step 128, loss 0.386207, accuracy 0.839844, precision 0.92, recall 0.8263473053892215
2019-03-18T23:08:53.525739: step 129, loss 0.335652, accuracy 0.867188, precision 0.9230769230769231, recall 0.8813559322033898
2019-03-18T23:08:55.035705: step 130, loss 0.365837, accuracy 0.824219, precision 0.8466257668711656, recall 0.8734177215189873
2019-03-18T23:08:56.608502: step 131, loss 0.374342, accuracy 0.835938, precision 0.9066666666666666, recall 0.8292682926829268
2019-03-18T23:08:58.079572: step 132, loss 0.408692, accuracy 0.824219, precision 0.8433734939759037, recall 0.8805031446540881
2019-03-18T23:08:59.567594: step 133, loss 0.39127, accuracy 0.84375, precision 0.8695652173913043, recall 0.8805031446540881
2019-03-18T23:09:01.073570: step 134, loss 0.426493, accuracy 0.820312, precision 0.845679012345679, recall 0.8670886075949367
2019-03-18T23:09:02.536660: step 135, loss 0.359707, accuracy 0.839844, precision 0.8774193548387097, recall 0.8607594936708861
2019-03-18T23:09:04.002743: step 136, loss 0.384768, accuracy 0.863281, precision 0.8797814207650273, recall 0.9252873563218391
2019-03-18T23:09:05.405994: step 137, loss 0.339315, accuracy 0.859375, precision 0.8787878787878788, recall 0.9006211180124224
2019-03-18T23:09:06.842156: step 138, loss 0.433914, accuracy 0.839844, precision 0.8901734104046243, recall 0.875
2019-03-18T23:09:08.267354: step 139, loss 0.418598, accuracy 0.828125, precision 0.9012345679012346, recall 0.8390804597701149
2019-03-18T23:09:09.745398: step 140, loss 0.303122, accuracy 0.894531, precision 0.9771428571428571, recall 0.8814432989690721
2019-03-18T23:09:11.028968: step 141, loss 0.371133, accuracy 0.867188, precision 0.936046511627907, recall 0.875
2019-03-18T23:09:12.398341: step 142, loss 0.280708, accuracy 0.886719, precision 0.9248554913294798, recall 0.9090909090909091
2019-03-18T23:09:13.819544: step 143, loss 0.347671, accuracy 0.847656, precision 0.9022988505747126, recall 0.8770949720670391
2019-03-18T23:09:15.356482: step 144, loss 0.335971, accuracy 0.84375, precision 0.8909090909090909, recall 0.8698224852071006
2019-03-18T23:09:16.742776: step 145, loss 0.407749, accuracy 0.847656, precision 0.9058823529411765, recall 0.8700564971751412
2019-03-18T23:09:18.121093: step 146, loss 0.296516, accuracy 0.878906, precision 0.8953488372093024, recall 0.9221556886227545
2019-03-18T23:09:19.619625: step 147, loss 0.299471, accuracy 0.890625, precision 0.935672514619883, recall 0.903954802259887
2019-03-18T23:09:20.341696: step 148, loss 0.252303, accuracy 0.918919, precision 0.9545454545454546, recall 0.9130434782608695
2019-03-18T23:09:21.747453: step 149, loss 0.317646, accuracy 0.835938, precision 0.8554913294797688, recall 0.896969696969697
2019-03-18T23:09:23.179628: step 150, loss 0.350472, accuracy 0.855469, precision 0.8924050632911392, recall 0.8757763975155279
2019-03-18T23:09:24.542036: step 151, loss 0.276902, accuracy 0.886719, precision 0.9132947976878613, recall 0.9186046511627907
2019-03-18T23:09:26.023606: step 152, loss 0.301658, accuracy 0.871094, precision 0.9047619047619048, recall 0.8994082840236687
2019-03-18T23:09:27.413891: step 153, loss 0.337223, accuracy 0.863281, precision 0.8963414634146342, recall 0.8909090909090909
2019-03-18T23:09:28.818137: step 154, loss 0.338744, accuracy 0.855469, precision 0.8957055214723927, recall 0.8795180722891566
2019-03-18T23:09:30.226375: step 155, loss 0.304938, accuracy 0.855469, precision 0.9036144578313253, recall 0.8771929824561403
2019-03-18T23:09:31.737340: step 156, loss 0.278348, accuracy 0.886719, precision 0.9197530864197531, recall 0.9030303030303031
2019-03-18T23:09:33.127638: step 157, loss 0.252551, accuracy 0.90625, precision 0.9316770186335404, recall 0.9202453987730062
2019-03-18T23:09:34.410210: step 158, loss 0.313387, accuracy 0.863281, precision 0.9151515151515152, recall 0.877906976744186
2019-03-18T23:09:35.732675: step 159, loss 0.305084, accuracy 0.890625, precision 0.9390243902439024, recall 0.8953488372093024
2019-03-18T23:09:37.102534: step 160, loss 0.307023, accuracy 0.886719, precision 0.9197530864197531, recall 0.9030303030303031
2019-03-18T23:09:38.516815: step 161, loss 0.311449, accuracy 0.871094, precision 0.8855421686746988, recall 0.9130434782608695
2019-03-18T23:09:39.864214: step 162, loss 0.249796, accuracy 0.898438, precision 0.935064935064935, recall 0.9
2019-03-18T23:09:41.127857: step 163, loss 0.252992, accuracy 0.894531, precision 0.9075144508670521, recall 0.9345238095238095
2019-03-18T23:09:42.574990: step 164, loss 0.350955, accuracy 0.859375, precision 0.8780487804878049, recall 0.9
2019-03-18T23:09:44.055619: step 165, loss 0.375069, accuracy 0.859375, precision 0.8922155688622755, recall 0.8922155688622755
2019-03-18T23:09:45.516779: step 166, loss 0.319351, accuracy 0.890625, precision 0.9349112426035503, recall 0.9028571428571428
2019-03-18T23:09:47.086105: step 167, loss 0.280035, accuracy 0.882812, precision 0.9151515151515152, recall 0.9041916167664671
2019-03-18T23:09:48.635475: step 168, loss 0.258922, accuracy 0.894531, precision 0.9473684210526315, recall 0.9
2019-03-18T23:09:50.126492: step 169, loss 0.265183, accuracy 0.90625, precision 0.9421965317919075, recall 0.9209039548022598
2019-03-18T23:09:51.629531: step 170, loss 0.331186, accuracy 0.875, precision 0.9216867469879518, recall 0.8895348837209303
2019-03-18T23:09:53.160444: step 171, loss 0.252482, accuracy 0.894531, precision 0.9464285714285714, recall 0.8983050847457628
2019-03-18T23:09:54.706362: step 172, loss 0.272022, accuracy 0.898438, precision 0.9285714285714286, recall 0.9176470588235294
2019-03-18T23:09:56.361962: step 173, loss 0.246103, accuracy 0.882812, precision 0.9161290322580645, recall 0.8930817610062893
2019-03-18T23:09:57.848516: step 174, loss 0.249808, accuracy 0.886719, precision 0.9050632911392406, recall 0.910828025477707
2019-03-18T23:09:59.277260: step 175, loss 0.365973, accuracy 0.863281, precision 0.9117647058823529, recall 0.8857142857142857
2019-03-18T23:10:00.679581: step 176, loss 0.333391, accuracy 0.851562, precision 0.875, recall 0.8963414634146342
2019-03-18T23:10:02.259388: step 177, loss 0.299566, accuracy 0.867188, precision 0.8901734104046243, recall 0.9112426035502958
2019-03-18T23:10:03.666628: step 178, loss 0.273827, accuracy 0.890625, precision 0.9101123595505618, recall 0.9310344827586207
2019-03-18T23:10:05.200609: step 179, loss 0.286436, accuracy 0.90625, precision 0.9464285714285714, recall 0.9137931034482759
2019-03-18T23:10:06.462750: step 180, loss 0.276683, accuracy 0.882812, precision 0.9244186046511628, recall 0.9034090909090909
2019-03-18T23:10:08.062558: step 181, loss 0.207167, accuracy 0.90625, precision 0.9491525423728814, recall 0.9180327868852459
2019-03-18T23:10:09.385024: step 182, loss 0.265752, accuracy 0.894531, precision 0.9485714285714286, recall 0.9021739130434783
2019-03-18T23:10:10.709485: step 183, loss 0.268443, accuracy 0.882812, precision 0.9404761904761905, recall 0.8876404494382022
2019-03-18T23:10:12.119811: step 184, loss 0.318288, accuracy 0.878906, precision 0.9386503067484663, recall 0.8793103448275862
2019-03-18T23:10:13.006461: step 185, loss 0.561292, accuracy 0.837838, precision 0.9583333333333334, recall 0.8214285714285714
2019-03-18T23:10:14.216251: step 186, loss 0.190716, accuracy 0.925781, precision 0.9464285714285714, recall 0.9408284023668639
2019-03-18T23:10:15.628983: step 187, loss 0.234135, accuracy 0.90625, precision 0.9382716049382716, recall 0.9156626506024096
2019-03-18T23:10:16.952471: step 188, loss 0.269651, accuracy 0.871094, precision 0.8977272727272727, recall 0.9132947976878613
2019-03-18T23:10:18.280921: step 189, loss 0.229239, accuracy 0.894531, precision 0.9101796407185628, recall 0.926829268292683
2019-03-18T23:10:19.714092: step 190, loss 0.282308, accuracy 0.867188, precision 0.8520710059171598, recall 0.9411764705882353
2019-03-18T23:10:21.146289: step 191, loss 0.246904, accuracy 0.910156, precision 0.9024390243902439, recall 0.9548387096774194
2019-03-18T23:10:22.722585: step 192, loss 0.222203, accuracy 0.898438, precision 0.901840490797546, recall 0.9363057324840764
2019-03-18T23:10:24.173707: step 193, loss 0.209745, accuracy 0.898438, precision 0.9273743016759777, recall 0.9273743016759777
2019-03-18T23:10:25.814374: step 194, loss 0.246445, accuracy 0.917969, precision 0.9506172839506173, recall 0.9221556886227545
2019-03-18T23:10:27.312397: step 195, loss 0.275262, accuracy 0.886719, precision 0.9418604651162791, recall 0.8950276243093923
2019-03-18T23:10:28.797429: step 196, loss 0.279177, accuracy 0.894531, precision 0.949685534591195, recall 0.888235294117647
2019-03-18T23:10:30.222240: step 197, loss 0.287242, accuracy 0.898438, precision 0.9344262295081968, recall 0.9243243243243243
2019-03-18T23:10:31.768122: step 198, loss 0.305887, accuracy 0.898438, precision 0.9457831325301205, recall 0.9022988505747126
2019-03-18T23:10:33.069644: step 199, loss 0.199927, accuracy 0.917969, precision 0.9415204678362573, recall 0.936046511627907
2019-03-18T23:10:34.530740: step 200, loss 0.218155, accuracy 0.902344, precision 0.9441340782122905, recall 0.9184782608695652
2019-03-18T23:10:35.988907: step 201, loss 0.242587, accuracy 0.898438, precision 0.94375, recall 0.8988095238095238
2019-03-18T23:10:37.457491: step 202, loss 0.251384, accuracy 0.894531, precision 0.9216867469879518, recall 0.9161676646706587
2019-03-18T23:10:38.753088: step 203, loss 0.254195, accuracy 0.902344, precision 0.9147727272727273, recall 0.9415204678362573
2019-03-18T23:10:40.122429: step 204, loss 0.213232, accuracy 0.910156, precision 0.9006211180124224, recall 0.9539473684210527
2019-03-18T23:10:41.632448: step 205, loss 0.233872, accuracy 0.902344, precision 0.9302325581395349, recall 0.9248554913294798
2019-03-18T23:10:43.138445: step 206, loss 0.22147, accuracy 0.898438, precision 0.9113924050632911, recall 0.9230769230769231
2019-03-18T23:10:44.585602: step 207, loss 0.278571, accuracy 0.871094, precision 0.9246575342465754, recall 0.8598726114649682
2019-03-18T23:10:46.017310: step 208, loss 0.164482, accuracy 0.941406, precision 0.9457831325301205, recall 0.9631901840490797
2019-03-18T23:10:47.523919: step 209, loss 0.244773, accuracy 0.90625, precision 0.9534883720930233, recall 0.9111111111111111
2019-03-18T23:10:48.897296: step 210, loss 0.212136, accuracy 0.902344, precision 0.9121621621621622, recall 0.9183673469387755
2019-03-18T23:10:50.329470: step 211, loss 0.175408, accuracy 0.929688, precision 0.9482758620689655, recall 0.9482758620689655
2019-03-18T23:10:51.765632: step 212, loss 0.225848, accuracy 0.914062, precision 0.9404761904761905, recall 0.9294117647058824
2019-03-18T23:10:53.178366: step 213, loss 0.220073, accuracy 0.910156, precision 0.9263803680981595, recall 0.9320987654320988
2019-03-18T23:10:54.754156: step 214, loss 0.200616, accuracy 0.914062, precision 0.949438202247191, recall 0.9285714285714286
2019-03-18T23:10:56.543405: step 215, loss 0.266498, accuracy 0.90625, precision 0.9382716049382716, recall 0.9156626506024096
2019-03-18T23:10:58.045390: step 216, loss 0.206694, accuracy 0.910156, precision 0.937888198757764, recall 0.9207317073170732
2019-03-18T23:10:59.552365: step 217, loss 0.166797, accuracy 0.933594, precision 0.9583333333333334, recall 0.9415204678362573
2019-03-18T23:11:00.968579: step 218, loss 0.25261, accuracy 0.898438, precision 0.9130434782608695, recall 0.9245283018867925
2019-03-18T23:11:02.334436: step 219, loss 0.182061, accuracy 0.925781, precision 0.95, recall 0.9325153374233128
2019-03-18T23:11:03.866856: step 220, loss 0.224127, accuracy 0.902344, precision 0.9293478260869565, recall 0.9344262295081968
2019-03-18T23:11:05.274094: step 221, loss 0.267282, accuracy 0.914062, precision 0.94375, recall 0.9207317073170732
2019-03-18T23:11:05.791711: step 222, loss 0.230643, accuracy 0.891892, precision 0.9642857142857143, recall 0.9
2019-03-18T23:11:07.306699: step 223, loss 0.239867, accuracy 0.886719, precision 0.925, recall 0.896969696969697
2019-03-18T23:11:08.895961: step 224, loss 0.181432, accuracy 0.929688, precision 0.943502824858757, recall 0.9542857142857143
2019-03-18T23:11:10.363041: step 225, loss 0.212425, accuracy 0.910156, precision 0.9491525423728814, recall 0.9230769230769231
2019-03-18T23:11:11.880005: step 226, loss 0.20609, accuracy 0.917969, precision 0.9281045751633987, recall 0.9342105263157895
2019-03-18T23:11:13.498237: step 227, loss 0.193402, accuracy 0.914062, precision 0.9314285714285714, recall 0.9421965317919075
2019-03-18T23:11:15.079013: step 228, loss 0.163256, accuracy 0.929688, precision 0.9529411764705882, recall 0.9418604651162791
2019-03-18T23:11:16.674749: step 229, loss 0.224302, accuracy 0.902344, precision 0.9171974522292994, recall 0.9230769230769231
2019-03-18T23:11:17.985253: step 230, loss 0.22155, accuracy 0.921875, precision 0.9593023255813954, recall 0.9269662921348315
2019-03-18T23:11:19.315204: step 231, loss 0.214522, accuracy 0.910156, precision 0.9617834394904459, recall 0.8988095238095238
2019-03-18T23:11:20.676645: step 232, loss 0.25689, accuracy 0.894531, precision 0.9375, recall 0.8982035928143712
2019-03-18T23:11:22.048079: step 233, loss 0.194016, accuracy 0.914062, precision 0.954248366013072, recall 0.906832298136646
2019-03-18T23:11:23.410453: step 234, loss 0.176129, accuracy 0.933594, precision 0.9523809523809523, recall 0.9574468085106383
2019-03-18T23:11:24.995271: step 235, loss 0.228738, accuracy 0.925781, precision 0.9408284023668639, recall 0.9464285714285714
2019-03-18T23:11:26.266386: step 236, loss 0.199891, accuracy 0.917969, precision 0.9222222222222223, recall 0.9595375722543352
2019-03-18T23:11:27.646696: step 237, loss 0.184155, accuracy 0.914062, precision 0.9190751445086706, recall 0.9520958083832335
2019-03-18T23:11:28.995604: step 238, loss 0.168756, accuracy 0.9375, precision 0.9580838323353293, recall 0.9467455621301775
2019-03-18T23:11:30.566119: step 239, loss 0.188048, accuracy 0.941406, precision 0.9808917197452229, recall 0.927710843373494
2019-03-18T23:11:31.927502: step 240, loss 0.256959, accuracy 0.917969, precision 0.9585798816568047, recall 0.9204545454545454
2019-03-18T23:11:33.338796: step 241, loss 0.195454, accuracy 0.914062, precision 0.9310344827586207, recall 0.9418604651162791
2019-03-18T23:11:34.682205: step 242, loss 0.155314, accuracy 0.941406, precision 0.9753086419753086, recall 0.9349112426035503
2019-03-18T23:11:36.185189: step 243, loss 0.167813, accuracy 0.925781, precision 0.9299363057324841, recall 0.948051948051948
2019-03-18T23:11:37.560025: step 244, loss 0.205847, accuracy 0.914062, precision 0.9281045751633987, recall 0.9281045751633987
2019-03-18T23:11:38.871030: step 245, loss 0.160195, accuracy 0.9375, precision 0.9523809523809523, recall 0.9523809523809523
2019-03-18T23:11:40.159121: step 246, loss 0.21468, accuracy 0.925781, precision 0.936046511627907, recall 0.9526627218934911
2019-03-18T23:11:41.494095: step 247, loss 0.242092, accuracy 0.914062, precision 0.9308176100628931, recall 0.9308176100628931
2019-03-18T23:11:42.832121: step 248, loss 0.141114, accuracy 0.960938, precision 0.9819277108433735, recall 0.9588235294117647
2019-03-18T23:11:44.213430: step 249, loss 0.215178, accuracy 0.90625, precision 0.9207317073170732, recall 0.9320987654320988
2019-03-18 23:11:46.511086: W tensorflow/core/framework/allocator.cc:122] Allocation of 99921600 exceeds 10% of system memory.
2019-03-18T23:11:45.773363: step 250, loss 0.233059, accuracy 0.921875, precision 0.9714285714285714, recall 0.918918918918919

Evaluation:
[[1480  122]
 [ 267  444]]
2019-03-18T23:11:48.585434: step 250, loss 0.413371, accuracy 0.83182, precision 0.9238451935081149, recall 0.8471665712650257

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968365\checkpoints\model-250

2019-03-18T23:11:50.758661: step 251, loss 0.187177, accuracy 0.925781, precision 0.9289940828402367, recall 0.9573170731707317
2019-03-18T23:11:52.188870: step 252, loss 0.206111, accuracy 0.910156, precision 0.9523809523809523, recall 0.9142857142857143
2019-03-18T23:11:53.619559: step 253, loss 0.191126, accuracy 0.929688, precision 0.9588235294117647, recall 0.9367816091954023
2019-03-18T23:11:54.711641: step 254, loss 0.213798, accuracy 0.902344, precision 0.9578313253012049, recall 0.8983050847457628
2019-03-18T23:11:56.272980: step 255, loss 0.18384, accuracy 0.941406, precision 0.9467455621301775, recall 0.963855421686747
2019-03-18T23:11:57.626873: step 256, loss 0.210308, accuracy 0.921875, precision 0.9186046511627907, recall 0.9634146341463414
2019-03-18T23:11:59.120928: step 257, loss 0.181774, accuracy 0.925781, precision 0.9325153374233128, recall 0.95
2019-03-18T23:12:00.683797: step 258, loss 0.176982, accuracy 0.945312, precision 0.9761904761904762, recall 0.9425287356321839
2019-03-18T23:12:01.409371: step 259, loss 0.16578, accuracy 0.891892, precision 0.9090909090909091, recall 0.9090909090909091
2019-03-18T23:12:02.663021: step 260, loss 0.17614, accuracy 0.933594, precision 0.9691358024691358, recall 0.9289940828402367
2019-03-18T23:12:04.134600: step 261, loss 0.148027, accuracy 0.949219, precision 0.9644970414201184, recall 0.9588235294117647
2019-03-18T23:12:05.633651: step 262, loss 0.164596, accuracy 0.925781, precision 0.9352941176470588, recall 0.9520958083832335
2019-03-18T23:12:07.139188: step 263, loss 0.179945, accuracy 0.921875, precision 0.9583333333333334, recall 0.9252873563218391
2019-03-18T23:12:08.554486: step 264, loss 0.146368, accuracy 0.949219, precision 0.9722222222222222, recall 0.9395973154362416
2019-03-18T23:12:10.009590: step 265, loss 0.212077, accuracy 0.90625, precision 0.9371069182389937, recall 0.9141104294478528
2019-03-18T23:12:11.565944: step 266, loss 0.111756, accuracy 0.96875, precision 0.9702380952380952, recall 0.9819277108433735
2019-03-18T23:12:12.572811: step 267, loss 0.137948, accuracy 0.953125, precision 0.9523809523809523, recall 0.975609756097561
2019-03-18T23:12:13.958257: step 268, loss 0.222546, accuracy 0.914062, precision 0.9038461538461539, recall 0.9527027027027027
2019-03-18T23:12:15.388167: step 269, loss 0.133595, accuracy 0.949219, precision 0.9529411764705882, recall 0.9700598802395209
2019-03-18T23:12:16.878043: step 270, loss 0.172844, accuracy 0.925781, precision 0.9390243902439024, recall 0.9447852760736196
2019-03-18T23:12:18.087874: step 271, loss 0.132114, accuracy 0.949219, precision 0.9704142011834319, recall 0.9534883720930233
2019-03-18T23:12:19.514570: step 272, loss 0.148808, accuracy 0.929688, precision 0.9573170731707317, recall 0.9345238095238095
2019-03-18T23:12:20.883473: step 273, loss 0.145145, accuracy 0.953125, precision 0.9693251533742331, recall 0.9575757575757575
2019-03-18T23:12:22.231891: step 274, loss 0.0913603, accuracy 0.964844, precision 0.9771428571428571, recall 0.9715909090909091
2019-03-18T23:12:23.730400: step 275, loss 0.139823, accuracy 0.945312, precision 0.9659090909090909, recall 0.9550561797752809
2019-03-18T23:12:25.237373: step 276, loss 0.188604, accuracy 0.9375, precision 0.963855421686747, recall 0.9411764705882353
2019-03-18T23:12:26.726393: step 277, loss 0.177491, accuracy 0.933594, precision 0.9675324675324676, recall 0.9254658385093167
2019-03-18T23:12:28.251829: step 278, loss 0.139811, accuracy 0.957031, precision 0.9555555555555556, recall 0.9828571428571429
2019-03-18T23:12:29.749826: step 279, loss 0.125761, accuracy 0.953125, precision 0.9590643274853801, recall 0.9704142011834319
2019-03-18T23:12:30.861855: step 280, loss 0.154751, accuracy 0.953125, precision 0.9649122807017544, recall 0.9649122807017544
2019-03-18T23:12:32.226717: step 281, loss 0.183776, accuracy 0.921875, precision 0.9342105263157895, recall 0.9342105263157895
2019-03-18T23:12:33.685328: step 282, loss 0.165212, accuracy 0.9375, precision 0.9464285714285714, recall 0.9578313253012049
2019-03-18T23:12:35.172909: step 283, loss 0.138805, accuracy 0.941406, precision 0.9464285714285714, recall 0.9636363636363636
2019-03-18T23:12:36.437530: step 284, loss 0.175067, accuracy 0.910156, precision 0.9490445859872612, recall 0.9085365853658537
2019-03-18T23:12:38.014316: step 285, loss 0.178974, accuracy 0.921875, precision 0.9593023255813954, recall 0.9269662921348315
2019-03-18T23:12:39.173220: step 286, loss 0.131969, accuracy 0.9375, precision 0.9371428571428572, recall 0.9704142011834319
2019-03-18T23:12:40.638454: step 287, loss 0.186485, accuracy 0.921875, precision 0.9289940828402367, recall 0.9515151515151515
2019-03-18T23:12:42.154909: step 288, loss 0.168641, accuracy 0.914062, precision 0.945054945054945, recall 0.9347826086956522
2019-03-18T23:12:43.555193: step 289, loss 0.150491, accuracy 0.945312, precision 0.9668508287292817, recall 0.9562841530054644
2019-03-18T23:12:44.852240: step 290, loss 0.164226, accuracy 0.933594, precision 0.9617834394904459, recall 0.9320987654320988
2019-03-18T23:12:46.220580: step 291, loss 0.14443, accuracy 0.949219, precision 0.9777777777777777, recall 0.9513513513513514
2019-03-18T23:12:47.728551: step 292, loss 0.173716, accuracy 0.925781, precision 0.9454545454545454, recall 0.9397590361445783
2019-03-18T23:12:49.260527: step 293, loss 0.167039, accuracy 0.914062, precision 0.9548387096774194, recall 0.9079754601226994
2019-03-18T23:12:50.727607: step 294, loss 0.161551, accuracy 0.941406, precision 0.9709302325581395, recall 0.943502824858757
2019-03-18T23:12:52.182233: step 295, loss 0.181239, accuracy 0.933594, precision 0.9349112426035503, recall 0.9634146341463414
2019-03-18T23:12:52.897837: step 296, loss 0.146513, accuracy 0.945946, precision 1.0, recall 0.92
2019-03-18T23:12:54.191380: step 297, loss 0.132641, accuracy 0.945312, precision 0.9565217391304348, recall 0.9565217391304348
2019-03-18T23:12:55.542793: step 298, loss 0.127994, accuracy 0.957031, precision 0.9585798816568047, recall 0.9759036144578314
2019-03-18T23:12:56.950145: step 299, loss 0.125841, accuracy 0.953125, precision 0.9473684210526315, recall 0.9818181818181818
2019-03-18T23:12:58.277112: step 300, loss 0.145657, accuracy 0.9375, precision 0.9467455621301775, recall 0.9580838323353293
2019-03-18T23:12:59.646453: step 301, loss 0.152565, accuracy 0.949219, precision 0.9464285714285714, recall 0.9754601226993865
2019-03-18T23:13:01.093101: step 302, loss 0.185341, accuracy 0.949219, precision 0.9515151515151515, recall 0.9691358024691358
2019-03-18T23:13:02.649452: step 303, loss 0.142733, accuracy 0.945312, precision 0.96045197740113, recall 0.96045197740113
2019-03-18T23:13:04.248801: step 304, loss 0.152409, accuracy 0.929688, precision 0.9540229885057471, recall 0.9431818181818182
2019-03-18T23:13:05.797174: step 305, loss 0.173964, accuracy 0.929688, precision 0.9607843137254902, recall 0.9245283018867925
2019-03-18T23:13:07.206409: step 306, loss 0.158471, accuracy 0.933594, precision 0.9757575757575757, recall 0.9252873563218391
2019-03-18T23:13:08.648067: step 307, loss 0.177033, accuracy 0.921875, precision 0.9698795180722891, recall 0.9147727272727273
2019-03-18T23:13:10.084230: step 308, loss 0.139343, accuracy 0.957031, precision 0.9824561403508771, recall 0.9545454545454546
2019-03-18T23:13:11.430143: step 309, loss 0.137, accuracy 0.945312, precision 0.959731543624161, recall 0.9470198675496688
2019-03-18T23:13:12.837435: step 310, loss 0.141397, accuracy 0.9375, precision 0.9447852760736196, recall 0.9565217391304348
2019-03-18T23:13:14.237262: step 311, loss 0.116417, accuracy 0.960938, precision 0.9700598802395209, recall 0.9700598802395209
2019-03-18T23:13:15.702347: step 312, loss 0.153462, accuracy 0.9375, precision 0.9441340782122905, recall 0.9657142857142857
2019-03-18T23:13:17.183390: step 313, loss 0.154204, accuracy 0.949219, precision 0.9382022471910112, recall 0.9881656804733728
2019-03-18T23:13:18.480923: step 314, loss 0.131542, accuracy 0.953125, precision 0.9753086419753086, recall 0.9518072289156626
2019-03-18T23:13:19.837810: step 315, loss 0.108953, accuracy 0.96875, precision 0.9691358024691358, recall 0.98125
2019-03-18T23:13:21.272973: step 316, loss 0.133122, accuracy 0.957031, precision 0.9588235294117647, recall 0.9760479041916168
2019-03-18T23:13:22.787925: step 317, loss 0.136448, accuracy 0.945312, precision 0.9491525423728814, recall 0.9710982658959537
2019-03-18T23:13:24.240566: step 318, loss 0.131419, accuracy 0.957031, precision 0.9613259668508287, recall 0.9775280898876404
2019-03-18T23:13:25.518192: step 319, loss 0.170726, accuracy 0.9375, precision 0.9539473684210527, recall 0.9415584415584416
2019-03-18T23:13:27.099484: step 320, loss 0.128729, accuracy 0.960938, precision 0.9878048780487805, recall 0.9529411764705882
2019-03-18T23:13:28.538147: step 321, loss 0.103884, accuracy 0.964844, precision 0.9585798816568047, recall 0.9878048780487805
2019-03-18T23:13:30.050131: step 322, loss 0.125112, accuracy 0.957031, precision 0.9698795180722891, recall 0.9640718562874252
2019-03-18T23:13:31.483817: step 323, loss 0.142145, accuracy 0.957031, precision 0.9817073170731707, recall 0.9526627218934911
2019-03-18T23:13:32.934464: step 324, loss 0.135442, accuracy 0.9375, precision 0.9649122807017544, recall 0.9428571428571428
2019-03-18T23:13:34.335749: step 325, loss 0.134728, accuracy 0.949219, precision 0.9693251533742331, recall 0.9518072289156626
2019-03-18T23:13:35.714575: step 326, loss 0.110895, accuracy 0.957031, precision 0.9753086419753086, recall 0.9575757575757575
2019-03-18T23:13:37.206588: step 327, loss 0.181371, accuracy 0.925781, precision 0.9333333333333333, recall 0.9506172839506173
2019-03-18T23:13:38.628816: step 328, loss 0.143562, accuracy 0.941406, precision 0.9526627218934911, recall 0.9583333333333334
2019-03-18T23:13:39.823622: step 329, loss 0.159538, accuracy 0.929688, precision 0.9393939393939394, recall 0.950920245398773
2019-03-18T23:13:41.218493: step 330, loss 0.174615, accuracy 0.925781, precision 0.9454545454545454, recall 0.9397590361445783
2019-03-18T23:13:42.627728: step 331, loss 0.133711, accuracy 0.960938, precision 0.9761904761904762, recall 0.9647058823529412
2019-03-18T23:13:44.047933: step 332, loss 0.107283, accuracy 0.96875, precision 0.9823529411764705, recall 0.9709302325581395
2019-03-18T23:13:44.841328: step 333, loss 0.155814, accuracy 0.972973, precision 0.9545454545454546, recall 1.0
2019-03-18T23:13:46.275495: step 334, loss 0.0935461, accuracy 0.972656, precision 0.9763313609467456, recall 0.9821428571428571
2019-03-18T23:13:47.535640: step 335, loss 0.116341, accuracy 0.957031, precision 0.9819277108433735, recall 0.9532163742690059
2019-03-18T23:13:48.718047: step 336, loss 0.0597304, accuracy 0.984375, precision 0.9830508474576272, recall 0.9942857142857143
2019-03-18T23:13:49.993636: step 337, loss 0.0896772, accuracy 0.96875, precision 0.9702380952380952, recall 0.9819277108433735
2019-03-18T23:13:51.326076: step 338, loss 0.113427, accuracy 0.96875, precision 0.9819277108433735, recall 0.9702380952380952
2019-03-18T23:13:52.875992: step 339, loss 0.155767, accuracy 0.945312, precision 0.9634146341463414, recall 0.9518072289156626
2019-03-18T23:13:54.350052: step 340, loss 0.131976, accuracy 0.964844, precision 0.9881656804733728, recall 0.9597701149425287
2019-03-18T23:13:55.781737: step 341, loss 0.144419, accuracy 0.945312, precision 0.9695121951219512, recall 0.9464285714285714
2019-03-18T23:13:57.227965: step 342, loss 0.127627, accuracy 0.9375, precision 0.9705882352941176, recall 0.9375
2019-03-18T23:13:58.581350: step 343, loss 0.118016, accuracy 0.957031, precision 0.9700598802395209, recall 0.9642857142857143
2019-03-18T23:13:59.842979: step 344, loss 0.106904, accuracy 0.957031, precision 0.9751552795031055, recall 0.9573170731707317
2019-03-18T23:14:01.294133: step 345, loss 0.111178, accuracy 0.957031, precision 0.9615384615384616, recall 0.967741935483871
2019-03-18T23:14:02.674952: step 346, loss 0.16525, accuracy 0.9375, precision 0.9329608938547486, recall 0.9766081871345029
2019-03-18T23:14:04.413408: step 347, loss 0.114197, accuracy 0.96875, precision 0.9741935483870968, recall 0.9741935483870968
2019-03-18T23:14:06.313386: step 348, loss 0.103668, accuracy 0.964844, precision 0.9634146341463414, recall 0.9813664596273292
2019-03-18T23:14:08.130530: step 349, loss 0.107063, accuracy 0.960938, precision 0.9518072289156626, recall 0.9875
2019-03-18T23:14:09.966733: step 350, loss 0.1204, accuracy 0.953125, precision 0.9540229885057471, recall 0.9764705882352941
2019-03-18T23:14:11.613946: step 351, loss 0.154688, accuracy 0.941406, precision 0.9553072625698324, recall 0.9606741573033708
2019-03-18T23:14:13.155894: step 352, loss 0.108909, accuracy 0.964844, precision 0.9691358024691358, recall 0.9751552795031055
2019-03-18T23:14:14.610635: step 353, loss 0.111828, accuracy 0.953125, precision 0.9754601226993865, recall 0.9520958083832335
2019-03-18T23:14:16.175503: step 354, loss 0.111519, accuracy 0.957031, precision 0.9710982658959537, recall 0.9655172413793104
2019-03-18T23:14:17.903907: step 355, loss 0.109099, accuracy 0.964844, precision 0.9695121951219512, recall 0.9754601226993865
2019-03-18T23:14:19.837876: step 356, loss 0.111761, accuracy 0.949219, precision 0.9573170731707317, recall 0.9631901840490797
2019-03-18T23:14:21.500969: step 357, loss 0.123816, accuracy 0.960938, precision 0.9938271604938271, recall 0.9470588235294117
2019-03-18T23:14:22.885271: step 358, loss 0.108789, accuracy 0.96875, precision 0.9833333333333333, recall 0.9725274725274725
2019-03-18T23:14:24.666515: step 359, loss 0.101822, accuracy 0.96875, precision 0.9766081871345029, recall 0.9766081871345029
2019-03-18T23:14:26.335053: step 360, loss 0.152672, accuracy 0.921875, precision 0.9464285714285714, recall 0.9352941176470588
2019-03-18T23:14:27.920814: step 361, loss 0.115437, accuracy 0.960938, precision 0.9693251533742331, recall 0.9693251533742331
2019-03-18T23:14:29.448732: step 362, loss 0.108317, accuracy 0.96875, precision 0.9803921568627451, recall 0.967741935483871
2019-03-18T23:14:31.301780: step 363, loss 0.0838942, accuracy 0.976562, precision 0.9877300613496932, recall 0.9757575757575757
2019-03-18T23:14:32.853633: step 364, loss 0.144021, accuracy 0.945312, precision 0.9329268292682927, recall 0.9807692307692307
2019-03-18T23:14:34.366590: step 365, loss 0.135417, accuracy 0.949219, precision 0.9691358024691358, recall 0.9515151515151515
2019-03-18T23:14:36.021169: step 366, loss 0.101346, accuracy 0.964844, precision 0.9710982658959537, recall 0.9767441860465116
2019-03-18T23:14:37.529140: step 367, loss 0.115175, accuracy 0.964844, precision 0.9597701149425287, recall 0.9881656804733728
2019-03-18T23:14:39.078001: step 368, loss 0.0721925, accuracy 0.988281, precision 0.9825581395348837, recall 1.0
2019-03-18T23:14:40.611901: step 369, loss 0.114303, accuracy 0.941406, precision 0.9575757575757575, recall 0.9518072289156626
2019-03-18T23:14:41.288094: step 370, loss 0.0683595, accuracy 1, precision 1.0, recall 1.0
2019-03-18T23:14:42.881836: step 371, loss 0.0919945, accuracy 0.96875, precision 0.9881656804733728, recall 0.9653179190751445
2019-03-18T23:14:44.242200: step 372, loss 0.0885081, accuracy 0.972656, precision 0.9820359281437125, recall 0.9761904761904762
2019-03-18T23:14:45.398112: step 373, loss 0.0897168, accuracy 0.96875, precision 0.9814814814814815, recall 0.9695121951219512
2019-03-18T23:14:46.861346: step 374, loss 0.110597, accuracy 0.953125, precision 0.9518072289156626, recall 0.9753086419753086
2019-03-18T23:14:48.339396: step 375, loss 0.0795785, accuracy 0.984375, precision 0.9879518072289156, recall 0.9879518072289156
2019-03-18T23:14:49.672834: step 376, loss 0.0702492, accuracy 0.96875, precision 0.9651162790697675, recall 0.9880952380952381
2019-03-18T23:14:51.109019: step 377, loss 0.0981355, accuracy 0.972656, precision 0.9826589595375722, recall 0.9770114942528736
2019-03-18T23:14:52.760583: step 378, loss 0.0894911, accuracy 0.960938, precision 0.9712643678160919, recall 0.9712643678160919
2019-03-18T23:14:54.249604: step 379, loss 0.0913979, accuracy 0.960938, precision 0.9878787878787879, recall 0.9532163742690059
2019-03-18T23:14:55.671803: step 380, loss 0.0950183, accuracy 0.960938, precision 0.975609756097561, recall 0.963855421686747
2019-03-18T23:14:56.961359: step 381, loss 0.112765, accuracy 0.957031, precision 0.9705882352941176, recall 0.9649122807017544
2019-03-18T23:14:58.399515: step 382, loss 0.0878356, accuracy 0.976562, precision 0.9709302325581395, recall 0.9940476190476191
2019-03-18T23:14:59.901501: step 383, loss 0.0980498, accuracy 0.960938, precision 0.9774011299435028, recall 0.9664804469273743
2019-03-18T23:15:01.300295: step 384, loss 0.128522, accuracy 0.957031, precision 0.968944099378882, recall 0.9629629629629629
2019-03-18T23:15:02.704053: step 385, loss 0.0994925, accuracy 0.976562, precision 0.9939393939393939, recall 0.9704142011834319
2019-03-18T23:15:04.088888: step 386, loss 0.10632, accuracy 0.976562, precision 0.9828571428571429, recall 0.9828571428571429
2019-03-18T23:15:05.566449: step 387, loss 0.140092, accuracy 0.941406, precision 0.9461077844311377, recall 0.9634146341463414
2019-03-18T23:15:07.137252: step 388, loss 0.0750251, accuracy 0.972656, precision 0.9655172413793104, recall 0.9940828402366864
2019-03-18T23:15:08.724014: step 389, loss 0.127102, accuracy 0.953125, precision 0.9588235294117647, recall 0.9702380952380952
2019-03-18T23:15:10.203060: step 390, loss 0.108573, accuracy 0.964844, precision 0.9746835443037974, recall 0.9685534591194969
2019-03-18T23:15:11.593345: step 391, loss 0.084052, accuracy 0.96875, precision 0.9709302325581395, recall 0.9823529411764705
2019-03-18T23:15:13.154218: step 392, loss 0.105303, accuracy 0.96875, precision 0.9821428571428571, recall 0.9705882352941176
2019-03-18T23:15:14.580406: step 393, loss 0.0749914, accuracy 0.984375, precision 0.9878048780487805, recall 0.9878048780487805
2019-03-18T23:15:15.974681: step 394, loss 0.0745877, accuracy 0.984375, precision 0.9875776397515528, recall 0.9875776397515528
2019-03-18T23:15:17.413834: step 395, loss 0.109504, accuracy 0.964844, precision 0.967741935483871, recall 0.974025974025974
2019-03-18T23:15:19.037495: step 396, loss 0.0870492, accuracy 0.972656, precision 0.9774011299435028, recall 0.9829545454545454
2019-03-18T23:15:20.541477: step 397, loss 0.102351, accuracy 0.964844, precision 0.9938650306748467, recall 0.9529411764705882
2019-03-18T23:15:22.073383: step 398, loss 0.087298, accuracy 0.976562, precision 0.9754601226993865, recall 0.9875776397515528
2019-03-18T23:15:23.674106: step 399, loss 0.091464, accuracy 0.976562, precision 0.9875, recall 0.9753086419753086
2019-03-18T23:15:25.159139: step 400, loss 0.0997525, accuracy 0.972656, precision 0.9939393939393939, recall 0.9647058823529412
2019-03-18T23:15:26.483610: step 401, loss 0.107503, accuracy 0.953125, precision 0.9695121951219512, recall 0.9578313253012049
2019-03-18T23:15:28.179067: step 402, loss 0.122071, accuracy 0.964844, precision 0.9693251533742331, recall 0.9753086419753086
2019-03-18T23:15:29.773806: step 403, loss 0.111258, accuracy 0.96875, precision 0.9559748427672956, recall 0.9934640522875817
2019-03-18T23:15:31.399620: step 404, loss 0.0844444, accuracy 0.96875, precision 0.9664804469273743, recall 0.9885714285714285
2019-03-18T23:15:32.959453: step 405, loss 0.0989526, accuracy 0.960938, precision 0.9819277108433735, recall 0.9588235294117647
2019-03-18T23:15:34.578650: step 406, loss 0.106459, accuracy 0.960938, precision 0.975, recall 0.9629629629629629
2019-03-18T23:15:35.402442: step 407, loss 0.109999, accuracy 0.945946, precision 0.9615384615384616, recall 0.9615384615384616
2019-03-18T23:15:36.764800: step 408, loss 0.0686609, accuracy 0.984375, precision 0.9881656804733728, recall 0.9881656804733728
2019-03-18T23:15:38.381481: step 409, loss 0.0983917, accuracy 0.960938, precision 0.9782608695652174, recall 0.967741935483871
2019-03-18T23:15:39.628151: step 410, loss 0.0629968, accuracy 0.992188, precision 0.9879518072289156, recall 1.0
2019-03-18T23:15:41.075283: step 411, loss 0.052673, accuracy 0.984375, precision 0.9815950920245399, recall 0.9937888198757764
2019-03-18T23:15:42.612178: step 412, loss 0.0869367, accuracy 0.972656, precision 0.9704142011834319, recall 0.9879518072289156
2019-03-18T23:15:44.137101: step 413, loss 0.0610717, accuracy 0.984375, precision 0.9817073170731707, recall 0.9938271604938271
2019-03-18T23:15:45.619141: step 414, loss 0.108611, accuracy 0.964844, precision 0.9771428571428571, recall 0.9715909090909091
2019-03-18T23:15:47.258760: step 415, loss 0.0986701, accuracy 0.957031, precision 0.9766081871345029, recall 0.9597701149425287
2019-03-18T23:15:48.829563: step 416, loss 0.0800658, accuracy 0.976562, precision 0.9873417721518988, recall 0.975
2019-03-18T23:15:50.323601: step 417, loss 0.0554609, accuracy 0.992188, precision 0.9880952380952381, recall 1.0
2019-03-18T23:15:51.827583: step 418, loss 0.078348, accuracy 0.980469, precision 0.9939024390243902, recall 0.9760479041916168
2019-03-18T23:15:53.196923: step 419, loss 0.0815101, accuracy 0.96875, precision 0.9698795180722891, recall 0.9817073170731707
2019-03-18T23:15:54.737806: step 420, loss 0.077453, accuracy 0.980469, precision 0.9940476190476191, recall 0.9766081871345029
2019-03-18T23:15:56.303622: step 421, loss 0.0747579, accuracy 0.980469, precision 0.9876543209876543, recall 0.9815950920245399
2019-03-18T23:15:57.713853: step 422, loss 0.117471, accuracy 0.953125, precision 0.9761904761904762, recall 0.9534883720930233
2019-03-18T23:15:59.290641: step 423, loss 0.0675281, accuracy 0.984375, precision 0.9825581395348837, recall 0.9941176470588236
2019-03-18T23:16:00.694959: step 424, loss 0.0866131, accuracy 0.96875, precision 0.9717514124293786, recall 0.9828571428571429
2019-03-18T23:16:02.018931: step 425, loss 0.0870293, accuracy 0.984375, precision 0.9939024390243902, recall 0.9819277108433735
2019-03-18T23:16:03.478032: step 426, loss 0.10088, accuracy 0.964844, precision 0.9612903225806452, recall 0.9802631578947368
2019-03-18T23:16:04.816456: step 427, loss 0.0648311, accuracy 0.980469, precision 0.975609756097561, recall 0.9937888198757764
2019-03-18T23:16:06.321433: step 428, loss 0.0963762, accuracy 0.964844, precision 0.9827586206896551, recall 0.9661016949152542
2019-03-18T23:16:07.873352: step 429, loss 0.0916983, accuracy 0.972656, precision 0.9770114942528736, recall 0.9826589595375722
2019-03-18T23:16:09.369401: step 430, loss 0.0948142, accuracy 0.964844, precision 0.967948717948718, recall 0.9741935483870968
2019-03-18T23:16:10.695851: step 431, loss 0.0781015, accuracy 0.988281, precision 0.9942528735632183, recall 0.9885714285714285
2019-03-18T23:16:12.141988: step 432, loss 0.0825346, accuracy 0.964844, precision 0.9693251533742331, recall 0.9753086419753086
2019-03-18T23:16:13.622035: step 433, loss 0.0783525, accuracy 0.972656, precision 0.9866666666666667, recall 0.9673202614379085
2019-03-18T23:16:15.009326: step 434, loss 0.0793077, accuracy 0.976562, precision 0.9814814814814815, recall 0.9814814814814815
2019-03-18T23:16:16.442496: step 435, loss 0.113244, accuracy 0.957031, precision 0.9515151515151515, recall 0.98125
2019-03-18T23:16:17.737038: step 436, loss 0.113051, accuracy 0.960938, precision 0.9595375722543352, recall 0.9822485207100592
2019-03-18T23:16:19.161233: step 437, loss 0.0894777, accuracy 0.972656, precision 0.9702380952380952, recall 0.9878787878787879
2019-03-18T23:16:20.601961: step 438, loss 0.0925798, accuracy 0.972656, precision 0.9675324675324676, recall 0.9867549668874173
2019-03-18T23:16:22.025665: step 439, loss 0.0654673, accuracy 0.980469, precision 0.9888268156424581, recall 0.9833333333333333
2019-03-18T23:16:23.672777: step 440, loss 0.0722556, accuracy 0.972656, precision 0.9719101123595506, recall 0.9885714285714285
2019-03-18T23:16:25.237106: step 441, loss 0.0752765, accuracy 0.980469, precision 0.9803921568627451, recall 0.9868421052631579
2019-03-18T23:16:26.736621: step 442, loss 0.0956377, accuracy 0.953125, precision 0.9698795180722891, recall 0.9583333333333334
2019-03-18T23:16:28.235625: step 443, loss 0.117245, accuracy 0.9375, precision 0.9476744186046512, recall 0.9588235294117647
2019-03-18T23:16:28.926913: step 444, loss 0.159984, accuracy 0.945946, precision 0.9583333333333334, recall 0.9583333333333334
2019-03-18T23:16:30.407471: step 445, loss 0.0863607, accuracy 0.960938, precision 0.98125, recall 0.9573170731707317
2019-03-18T23:16:31.979350: step 446, loss 0.0689417, accuracy 0.984375, precision 0.9943502824858758, recall 0.9832402234636871
2019-03-18T23:16:33.413581: step 447, loss 0.0923589, accuracy 0.96875, precision 0.9824561403508771, recall 0.9710982658959537
2019-03-18T23:16:35.094598: step 448, loss 0.0665595, accuracy 0.980469, precision 0.9940476190476191, recall 0.9766081871345029
2019-03-18T23:16:36.662409: step 449, loss 0.0836724, accuracy 0.972656, precision 0.9642857142857143, recall 0.9938650306748467
2019-03-18T23:16:38.736975: step 450, loss 0.090446, accuracy 0.96875, precision 0.9666666666666667, recall 0.9886363636363636
2019-03-18T23:16:40.918655: step 451, loss 0.102972, accuracy 0.960938, precision 0.9704142011834319, recall 0.9704142011834319
2019-03-18T23:16:43.016302: step 452, loss 0.0571529, accuracy 0.988281, precision 0.9826589595375722, recall 1.0
2019-03-18T23:16:44.713362: step 453, loss 0.0535766, accuracy 0.988281, precision 0.9936305732484076, recall 0.9873417721518988
2019-03-18T23:16:46.264218: step 454, loss 0.0739939, accuracy 0.984375, precision 0.9828571428571429, recall 0.9942196531791907
2019-03-18T23:16:47.815073: step 455, loss 0.0752695, accuracy 0.964844, precision 0.9702380952380952, recall 0.9760479041916168
2019-03-18T23:16:49.518522: step 456, loss 0.0724632, accuracy 0.984375, precision 0.9936708860759493, recall 0.98125
2019-03-18T23:16:51.293777: step 457, loss 0.0546011, accuracy 0.988281, precision 0.9938271604938271, recall 0.9877300613496932
2019-03-18T23:16:53.195721: step 458, loss 0.0882864, accuracy 0.96875, precision 0.974025974025974, recall 0.974025974025974
2019-03-18T23:16:54.953024: step 459, loss 0.0811361, accuracy 0.96875, precision 0.9753086419753086, recall 0.9753086419753086
2019-03-18T23:16:56.571319: step 460, loss 0.113559, accuracy 0.960938, precision 0.9772727272727273, recall 0.9662921348314607
2019-03-18T23:16:58.447326: step 461, loss 0.051054, accuracy 0.980469, precision 0.9883720930232558, recall 0.9826589595375722
2019-03-18T23:16:59.987753: step 462, loss 0.0672936, accuracy 0.984375, precision 1.0, recall 0.9754601226993865
2019-03-18T23:17:01.561649: step 463, loss 0.0725689, accuracy 0.972656, precision 0.9748427672955975, recall 0.9810126582278481
2019-03-18T23:17:03.030723: step 464, loss 0.0918654, accuracy 0.972656, precision 0.9875776397515528, recall 0.9695121951219512
2019-03-18T23:17:04.470299: step 465, loss 0.0781817, accuracy 0.984375, precision 0.9876543209876543, recall 0.9876543209876543
2019-03-18T23:17:05.737231: step 466, loss 0.0612175, accuracy 0.988281, precision 0.9940828402366864, recall 0.9882352941176471
2019-03-18T23:17:07.066260: step 467, loss 0.0915037, accuracy 0.972656, precision 0.9746835443037974, recall 0.9808917197452229
2019-03-18T23:17:08.445172: step 468, loss 0.0913464, accuracy 0.960938, precision 0.9464285714285714, recall 0.99375
2019-03-18T23:17:09.719942: step 469, loss 0.049616, accuracy 0.980469, precision 0.9751552795031055, recall 0.9936708860759493
2019-03-18T23:17:11.155622: step 470, loss 0.0562645, accuracy 0.980469, precision 0.9823529411764705, recall 0.9881656804733728
2019-03-18T23:17:12.857148: step 471, loss 0.0508802, accuracy 0.980469, precision 0.9772727272727273, recall 0.9942196531791907
2019-03-18T23:17:14.473405: step 472, loss 0.0682477, accuracy 0.96875, precision 0.9715909090909091, recall 0.9827586206896551
2019-03-18T23:17:15.871235: step 473, loss 0.0964159, accuracy 0.964844, precision 0.9710982658959537, recall 0.9767441860465116
2019-03-18T23:17:17.295988: step 474, loss 0.106029, accuracy 0.96875, precision 0.9814814814814815, recall 0.9695121951219512
2019-03-18T23:17:18.916287: step 475, loss 0.0574199, accuracy 0.984375, precision 0.987012987012987, recall 0.987012987012987
2019-03-18T23:17:20.451737: step 476, loss 0.0945537, accuracy 0.96875, precision 0.9811320754716981, recall 0.968944099378882
2019-03-18T23:17:21.922708: step 477, loss 0.0621957, accuracy 0.988281, precision 0.9888888888888889, recall 0.994413407821229
2019-03-18T23:17:23.422740: step 478, loss 0.0873702, accuracy 0.96875, precision 0.9817073170731707, recall 0.9698795180722891
2019-03-18T23:17:24.733261: step 479, loss 0.0485695, accuracy 0.992188, precision 1.0, recall 0.9882352941176471
2019-03-18T23:17:25.858882: step 480, loss 0.0451735, accuracy 0.988281, precision 0.9943820224719101, recall 0.9888268156424581
2019-03-18T23:17:26.844919: step 481, loss 0.0515463, accuracy 1, precision 1.0, recall 1.0
2019-03-18T23:17:28.295656: step 482, loss 0.0503062, accuracy 0.988281, precision 0.9877300613496932, recall 0.9938271604938271
2019-03-18T23:17:29.583215: step 483, loss 0.0627105, accuracy 0.96875, precision 0.9602272727272727, recall 0.9941176470588236
2019-03-18T23:17:30.908674: step 484, loss 0.0703277, accuracy 0.96875, precision 0.9565217391304348, recall 0.9935483870967742
2019-03-18T23:17:32.085059: step 485, loss 0.0779842, accuracy 0.972656, precision 0.967032967032967, recall 0.9943502824858758
2019-03-18T23:17:33.489307: step 486, loss 0.0397183, accuracy 0.996094, precision 0.9935064935064936, recall 1.0
2019-03-18T23:17:34.801799: step 487, loss 0.0592534, accuracy 0.972656, precision 0.9657142857142857, recall 0.9941176470588236
2019-03-18T23:17:36.218016: step 488, loss 0.0521864, accuracy 0.988281, precision 0.9819277108433735, recall 1.0
2019-03-18T23:17:37.269254: step 489, loss 0.0737305, accuracy 0.972656, precision 0.9702380952380952, recall 0.9878787878787879
2019-03-18T23:17:38.504465: step 490, loss 0.0439421, accuracy 0.988281, precision 0.9880239520958084, recall 0.9939759036144579
2019-03-18T23:17:39.967584: step 491, loss 0.0458331, accuracy 0.980469, precision 0.9939393939393939, recall 0.9761904761904762
2019-03-18T23:17:41.407277: step 492, loss 0.0605968, accuracy 0.976562, precision 0.9940119760479041, recall 0.9707602339181286
2019-03-18T23:17:42.843979: step 493, loss 0.0632151, accuracy 0.972656, precision 0.9885057471264368, recall 0.9717514124293786
2019-03-18T23:17:44.882619: step 494, loss 0.0613904, accuracy 0.976562, precision 1.0, recall 0.967032967032967
2019-03-18T23:17:46.910224: step 495, loss 0.0837502, accuracy 0.972656, precision 0.9883040935672515, recall 0.9712643678160919
2019-03-18T23:17:48.893435: step 496, loss 0.06463, accuracy 0.984375, precision 1.0, recall 0.9767441860465116
2019-03-18T23:17:50.635313: step 497, loss 0.0534852, accuracy 0.980469, precision 0.9704142011834319, recall 1.0
2019-03-18T23:17:52.268948: step 498, loss 0.0594996, accuracy 0.988281, precision 0.9877300613496932, recall 0.9938271604938271
2019-03-18T23:17:53.971398: step 499, loss 0.0643769, accuracy 0.980469, precision 0.9811320754716981, recall 0.9873417721518988
2019-03-18T23:17:55.652905: step 500, loss 0.0713328, accuracy 0.976562, precision 0.9714285714285714, recall 0.9941520467836257

Evaluation:
2019-03-18 23:17:55.656248: W tensorflow/core/framework/allocator.cc:122] Allocation of 99921600 exceeds 10% of system memory.
[[1424  178]
 [ 211  500]]
2019-03-18T23:17:58.378622: step 500, loss 0.460978, accuracy 0.83182, precision 0.8888888888888888, recall 0.8709480122324159

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968365\checkpoints\model-500


Process finished with exit code 0
