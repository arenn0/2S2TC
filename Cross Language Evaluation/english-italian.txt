"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
English Italian
GloVe model loaded
Pretrained Embedding: GloVe
Italian: False
Loading data...
11566
Max Document length: 37
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:98: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 1
Train/Dev split: 9253/2313
2019-03-18 23:06:01.520710: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
35
34
33
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:485: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968365

2019-03-18T23:06:11.574989: step 1, loss 2.73878, accuracy 0.460938, precision 0.35119047619047616, recall 0.6704545454545454
2019-03-18T23:06:13.677877: step 2, loss 4.30552, accuracy 0.617188, precision 0.9810126582278481, recall 0.62
2019-03-18T23:06:14.903603: step 3, loss 3.31711, accuracy 0.640625, precision 0.9573170731707317, recall 0.6487603305785123
2019-03-18T23:06:16.868353: step 4, loss 2.04536, accuracy 0.613281, precision 0.797752808988764, recall 0.6926829268292682
2019-03-18T23:06:18.648595: step 5, loss 2.0541, accuracy 0.527344, precision 0.4860335195530726, recall 0.75
2019-03-18T23:06:20.613344: step 6, loss 2.39702, accuracy 0.457031, precision 0.36809815950920244, recall 0.625
2019-03-18T23:06:22.396579: step 7, loss 1.94355, accuracy 0.523438, precision 0.49101796407185627, recall 0.6890756302521008
2019-03-18T23:06:23.900562: step 8, loss 1.97269, accuracy 0.539062, precision 0.6457142857142857, recall 0.6686390532544378
2019-03-18T23:06:25.480340: step 9, loss 1.80681, accuracy 0.636719, precision 0.8159509202453987, recall 0.6785714285714286
2019-03-18T23:06:26.928472: step 10, loss 1.93864, accuracy 0.628906, precision 0.86, recall 0.6354679802955665
2019-03-18T23:06:28.386574: step 11, loss 1.76326, accuracy 0.664062, precision 0.8690476190476191, recall 0.6952380952380952
2019-03-18T23:06:29.658176: step 12, loss 1.44454, accuracy 0.679688, precision 0.8228571428571428, recall 0.7384615384615385
2019-03-18T23:06:31.015548: step 13, loss 1.6363, accuracy 0.597656, precision 0.7048192771084337, recall 0.6842105263157895
2019-03-18T23:06:32.281168: step 14, loss 1.70162, accuracy 0.558594, precision 0.562874251497006, recall 0.7014925373134329
2019-03-18T23:06:33.561744: step 15, loss 1.4879, accuracy 0.578125, precision 0.5033112582781457, recall 0.6972477064220184
2019-03-18T23:06:34.838334: step 16, loss 1.69513, accuracy 0.578125, precision 0.572289156626506, recall 0.7196969696969697
2019-03-18T23:06:36.033142: step 17, loss 1.33815, accuracy 0.613281, precision 0.6428571428571429, recall 0.7748344370860927
2019-03-18T23:06:37.204012: step 18, loss 1.44474, accuracy 0.632812, precision 0.7839506172839507, recall 0.6827956989247311
2019-03-18T23:06:38.409790: step 19, loss 1.43221, accuracy 0.675781, precision 0.8535031847133758, recall 0.6907216494845361
2019-03-18T23:06:39.602603: step 20, loss 1.42869, accuracy 0.625, precision 0.8045977011494253, recall 0.693069306930693
2019-03-18T23:06:40.690695: step 21, loss 1.36799, accuracy 0.644531, precision 0.806060606060606, recall 0.6927083333333334
2019-03-18T23:06:41.840622: step 22, loss 1.00649, accuracy 0.65625, precision 0.8022598870056498, recall 0.7282051282051282
2019-03-18T23:06:42.884832: step 23, loss 1.11118, accuracy 0.675781, precision 0.7586206896551724, recall 0.7630057803468208
2019-03-18T23:06:44.030770: step 24, loss 1.16817, accuracy 0.640625, precision 0.718562874251497, recall 0.7272727272727273
2019-03-18T23:06:45.100910: step 25, loss 1.09275, accuracy 0.703125, precision 0.7018633540372671, recall 0.8014184397163121
2019-03-18T23:06:46.102235: step 26, loss 1.0711, accuracy 0.644531, precision 0.71875, recall 0.7142857142857143
2019-03-18T23:06:47.212270: step 27, loss 1.32301, accuracy 0.621094, precision 0.6289308176100629, recall 0.7246376811594203
2019-03-18T23:06:48.393113: step 28, loss 1.21763, accuracy 0.65625, precision 0.6815286624203821, recall 0.7379310344827587
2019-03-18T23:06:49.521098: step 29, loss 1.1653, accuracy 0.6875, precision 0.8012422360248447, recall 0.7288135593220338
2019-03-18T23:06:50.649085: step 30, loss 0.963931, accuracy 0.710938, precision 0.8248587570621468, recall 0.7724867724867724
2019-03-18T23:06:51.779066: step 31, loss 0.915838, accuracy 0.695312, precision 0.7844311377245509, recall 0.7572254335260116
2019-03-18T23:06:52.904059: step 32, loss 1.11784, accuracy 0.65625, precision 0.8023952095808383, recall 0.708994708994709
2019-03-18T23:06:53.943282: step 33, loss 1.00864, accuracy 0.703125, precision 0.8, recall 0.7542857142857143
2019-03-18T23:06:55.148066: step 34, loss 0.983319, accuracy 0.695312, precision 0.7441860465116279, recall 0.7901234567901234
2019-03-18T23:06:56.337883: step 35, loss 0.804051, accuracy 0.707031, precision 0.7873563218390804, recall 0.7828571428571428
2019-03-18T23:06:57.393063: step 36, loss 0.850063, accuracy 0.714844, precision 0.7724550898203593, recall 0.7865853658536586
2019-03-18T23:06:57.845855: step 37, loss 0.736103, accuracy 0.756757, precision 0.7586206896551724, recall 0.9166666666666666
2019-03-18T23:06:58.888069: step 38, loss 0.628252, accuracy 0.761719, precision 0.8620689655172413, recall 0.8021390374331551
2019-03-18T23:07:00.120775: step 39, loss 0.792004, accuracy 0.699219, precision 0.8070175438596491, recall 0.7582417582417582
2019-03-18T23:07:01.226819: step 40, loss 0.819346, accuracy 0.785156, precision 0.8804347826086957, recall 0.8307692307692308
2019-03-18T23:07:02.382732: step 41, loss 0.836885, accuracy 0.707031, precision 0.8490566037735849, recall 0.7258064516129032
2019-03-18T23:07:03.457857: step 42, loss 0.887424, accuracy 0.730469, precision 0.8675496688741722, recall 0.7277777777777777
2019-03-18T23:07:04.577864: step 43, loss 0.766344, accuracy 0.714844, precision 0.7784431137724551, recall 0.7831325301204819
2019-03-18T23:07:05.667952: step 44, loss 0.783965, accuracy 0.761719, precision 0.8344370860927153, recall 0.7777777777777778
2019-03-18T23:07:06.695207: step 45, loss 0.749535, accuracy 0.742188, precision 0.7701149425287356, recall 0.8375
2019-03-18T23:07:07.801254: step 46, loss 0.688196, accuracy 0.753906, precision 0.7456647398843931, recall 0.8716216216216216
2019-03-18T23:07:08.897322: step 47, loss 0.630623, accuracy 0.757812, precision 0.7987012987012987, recall 0.7987012987012987
2019-03-18T23:07:10.095121: step 48, loss 0.669319, accuracy 0.75, precision 0.8121212121212121, recall 0.8023952095808383
2019-03-18T23:07:11.228095: step 49, loss 0.642314, accuracy 0.773438, precision 0.8228571428571428, recall 0.8421052631578947
2019-03-18T23:07:12.380017: step 50, loss 0.777998, accuracy 0.734375, precision 0.8198757763975155, recall 0.7719298245614035
2019-03-18T23:07:13.528945: step 51, loss 0.576475, accuracy 0.792969, precision 0.8722222222222222, recall 0.839572192513369
2019-03-18T23:07:14.700814: step 52, loss 0.72363, accuracy 0.703125, precision 0.8220858895705522, recall 0.7403314917127072
2019-03-18T23:07:15.815834: step 53, loss 0.572871, accuracy 0.769531, precision 0.8136645962732919, recall 0.81875
2019-03-18T23:07:16.965761: step 54, loss 0.760685, accuracy 0.757812, precision 0.84472049689441, recall 0.7861271676300579
2019-03-18T23:07:18.149599: step 55, loss 0.603349, accuracy 0.742188, precision 0.8068181818181818, recall 0.8160919540229885
2019-03-18T23:07:19.283568: step 56, loss 0.597256, accuracy 0.761719, precision 0.875, recall 0.7734806629834254
2019-03-18T23:07:20.413550: step 57, loss 0.843718, accuracy 0.703125, precision 0.7987804878048781, recall 0.7528735632183908
2019-03-18T23:07:21.576441: step 58, loss 0.519469, accuracy 0.792969, precision 0.8197674418604651, recall 0.8650306748466258
2019-03-18T23:07:22.717393: step 59, loss 0.641515, accuracy 0.75, precision 0.8301886792452831, recall 0.7810650887573964
2019-03-18T23:07:23.790525: step 60, loss 0.638751, accuracy 0.765625, precision 0.8104575163398693, recall 0.8
2019-03-18T23:07:24.907541: step 61, loss 0.624701, accuracy 0.765625, precision 0.8, recall 0.8395061728395061
2019-03-18T23:07:26.005607: step 62, loss 0.630597, accuracy 0.789062, precision 0.8571428571428571, recall 0.8275862068965517
2019-03-18T23:07:27.108659: step 63, loss 0.506145, accuracy 0.808594, precision 0.8222222222222222, recall 0.896969696969697
2019-03-18T23:07:28.225674: step 64, loss 0.630065, accuracy 0.746094, precision 0.8218390804597702, recall 0.807909604519774
2019-03-18T23:07:29.385575: step 65, loss 0.592112, accuracy 0.753906, precision 0.8141025641025641, recall 0.7888198757763976
2019-03-18T23:07:30.814756: step 66, loss 0.592239, accuracy 0.757812, precision 0.8470588235294118, recall 0.8
2019-03-18T23:07:32.293804: step 67, loss 0.716554, accuracy 0.730469, precision 0.7988165680473372, recall 0.7941176470588235
2019-03-18T23:07:33.852640: step 68, loss 0.58175, accuracy 0.792969, precision 0.8867924528301887, recall 0.8011363636363636
2019-03-18T23:07:35.378561: step 69, loss 0.505473, accuracy 0.808594, precision 0.8870056497175142, recall 0.8440860215053764
2019-03-18T23:07:36.779815: step 70, loss 0.50316, accuracy 0.8125, precision 0.8719512195121951, recall 0.8411764705882353
2019-03-18T23:07:38.254874: step 71, loss 0.58917, accuracy 0.769531, precision 0.8074866310160428, recall 0.867816091954023
2019-03-18T23:07:39.798749: step 72, loss 0.540268, accuracy 0.761719, precision 0.7716049382716049, recall 0.8389261744966443
2019-03-18T23:07:41.406453: step 73, loss 0.670476, accuracy 0.738281, precision 0.8048780487804879, recall 0.7904191616766467
2019-03-18T23:07:42.157446: step 74, loss 0.88562, accuracy 0.72973, precision 0.75, recall 0.8181818181818182
2019-03-18T23:07:43.537759: step 75, loss 0.525656, accuracy 0.8125, precision 0.9239766081871345, recall 0.8186528497409327
2019-03-18T23:07:45.093601: step 76, loss 0.580465, accuracy 0.777344, precision 0.8703703703703703, recall 0.7966101694915254
2019-03-18T23:07:46.513807: step 77, loss 0.567406, accuracy 0.773438, precision 0.8910256410256411, recall 0.7722222222222223
2019-03-18T23:07:47.794385: step 78, loss 0.487914, accuracy 0.78125, precision 0.8289473684210527, recall 0.8076923076923077
2019-03-18T23:07:49.358209: step 79, loss 0.482038, accuracy 0.816406, precision 0.8901734104046243, recall 0.8461538461538461
2019-03-18T23:07:50.753476: step 80, loss 0.531341, accuracy 0.792969, precision 0.8291139240506329, recall 0.8343949044585988
2019-03-18T23:07:52.257457: step 81, loss 0.429935, accuracy 0.804688, precision 0.8493975903614458, recall 0.8493975903614458
2019-03-18T23:07:53.710574: step 82, loss 0.472437, accuracy 0.804688, precision 0.8352941176470589, recall 0.8658536585365854
2019-03-18T23:07:55.298332: step 83, loss 0.447289, accuracy 0.792969, precision 0.8364779874213837, recall 0.83125
2019-03-18T23:07:56.821264: step 84, loss 0.519335, accuracy 0.796875, precision 0.8242424242424242, recall 0.8553459119496856
2019-03-18T23:07:58.450909: step 85, loss 0.482226, accuracy 0.773438, precision 0.8172043010752689, recall 0.8636363636363636
2019-03-18T23:07:59.791327: step 86, loss 0.430763, accuracy 0.800781, precision 0.8620689655172413, recall 0.847457627118644
2019-03-18T23:08:01.230480: step 87, loss 0.345538, accuracy 0.871094, precision 0.9467455621301775, recall 0.8695652173913043
2019-03-18T23:08:02.747427: step 88, loss 0.46588, accuracy 0.828125, precision 0.9147727272727273, recall 0.8473684210526315
2019-03-18T23:08:04.262386: step 89, loss 0.480084, accuracy 0.792969, precision 0.9151515151515152, recall 0.7947368421052632
2019-03-18T23:08:05.725471: step 90, loss 0.381734, accuracy 0.832031, precision 0.8674033149171271, recall 0.8920454545454546
2019-03-18T23:08:07.230448: step 91, loss 0.436536, accuracy 0.820312, precision 0.9080459770114943, recall 0.8404255319148937
2019-03-18T23:08:08.587820: step 92, loss 0.459734, accuracy 0.804688, precision 0.8390804597701149, recall 0.8690476190476191
2019-03-18T23:08:10.024980: step 93, loss 0.540446, accuracy 0.769531, precision 0.8552631578947368, recall 0.7784431137724551
2019-03-18T23:08:11.372380: step 94, loss 0.489096, accuracy 0.78125, precision 0.8603351955307262, recall 0.8324324324324325
2019-03-18T23:08:12.825497: step 95, loss 0.427825, accuracy 0.839844, precision 0.896969696969697, recall 0.8604651162790697
2019-03-18T23:08:14.212790: step 96, loss 0.413021, accuracy 0.824219, precision 0.8695652173913043, recall 0.8839779005524862
2019-03-18T23:08:15.596094: step 97, loss 0.455049, accuracy 0.816406, precision 0.8653846153846154, recall 0.8385093167701864
2019-03-18T23:08:17.236710: step 98, loss 0.422965, accuracy 0.828125, precision 0.8466257668711656, recall 0.8789808917197452
2019-03-18T23:08:18.698802: step 99, loss 0.398723, accuracy 0.84375, precision 0.8633540372670807, recall 0.8853503184713376
2019-03-18T23:08:20.038224: step 100, loss 0.40059, accuracy 0.804688, precision 0.8607594936708861, recall 0.8292682926829268
2019-03-18T23:08:21.411554: step 101, loss 0.449804, accuracy 0.820312, precision 0.9032258064516129, recall 0.8187134502923976
2019-03-18T23:08:22.972383: step 102, loss 0.513793, accuracy 0.796875, precision 0.84, recall 0.8596491228070176
2019-03-18T23:08:24.450433: step 103, loss 0.563298, accuracy 0.765625, precision 0.8, recall 0.8104575163398693
2019-03-18T23:08:25.920506: step 104, loss 0.373545, accuracy 0.835938, precision 0.8685714285714285, recall 0.8888888888888888
2019-03-18T23:08:27.450417: step 105, loss 0.479088, accuracy 0.824219, precision 0.8874172185430463, recall 0.8271604938271605
2019-03-18T23:08:29.064105: step 106, loss 0.330096, accuracy 0.871094, precision 0.8959537572254336, recall 0.9117647058823529
2019-03-18T23:08:30.811436: step 107, loss 0.544731, accuracy 0.757812, precision 0.8352941176470589, recall 0.8068181818181818
2019-03-18T23:08:32.281507: step 108, loss 0.576515, accuracy 0.792969, precision 0.874251497005988, recall 0.8202247191011236
2019-03-18T23:08:33.810423: step 109, loss 0.413937, accuracy 0.832031, precision 0.884393063583815, recall 0.8693181818181818
2019-03-18T23:08:35.493924: step 110, loss 0.428049, accuracy 0.839844, precision 0.8641975308641975, recall 0.8805031446540881
2019-03-18T23:08:36.297777: step 111, loss 0.372128, accuracy 0.864865, precision 0.9259259259259259, recall 0.8928571428571429
2019-03-18T23:08:37.930412: step 112, loss 0.293917, accuracy 0.882812, precision 0.9047619047619048, recall 0.9156626506024096
2019-03-18T23:08:39.450358: step 113, loss 0.298497, accuracy 0.871094, precision 0.9190751445086706, recall 0.8932584269662921
2019-03-18T23:08:41.289438: step 114, loss 0.406742, accuracy 0.847656, precision 0.9171974522292994, recall 0.8470588235294118
2019-03-18T23:08:42.952991: step 115, loss 0.389903, accuracy 0.8125, precision 0.8588957055214724, recall 0.8484848484848485
2019-03-18T23:08:44.477917: step 116, loss 0.37441, accuracy 0.847656, precision 0.9151515151515152, recall 0.8579545454545454
2019-03-18T23:08:46.025782: step 117, loss 0.399486, accuracy 0.832031, precision 0.8579881656804734, recall 0.8841463414634146
2019-03-18T23:08:47.581623: step 118, loss 0.368071, accuracy 0.839844, precision 0.867816091954023, recall 0.893491124260355
2019-03-18T23:08:49.073637: step 119, loss 0.404482, accuracy 0.808594, precision 0.8353658536585366, recall 0.8616352201257862
2019-03-18T23:08:50.578615: step 120, loss 0.411966, accuracy 0.832031, precision 0.8909090909090909, recall 0.8546511627906976
2019-03-18T23:08:52.066642: step 121, loss 0.455469, accuracy 0.808594, precision 0.8765432098765432, recall 0.8304093567251462
2019-03-18T23:08:53.609515: step 122, loss 0.394158, accuracy 0.835938, precision 0.8975903614457831, recall 0.8563218390804598
2019-03-18T23:08:55.131449: step 123, loss 0.433972, accuracy 0.835938, precision 0.8713450292397661, recall 0.8816568047337278
2019-03-18T23:08:56.712226: step 124, loss 0.387959, accuracy 0.84375, precision 0.9298245614035088, recall 0.8502673796791443
2019-03-18T23:08:58.264078: step 125, loss 0.290707, accuracy 0.875, precision 0.8977272727272727, recall 0.9186046511627907
2019-03-18T23:08:59.895718: step 126, loss 0.395717, accuracy 0.84375, precision 0.9325153374233128, recall 0.8397790055248618
2019-03-18T23:09:01.437598: step 127, loss 0.324874, accuracy 0.863281, precision 0.9341317365269461, recall 0.8666666666666667
2019-03-18T23:09:02.892714: step 128, loss 0.382389, accuracy 0.824219, precision 0.9030303030303031, recall 0.8370786516853933
2019-03-18T23:09:04.558259: step 129, loss 0.404045, accuracy 0.847656, precision 0.8841463414634146, recall 0.8787878787878788
2019-03-18T23:09:06.012373: step 130, loss 0.383462, accuracy 0.835938, precision 0.8895348837209303, recall 0.8693181818181818
2019-03-18T23:09:07.765703: step 131, loss 0.418739, accuracy 0.816406, precision 0.8218390804597702, recall 0.89937106918239
2019-03-18T23:09:09.337488: step 132, loss 0.351154, accuracy 0.878906, precision 0.88125, recall 0.9215686274509803
2019-03-18T23:09:11.141666: step 133, loss 0.38765, accuracy 0.832031, precision 0.8654970760233918, recall 0.8809523809523809
2019-03-18T23:09:12.656651: step 134, loss 0.318503, accuracy 0.855469, precision 0.9017341040462428, recall 0.8863636363636364
2019-03-18T23:09:14.007086: step 135, loss 0.346919, accuracy 0.863281, precision 0.8888888888888888, recall 0.9047619047619048
2019-03-18T23:09:15.536000: step 136, loss 0.319258, accuracy 0.855469, precision 0.9130434782608695, recall 0.8647058823529412
2019-03-18T23:09:17.033000: step 137, loss 0.338148, accuracy 0.835938, precision 0.9101796407185628, recall 0.8491620111731844
2019-03-18T23:09:18.507062: step 138, loss 0.343521, accuracy 0.855469, precision 0.8982035928143712, recall 0.8823529411764706
2019-03-18T23:09:19.973680: step 139, loss 0.368086, accuracy 0.855469, precision 0.9310344827586207, recall 0.8663101604278075
2019-03-18T23:09:21.375933: step 140, loss 0.373619, accuracy 0.84375, precision 0.9202453987730062, recall 0.847457627118644
2019-03-18T23:09:22.907356: step 141, loss 0.459443, accuracy 0.804688, precision 0.8901734104046243, recall 0.8324324324324325
2019-03-18T23:09:24.542036: step 142, loss 0.344127, accuracy 0.855469, precision 0.9041916167664671, recall 0.877906976744186
2019-03-18T23:09:25.967753: step 143, loss 0.386631, accuracy 0.847656, precision 0.8909090909090909, recall 0.875
2019-03-18T23:09:27.516615: step 144, loss 0.356496, accuracy 0.84375, precision 0.8908045977011494, recall 0.8806818181818182
2019-03-18T23:09:29.055504: step 145, loss 0.314733, accuracy 0.855469, precision 0.8838709677419355, recall 0.8782051282051282
2019-03-18T23:09:30.530562: step 146, loss 0.365911, accuracy 0.863281, precision 0.8957055214723927, recall 0.8902439024390244
2019-03-18T23:09:32.116325: step 147, loss 0.310995, accuracy 0.867188, precision 0.8846153846153846, recall 0.8961038961038961
2019-03-18T23:09:32.685818: step 148, loss 0.20233, accuracy 0.945946, precision 0.9130434782608695, recall 1.0
2019-03-18T23:09:34.318455: step 149, loss 0.311742, accuracy 0.871094, precision 0.9298245614035088, recall 0.8833333333333333
2019-03-18T23:09:35.871305: step 150, loss 0.313365, accuracy 0.867188, precision 0.877906976744186, recall 0.9207317073170732
2019-03-18T23:09:37.590231: step 151, loss 0.297504, accuracy 0.878906, precision 0.9190751445086706, recall 0.9034090909090909
2019-03-18T23:09:39.306705: step 152, loss 0.362802, accuracy 0.851562, precision 0.9121621621621622, recall 0.84375
2019-03-18T23:09:40.854587: step 153, loss 0.315726, accuracy 0.855469, precision 0.8846153846153846, recall 0.8789808917197452
2019-03-18T23:09:42.441347: step 154, loss 0.192668, accuracy 0.941406, precision 0.9753086419753086, recall 0.9349112426035503
2019-03-18T23:09:44.148374: step 155, loss 0.364339, accuracy 0.84375, precision 0.9132947976878613, recall 0.8633879781420765
2019-03-18T23:09:45.800021: step 156, loss 0.222979, accuracy 0.898438, precision 0.9444444444444444, recall 0.9
2019-03-18T23:09:47.315491: step 157, loss 0.292026, accuracy 0.878906, precision 0.9074074074074074, recall 0.901840490797546
2019-03-18T23:09:48.772109: step 158, loss 0.298371, accuracy 0.871094, precision 0.9107142857142857, recall 0.8947368421052632
2019-03-18T23:09:50.182342: step 159, loss 0.302598, accuracy 0.867188, precision 0.8930817610062893, recall 0.8930817610062893
2019-03-18T23:09:51.951669: step 160, loss 0.291017, accuracy 0.90625, precision 0.9310344827586207, recall 0.9310344827586207
2019-03-18T23:09:53.657165: step 161, loss 0.294348, accuracy 0.847656, precision 0.879746835443038, recall 0.8742138364779874
2019-03-18T23:09:55.018528: step 162, loss 0.220716, accuracy 0.925781, precision 0.9209039548022598, recall 0.9702380952380952
2019-03-18T23:09:56.603317: step 163, loss 0.286431, accuracy 0.878906, precision 0.9181286549707602, recall 0.9022988505747126
2019-03-18T23:09:58.296369: step 164, loss 0.295932, accuracy 0.890625, precision 0.9216867469879518, recall 0.9107142857142857
2019-03-18T23:09:59.819878: step 165, loss 0.335796, accuracy 0.882812, precision 0.9506172839506173, recall 0.875
2019-03-18T23:10:01.479471: step 166, loss 0.237601, accuracy 0.921875, precision 0.9540229885057471, recall 0.9325842696629213
2019-03-18T23:10:02.923613: step 167, loss 0.315095, accuracy 0.859375, precision 0.8950617283950617, recall 0.8841463414634146
2019-03-18T23:10:04.327881: step 168, loss 0.3638, accuracy 0.835938, precision 0.92, recall 0.8518518518518519
2019-03-18T23:10:05.889281: step 169, loss 0.273205, accuracy 0.878906, precision 0.9607843137254902, recall 0.8546511627906976
2019-03-18T23:10:07.288051: step 170, loss 0.349595, accuracy 0.859375, precision 0.9069767441860465, recall 0.8863636363636364
2019-03-18T23:10:08.829508: step 171, loss 0.272544, accuracy 0.902344, precision 0.9523809523809523, recall 0.903954802259887
2019-03-18T23:10:10.270657: step 172, loss 0.297993, accuracy 0.875, precision 0.912568306010929, recall 0.912568306010929
2019-03-18T23:10:12.048461: step 173, loss 0.277425, accuracy 0.882812, precision 0.8719512195121951, recall 0.9407894736842105
2019-03-18T23:10:13.823301: step 174, loss 0.274601, accuracy 0.882812, precision 0.8855421686746988, recall 0.930379746835443
2019-03-18T23:10:15.356711: step 175, loss 0.274933, accuracy 0.890625, precision 0.9074074074074074, recall 0.91875
2019-03-18T23:10:16.869693: step 176, loss 0.289237, accuracy 0.871094, precision 0.8719512195121951, recall 0.9225806451612903
2019-03-18T23:10:18.513300: step 177, loss 0.320368, accuracy 0.847656, precision 0.8855421686746988, recall 0.8802395209580839
2019-03-18T23:10:20.028762: step 178, loss 0.284648, accuracy 0.886719, precision 0.9281437125748503, recall 0.9011627906976745
2019-03-18T23:10:21.458454: step 179, loss 0.333221, accuracy 0.863281, precision 0.9333333333333333, recall 0.8795811518324608
2019-03-18T23:10:22.789406: step 180, loss 0.236259, accuracy 0.90625, precision 0.9473684210526315, recall 0.9152542372881356
2019-03-18T23:10:24.352230: step 181, loss 0.335106, accuracy 0.867188, precision 0.9487179487179487, recall 0.8505747126436781
2019-03-18T23:10:26.011845: step 182, loss 0.301599, accuracy 0.863281, precision 0.949685534591195, recall 0.848314606741573
2019-03-18T23:10:27.490920: step 183, loss 0.287227, accuracy 0.84375, precision 0.8895348837209303, recall 0.8793103448275862
2019-03-18T23:10:29.029808: step 184, loss 0.340444, accuracy 0.859375, precision 0.861878453038674, recall 0.9341317365269461
2019-03-18T23:10:29.798752: step 185, loss 0.347137, accuracy 0.837838, precision 0.8260869565217391, recall 0.9047619047619048
2019-03-18T23:10:31.386143: step 186, loss 0.197157, accuracy 0.921875, precision 0.9294871794871795, recall 0.9415584415584416
2019-03-18T23:10:33.113526: step 187, loss 0.293983, accuracy 0.882812, precision 0.8922155688622755, recall 0.9254658385093167
2019-03-18T23:10:34.675354: step 188, loss 0.258152, accuracy 0.886719, precision 0.9415204678362573, recall 0.8944444444444445
2019-03-18T23:10:36.443201: step 189, loss 0.257836, accuracy 0.898438, precision 0.9371428571428572, recall 0.9162011173184358
2019-03-18T23:10:38.033950: step 190, loss 0.266422, accuracy 0.875, precision 0.9044585987261147, recall 0.8930817610062893
2019-03-18T23:10:39.676620: step 191, loss 0.285769, accuracy 0.886719, precision 0.9404761904761905, recall 0.8926553672316384
2019-03-18T23:10:41.236965: step 192, loss 0.240558, accuracy 0.898438, precision 0.9285714285714286, recall 0.9176470588235294
2019-03-18T23:10:42.839731: step 193, loss 0.226928, accuracy 0.921875, precision 0.9559748427672956, recall 0.9212121212121213
2019-03-18T23:10:44.440982: step 194, loss 0.210269, accuracy 0.917969, precision 0.9393939393939394, recall 0.9337349397590361
2019-03-18T23:10:46.255673: step 195, loss 0.25717, accuracy 0.898438, precision 0.9457831325301205, recall 0.9022988505747126
2019-03-18T23:10:47.996684: step 196, loss 0.30271, accuracy 0.890625, precision 0.9, recall 0.9230769230769231
2019-03-18T23:10:49.665244: step 197, loss 0.276115, accuracy 0.890625, precision 0.9226190476190477, recall 0.9117647058823529
2019-03-18T23:10:51.204132: step 198, loss 0.250964, accuracy 0.894531, precision 0.9438202247191011, recall 0.9081081081081082
2019-03-18T23:10:52.810350: step 199, loss 0.230114, accuracy 0.910156, precision 0.9717514124293786, recall 0.9052631578947369
2019-03-18T23:10:54.650432: step 200, loss 0.258469, accuracy 0.902344, precision 0.9349112426035503, recall 0.9186046511627907
2019-03-18T23:10:56.591277: step 201, loss 0.229643, accuracy 0.910156, precision 0.9595375722543352, recall 0.9120879120879121
2019-03-18T23:10:58.209954: step 202, loss 0.195965, accuracy 0.914062, precision 0.9117647058823529, recall 0.9567901234567902
2019-03-18T23:10:59.727895: step 203, loss 0.216302, accuracy 0.894531, precision 0.9141104294478528, recall 0.9197530864197531
2019-03-18T23:11:01.427354: step 204, loss 0.171953, accuracy 0.9375, precision 0.9397590361445783, recall 0.9629629629629629
2019-03-18T23:11:02.831620: step 205, loss 0.21171, accuracy 0.925781, precision 0.9493670886075949, recall 0.9316770186335404
2019-03-18T23:11:04.300697: step 206, loss 0.214006, accuracy 0.921875, precision 0.9763313609467456, recall 0.9116022099447514
2019-03-18T23:11:05.687989: step 207, loss 0.203885, accuracy 0.910156, precision 0.9265536723163842, recall 0.9425287356321839
2019-03-18T23:11:07.458293: step 208, loss 0.23685, accuracy 0.90625, precision 0.9190751445086706, recall 0.9408284023668639
2019-03-18T23:11:09.081465: step 209, loss 0.230143, accuracy 0.898438, precision 0.9518072289156626, recall 0.8977272727272727
2019-03-18T23:11:10.637308: step 210, loss 0.228496, accuracy 0.914062, precision 0.9534883720930233, recall 0.9213483146067416
2019-03-18T23:11:12.245588: step 211, loss 0.362951, accuracy 0.867188, precision 0.8994413407821229, recall 0.9096045197740112
2019-03-18T23:11:13.908143: step 212, loss 0.229766, accuracy 0.898438, precision 0.9390243902439024, recall 0.9058823529411765
2019-03-18T23:11:15.543773: step 213, loss 0.218579, accuracy 0.914062, precision 0.9371428571428572, recall 0.9371428571428572
2019-03-18T23:11:17.182393: step 214, loss 0.261669, accuracy 0.898438, precision 0.9404761904761905, recall 0.9080459770114943
2019-03-18T23:11:18.728771: step 215, loss 0.226003, accuracy 0.921875, precision 0.9512195121951219, recall 0.9285714285714286
2019-03-18T23:11:20.305632: step 216, loss 0.250848, accuracy 0.894531, precision 0.9177215189873418, recall 0.9119496855345912
2019-03-18T23:11:21.881519: step 217, loss 0.206046, accuracy 0.90625, precision 0.9325153374233128, recall 0.9212121212121213
2019-03-18T23:11:23.572021: step 218, loss 0.262738, accuracy 0.894531, precision 0.9197530864197531, recall 0.9141104294478528
2019-03-18T23:11:25.107970: step 219, loss 0.264145, accuracy 0.910156, precision 0.9570552147239264, recall 0.9069767441860465
2019-03-18T23:11:26.536663: step 220, loss 0.234896, accuracy 0.921875, precision 0.9221556886227545, recall 0.9565217391304348
2019-03-18T23:11:28.089515: step 221, loss 0.242802, accuracy 0.902344, precision 0.9367088607594937, recall 0.9079754601226994
2019-03-18T23:11:28.766704: step 222, loss 0.316978, accuracy 0.864865, precision 0.9, recall 0.8571428571428571
2019-03-18T23:11:30.205882: step 223, loss 0.184, accuracy 0.925781, precision 0.9235294117647059, recall 0.9631901840490797
2019-03-18T23:11:31.522074: step 224, loss 0.179411, accuracy 0.9375, precision 0.9404761904761905, recall 0.9634146341463414
2019-03-18T23:11:33.401628: step 225, loss 0.228616, accuracy 0.921875, precision 0.9405405405405406, recall 0.9508196721311475
2019-03-18T23:11:35.056207: step 226, loss 0.210273, accuracy 0.933594, precision 0.9518072289156626, recall 0.9461077844311377
2019-03-18T23:11:36.508834: step 227, loss 0.166439, accuracy 0.933594, precision 0.9512195121951219, recall 0.9454545454545454
2019-03-18T23:11:38.084134: step 228, loss 0.227474, accuracy 0.917969, precision 0.949685534591195, recall 0.9207317073170732
2019-03-18T23:11:39.590617: step 229, loss 0.199375, accuracy 0.914062, precision 0.9588235294117647, recall 0.9157303370786517
2019-03-18T23:11:41.051765: step 230, loss 0.224233, accuracy 0.902344, precision 0.9464285714285714, recall 0.9085714285714286
2019-03-18T23:11:42.552869: step 231, loss 0.201913, accuracy 0.933594, precision 0.9820359281437125, recall 0.9213483146067416
2019-03-18T23:11:44.165557: step 232, loss 0.191161, accuracy 0.914062, precision 0.950920245398773, recall 0.9171597633136095
2019-03-18T23:11:45.781341: step 233, loss 0.195934, accuracy 0.921875, precision 0.9636363636363636, recall 0.9190751445086706
2019-03-18T23:11:47.247424: step 234, loss 0.147401, accuracy 0.9375, precision 0.9590643274853801, recall 0.9479768786127167
2019-03-18T23:11:48.835797: step 235, loss 0.198702, accuracy 0.90625, precision 0.926829268292683, recall 0.926829268292683
2019-03-18T23:11:50.394631: step 236, loss 0.225123, accuracy 0.902344, precision 0.9290322580645162, recall 0.9113924050632911
2019-03-18T23:11:51.897649: step 237, loss 0.208682, accuracy 0.910156, precision 0.9195402298850575, recall 0.9467455621301775
2019-03-18T23:11:53.420092: step 238, loss 0.21785, accuracy 0.925781, precision 0.9302325581395349, recall 0.9580838323353293
2019-03-18T23:11:54.802399: step 239, loss 0.178704, accuracy 0.925781, precision 0.9371069182389937, recall 0.9430379746835443
2019-03-18T23:11:56.319856: step 240, loss 0.191866, accuracy 0.925781, precision 0.9393939393939394, recall 0.9451219512195121
2019-03-18T23:11:57.803401: step 241, loss 0.190234, accuracy 0.917969, precision 0.9308176100628931, recall 0.9367088607594937
2019-03-18T23:11:59.474002: step 242, loss 0.189326, accuracy 0.9375, precision 0.9421965317919075, recall 0.9644970414201184
2019-03-18T23:12:01.106187: step 243, loss 0.218856, accuracy 0.90625, precision 0.9363057324840764, recall 0.9130434782608695
2019-03-18T23:12:02.700920: step 244, loss 0.266243, accuracy 0.875, precision 0.8983050847457628, recall 0.9190751445086706
2019-03-18T23:12:04.175001: step 245, loss 0.199729, accuracy 0.90625, precision 0.9518072289156626, recall 0.9080459770114943
2019-03-18T23:12:05.702005: step 246, loss 0.175635, accuracy 0.933594, precision 0.9709302325581395, recall 0.9329608938547486
2019-03-18T23:12:07.295769: step 247, loss 0.204811, accuracy 0.894531, precision 0.9506172839506173, recall 0.8901734104046243
2019-03-18T23:12:08.943440: step 248, loss 0.237667, accuracy 0.910156, precision 0.95, recall 0.9101796407185628
2019-03-18T23:12:10.529202: step 249, loss 0.200157, accuracy 0.902344, precision 0.9325842696629213, recall 0.9273743016759777
2019-03-18T23:12:12.019731: step 250, loss 0.152463, accuracy 0.933594, precision 0.9580838323353293, recall 0.9411764705882353

Evaluation:
[[889 713]
 [420 291]]
2019-03-18T23:12:15.023411: step 250, loss 1.10615, accuracy 0.51016, precision 0.5549313358302123, recall 0.679144385026738

2019-03-18 23:12:12.954022: W tensorflow/core/framework/allocator.cc:122] Allocation of 102697200 exceeds 10% of system memory.
Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968365\checkpoints\model-250

2019-03-18T23:12:17.611085: step 251, loss 0.188179, accuracy 0.9375, precision 0.9390243902439024, recall 0.9625
2019-03-18T23:12:19.115128: step 252, loss 0.246149, accuracy 0.917969, precision 0.9345238095238095, recall 0.9401197604790419
2019-03-18T23:12:20.581280: step 253, loss 0.161775, accuracy 0.917969, precision 0.9378531073446328, recall 0.9431818181818182
2019-03-18T23:12:22.136147: step 254, loss 0.25024, accuracy 0.898438, precision 0.9146341463414634, recall 0.9259259259259259
2019-03-18T23:12:23.670560: step 255, loss 0.213834, accuracy 0.921875, precision 0.9230769230769231, recall 0.9570552147239264
2019-03-18T23:12:25.122679: step 256, loss 0.253946, accuracy 0.90625, precision 0.9371069182389937, recall 0.9141104294478528
2019-03-18T23:12:26.678521: step 257, loss 0.180426, accuracy 0.929688, precision 0.9534883720930233, recall 0.9425287356321839
2019-03-18T23:12:28.377493: step 258, loss 0.175313, accuracy 0.933594, precision 0.9490445859872612, recall 0.9430379746835443
2019-03-18T23:12:29.163394: step 259, loss 0.088351, accuracy 0.972973, precision 1.0, recall 0.9629629629629629
2019-03-18T23:12:30.512787: step 260, loss 0.194879, accuracy 0.90625, precision 0.9532163742690059, recall 0.9106145251396648
2019-03-18T23:12:32.161892: step 261, loss 0.163747, accuracy 0.929688, precision 0.9479768786127167, recall 0.9479768786127167
2019-03-18T23:12:33.799024: step 262, loss 0.165027, accuracy 0.925781, precision 0.9558011049723757, recall 0.9402173913043478
2019-03-18T23:12:35.438200: step 263, loss 0.135747, accuracy 0.949219, precision 0.9668508287292817, recall 0.9615384615384616
2019-03-18T23:12:36.936197: step 264, loss 0.166867, accuracy 0.921875, precision 0.9542857142857143, recall 0.9329608938547486
2019-03-18T23:12:38.552878: step 265, loss 0.173123, accuracy 0.933594, precision 0.9487179487179487, recall 0.9426751592356688
2019-03-18T23:12:40.034552: step 266, loss 0.141057, accuracy 0.945312, precision 0.9473684210526315, recall 0.9700598802395209
2019-03-18T23:12:41.616353: step 267, loss 0.156171, accuracy 0.929688, precision 0.9575757575757575, recall 0.9349112426035503
2019-03-18T23:12:43.179686: step 268, loss 0.143718, accuracy 0.945312, precision 0.9556962025316456, recall 0.9556962025316456
2019-03-18T23:12:44.587944: step 269, loss 0.236844, accuracy 0.886719, precision 0.9127906976744186, recall 0.9181286549707602
2019-03-18T23:12:46.072975: step 270, loss 0.155231, accuracy 0.957031, precision 0.9642857142857143, recall 0.9700598802395209
2019-03-18T23:12:47.560001: step 271, loss 0.132544, accuracy 0.949219, precision 0.9505494505494505, recall 0.9774011299435028
2019-03-18T23:12:49.153812: step 272, loss 0.142232, accuracy 0.949219, precision 0.9822485207100592, recall 0.9431818181818182
2019-03-18T23:12:50.634854: step 273, loss 0.154443, accuracy 0.945312, precision 0.9649122807017544, recall 0.953757225433526
2019-03-18T23:12:52.294450: step 274, loss 0.15095, accuracy 0.945312, precision 0.9717514124293786, recall 0.9502762430939227
2019-03-18T23:12:53.809401: step 275, loss 0.182141, accuracy 0.925781, precision 0.9871794871794872, recall 0.9005847953216374
2019-03-18T23:12:55.250573: step 276, loss 0.170542, accuracy 0.914062, precision 0.9548387096774194, recall 0.9079754601226994
2019-03-18T23:12:56.852406: step 277, loss 0.178491, accuracy 0.925781, precision 0.9448275862068966, recall 0.9256756756756757
2019-03-18T23:12:58.454638: step 278, loss 0.170631, accuracy 0.933594, precision 0.9444444444444444, recall 0.9503105590062112
2019-03-18T23:12:59.931693: step 279, loss 0.157521, accuracy 0.9375, precision 0.94375, recall 0.9556962025316456
2019-03-18T23:13:01.367877: step 280, loss 0.188518, accuracy 0.929688, precision 0.9487179487179487, recall 0.9367088607594937
2019-03-18T23:13:02.870863: step 281, loss 0.137863, accuracy 0.949219, precision 0.9647058823529412, recall 0.9590643274853801
2019-03-18T23:13:04.349531: step 282, loss 0.21212, accuracy 0.90625, precision 0.8816568047337278, recall 0.9738562091503268
2019-03-18T23:13:05.902892: step 283, loss 0.13571, accuracy 0.960938, precision 0.9602272727272727, recall 0.9825581395348837
2019-03-18T23:13:07.476687: step 284, loss 0.177582, accuracy 0.921875, precision 0.9166666666666666, recall 0.9625
2019-03-18T23:13:08.940288: step 285, loss 0.165724, accuracy 0.945312, precision 0.9580838323353293, recall 0.9580838323353293
2019-03-18T23:13:10.626292: step 286, loss 0.181272, accuracy 0.914062, precision 0.9488636363636364, recall 0.9277777777777778
2019-03-18T23:13:12.054528: step 287, loss 0.154403, accuracy 0.941406, precision 0.9702380952380952, recall 0.9421965317919075
2019-03-18T23:13:13.566996: step 288, loss 0.158366, accuracy 0.933594, precision 0.9760479041916168, recall 0.9261363636363636
2019-03-18T23:13:15.111925: step 289, loss 0.147444, accuracy 0.957031, precision 0.9775280898876404, recall 0.9613259668508287
2019-03-18T23:13:16.638845: step 290, loss 0.198421, accuracy 0.914062, precision 0.9814814814814815, recall 0.8932584269662921
2019-03-18T23:13:18.240565: step 291, loss 0.183389, accuracy 0.910156, precision 0.930379746835443, recall 0.9245283018867925
2019-03-18T23:13:19.906627: step 292, loss 0.138885, accuracy 0.957031, precision 0.9590643274853801, recall 0.9761904761904762
2019-03-18T23:13:21.326830: step 293, loss 0.148342, accuracy 0.953125, precision 0.9722222222222222, recall 0.9459459459459459
2019-03-18T23:13:22.953483: step 294, loss 0.145721, accuracy 0.949219, precision 0.9629629629629629, recall 0.9570552147239264
2019-03-18T23:13:24.614560: step 295, loss 0.205961, accuracy 0.914062, precision 0.9294117647058824, recall 0.9404761904761905
2019-03-18T23:13:25.237893: step 296, loss 0.0973478, accuracy 0.972973, precision 0.9545454545454546, recall 1.0
2019-03-18T23:13:26.904487: step 297, loss 0.100583, accuracy 0.980469, precision 0.9821428571428571, recall 0.9880239520958084
2019-03-18T23:13:28.422456: step 298, loss 0.158861, accuracy 0.941406, precision 0.9573170731707317, recall 0.9515151515151515
2019-03-18T23:13:29.871608: step 299, loss 0.110565, accuracy 0.964844, precision 0.9534883720930233, recall 0.9939393939393939
2019-03-18T23:13:31.395056: step 300, loss 0.138986, accuracy 0.953125, precision 0.9515151515151515, recall 0.9751552795031055
2019-03-18T23:13:33.102017: step 301, loss 0.147011, accuracy 0.945312, precision 0.9382716049382716, recall 0.9743589743589743
2019-03-18T23:13:34.648912: step 302, loss 0.154245, accuracy 0.949219, precision 0.9810126582278481, recall 0.9393939393939394
2019-03-18T23:13:36.282058: step 303, loss 0.148194, accuracy 0.941406, precision 0.9634146341463414, recall 0.9461077844311377
2019-03-18T23:13:37.610020: step 304, loss 0.110912, accuracy 0.964844, precision 0.9827586206896551, recall 0.9661016949152542
2019-03-18T23:13:39.245167: step 305, loss 0.121995, accuracy 0.96875, precision 0.9700598802395209, recall 0.9818181818181818
2019-03-18T23:13:40.903799: step 306, loss 0.163125, accuracy 0.929688, precision 0.9710982658959537, recall 0.9281767955801105
2019-03-18T23:13:42.415294: step 307, loss 0.163502, accuracy 0.941406, precision 0.9640718562874252, recall 0.9470588235294117
2019-03-18T23:13:43.868416: step 308, loss 0.115052, accuracy 0.964844, precision 0.9705882352941176, recall 0.9763313609467456
2019-03-18T23:13:45.349968: step 309, loss 0.154225, accuracy 0.941406, precision 0.963855421686747, recall 0.9467455621301775
2019-03-18T23:13:46.961175: step 310, loss 0.140741, accuracy 0.945312, precision 0.9425287356321839, recall 0.9761904761904762
2019-03-18T23:13:48.464721: step 311, loss 0.171387, accuracy 0.925781, precision 0.9640718562874252, recall 0.9252873563218391
2019-03-18T23:13:49.895898: step 312, loss 0.102124, accuracy 0.964844, precision 0.9704142011834319, recall 0.9761904761904762
2019-03-18T23:13:51.568430: step 313, loss 0.135003, accuracy 0.945312, precision 0.9550561797752809, recall 0.9659090909090909
2019-03-18T23:13:53.217081: step 314, loss 0.12086, accuracy 0.964844, precision 0.9814814814814815, recall 0.9636363636363636
2019-03-18T23:13:54.698123: step 315, loss 0.207804, accuracy 0.910156, precision 0.9294871794871795, recall 0.9235668789808917
2019-03-18T23:13:56.162259: step 316, loss 0.159604, accuracy 0.9375, precision 0.9753086419753086, recall 0.9294117647058824
2019-03-18T23:13:57.799439: step 317, loss 0.14897, accuracy 0.929688, precision 0.9263803680981595, recall 0.9617834394904459
2019-03-18T23:13:59.383206: step 318, loss 0.140099, accuracy 0.941406, precision 0.9570552147239264, recall 0.9512195121951219
2019-03-18T23:14:00.896163: step 319, loss 0.124067, accuracy 0.964844, precision 0.9647058823529412, recall 0.9820359281437125
2019-03-18T23:14:02.513385: step 320, loss 0.168212, accuracy 0.933594, precision 0.9337349397590361, recall 0.9627329192546584
2019-03-18T23:14:04.580961: step 321, loss 0.146839, accuracy 0.957031, precision 0.9714285714285714, recall 0.9659090909090909
2019-03-18T23:14:06.760192: step 322, loss 0.153002, accuracy 0.945312, precision 0.9733333333333334, recall 0.9358974358974359
2019-03-18T23:14:08.941451: step 323, loss 0.152799, accuracy 0.929688, precision 0.963855421686747, recall 0.9302325581395349
2019-03-18T23:14:10.695855: step 324, loss 0.175459, accuracy 0.929688, precision 0.9556962025316456, recall 0.9320987654320988
2019-03-18T23:14:12.713048: step 325, loss 0.157785, accuracy 0.914062, precision 0.9542857142857143, recall 0.9226519337016574
2019-03-18T23:14:14.544810: step 326, loss 0.0921542, accuracy 0.953125, precision 0.96, recall 0.9710982658959537
2019-03-18T23:14:16.575439: step 327, loss 0.124215, accuracy 0.957031, precision 0.9822485207100592, recall 0.9540229885057471
2019-03-18T23:14:18.338297: step 328, loss 0.216558, accuracy 0.917969, precision 0.9230769230769231, recall 0.9512195121951219
2019-03-18T23:14:20.368458: step 329, loss 0.114883, accuracy 0.957031, precision 0.96, recall 0.9767441860465116
2019-03-18T23:14:22.051499: step 330, loss 0.0931854, accuracy 0.96875, precision 0.9883040935672515, recall 0.9657142857142857
2019-03-18T23:14:23.947436: step 331, loss 0.107522, accuracy 0.960938, precision 0.9938650306748467, recall 0.9473684210526315
2019-03-18T23:14:25.589045: step 332, loss 0.146224, accuracy 0.953125, precision 0.9640718562874252, recall 0.9640718562874252
2019-03-18T23:14:26.731991: step 333, loss 0.207849, accuracy 0.891892, precision 0.8421052631578947, recall 0.9411764705882353
2019-03-18T23:14:28.366622: step 334, loss 0.125074, accuracy 0.964844, precision 0.9649122807017544, recall 0.9821428571428571
2019-03-18T23:14:30.215683: step 335, loss 0.119485, accuracy 0.949219, precision 0.9337349397590361, recall 0.9872611464968153
2019-03-18T23:14:32.010886: step 336, loss 0.118849, accuracy 0.957031, precision 0.9683544303797469, recall 0.9622641509433962
2019-03-18T23:14:33.472981: step 337, loss 0.0971608, accuracy 0.96875, precision 0.9759036144578314, recall 0.9759036144578314
2019-03-18T23:14:35.283142: step 338, loss 0.124229, accuracy 0.945312, precision 0.9405405405405406, recall 0.9830508474576272
2019-03-18T23:14:36.933731: step 339, loss 0.111243, accuracy 0.964844, precision 0.9620253164556962, recall 0.9806451612903225
2019-03-18T23:14:38.549413: step 340, loss 0.121822, accuracy 0.960938, precision 0.9554140127388535, recall 0.9803921568627451
2019-03-18T23:14:40.134179: step 341, loss 0.123967, accuracy 0.953125, precision 0.9683544303797469, recall 0.95625
2019-03-18T23:14:41.701989: step 342, loss 0.109517, accuracy 0.960938, precision 0.9941176470588236, recall 0.949438202247191
2019-03-18T23:14:43.287751: step 343, loss 0.0874631, accuracy 0.964844, precision 0.9836956521739131, recall 0.9679144385026738
2019-03-18T23:14:44.794724: step 344, loss 0.112014, accuracy 0.960938, precision 0.9885057471264368, recall 0.9555555555555556
2019-03-18T23:14:46.187006: step 345, loss 0.134169, accuracy 0.949219, precision 0.9649122807017544, recall 0.9593023255813954
2019-03-18T23:14:47.940465: step 346, loss 0.129135, accuracy 0.949219, precision 0.9821428571428571, recall 0.9428571428571428
2019-03-18T23:14:49.586066: step 347, loss 0.103773, accuracy 0.960938, precision 0.968944099378882, recall 0.968944099378882
2019-03-18T23:14:51.244634: step 348, loss 0.116946, accuracy 0.960938, precision 0.9761904761904762, recall 0.9647058823529412
2019-03-18T23:14:53.151539: step 349, loss 0.109016, accuracy 0.960938, precision 0.9808917197452229, recall 0.9565217391304348
2019-03-18T23:14:54.893882: step 350, loss 0.114554, accuracy 0.953125, precision 0.9526627218934911, recall 0.9757575757575757
2019-03-18T23:14:56.552450: step 351, loss 0.159487, accuracy 0.933594, precision 0.9318181818181818, recall 0.9704142011834319
2019-03-18T23:14:58.188080: step 352, loss 0.101129, accuracy 0.953125, precision 0.9482758620689655, recall 0.9821428571428571
2019-03-18T23:14:59.798776: step 353, loss 0.108564, accuracy 0.972656, precision 0.9683544303797469, recall 0.9870967741935484
2019-03-18T23:15:01.672811: step 354, loss 0.144221, accuracy 0.933594, precision 0.9479768786127167, recall 0.9534883720930233
2019-03-18T23:15:03.152391: step 355, loss 0.103088, accuracy 0.960938, precision 0.9634146341463414, recall 0.9753086419753086
2019-03-18T23:15:04.716211: step 356, loss 0.100715, accuracy 0.972656, precision 0.9803921568627451, recall 0.974025974025974
2019-03-18T23:15:06.169838: step 357, loss 0.109622, accuracy 0.957031, precision 0.9875776397515528, recall 0.9464285714285714
2019-03-18T23:15:07.703738: step 358, loss 0.119169, accuracy 0.941406, precision 0.9588235294117647, recall 0.9532163742690059
2019-03-18T23:15:09.350339: step 359, loss 0.166586, accuracy 0.921875, precision 0.9649122807017544, recall 0.9217877094972067
2019-03-18T23:15:10.926128: step 360, loss 0.127018, accuracy 0.964844, precision 0.983957219251337, recall 0.968421052631579
2019-03-18T23:15:12.473038: step 361, loss 0.128856, accuracy 0.949219, precision 0.9705882352941176, recall 0.953757225433526
2019-03-18T23:15:14.091713: step 362, loss 0.115505, accuracy 0.960938, precision 0.9753086419753086, recall 0.9634146341463414
2019-03-18T23:15:15.877938: step 363, loss 0.087362, accuracy 0.96875, precision 0.9880952380952381, recall 0.9651162790697675
2019-03-18T23:15:17.664165: step 364, loss 0.104984, accuracy 0.964844, precision 0.9760479041916168, recall 0.9702380952380952
2019-03-18T23:15:19.327720: step 365, loss 0.1042, accuracy 0.960938, precision 0.9585798816568047, recall 0.9818181818181818
2019-03-18T23:15:21.074055: step 366, loss 0.13608, accuracy 0.957031, precision 0.9447852760736196, recall 0.9871794871794872
2019-03-18T23:15:22.580031: step 367, loss 0.138048, accuracy 0.953125, precision 0.9506172839506173, recall 0.9746835443037974
2019-03-18T23:15:24.091989: step 368, loss 0.138414, accuracy 0.949219, precision 0.9805194805194806, recall 0.937888198757764
2019-03-18T23:15:25.655810: step 369, loss 0.100387, accuracy 0.957031, precision 0.9757575757575757, recall 0.9583333333333334
2019-03-18T23:15:26.410793: step 370, loss 0.0520124, accuracy 1, precision 1.0, recall 1.0
2019-03-18T23:15:27.908791: step 371, loss 0.110775, accuracy 0.960938, precision 0.9512195121951219, recall 0.9873417721518988
2019-03-18T23:15:29.643158: step 372, loss 0.103898, accuracy 0.964844, precision 0.9702380952380952, recall 0.9760479041916168
2019-03-18T23:15:31.309866: step 373, loss 0.109339, accuracy 0.960938, precision 0.963855421686747, recall 0.975609756097561
2019-03-18T23:15:32.839772: step 374, loss 0.107291, accuracy 0.96875, precision 0.9710982658959537, recall 0.9824561403508771
2019-03-18T23:15:34.259496: step 375, loss 0.0884568, accuracy 0.980469, precision 0.9759036144578314, recall 0.9938650306748467
2019-03-18T23:15:36.048716: step 376, loss 0.0727894, accuracy 0.984375, precision 0.9940476190476191, recall 0.9823529411764705
2019-03-18T23:15:37.587602: step 377, loss 0.111643, accuracy 0.960938, precision 0.9883720930232558, recall 0.9550561797752809
2019-03-18T23:15:39.222234: step 378, loss 0.0707484, accuracy 0.984375, precision 0.9824561403508771, recall 0.9940828402366864
2019-03-18T23:15:40.777079: step 379, loss 0.084936, accuracy 0.980469, precision 0.9832402234636871, recall 0.9887640449438202
2019-03-18T23:15:42.338906: step 380, loss 0.0806302, accuracy 0.976562, precision 0.9766081871345029, recall 0.9881656804733728
2019-03-18T23:15:43.968556: step 381, loss 0.0983292, accuracy 0.960938, precision 0.9693251533742331, recall 0.9693251533742331
2019-03-18T23:15:45.719872: step 382, loss 0.0897391, accuracy 0.96875, precision 0.9715909090909091, recall 0.9827586206896551
2019-03-18T23:15:47.378442: step 383, loss 0.111367, accuracy 0.957031, precision 0.963855421686747, recall 0.9696969696969697
2019-03-18T23:15:49.009083: step 384, loss 0.0975393, accuracy 0.960938, precision 0.9883720930232558, recall 0.9550561797752809
2019-03-18T23:15:50.617818: step 385, loss 0.118308, accuracy 0.949219, precision 0.9636363636363636, recall 0.9578313253012049
2019-03-18T23:15:52.096863: step 386, loss 0.0977793, accuracy 0.964844, precision 0.9878787878787879, recall 0.9588235294117647
2019-03-18T23:15:53.721521: step 387, loss 0.0915072, accuracy 0.980469, precision 0.9824561403508771, recall 0.9882352941176471
2019-03-18T23:15:55.317259: step 388, loss 0.0858115, accuracy 0.964844, precision 0.975609756097561, recall 0.9696969696969697
2019-03-18T23:15:56.541985: step 389, loss 0.094935, accuracy 0.976562, precision 0.9884393063583815, recall 0.9771428571428571
2019-03-18T23:15:58.133734: step 390, loss 0.0841397, accuracy 0.976562, precision 0.9754601226993865, recall 0.9875776397515528
2019-03-18T23:15:59.821222: step 391, loss 0.124439, accuracy 0.949219, precision 0.9440993788819876, recall 0.9743589743589743
2019-03-18T23:16:01.498322: step 392, loss 0.113605, accuracy 0.957031, precision 0.9634146341463414, recall 0.9693251533742331
2019-03-18T23:16:02.953434: step 393, loss 0.126794, accuracy 0.96875, precision 0.9751552795031055, recall 0.9751552795031055
2019-03-18T23:16:04.358678: step 394, loss 0.0773812, accuracy 0.976562, precision 0.9883040935672515, recall 0.976878612716763
2019-03-18T23:16:05.903551: step 395, loss 0.0940726, accuracy 0.964844, precision 0.975, recall 0.968944099378882
2019-03-18T23:16:07.520295: step 396, loss 0.0966947, accuracy 0.972656, precision 0.98125, recall 0.9751552795031055
2019-03-18T23:16:09.165940: step 397, loss 0.0954429, accuracy 0.972656, precision 0.9795918367346939, recall 0.972972972972973
2019-03-18T23:16:10.927234: step 398, loss 0.104879, accuracy 0.964844, precision 0.9642857142857143, recall 0.9818181818181818
2019-03-18T23:16:12.687530: step 399, loss 0.0805074, accuracy 0.972656, precision 0.9775280898876404, recall 0.9830508474576272
2019-03-18T23:16:14.155607: step 400, loss 0.0725992, accuracy 0.992188, precision 0.9937888198757764, recall 0.9937888198757764
2019-03-18T23:16:15.713445: step 401, loss 0.0566473, accuracy 0.984375, precision 0.9832402234636871, recall 0.9943502824858758
2019-03-18T23:16:17.108716: step 402, loss 0.11566, accuracy 0.957031, precision 0.9880239520958084, recall 0.9482758620689655
2019-03-18T23:16:18.646606: step 403, loss 0.106915, accuracy 0.96875, precision 0.9876543209876543, recall 0.963855421686747
2019-03-18T23:16:20.132217: step 404, loss 0.104055, accuracy 0.957031, precision 0.9813664596273292, recall 0.9518072289156626
2019-03-18T23:16:21.787793: step 405, loss 0.0782717, accuracy 0.972656, precision 0.9702380952380952, recall 0.9878787878787879
2019-03-18T23:16:23.334680: step 406, loss 0.083631, accuracy 0.964844, precision 0.9702380952380952, recall 0.9760479041916168
2019-03-18T23:16:24.240769: step 407, loss 0.276727, accuracy 0.891892, precision 1.0, recall 0.8333333333333334
2019-03-18T23:16:25.761706: step 408, loss 0.0807431, accuracy 0.972656, precision 0.9640718562874252, recall 0.9938271604938271
2019-03-18T23:16:27.483626: step 409, loss 0.0842369, accuracy 0.96875, precision 0.9583333333333334, recall 0.9938271604938271
2019-03-18T23:16:28.737420: step 410, loss 0.090425, accuracy 0.972656, precision 0.9763313609467456, recall 0.9821428571428571
2019-03-18T23:16:30.344126: step 411, loss 0.0881425, accuracy 0.96875, precision 0.9575757575757575, recall 0.9937106918238994
2019-03-18T23:16:31.848700: step 412, loss 0.0870363, accuracy 0.964844, precision 0.9786096256684492, recall 0.973404255319149
2019-03-18T23:16:33.463448: step 413, loss 0.0790582, accuracy 0.984375, precision 0.9817073170731707, recall 0.9938271604938271
2019-03-18T23:16:35.045729: step 414, loss 0.0695424, accuracy 0.984375, precision 0.9942196531791907, recall 0.9828571428571429
2019-03-18T23:16:36.832978: step 415, loss 0.126377, accuracy 0.949219, precision 0.9822485207100592, recall 0.9431818181818182
2019-03-18T23:16:39.136418: step 416, loss 0.13311, accuracy 0.957031, precision 0.975, recall 0.9570552147239264
2019-03-18T23:16:41.807949: step 417, loss 0.104098, accuracy 0.957031, precision 0.9935897435897436, recall 0.9393939393939394
2019-03-18T23:16:44.064097: step 418, loss 0.0790006, accuracy 0.964844, precision 0.9719101123595506, recall 0.9774011299435028
2019-03-18T23:16:45.779513: step 419, loss 0.082032, accuracy 0.96875, precision 0.9940476190476191, recall 0.9597701149425287
2019-03-18T23:16:47.371259: step 420, loss 0.0581803, accuracy 0.988281, precision 0.9941520467836257, recall 0.9883720930232558
2019-03-18T23:16:49.178431: step 421, loss 0.0756545, accuracy 0.972656, precision 0.9751552795031055, recall 0.98125
2019-03-18T23:16:51.159140: step 422, loss 0.104403, accuracy 0.960938, precision 0.9698795180722891, recall 0.9698795180722891
2019-03-18T23:16:53.287475: step 423, loss 0.0749754, accuracy 0.976562, precision 0.9764705882352941, recall 0.9880952380952381
2019-03-18T23:16:55.001894: step 424, loss 0.0732067, accuracy 0.984375, precision 0.9939759036144579, recall 0.9821428571428571
2019-03-18T23:16:56.950307: step 425, loss 0.0743126, accuracy 0.980469, precision 0.98125, recall 0.9874213836477987
2019-03-18T23:16:58.729573: step 426, loss 0.108607, accuracy 0.953125, precision 0.9454545454545454, recall 0.9811320754716981
2019-03-18T23:17:00.401753: step 427, loss 0.0763117, accuracy 0.96875, precision 0.9695121951219512, recall 0.9814814814814815
2019-03-18T23:17:01.912712: step 428, loss 0.0786998, accuracy 0.976562, precision 0.9940119760479041, recall 0.9707602339181286
2019-03-18T23:17:03.506961: step 429, loss 0.0821513, accuracy 0.964844, precision 0.9717514124293786, recall 0.9772727272727273
2019-03-18T23:17:04.918103: step 430, loss 0.056588, accuracy 0.976562, precision 0.9832402234636871, recall 0.9832402234636871
2019-03-18T23:17:06.557041: step 431, loss 0.122025, accuracy 0.949219, precision 0.974025974025974, recall 0.9433962264150944
2019-03-18T23:17:08.073654: step 432, loss 0.102963, accuracy 0.96875, precision 0.9710982658959537, recall 0.9824561403508771
2019-03-18T23:17:09.651126: step 433, loss 0.0751824, accuracy 0.96875, precision 0.9807692307692307, recall 0.9683544303797469
2019-03-18T23:17:11.263386: step 434, loss 0.100063, accuracy 0.96875, precision 0.9825581395348837, recall 0.9712643678160919
2019-03-18T23:17:13.021709: step 435, loss 0.122834, accuracy 0.964844, precision 0.9877300613496932, recall 0.9583333333333334
2019-03-18T23:17:14.595114: step 436, loss 0.0815434, accuracy 0.976562, precision 0.9824561403508771, recall 0.9824561403508771
2019-03-18T23:17:16.167954: step 437, loss 0.108922, accuracy 0.96875, precision 0.9738562091503268, recall 0.9738562091503268
2019-03-18T23:17:17.658111: step 438, loss 0.101277, accuracy 0.972656, precision 0.9763313609467456, recall 0.9821428571428571
2019-03-18T23:17:19.121739: step 439, loss 0.0923039, accuracy 0.964844, precision 0.9619565217391305, recall 0.9888268156424581
2019-03-18T23:17:20.653028: step 440, loss 0.13109, accuracy 0.957031, precision 0.9454545454545454, recall 0.9873417721518988
2019-03-18T23:17:22.285740: step 441, loss 0.0909812, accuracy 0.964844, precision 0.9629629629629629, recall 0.9811320754716981
2019-03-18T23:17:23.825685: step 442, loss 0.0936368, accuracy 0.96875, precision 0.9746835443037974, recall 0.9746835443037974
2019-03-18T23:17:25.233431: step 443, loss 0.0930608, accuracy 0.964844, precision 0.9811320754716981, recall 0.9629629629629629
2019-03-18T23:17:25.909352: step 444, loss 0.101149, accuracy 0.945946, precision 1.0, recall 0.92
2019-03-18T23:17:27.404525: step 445, loss 0.0681245, accuracy 0.984375, precision 0.9817073170731707, recall 0.9938271604938271
2019-03-18T23:17:29.047646: step 446, loss 0.0959747, accuracy 0.980469, precision 0.9827586206896551, recall 0.9884393063583815
2019-03-18T23:17:30.614459: step 447, loss 0.0657531, accuracy 0.980469, precision 0.9939024390243902, recall 0.9760479041916168
2019-03-18T23:17:32.152878: step 448, loss 0.100308, accuracy 0.976562, precision 0.9820359281437125, recall 0.9820359281437125
2019-03-18T23:17:33.788508: step 449, loss 0.0909937, accuracy 0.960938, precision 0.9757575757575757, recall 0.9640718562874252
2019-03-18T23:17:35.378261: step 450, loss 0.0796364, accuracy 0.984375, precision 0.9818181818181818, recall 0.9938650306748467
2019-03-18T23:17:36.837409: step 451, loss 0.0691881, accuracy 0.972656, precision 0.9644970414201184, recall 0.9939024390243902
2019-03-18T23:17:38.078602: step 452, loss 0.0675366, accuracy 0.980469, precision 0.9817073170731707, recall 0.9877300613496932
2019-03-18T23:17:39.565657: step 453, loss 0.0594999, accuracy 0.988281, precision 0.9939759036144579, recall 0.9880239520958084
2019-03-18T23:17:41.229240: step 454, loss 0.0731178, accuracy 0.980469, precision 0.9822485207100592, recall 0.9880952380952381
2019-03-18T23:17:42.903821: step 455, loss 0.06853, accuracy 0.96875, precision 0.975609756097561, recall 0.975609756097561
2019-03-18T23:17:45.311985: step 456, loss 0.060091, accuracy 0.984375, precision 0.9880239520958084, recall 0.9880239520958084
2019-03-18T23:17:47.660222: step 457, loss 0.0658814, accuracy 0.984375, precision 0.9878048780487805, recall 0.9878048780487805
2019-03-18T23:17:49.813487: step 458, loss 0.0782662, accuracy 0.972656, precision 0.9714285714285714, recall 0.9883720930232558
2019-03-18T23:17:51.890957: step 459, loss 0.0753347, accuracy 0.964844, precision 0.9627329192546584, recall 0.9810126582278481
2019-03-18T23:17:53.563488: step 460, loss 0.091042, accuracy 0.964844, precision 0.9760479041916168, recall 0.9702380952380952
2019-03-18T23:17:55.274915: step 461, loss 0.0876871, accuracy 0.984375, precision 0.9943181818181818, recall 0.9831460674157303
2019-03-18T23:17:57.094053: step 462, loss 0.0743833, accuracy 0.976562, precision 0.9625, recall 1.0
2019-03-18T23:17:59.006942: step 463, loss 0.0662776, accuracy 0.984375, precision 0.9947089947089947, recall 0.9842931937172775
2019-03-18T23:18:00.363318: step 464, loss 0.0855536, accuracy 0.96875, precision 0.975609756097561, recall 0.975609756097561
2019-03-18T23:18:01.631928: step 465, loss 0.0740121, accuracy 0.976562, precision 0.9941176470588236, recall 0.9712643678160919
2019-03-18T23:18:02.995284: step 466, loss 0.0807803, accuracy 0.964844, precision 0.9642857142857143, recall 0.9818181818181818
2019-03-18T23:18:03.942753: step 467, loss 0.0684656, accuracy 0.984375, precision 0.9881656804733728, recall 0.9881656804733728
2019-03-18T23:18:04.931133: step 468, loss 0.0862553, accuracy 0.960938, precision 0.9705882352941176, recall 0.9705882352941176
2019-03-18T23:18:05.899059: step 469, loss 0.0676444, accuracy 0.980469, precision 0.9820359281437125, recall 0.9879518072289156
2019-03-18T23:18:06.897474: step 470, loss 0.0469994, accuracy 0.988281, precision 0.9936708860759493, recall 0.9874213836477987
2019-03-18T23:18:07.878851: step 471, loss 0.0715438, accuracy 0.980469, precision 0.9771428571428571, recall 0.9941860465116279
2019-03-18T23:18:08.946331: step 472, loss 0.0964981, accuracy 0.976562, precision 0.975, recall 0.9873417721518988
2019-03-18T23:18:10.247931: step 473, loss 0.0959443, accuracy 0.960938, precision 0.9506172839506173, recall 0.9871794871794872
2019-03-18T23:18:11.239789: step 474, loss 0.0892482, accuracy 0.976562, precision 0.99375, recall 0.9695121951219512
2019-03-18T23:18:12.205682: step 475, loss 0.0715821, accuracy 0.980469, precision 0.9876543209876543, recall 0.9815950920245399
2019-03-18T23:18:13.248468: step 476, loss 0.0837617, accuracy 0.972656, precision 0.9715909090909091, recall 0.9884393063583815
2019-03-18T23:18:14.210891: step 477, loss 0.088214, accuracy 0.96875, precision 0.975609756097561, recall 0.975609756097561
2019-03-18T23:18:14.769945: step 478, loss 0.0614673, accuracy 0.984375, precision 0.9880952380952381, recall 0.9880952380952381
2019-03-18T23:18:15.614716: step 479, loss 0.0822348, accuracy 0.96875, precision 0.9936305732484076, recall 0.9570552147239264
2019-03-18T23:18:16.514333: step 480, loss 0.0846899, accuracy 0.976562, precision 0.9817073170731707, recall 0.9817073170731707
2019-03-18T23:18:16.897574: step 481, loss 0.0658024, accuracy 1, precision 1.0, recall 1.0
2019-03-18T23:18:17.754815: step 482, loss 0.056758, accuracy 0.984375, precision 0.9828571428571429, recall 0.9942196531791907
2019-03-18T23:18:18.660964: step 483, loss 0.0746523, accuracy 0.984375, precision 0.9880239520958084, recall 0.9880239520958084
2019-03-18T23:18:19.472444: step 484, loss 0.0691996, accuracy 0.972656, precision 0.9825581395348837, recall 0.976878612716763
2019-03-18T23:18:20.281098: step 485, loss 0.0864485, accuracy 0.96875, precision 0.9764705882352941, recall 0.9764705882352941
2019-03-18T23:18:21.089778: step 486, loss 0.0713526, accuracy 0.96875, precision 0.9761904761904762, recall 0.9761904761904762
2019-03-18T23:18:21.932627: step 487, loss 0.0485255, accuracy 0.992188, precision 0.9940476190476191, recall 0.9940476190476191
2019-03-18T23:18:22.767913: step 488, loss 0.0566647, accuracy 0.992188, precision 0.9932885906040269, recall 0.9932885906040269
2019-03-18T23:18:23.576940: step 489, loss 0.0572122, accuracy 0.988281, precision 0.9885057471264368, recall 0.9942196531791907
2019-03-18T23:18:24.304013: step 490, loss 0.0915232, accuracy 0.96875, precision 0.9698795180722891, recall 0.9817073170731707
2019-03-18T23:18:25.102875: step 491, loss 0.074061, accuracy 0.976562, precision 0.9808917197452229, recall 0.9808917197452229
2019-03-18T23:18:25.820467: step 492, loss 0.0676171, accuracy 0.980469, precision 0.9941176470588236, recall 0.976878612716763
2019-03-18T23:18:26.572458: step 493, loss 0.0707991, accuracy 0.980469, precision 1.0, recall 0.9693251533742331
2019-03-18T23:18:27.416270: step 494, loss 0.0532171, accuracy 0.988281, precision 0.9939024390243902, recall 0.9878787878787879
2019-03-18T23:18:28.179230: step 495, loss 0.0484261, accuracy 0.992188, precision 0.9880952380952381, recall 1.0
2019-03-18T23:18:28.862912: step 496, loss 0.0625785, accuracy 0.980469, precision 0.9875776397515528, recall 0.9814814814814815
2019-03-18T23:18:29.659786: step 497, loss 0.0766687, accuracy 0.96875, precision 0.9629629629629629, recall 0.9873417721518988
2019-03-18T23:18:30.415762: step 498, loss 0.0504341, accuracy 0.996094, precision 1.0, recall 0.9941520467836257
2019-03-18T23:18:31.175911: step 499, loss 0.0651237, accuracy 0.984375, precision 0.988950276243094, recall 0.988950276243094
2019-03-18T23:18:31.855097: step 500, loss 0.0711477, accuracy 0.972656, precision 0.9642857142857143, recall 0.9938650306748467

Evaluation:
2019-03-18 23:18:31.857612: W tensorflow/core/framework/allocator.cc:122] Allocation of 102697200 exceeds 10% of system memory.
[[861 741]
 [391 320]]
2019-03-18T23:18:33.141706: step 500, loss 1.40036, accuracy 0.510592, precision 0.5374531835205992, recall 0.6876996805111821

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968365\checkpoints\model-500


Process finished with exit code 0
