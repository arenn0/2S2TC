"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
Italian Italian
GloVe model loaded
Pretrained Embedding: GloVe
Italian: False
Loading data...
11566
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:98: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Max Document length: 37
Vocabulary Size: 1
Train/Dev split: 9253/2313
2019-03-18 23:07:17.768067: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:485: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

35
34
33
Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968444

2019-03-18T23:07:30.477656: step 1, loss 3.37316, accuracy 0.582031, precision 0.879746835443038, recall 0.6123348017621145
2019-03-18T23:07:31.658509: step 2, loss 4.87661, accuracy 0.378906, precision 0.13714285714285715, recall 0.75
2019-03-18T23:07:32.631900: step 3, loss 2.64349, accuracy 0.503906, precision 0.44871794871794873, recall 0.6306306306306306
2019-03-18T23:07:33.712015: step 4, loss 2.37724, accuracy 0.601562, precision 0.7885714285714286, recall 0.6798029556650246
2019-03-18T23:07:34.997579: step 5, loss 3.82462, accuracy 0.605469, precision 0.9407894736842105, recall 0.6085106382978723
2019-03-18T23:07:36.060738: step 6, loss 2.58391, accuracy 0.660156, precision 0.9289940828402367, recall 0.6767241379310345
2019-03-18T23:07:37.191715: step 7, loss 3.02161, accuracy 0.570312, precision 0.8238993710691824, recall 0.6150234741784038
2019-03-18T23:07:38.355605: step 8, loss 1.65045, accuracy 0.636719, precision 0.6739130434782609, recall 0.7898089171974523
2019-03-18T23:07:39.711981: step 9, loss 2.35815, accuracy 0.507812, precision 0.5266272189349113, recall 0.6592592592592592
2019-03-18T23:07:41.094287: step 10, loss 1.84937, accuracy 0.566406, precision 0.4913294797687861, recall 0.7870370370370371
2019-03-18T23:07:42.271142: step 11, loss 1.84287, accuracy 0.5625, precision 0.4968944099378882, recall 0.7207207207207207
2019-03-18T23:07:43.483902: step 12, loss 1.93916, accuracy 0.554688, precision 0.6265060240963856, recall 0.6666666666666666
2019-03-18T23:07:44.778444: step 13, loss 1.96755, accuracy 0.617188, precision 0.8404907975460123, recall 0.6555023923444976
2019-03-18T23:07:45.964273: step 14, loss 1.81465, accuracy 0.652344, precision 0.852760736196319, recall 0.6813725490196079
2019-03-18T23:07:47.101236: step 15, loss 1.84669, accuracy 0.613281, precision 0.803680981595092, recall 0.6616161616161617
2019-03-18T23:07:48.125499: step 16, loss 1.67176, accuracy 0.640625, precision 0.7926829268292683, recall 0.6914893617021277
2019-03-18T23:07:49.201623: step 17, loss 1.40397, accuracy 0.636719, precision 0.7272727272727273, recall 0.7142857142857143
2019-03-18T23:07:50.506137: step 18, loss 1.54057, accuracy 0.625, precision 0.6744186046511628, recall 0.7435897435897436
2019-03-18T23:07:51.836583: step 19, loss 1.58415, accuracy 0.605469, precision 0.6043956043956044, recall 0.7913669064748201
2019-03-18T23:07:52.643426: step 20, loss 1.49491, accuracy 0.597656, precision 0.6193548387096774, recall 0.6857142857142857
2019-03-18T23:07:53.868155: step 21, loss 1.36189, accuracy 0.644531, precision 0.6975308641975309, recall 0.7290322580645161
2019-03-18T23:07:55.085900: step 22, loss 1.26551, accuracy 0.664062, precision 0.7455621301775148, recall 0.7455621301775148
2019-03-18T23:07:56.382434: step 23, loss 1.30334, accuracy 0.628906, precision 0.7485380116959064, recall 0.7111111111111111
2019-03-18T23:07:57.654037: step 24, loss 1.2192, accuracy 0.691406, precision 0.8333333333333334, recall 0.7329842931937173
2019-03-18T23:07:58.847848: step 25, loss 1.31354, accuracy 0.703125, precision 0.8192090395480226, recall 0.7671957671957672
2019-03-18T23:07:59.915992: step 26, loss 1.0877, accuracy 0.714844, precision 0.8372093023255814, recall 0.7619047619047619
2019-03-18T23:08:01.047968: step 27, loss 1.21987, accuracy 0.71875, precision 0.8793103448275862, recall 0.75
2019-03-18T23:08:02.261725: step 28, loss 1.01739, accuracy 0.726562, precision 0.8176795580110497, recall 0.8
2019-03-18T23:08:03.430602: step 29, loss 1.28292, accuracy 0.621094, precision 0.6948051948051948, recall 0.6815286624203821
2019-03-18T23:08:04.434917: step 30, loss 1.2148, accuracy 0.628906, precision 0.6390532544378699, recall 0.7605633802816901
2019-03-18T23:08:05.584845: step 31, loss 1.4132, accuracy 0.621094, precision 0.6305732484076433, recall 0.717391304347826
2019-03-18T23:08:06.843481: step 32, loss 1.01957, accuracy 0.714844, precision 0.7660818713450293, recall 0.7987804878048781
2019-03-18T23:08:08.160961: step 33, loss 1.00922, accuracy 0.667969, precision 0.7, recall 0.7777777777777778
2019-03-18T23:08:09.385689: step 34, loss 1.02857, accuracy 0.675781, precision 0.7682926829268293, recall 0.7368421052631579
2019-03-18T23:08:10.487743: step 35, loss 1.01496, accuracy 0.675781, precision 0.810126582278481, recall 0.7071823204419889
2019-03-18T23:08:11.738403: step 36, loss 0.924904, accuracy 0.707031, precision 0.823170731707317, recall 0.7458563535911602
2019-03-18T23:08:12.204157: step 37, loss 0.891044, accuracy 0.675676, precision 0.8148148148148148, recall 0.7586206896551724
2019-03-18T23:08:13.446836: step 38, loss 0.900245, accuracy 0.703125, precision 0.8353658536585366, recall 0.7365591397849462
2019-03-18T23:08:14.615713: step 39, loss 0.900377, accuracy 0.761719, precision 0.8452380952380952, recall 0.8022598870056498
2019-03-18T23:08:15.751677: step 40, loss 0.76428, accuracy 0.714844, precision 0.7586206896551724, recall 0.8098159509202454
2019-03-18T23:08:16.978400: step 41, loss 0.842058, accuracy 0.71875, precision 0.7407407407407407, recall 0.8
2019-03-18T23:08:18.115361: step 42, loss 0.978637, accuracy 0.667969, precision 0.7241379310344828, recall 0.7730061349693251
2019-03-18T23:08:19.124665: step 43, loss 0.881343, accuracy 0.695312, precision 0.803921568627451, recall 0.7192982456140351
2019-03-18T23:08:20.347398: step 44, loss 0.697172, accuracy 0.769531, precision 0.8362573099415205, recall 0.8218390804597702
2019-03-18T23:08:21.568135: step 45, loss 0.831926, accuracy 0.71875, precision 0.8275862068965517, recall 0.7741935483870968
2019-03-18T23:08:22.753966: step 46, loss 0.789576, accuracy 0.769531, precision 0.8255813953488372, recall 0.8304093567251462
2019-03-18T23:08:24.042523: step 47, loss 0.925028, accuracy 0.71875, precision 0.791907514450867, recall 0.791907514450867
2019-03-18T23:08:25.390920: step 48, loss 0.819164, accuracy 0.707031, precision 0.7852760736196319, recall 0.7619047619047619
2019-03-18T23:08:26.770234: step 49, loss 0.704915, accuracy 0.765625, precision 0.8079470198675497, recall 0.7973856209150327
2019-03-18T23:08:27.997954: step 50, loss 1.08317, accuracy 0.6875, precision 0.7738095238095238, recall 0.7558139534883721
2019-03-18T23:08:29.335380: step 51, loss 0.774645, accuracy 0.742188, precision 0.8333333333333334, recall 0.7647058823529411
2019-03-18T23:08:30.553126: step 52, loss 0.650456, accuracy 0.761719, precision 0.8271604938271605, recall 0.8023952095808383
2019-03-18T23:08:31.851656: step 53, loss 0.695335, accuracy 0.738281, precision 0.8083832335329342, recall 0.7941176470588235
2019-03-18T23:08:33.218005: step 54, loss 0.826036, accuracy 0.730469, precision 0.7857142857142857, recall 0.8
2019-03-18T23:08:34.740935: step 55, loss 0.735109, accuracy 0.722656, precision 0.7962962962962963, recall 0.7724550898203593
2019-03-18T23:08:35.915796: step 56, loss 0.723992, accuracy 0.722656, precision 0.7912087912087912, recall 0.8135593220338984
2019-03-18T23:08:37.255219: step 57, loss 0.705568, accuracy 0.757812, precision 0.8095238095238095, recall 0.8192771084337349
2019-03-18T23:08:38.384202: step 58, loss 0.577898, accuracy 0.757812, precision 0.8089887640449438, recall 0.8372093023255814
2019-03-18T23:08:39.602944: step 59, loss 0.801197, accuracy 0.734375, precision 0.8588235294117647, recall 0.7684210526315789
2019-03-18T23:08:40.906460: step 60, loss 0.773254, accuracy 0.695312, precision 0.8220858895705522, recall 0.73224043715847
2019-03-18T23:08:42.107251: step 61, loss 0.822252, accuracy 0.722656, precision 0.8452380952380952, recall 0.7593582887700535
2019-03-18T23:08:43.461633: step 62, loss 0.703733, accuracy 0.773438, precision 0.8269230769230769, recall 0.80625
2019-03-18T23:08:44.986558: step 63, loss 0.568257, accuracy 0.789062, precision 0.8579881656804734, recall 0.8285714285714286
2019-03-18T23:08:46.349915: step 64, loss 0.811393, accuracy 0.726562, precision 0.7873563218390804, recall 0.8058823529411765
2019-03-18T23:08:47.500844: step 65, loss 0.804234, accuracy 0.699219, precision 0.7365269461077845, recall 0.7884615384615384
2019-03-18T23:08:48.787400: step 66, loss 0.718145, accuracy 0.757812, precision 0.7951807228915663, recall 0.825
2019-03-18T23:08:50.046038: step 67, loss 0.687645, accuracy 0.773438, precision 0.8170731707317073, recall 0.8271604938271605
2019-03-18T23:08:51.235860: step 68, loss 0.874863, accuracy 0.730469, precision 0.8203592814371258, recall 0.7784090909090909
2019-03-18T23:08:52.281065: step 69, loss 0.758297, accuracy 0.703125, precision 0.8187134502923976, recall 0.7567567567567568
2019-03-18T23:08:53.451937: step 70, loss 0.696534, accuracy 0.722656, precision 0.85625, recall 0.7405405405405405
2019-03-18T23:08:54.935971: step 71, loss 0.86309, accuracy 0.699219, precision 0.8301886792452831, recall 0.7252747252747253
2019-03-18T23:08:56.365153: step 72, loss 0.639387, accuracy 0.777344, precision 0.8055555555555556, recall 0.8682634730538922
2019-03-18T23:08:57.580904: step 73, loss 0.694313, accuracy 0.71875, precision 0.7469135802469136, recall 0.7960526315789473
2019-03-18T23:08:57.943933: step 74, loss 0.746699, accuracy 0.702703, precision 0.7307692307692307, recall 0.8260869565217391
2019-03-18T23:08:59.332223: step 75, loss 0.549523, accuracy 0.753906, precision 0.8092485549132948, recall 0.8235294117647058
2019-03-18T23:09:00.547975: step 76, loss 0.731882, accuracy 0.746094, precision 0.83125, recall 0.7777777777777778
2019-03-18T23:09:01.549299: step 77, loss 0.517291, accuracy 0.808594, precision 0.9393939393939394, recall 0.7989690721649485
2019-03-18T23:09:02.797963: step 78, loss 0.642015, accuracy 0.757812, precision 0.8333333333333334, recall 0.7941176470588235
2019-03-18T23:09:04.066572: step 79, loss 0.600049, accuracy 0.789062, precision 0.8802395209580839, recall 0.8121546961325967
2019-03-18T23:09:05.524676: step 80, loss 0.620394, accuracy 0.753906, precision 0.8076923076923077, recall 0.7924528301886793
2019-03-18T23:09:06.893020: step 81, loss 0.716238, accuracy 0.730469, precision 0.7757575757575758, recall 0.8
2019-03-18T23:09:08.107775: step 82, loss 0.600135, accuracy 0.753906, precision 0.7797619047619048, recall 0.8343949044585988
2019-03-18T23:09:09.337488: step 83, loss 0.57214, accuracy 0.78125, precision 0.81875, recall 0.8291139240506329
2019-03-18T23:09:10.647987: step 84, loss 0.549618, accuracy 0.777344, precision 0.834319526627219, recall 0.8294117647058824
2019-03-18T23:09:12.075204: step 85, loss 0.584452, accuracy 0.753906, precision 0.8322147651006712, recall 0.7654320987654321
2019-03-18T23:09:13.302924: step 86, loss 0.643634, accuracy 0.753906, precision 0.8875, recall 0.7593582887700535
2019-03-18T23:09:14.681286: step 87, loss 0.558754, accuracy 0.78125, precision 0.8305084745762712, recall 0.8497109826589595
2019-03-18T23:09:16.009745: step 88, loss 0.521282, accuracy 0.792969, precision 0.8280254777070064, recall 0.8333333333333334
2019-03-18T23:09:17.172626: step 89, loss 0.45405, accuracy 0.800781, precision 0.8910256410256411, recall 0.8034682080924855
2019-03-18T23:09:18.313578: step 90, loss 0.595682, accuracy 0.773438, precision 0.8023952095808383, recall 0.8427672955974843
2019-03-18T23:09:19.506390: step 91, loss 0.539164, accuracy 0.789062, precision 0.8260869565217391, recall 0.8364779874213837
2019-03-18T23:09:20.703729: step 92, loss 0.423126, accuracy 0.820312, precision 0.8402366863905325, recall 0.8819875776397516
2019-03-18T23:09:21.870127: step 93, loss 0.59646, accuracy 0.75, precision 0.861271676300578, recall 0.7883597883597884
2019-03-18T23:09:23.014069: step 94, loss 0.542316, accuracy 0.773438, precision 0.8518518518518519, recall 0.8023255813953488
2019-03-18T23:09:24.320627: step 95, loss 0.523638, accuracy 0.765625, precision 0.8596491228070176, recall 0.8032786885245902
2019-03-18T23:09:25.578775: step 96, loss 0.487935, accuracy 0.816406, precision 0.8907103825136612, recall 0.8578947368421053
2019-03-18T23:09:26.885303: step 97, loss 0.606414, accuracy 0.746094, precision 0.8466257668711656, recall 0.7752808988764045
2019-03-18T23:09:27.997331: step 98, loss 0.608147, accuracy 0.777344, precision 0.8625, recall 0.7976878612716763
2019-03-18T23:09:29.177178: step 99, loss 0.498689, accuracy 0.8125, precision 0.867816091954023, recall 0.8579545454545454
2019-03-18T23:09:30.269261: step 100, loss 0.528141, accuracy 0.8125, precision 0.8651685393258427, recall 0.8651685393258427
2019-03-18T23:09:31.554825: step 101, loss 0.582777, accuracy 0.746094, precision 0.8072289156626506, recall 0.8023952095808383
2019-03-18T23:09:32.686815: step 102, loss 0.588163, accuracy 0.734375, precision 0.838150289017341, recall 0.7837837837837838
2019-03-18T23:09:33.969387: step 103, loss 0.55024, accuracy 0.816406, precision 0.8666666666666667, recall 0.8715083798882681
2019-03-18T23:09:35.252957: step 104, loss 0.674313, accuracy 0.777344, precision 0.8509316770186336, recall 0.8058823529411765
2019-03-18T23:09:36.281728: step 105, loss 0.552857, accuracy 0.765625, precision 0.8439306358381503, recall 0.8156424581005587
2019-03-18T23:09:37.399740: step 106, loss 0.373188, accuracy 0.835938, precision 0.8786127167630058, recall 0.8786127167630058
2019-03-18T23:09:38.810031: step 107, loss 0.43897, accuracy 0.792969, precision 0.8902439024390244, recall 0.8066298342541437
2019-03-18T23:09:40.221280: step 108, loss 0.466677, accuracy 0.773438, precision 0.8373493975903614, recall 0.8176470588235294
2019-03-18T23:09:41.491884: step 109, loss 0.490837, accuracy 0.800781, precision 0.8206521739130435, recall 0.893491124260355
2019-03-18T23:09:42.626852: step 110, loss 0.413857, accuracy 0.84375, precision 0.8666666666666667, recall 0.8881987577639752
2019-03-18T23:09:43.234787: step 111, loss 0.655875, accuracy 0.702703, precision 0.9090909090909091, recall 0.6896551724137931
2019-03-18T23:09:44.255089: step 112, loss 0.392072, accuracy 0.832031, precision 0.91875, recall 0.8305084745762712
2019-03-18T23:09:45.513786: step 113, loss 0.414149, accuracy 0.839844, precision 0.8888888888888888, recall 0.8735632183908046
2019-03-18T23:09:46.943486: step 114, loss 0.561334, accuracy 0.757812, precision 0.7810650887573964, recall 0.8407643312101911
2019-03-18T23:09:48.360211: step 115, loss 0.491259, accuracy 0.742188, precision 0.7393939393939394, recall 0.8413793103448276
2019-03-18T23:09:49.526097: step 116, loss 0.430233, accuracy 0.824219, precision 0.8376623376623377, recall 0.8657718120805369
2019-03-18T23:09:50.586807: step 117, loss 0.441189, accuracy 0.808594, precision 0.8333333333333334, recall 0.8875739644970414
2019-03-18T23:09:51.735255: step 118, loss 0.466807, accuracy 0.820312, precision 0.922077922077922, recall 0.8068181818181818
2019-03-18T23:09:53.158445: step 119, loss 0.375277, accuracy 0.832031, precision 0.8975903614457831, recall 0.8514285714285714
2019-03-18T23:09:54.392202: step 120, loss 0.483752, accuracy 0.789062, precision 0.8424242424242424, recall 0.8323353293413174
2019-03-18T23:09:55.497273: step 121, loss 0.530191, accuracy 0.804688, precision 0.8830409356725146, recall 0.8342541436464088
2019-03-18T23:09:56.772863: step 122, loss 0.463976, accuracy 0.804688, precision 0.9011627906976745, recall 0.824468085106383
2019-03-18T23:09:58.031028: step 123, loss 0.418311, accuracy 0.804688, precision 0.8922155688622755, recall 0.8232044198895028
2019-03-18T23:09:59.512628: step 124, loss 0.456433, accuracy 0.8125, precision 0.8734939759036144, recall 0.8430232558139535
2019-03-18T23:10:00.897997: step 125, loss 0.497919, accuracy 0.816406, precision 0.8860759493670886, recall 0.8284023668639053
2019-03-18T23:10:02.048950: step 126, loss 0.472484, accuracy 0.800781, precision 0.8727272727272727, recall 0.8275862068965517
2019-03-18T23:10:03.168960: step 127, loss 0.418877, accuracy 0.832031, precision 0.8597560975609756, recall 0.8757763975155279
2019-03-18T23:10:04.497428: step 128, loss 0.342201, accuracy 0.867188, precision 0.8690476190476191, recall 0.9240506329113924
2019-03-18T23:10:05.739691: step 129, loss 0.384074, accuracy 0.839844, precision 0.8597560975609756, recall 0.8867924528301887
2019-03-18T23:10:06.977374: step 130, loss 0.333637, accuracy 0.847656, precision 0.8827160493827161, recall 0.8773006134969326
2019-03-18T23:10:08.261027: step 131, loss 0.453018, accuracy 0.820312, precision 0.872093023255814, recall 0.8620689655172413
2019-03-18T23:10:09.547589: step 132, loss 0.438643, accuracy 0.792969, precision 0.8354430379746836, recall 0.8301886792452831
2019-03-18T23:10:10.658620: step 133, loss 0.420237, accuracy 0.835938, precision 0.8853503184713376, recall 0.852760736196319
2019-03-18T23:10:11.945736: step 134, loss 0.401196, accuracy 0.824219, precision 0.9259259259259259, recall 0.819672131147541
2019-03-18T23:10:13.327625: step 135, loss 0.470391, accuracy 0.789062, precision 0.8493975903614458, recall 0.8294117647058824
2019-03-18T23:10:14.450627: step 136, loss 0.380017, accuracy 0.804688, precision 0.8809523809523809, recall 0.8314606741573034
2019-03-18T23:10:15.513293: step 137, loss 0.439951, accuracy 0.820312, precision 0.8729281767955801, recall 0.8729281767955801
2019-03-18T23:10:16.811855: step 138, loss 0.423982, accuracy 0.832031, precision 0.8850574712643678, recall 0.8700564971751412
2019-03-18T23:10:18.122344: step 139, loss 0.350767, accuracy 0.847656, precision 0.8863636363636364, recall 0.8914285714285715
2019-03-18T23:10:19.473734: step 140, loss 0.445337, accuracy 0.828125, precision 0.906832298136646, recall 0.8342857142857143
2019-03-18T23:10:20.713449: step 141, loss 0.40409, accuracy 0.816406, precision 0.881578947368421, recall 0.8220858895705522
2019-03-18T23:10:21.994024: step 142, loss 0.31052, accuracy 0.863281, precision 0.8797814207650273, recall 0.9252873563218391
2019-03-18T23:10:23.131493: step 143, loss 0.444302, accuracy 0.808594, precision 0.8516129032258064, recall 0.8354430379746836
2019-03-18T23:10:24.255488: step 144, loss 0.400207, accuracy 0.816406, precision 0.8713450292397661, recall 0.8563218390804598
2019-03-18T23:10:25.529085: step 145, loss 0.385934, accuracy 0.847656, precision 0.9213483146067416, recall 0.8677248677248677
2019-03-18T23:10:26.779312: step 146, loss 0.327707, accuracy 0.886719, precision 0.9243243243243243, recall 0.9193548387096774
2019-03-18T23:10:28.069376: step 147, loss 0.328003, accuracy 0.859375, precision 0.8902439024390244, recall 0.8902439024390244
2019-03-18T23:10:28.512196: step 148, loss 0.342931, accuracy 0.810811, precision 0.8571428571428571, recall 0.8888888888888888
2019-03-18T23:10:29.757863: step 149, loss 0.351053, accuracy 0.875, precision 0.98125, recall 0.8440860215053764
2019-03-18T23:10:31.195651: step 150, loss 0.361336, accuracy 0.84375, precision 0.8963414634146342, recall 0.8647058823529412
2019-03-18T23:10:32.480219: step 151, loss 0.36808, accuracy 0.863281, precision 0.9364161849710982, recall 0.8709677419354839
2019-03-18T23:10:33.565320: step 152, loss 0.331871, accuracy 0.839844, precision 0.8975903614457831, recall 0.861271676300578
2019-03-18T23:10:34.852878: step 153, loss 0.413532, accuracy 0.855469, precision 0.88125, recall 0.8867924528301887
2019-03-18T23:10:36.126538: step 154, loss 0.306059, accuracy 0.863281, precision 0.9005847953216374, recall 0.8953488372093024
2019-03-18T23:10:37.456492: step 155, loss 0.383265, accuracy 0.816406, precision 0.8597560975609756, recall 0.8545454545454545
2019-03-18T23:10:38.670309: step 156, loss 0.330274, accuracy 0.855469, precision 0.8735632183908046, recall 0.9101796407185628
2019-03-18T23:10:39.954876: step 157, loss 0.441508, accuracy 0.820312, precision 0.8679245283018868, recall 0.8466257668711656
2019-03-18T23:10:41.239963: step 158, loss 0.344752, accuracy 0.839844, precision 0.8765432098765432, recall 0.8711656441717791
2019-03-18T23:10:42.739000: step 159, loss 0.377913, accuracy 0.820312, precision 0.8875, recall 0.8352941176470589
2019-03-18T23:10:44.187662: step 160, loss 0.404873, accuracy 0.820312, precision 0.875, recall 0.8651685393258427
2019-03-18T23:10:45.449826: step 161, loss 0.294189, accuracy 0.875, precision 0.9340659340659341, recall 0.8947368421052632
2019-03-18T23:10:46.434195: step 162, loss 0.356684, accuracy 0.835938, precision 0.9036144578313253, recall 0.8522727272727273
2019-03-18T23:10:47.782257: step 163, loss 0.329337, accuracy 0.859375, precision 0.9195402298850575, recall 0.8791208791208791
2019-03-18T23:10:49.348092: step 164, loss 0.378, accuracy 0.828125, precision 0.9034090909090909, recall 0.8548387096774194
2019-03-18T23:10:50.683523: step 165, loss 0.394521, accuracy 0.847656, precision 0.8938547486033519, recall 0.8888888888888888
2019-03-18T23:10:51.910246: step 166, loss 0.380358, accuracy 0.839844, precision 0.9202453987730062, recall 0.8426966292134831
2019-03-18T23:10:53.227235: step 167, loss 0.393128, accuracy 0.832031, precision 0.9090909090909091, recall 0.8426966292134831
2019-03-18T23:10:54.540725: step 168, loss 0.266767, accuracy 0.882812, precision 0.9186046511627907, recall 0.9080459770114943
2019-03-18T23:10:56.197333: step 169, loss 0.337281, accuracy 0.859375, precision 0.8848484848484849, recall 0.8957055214723927
2019-03-18T23:10:57.569663: step 170, loss 0.38717, accuracy 0.851562, precision 0.8650306748466258, recall 0.8980891719745223
2019-03-18T23:10:58.695661: step 171, loss 0.332609, accuracy 0.839844, precision 0.8742857142857143, recall 0.8895348837209303
2019-03-18T23:10:59.859544: step 172, loss 0.304617, accuracy 0.859375, precision 0.9248554913294798, recall 0.8743169398907104
2019-03-18T23:11:01.158074: step 173, loss 0.351503, accuracy 0.851562, precision 0.877906976744186, recall 0.8988095238095238
2019-03-18T23:11:02.436168: step 174, loss 0.325939, accuracy 0.851562, precision 0.9345238095238095, recall 0.8532608695652174
2019-03-18T23:11:03.661403: step 175, loss 0.375474, accuracy 0.832031, precision 0.8867924528301887, recall 0.8493975903614458
2019-03-18T23:11:04.812329: step 176, loss 0.320378, accuracy 0.882812, precision 0.9383561643835616, recall 0.8670886075949367
2019-03-18T23:11:05.805674: step 177, loss 0.300208, accuracy 0.851562, precision 0.896969696969697, recall 0.8757396449704142
2019-03-18T23:11:07.171542: step 178, loss 0.320083, accuracy 0.878906, precision 0.9117647058823529, recall 0.9064327485380117
2019-03-18T23:11:08.561853: step 179, loss 0.368743, accuracy 0.835938, precision 0.8509316770186336, recall 0.8838709677419355
2019-03-18T23:11:09.651942: step 180, loss 0.38594, accuracy 0.828125, precision 0.8588957055214724, recall 0.8695652173913043
2019-03-18T23:11:10.739036: step 181, loss 0.383652, accuracy 0.828125, precision 0.7962962962962963, recall 0.9214285714285714
2019-03-18T23:11:11.967771: step 182, loss 0.376707, accuracy 0.835938, precision 0.8771929824561403, recall 0.8771929824561403
2019-03-18T23:11:13.416455: step 183, loss 0.381645, accuracy 0.832031, precision 0.9135802469135802, recall 0.8361581920903954
2019-03-18T23:11:14.849626: step 184, loss 0.413554, accuracy 0.847656, precision 0.9216867469879518, recall 0.8547486033519553
2019-03-18T23:11:15.148826: step 185, loss 0.525499, accuracy 0.837838, precision 0.92, recall 0.8518518518518519
2019-03-18T23:11:16.618900: step 186, loss 0.279645, accuracy 0.890625, precision 0.9392265193370166, recall 0.9090909090909091
2019-03-18T23:11:18.042095: step 187, loss 0.248424, accuracy 0.882812, precision 0.9431818181818182, recall 0.8924731182795699
2019-03-18T23:11:19.153636: step 188, loss 0.337383, accuracy 0.859375, precision 0.9493670886075949, recall 0.8426966292134831
2019-03-18T23:11:20.431296: step 189, loss 0.289412, accuracy 0.875, precision 0.9096385542168675, recall 0.8988095238095238
2019-03-18T23:11:21.686451: step 190, loss 0.276413, accuracy 0.90625, precision 0.9186046511627907, recall 0.9404761904761905
2019-03-18T23:11:23.091306: step 191, loss 0.285547, accuracy 0.882812, precision 0.9036144578313253, recall 0.9146341463414634
2019-03-18T23:11:24.514014: step 192, loss 0.299289, accuracy 0.851562, precision 0.8571428571428571, recall 0.9113924050632911
2019-03-18T23:11:25.773701: step 193, loss 0.264546, accuracy 0.894531, precision 0.9156626506024096, recall 0.9212121212121213
2019-03-18T23:11:27.133068: step 194, loss 0.327668, accuracy 0.871094, precision 0.9024390243902439, recall 0.896969696969697
2019-03-18T23:11:28.041641: step 195, loss 0.272888, accuracy 0.875, precision 0.9030303030303031, recall 0.9030303030303031
2019-03-18T23:11:29.262403: step 196, loss 0.319723, accuracy 0.863281, precision 0.9358974358974359, recall 0.8538011695906432
2019-03-18T23:11:30.457409: step 197, loss 0.252906, accuracy 0.898438, precision 0.9464285714285714, recall 0.9034090909090909
2019-03-18T23:11:31.586900: step 198, loss 0.288192, accuracy 0.878906, precision 0.9397590361445783, recall 0.8813559322033898
2019-03-18T23:11:33.042588: step 199, loss 0.314821, accuracy 0.902344, precision 0.930379746835443, recall 0.9130434782608695
2019-03-18T23:11:34.393975: step 200, loss 0.313242, accuracy 0.882812, precision 0.8876404494382022, recall 0.9404761904761905
2019-03-18T23:11:35.398292: step 201, loss 0.331745, accuracy 0.855469, precision 0.8888888888888888, recall 0.8941176470588236
2019-03-18T23:11:36.636493: step 202, loss 0.319953, accuracy 0.863281, precision 0.9259259259259259, recall 0.8670520231213873
2019-03-18T23:11:37.839788: step 203, loss 0.302152, accuracy 0.867188, precision 0.9041916167664671, recall 0.893491124260355
2019-03-18T23:11:39.268967: step 204, loss 0.28893, accuracy 0.886719, precision 0.9401197604790419, recall 0.8920454545454546
2019-03-18T23:11:40.640833: step 205, loss 0.292734, accuracy 0.878906, precision 0.9404761904761905, recall 0.88268156424581
2019-03-18T23:11:42.040635: step 206, loss 0.31945, accuracy 0.878906, precision 0.9578313253012049, recall 0.8688524590163934
2019-03-18T23:11:43.278926: step 207, loss 0.30818, accuracy 0.867188, precision 0.8922155688622755, recall 0.9030303030303031
2019-03-18T23:11:44.339094: step 208, loss 0.298225, accuracy 0.855469, precision 0.8675496688741722, recall 0.8851351351351351
2019-03-18T23:11:45.540984: step 209, loss 0.255678, accuracy 0.898438, precision 0.9289940828402367, recall 0.9181286549707602
2019-03-18T23:11:46.736791: step 210, loss 0.280346, accuracy 0.867188, precision 0.9047619047619048, recall 0.8941176470588236
2019-03-18T23:11:48.301149: step 211, loss 0.274992, accuracy 0.890625, precision 0.9212121212121213, recall 0.9101796407185628
2019-03-18T23:11:49.363388: step 212, loss 0.30931, accuracy 0.875, precision 0.9161290322580645, recall 0.8819875776397516
2019-03-18T23:11:50.529272: step 213, loss 0.350865, accuracy 0.847656, precision 0.9235294117647059, recall 0.8579234972677595
2019-03-18T23:11:51.762011: step 214, loss 0.241562, accuracy 0.886719, precision 0.9122807017543859, recall 0.9176470588235294
2019-03-18T23:11:53.022643: step 215, loss 0.326367, accuracy 0.859375, precision 0.867816091954023, recall 0.9207317073170732
2019-03-18T23:11:54.185048: step 216, loss 0.29081, accuracy 0.875, precision 0.9207317073170732, recall 0.888235294117647
2019-03-18T23:11:55.477107: step 217, loss 0.299893, accuracy 0.867188, precision 0.9202453987730062, recall 0.8771929824561403
2019-03-18T23:11:56.738737: step 218, loss 0.254214, accuracy 0.898438, precision 0.9166666666666666, recall 0.927710843373494
2019-03-18T23:11:57.915148: step 219, loss 0.341827, accuracy 0.84375, precision 0.9127906976744186, recall 0.8626373626373627
2019-03-18T23:11:59.168798: step 220, loss 0.254461, accuracy 0.886719, precision 0.9451219512195121, recall 0.8857142857142857
2019-03-18T23:12:00.541180: step 221, loss 0.278565, accuracy 0.871094, precision 0.8729281767955801, recall 0.9404761904761905
2019-03-18T23:12:00.933642: step 222, loss 0.331512, accuracy 0.864865, precision 0.9047619047619048, recall 0.8636363636363636
2019-03-18T23:12:02.205244: step 223, loss 0.241137, accuracy 0.921875, precision 0.9625668449197861, recall 0.9326424870466321
2019-03-18T23:12:03.579571: step 224, loss 0.255212, accuracy 0.921875, precision 0.975609756097561, recall 0.9090909090909091
2019-03-18T23:12:04.854216: step 225, loss 0.290702, accuracy 0.878906, precision 0.9085365853658537, recall 0.9030303030303031
2019-03-18T23:12:06.084008: step 226, loss 0.265807, accuracy 0.886719, precision 0.9197530864197531, recall 0.9030303030303031
2019-03-18T23:12:07.245903: step 227, loss 0.278281, accuracy 0.863281, precision 0.8950617283950617, recall 0.8895705521472392
2019-03-18T23:12:08.617311: step 228, loss 0.273565, accuracy 0.898438, precision 0.8922155688622755, recall 0.9490445859872612
2019-03-18T23:12:09.949751: step 229, loss 0.32273, accuracy 0.855469, precision 0.8662790697674418, recall 0.9141104294478528
2019-03-18T23:12:11.186446: step 230, loss 0.254455, accuracy 0.902344, precision 0.9352941176470588, recall 0.9190751445086706
2019-03-18T23:12:12.335886: step 231, loss 0.253583, accuracy 0.890625, precision 0.9325153374233128, recall 0.8994082840236687
2019-03-18T23:12:13.470043: step 232, loss 0.275264, accuracy 0.878906, precision 0.9171597633136095, recall 0.9011627906976745
2019-03-18T23:12:14.589570: step 233, loss 0.258398, accuracy 0.875, precision 0.9363057324840764, recall 0.8698224852071006
2019-03-18T23:12:15.889177: step 234, loss 0.25544, accuracy 0.894531, precision 0.927710843373494, recall 0.9112426035502958
2019-03-18T23:12:17.206168: step 235, loss 0.284055, accuracy 0.863281, precision 0.9461077844311377, recall 0.8586956521739131
2019-03-18T23:12:18.401036: step 236, loss 0.28305, accuracy 0.894531, precision 0.926829268292683, recall 0.9101796407185628
2019-03-18T23:12:19.434784: step 237, loss 0.377408, accuracy 0.839844, precision 0.8863636363636364, recall 0.8813559322033898
2019-03-18T23:12:20.665057: step 238, loss 0.234782, accuracy 0.90625, precision 0.9212121212121213, recall 0.9325153374233128
2019-03-18T23:12:22.034905: step 239, loss 0.269472, accuracy 0.878906, precision 0.9022988505747126, recall 0.9181286549707602
2019-03-18T23:12:23.425701: step 240, loss 0.199882, accuracy 0.933594, precision 0.9695121951219512, recall 0.9298245614035088
2019-03-18T23:12:24.749678: step 241, loss 0.273799, accuracy 0.871094, precision 0.9090909090909091, recall 0.8928571428571429
2019-03-18T23:12:25.791891: step 242, loss 0.279908, accuracy 0.863281, precision 0.9111111111111111, recall 0.8961748633879781
2019-03-18T23:12:26.955783: step 243, loss 0.202213, accuracy 0.921875, precision 0.926829268292683, recall 0.95
2019-03-18T23:12:28.166058: step 244, loss 0.272135, accuracy 0.882812, precision 0.8988095238095238, recall 0.9207317073170732
2019-03-18T23:12:29.400760: step 245, loss 0.277453, accuracy 0.871094, precision 0.9415204678362573, recall 0.875
2019-03-18T23:12:30.475886: step 246, loss 0.197474, accuracy 0.914062, precision 0.9281767955801105, recall 0.9491525423728814
2019-03-18T23:12:31.873660: step 247, loss 0.234477, accuracy 0.910156, precision 0.953757225433526, recall 0.9166666666666666
2019-03-18T23:12:33.039546: step 248, loss 0.24281, accuracy 0.890625, precision 0.9329268292682927, recall 0.9
2019-03-18T23:12:33.994503: step 249, loss 0.237932, accuracy 0.90625, precision 0.9454545454545454, recall 0.9122807017543859
2019-03-18T23:12:35.170915: step 250, loss 0.258419, accuracy 0.886719, precision 0.9520958083832335, recall 0.8833333333333333

Evaluation:
[[1482  120]
 [ 295  416]]
2019-03-18T23:12:38.233732: step 250, loss 0.422779, accuracy 0.820579, precision 0.9250936329588015, recall 0.8339898705683737

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968444\checkpoints\model-250

2019-03-18T23:12:40.256475: step 251, loss 0.250613, accuracy 0.894531, precision 0.9515151515151515, recall 0.8920454545454546
2019-03-18T23:12:41.465754: step 252, loss 0.349484, accuracy 0.835938, precision 0.8947368421052632, recall 0.8395061728395061
2019-03-18T23:12:42.814661: step 253, loss 0.211328, accuracy 0.917969, precision 0.9259259259259259, recall 0.9433962264150944
2019-03-18T23:12:44.066336: step 254, loss 0.297164, accuracy 0.871094, precision 0.8703703703703703, recall 0.9215686274509803
2019-03-18T23:12:45.365864: step 255, loss 0.23568, accuracy 0.917969, precision 0.9166666666666666, recall 0.9470198675496688
2019-03-18T23:12:46.558677: step 256, loss 0.318834, accuracy 0.875, precision 0.9245283018867925, recall 0.8802395209580839
2019-03-18T23:12:47.662727: step 257, loss 0.244834, accuracy 0.898438, precision 0.8926553672316384, recall 0.9575757575757575
2019-03-18T23:12:48.986259: step 258, loss 0.294323, accuracy 0.875, precision 0.9390243902439024, recall 0.875
2019-03-18T23:12:49.460991: step 259, loss 0.201701, accuracy 0.945946, precision 1.0, recall 0.9230769230769231
2019-03-18T23:12:50.573020: step 260, loss 0.206279, accuracy 0.90625, precision 0.9529411764705882, recall 0.9101123595505618
2019-03-18T23:12:51.928909: step 261, loss 0.223946, accuracy 0.917969, precision 0.9333333333333333, recall 0.9271523178807947
2019-03-18T23:12:53.242916: step 262, loss 0.202896, accuracy 0.917969, precision 0.9447852760736196, recall 0.927710843373494
2019-03-18T23:12:54.294617: step 263, loss 0.204371, accuracy 0.902344, precision 0.9318181818181818, recall 0.9265536723163842
2019-03-18T23:12:55.378233: step 264, loss 0.24174, accuracy 0.898438, precision 0.9127906976744186, recall 0.9345238095238095
2019-03-18T23:12:56.729734: step 265, loss 0.22965, accuracy 0.921875, precision 0.9426751592356688, recall 0.9308176100628931
2019-03-18T23:12:58.005836: step 266, loss 0.254339, accuracy 0.882812, precision 0.9085714285714286, recall 0.9190751445086706
2019-03-18T23:12:59.381161: step 267, loss 0.224427, accuracy 0.898438, precision 0.935672514619883, recall 0.9142857142857143
2019-03-18T23:13:00.781931: step 268, loss 0.198286, accuracy 0.933594, precision 0.9520958083832335, recall 0.9464285714285714
2019-03-18T23:13:01.920401: step 269, loss 0.215327, accuracy 0.917969, precision 0.9548387096774194, recall 0.9135802469135802
2019-03-18T23:13:03.006009: step 270, loss 0.22334, accuracy 0.925781, precision 0.9512195121951219, recall 0.9341317365269461
2019-03-18T23:13:04.161039: step 271, loss 0.213064, accuracy 0.921875, precision 0.9397590361445783, recall 0.9397590361445783
2019-03-18T23:13:05.525898: step 272, loss 0.252372, accuracy 0.890625, precision 0.9440993788819876, recall 0.8888888888888888
2019-03-18T23:13:06.772568: step 273, loss 0.227679, accuracy 0.90625, precision 0.96, recall 0.9081081081081082
2019-03-18T23:13:07.752948: step 274, loss 0.22522, accuracy 0.910156, precision 0.9411764705882353, recall 0.9248554913294798
2019-03-18T23:13:08.877455: step 275, loss 0.222111, accuracy 0.90625, precision 0.9269662921348315, recall 0.9375
2019-03-18T23:13:10.201916: step 276, loss 0.177367, accuracy 0.9375, precision 0.9269662921348315, recall 0.9821428571428571
2019-03-18T23:13:11.535861: step 277, loss 0.237773, accuracy 0.90625, precision 0.9141104294478528, recall 0.9371069182389937
2019-03-18T23:13:12.876331: step 278, loss 0.235247, accuracy 0.921875, precision 0.9520958083832335, recall 0.9298245614035088
2019-03-18T23:13:14.069710: step 279, loss 0.21847, accuracy 0.914062, precision 0.9487179487179487, recall 0.9135802469135802
2019-03-18T23:13:15.302415: step 280, loss 0.236164, accuracy 0.90625, precision 0.9454545454545454, recall 0.9122807017543859
2019-03-18T23:13:16.497223: step 281, loss 0.201834, accuracy 0.921875, precision 0.9393939393939394, recall 0.9393939393939394
2019-03-18T23:13:17.856592: step 282, loss 0.244589, accuracy 0.886719, precision 0.9192546583850931, recall 0.9024390243902439
2019-03-18T23:13:19.234907: step 283, loss 0.214243, accuracy 0.910156, precision 0.9318181818181818, recall 0.9371428571428572
2019-03-18T23:13:20.410278: step 284, loss 0.226407, accuracy 0.921875, precision 0.9447852760736196, recall 0.9333333333333333
2019-03-18T23:13:21.616056: step 285, loss 0.227741, accuracy 0.886719, precision 0.9345238095238095, recall 0.8971428571428571
2019-03-18T23:13:22.651291: step 286, loss 0.271005, accuracy 0.898438, precision 0.9341317365269461, recall 0.9122807017543859
2019-03-18T23:13:23.954325: step 287, loss 0.241982, accuracy 0.917969, precision 0.9415204678362573, recall 0.936046511627907
2019-03-18T23:13:25.100261: step 288, loss 0.195663, accuracy 0.914062, precision 0.95, recall 0.9156626506024096
2019-03-18T23:13:26.355954: step 289, loss 0.192585, accuracy 0.914062, precision 0.9195402298850575, recall 0.9523809523809523
2019-03-18T23:13:27.414640: step 290, loss 0.196511, accuracy 0.925781, precision 0.9371069182389937, recall 0.9430379746835443
2019-03-18T23:13:28.718664: step 291, loss 0.206468, accuracy 0.933594, precision 0.950920245398773, recall 0.9451219512195121
2019-03-18T23:13:29.939427: step 292, loss 0.262861, accuracy 0.878906, precision 0.9111111111111111, recall 0.9162011173184358
2019-03-18T23:13:31.263405: step 293, loss 0.253284, accuracy 0.898438, precision 0.9444444444444444, recall 0.9
2019-03-18T23:13:32.614321: step 294, loss 0.246251, accuracy 0.882812, precision 0.9382716049382716, recall 0.8837209302325582
2019-03-18T23:13:33.889912: step 295, loss 0.215072, accuracy 0.910156, precision 0.9485714285714286, recall 0.9222222222222223
2019-03-18T23:13:34.266932: step 296, loss 0.195231, accuracy 0.918919, precision 1.0, recall 0.9
2019-03-18T23:13:35.497157: step 297, loss 0.189569, accuracy 0.917969, precision 0.950920245398773, recall 0.9226190476190477
2019-03-18T23:13:36.757788: step 298, loss 0.212977, accuracy 0.917969, precision 0.9367088607594937, recall 0.9308176100628931
2019-03-18T23:13:37.952621: step 299, loss 0.227795, accuracy 0.90625, precision 0.9181286549707602, recall 0.9401197604790419
2019-03-18T23:13:39.120501: step 300, loss 0.238921, accuracy 0.914062, precision 0.9337349397590361, recall 0.9337349397590361
2019-03-18T23:13:40.460474: step 301, loss 0.166257, accuracy 0.9375, precision 0.9482758620689655, recall 0.9593023255813954
2019-03-18T23:13:41.678265: step 302, loss 0.202219, accuracy 0.917969, precision 0.9349112426035503, recall 0.9404761904761905
2019-03-18T23:13:42.835174: step 303, loss 0.176807, accuracy 0.917969, precision 0.9375, recall 0.9523809523809523
2019-03-18T23:13:44.047933: step 304, loss 0.207817, accuracy 0.917969, precision 0.9390243902439024, recall 0.9333333333333333
2019-03-18T23:13:45.444714: step 305, loss 0.204628, accuracy 0.914062, precision 0.9487179487179487, recall 0.9135802469135802
2019-03-18T23:13:46.723809: step 306, loss 0.173498, accuracy 0.941406, precision 0.9820359281437125, recall 0.9318181818181818
2019-03-18T23:13:48.003954: step 307, loss 0.19282, accuracy 0.929688, precision 0.9819277108433735, recall 0.9157303370786517
2019-03-18T23:13:48.968381: step 308, loss 0.184141, accuracy 0.925781, precision 0.9651162790697675, recall 0.9273743016759777
2019-03-18T23:13:50.131269: step 309, loss 0.207467, accuracy 0.917969, precision 0.9310344827586207, recall 0.9473684210526315
2019-03-18T23:13:51.368960: step 310, loss 0.16849, accuracy 0.945312, precision 0.9404761904761905, recall 0.9753086419753086
2019-03-18T23:13:52.774264: step 311, loss 0.175927, accuracy 0.929688, precision 0.9259259259259259, recall 0.9615384615384616
2019-03-18T23:13:54.028913: step 312, loss 0.187515, accuracy 0.921875, precision 0.9411764705882353, recall 0.9411764705882353
2019-03-18T23:13:55.222721: step 313, loss 0.213829, accuracy 0.910156, precision 0.9314285714285714, recall 0.9367816091954023
2019-03-18T23:13:56.233070: step 314, loss 0.224447, accuracy 0.910156, precision 0.925, recall 0.9308176100628931
2019-03-18T23:13:57.470318: step 315, loss 0.19319, accuracy 0.925781, precision 0.9320987654320988, recall 0.949685534591195
2019-03-18T23:13:58.727958: step 316, loss 0.206154, accuracy 0.921875, precision 0.9722222222222222, recall 0.9210526315789473
2019-03-18T23:13:59.783138: step 317, loss 0.188151, accuracy 0.933594, precision 0.9764705882352941, recall 0.9273743016759777
2019-03-18T23:14:01.008895: step 318, loss 0.204205, accuracy 0.933594, precision 0.9875, recall 0.9132947976878613
2019-03-18T23:14:02.307425: step 319, loss 0.177539, accuracy 0.929688, precision 0.9818181818181818, recall 0.9152542372881356
2019-03-18T23:14:03.678272: step 320, loss 0.207879, accuracy 0.914062, precision 0.9454545454545454, recall 0.9230769230769231
2019-03-18T23:14:05.537459: step 321, loss 0.170905, accuracy 0.925781, precision 0.9487179487179487, recall 0.9308176100628931
2019-03-18T23:14:07.107265: step 322, loss 0.196876, accuracy 0.921875, precision 0.9075144508670521, recall 0.9751552795031055
2019-03-18T23:14:08.618801: step 323, loss 0.208319, accuracy 0.914062, precision 0.9329268292682927, recall 0.9329268292682927
2019-03-18T23:14:10.237010: step 324, loss 0.23222, accuracy 0.894531, precision 0.8863636363636364, recall 0.9570552147239264
2019-03-18T23:14:11.761552: step 325, loss 0.249652, accuracy 0.910156, precision 0.9634146341463414, recall 0.9028571428571428
2019-03-18T23:14:13.333419: step 326, loss 0.172388, accuracy 0.929688, precision 0.9363057324840764, recall 0.9483870967741935
2019-03-18T23:14:14.763226: step 327, loss 0.262106, accuracy 0.894531, precision 0.9235668789808917, recall 0.90625
2019-03-18T23:14:16.243322: step 328, loss 0.179255, accuracy 0.925781, precision 0.9523809523809523, recall 0.935672514619883
2019-03-18T23:14:17.823123: step 329, loss 0.192403, accuracy 0.914062, precision 0.9285714285714286, recall 0.9397590361445783
2019-03-18T23:14:19.488808: step 330, loss 0.171271, accuracy 0.925781, precision 0.9702380952380952, recall 0.9209039548022598
2019-03-18T23:14:20.948448: step 331, loss 0.183715, accuracy 0.929688, precision 0.9683544303797469, recall 0.9216867469879518
2019-03-18T23:14:22.349701: step 332, loss 0.206945, accuracy 0.910156, precision 0.9294117647058824, recall 0.9349112426035503
2019-03-18T23:14:22.771574: step 333, loss 0.223167, accuracy 0.891892, precision 0.875, recall 0.9545454545454546
2019-03-18T23:14:24.490980: step 334, loss 0.218921, accuracy 0.890625, precision 0.9454545454545454, recall 0.8914285714285715
2019-03-18T23:14:25.824416: step 335, loss 0.178143, accuracy 0.941406, precision 0.9673202614379085, recall 0.9367088607594937
2019-03-18T23:14:27.223678: step 336, loss 0.200796, accuracy 0.925781, precision 0.9411764705882353, recall 0.935064935064935
2019-03-18T23:14:28.595012: step 337, loss 0.174172, accuracy 0.929688, precision 0.9371069182389937, recall 0.9490445859872612
2019-03-18T23:14:30.098994: step 338, loss 0.168615, accuracy 0.9375, precision 0.9345238095238095, recall 0.9691358024691358
2019-03-18T23:14:31.554105: step 339, loss 0.154692, accuracy 0.957031, precision 0.9707602339181286, recall 0.9651162790697675
2019-03-18T23:14:32.949377: step 340, loss 0.2042, accuracy 0.921875, precision 0.9244186046511628, recall 0.9578313253012049
2019-03-18T23:14:34.539129: step 341, loss 0.194726, accuracy 0.925781, precision 0.9629629629629629, recall 0.9230769230769231
2019-03-18T23:14:35.869574: step 342, loss 0.195354, accuracy 0.9375, precision 0.9698795180722891, recall 0.936046511627907
2019-03-18T23:14:37.137187: step 343, loss 0.184769, accuracy 0.929688, precision 0.9534883720930233, recall 0.9425287356321839
2019-03-18T23:14:38.508523: step 344, loss 0.140191, accuracy 0.960938, precision 0.9888268156424581, recall 0.9567567567567568
2019-03-18T23:14:39.890829: step 345, loss 0.132494, accuracy 0.957031, precision 0.9567567567567568, recall 0.9833333333333333
2019-03-18T23:14:41.220276: step 346, loss 0.186126, accuracy 0.917969, precision 0.9352941176470588, recall 0.9408284023668639
2019-03-18T23:14:42.595601: step 347, loss 0.151615, accuracy 0.9375, precision 0.9695121951219512, recall 0.9352941176470588
2019-03-18T23:14:43.767470: step 348, loss 0.188106, accuracy 0.925781, precision 0.9467455621301775, recall 0.9411764705882353
2019-03-18T23:14:44.872516: step 349, loss 0.159957, accuracy 0.941406, precision 0.9479768786127167, recall 0.9647058823529412
2019-03-18T23:14:46.294860: step 350, loss 0.147482, accuracy 0.941406, precision 0.9526627218934911, recall 0.9583333333333334
2019-03-18T23:14:47.840730: step 351, loss 0.213753, accuracy 0.925781, precision 0.9655172413793104, recall 0.9090909090909091
2019-03-18T23:14:49.180150: step 352, loss 0.133315, accuracy 0.953125, precision 0.9653179190751445, recall 0.9653179190751445
2019-03-18T23:14:50.280211: step 353, loss 0.182557, accuracy 0.929688, precision 0.9741935483870968, recall 0.9151515151515152
2019-03-18T23:14:51.402212: step 354, loss 0.223057, accuracy 0.898438, precision 0.9349112426035503, recall 0.9132947976878613
2019-03-18T23:14:52.978001: step 355, loss 0.209211, accuracy 0.894531, precision 0.8902439024390244, recall 0.9419354838709677
2019-03-18T23:14:54.417156: step 356, loss 0.204873, accuracy 0.921875, precision 0.9085714285714286, recall 0.9754601226993865
2019-03-18T23:14:55.589024: step 357, loss 0.182288, accuracy 0.925781, precision 0.9476744186046512, recall 0.9421965317919075
2019-03-18T23:14:56.672131: step 358, loss 0.173246, accuracy 0.933594, precision 0.9634146341463414, recall 0.9349112426035503
2019-03-18T23:14:57.938746: step 359, loss 0.172549, accuracy 0.933594, precision 0.9545454545454546, recall 0.9491525423728814
2019-03-18T23:14:59.344988: step 360, loss 0.16495, accuracy 0.929688, precision 0.9485714285714286, recall 0.9485714285714286
2019-03-18T23:15:00.548771: step 361, loss 0.214361, accuracy 0.902344, precision 0.9506172839506173, recall 0.9005847953216374
2019-03-18T23:15:01.600494: step 362, loss 0.210884, accuracy 0.917969, precision 0.9454545454545454, recall 0.9285714285714286
2019-03-18T23:15:02.929985: step 363, loss 0.207285, accuracy 0.910156, precision 0.9545454545454546, recall 0.901840490797546
2019-03-18T23:15:04.188623: step 364, loss 0.157113, accuracy 0.9375, precision 0.9629629629629629, recall 0.9397590361445783
2019-03-18T23:15:05.465719: step 365, loss 0.235772, accuracy 0.90625, precision 0.972972972972973, recall 0.8780487804878049
2019-03-18T23:15:06.605674: step 366, loss 0.179844, accuracy 0.929688, precision 0.9273743016759777, recall 0.9707602339181286
2019-03-18T23:15:07.871292: step 367, loss 0.177014, accuracy 0.917969, precision 0.9106145251396648, recall 0.9702380952380952
2019-03-18T23:15:09.175805: step 368, loss 0.211037, accuracy 0.917969, precision 0.926829268292683, recall 0.9440993788819876
2019-03-18T23:15:10.515227: step 369, loss 0.150183, accuracy 0.941406, precision 0.9431818181818182, recall 0.9707602339181286
2019-03-18T23:15:10.971010: step 370, loss 0.11261, accuracy 0.945946, precision 0.92, recall 1.0
2019-03-18T23:15:12.324435: step 371, loss 0.163124, accuracy 0.933594, precision 0.9491525423728814, recall 0.9545454545454546
2019-03-18T23:15:13.762591: step 372, loss 0.188337, accuracy 0.925781, precision 0.9695121951219512, recall 0.9190751445086706
2019-03-18T23:15:15.078075: step 373, loss 0.130619, accuracy 0.953125, precision 1.0, recall 0.9344262295081968
2019-03-18T23:15:16.300809: step 374, loss 0.17236, accuracy 0.929688, precision 0.9883040935672515, recall 0.9135135135135135
2019-03-18T23:15:17.465695: step 375, loss 0.146793, accuracy 0.9375, precision 0.9835164835164835, recall 0.9322916666666666
2019-03-18T23:15:18.884907: step 376, loss 0.134031, accuracy 0.945312, precision 0.9830508474576272, recall 0.9405405405405406
2019-03-18T23:15:20.448725: step 377, loss 0.130619, accuracy 0.960938, precision 0.9803921568627451, recall 0.9554140127388535
2019-03-18T23:15:21.836018: step 378, loss 0.108738, accuracy 0.96875, precision 0.9754601226993865, recall 0.9754601226993865
2019-03-18T23:15:23.179428: step 379, loss 0.165427, accuracy 0.941406, precision 0.9515151515151515, recall 0.9573170731707317
2019-03-18T23:15:24.397177: step 380, loss 0.194375, accuracy 0.933594, precision 0.9047619047619048, recall 0.9934640522875817
2019-03-18T23:15:25.671769: step 381, loss 0.112046, accuracy 0.96875, precision 0.9748427672955975, recall 0.9748427672955975
2019-03-18T23:15:26.893505: step 382, loss 0.186369, accuracy 0.933594, precision 0.9397590361445783, recall 0.9570552147239264
2019-03-18T23:15:28.078339: step 383, loss 0.15958, accuracy 0.949219, precision 0.9644970414201184, recall 0.9588235294117647
2019-03-18T23:15:29.415763: step 384, loss 0.111279, accuracy 0.960938, precision 0.96875, recall 0.96875
2019-03-18T23:15:30.843106: step 385, loss 0.169359, accuracy 0.929688, precision 0.975609756097561, recall 0.9195402298850575
2019-03-18T23:15:32.120694: step 386, loss 0.134046, accuracy 0.957031, precision 0.9625, recall 0.9685534591194969
2019-03-18T23:15:33.274620: step 387, loss 0.177369, accuracy 0.929688, precision 0.9874213836477987, recall 0.9075144508670521
2019-03-18T23:15:34.389149: step 388, loss 0.13599, accuracy 0.960938, precision 0.9876543209876543, recall 0.9523809523809523
2019-03-18T23:15:35.671722: step 389, loss 0.179905, accuracy 0.929688, precision 0.9325153374233128, recall 0.9559748427672956
2019-03-18T23:15:37.270449: step 390, loss 0.135092, accuracy 0.953125, precision 0.9590643274853801, recall 0.9704142011834319
2019-03-18T23:15:38.737530: step 391, loss 0.173343, accuracy 0.925781, precision 0.935672514619883, recall 0.9523809523809523
2019-03-18T23:15:39.920371: step 392, loss 0.159626, accuracy 0.933594, precision 0.9606741573033708, recall 0.9447513812154696
2019-03-18T23:15:40.959592: step 393, loss 0.115071, accuracy 0.964844, precision 0.9719101123595506, recall 0.9774011299435028
2019-03-18T23:15:42.162378: step 394, loss 0.166338, accuracy 0.929688, precision 0.9651162790697675, recall 0.9325842696629213
2019-03-18T23:15:43.564630: step 395, loss 0.139829, accuracy 0.957031, precision 0.9824561403508771, recall 0.9545454545454546
2019-03-18T23:15:44.726528: step 396, loss 0.145448, accuracy 0.953125, precision 0.9760479041916168, recall 0.9532163742690059
2019-03-18T23:15:45.835563: step 397, loss 0.176838, accuracy 0.917969, precision 0.9157303370786517, recall 0.9644970414201184
2019-03-18T23:15:47.134092: step 398, loss 0.132798, accuracy 0.949219, precision 0.9490445859872612, recall 0.9675324675324676
2019-03-18T23:15:48.480502: step 399, loss 0.148409, accuracy 0.957031, precision 0.9754601226993865, recall 0.9578313253012049
2019-03-18T23:15:49.795018: step 400, loss 0.111591, accuracy 0.964844, precision 0.9772727272727273, recall 0.9717514124293786
2019-03-18T23:15:50.867150: step 401, loss 0.171252, accuracy 0.929688, precision 0.9503105590062112, recall 0.9386503067484663
2019-03-18T23:15:52.159695: step 402, loss 0.179687, accuracy 0.925781, precision 0.9079754601226994, recall 0.9736842105263158
2019-03-18T23:15:53.325580: step 403, loss 0.127576, accuracy 0.957031, precision 0.9551282051282052, recall 0.9738562091503268
2019-03-18T23:15:54.658019: step 404, loss 0.170013, accuracy 0.925781, precision 0.9212121212121213, recall 0.9620253164556962
2019-03-18T23:15:56.042320: step 405, loss 0.151848, accuracy 0.949219, precision 0.9515151515151515, recall 0.9691358024691358
2019-03-18T23:15:57.263062: step 406, loss 0.108614, accuracy 0.972656, precision 0.9820359281437125, recall 0.9761904761904762
2019-03-18T23:15:57.626087: step 407, loss 0.154211, accuracy 0.945946, precision 1.0, recall 0.9090909090909091
2019-03-18T23:15:58.955536: step 408, loss 0.133217, accuracy 0.964844, precision 0.9826589595375722, recall 0.9659090909090909
2019-03-18T23:16:00.186307: step 409, loss 0.134642, accuracy 0.953125, precision 0.9753086419753086, recall 0.9518072289156626
2019-03-18T23:16:01.454444: step 410, loss 0.141023, accuracy 0.949219, precision 0.9695121951219512, recall 0.9520958083832335
2019-03-18T23:16:02.631295: step 411, loss 0.127704, accuracy 0.945312, precision 0.9559748427672956, recall 0.9559748427672956
2019-03-18T23:16:03.896912: step 412, loss 0.14648, accuracy 0.953125, precision 0.9578313253012049, recall 0.9695121951219512
2019-03-18T23:16:05.141587: step 413, loss 0.131012, accuracy 0.953125, precision 0.9570552147239264, recall 0.968944099378882
2019-03-18T23:16:06.282539: step 414, loss 0.133451, accuracy 0.941406, precision 0.9230769230769231, recall 0.9795918367346939
2019-03-18T23:16:07.478408: step 415, loss 0.119754, accuracy 0.960938, precision 0.9573170731707317, recall 0.98125
2019-03-18T23:16:08.890634: step 416, loss 0.130891, accuracy 0.941406, precision 0.9393939393939394, recall 0.96875
2019-03-18T23:16:10.304896: step 417, loss 0.105079, accuracy 0.976562, precision 0.9822485207100592, recall 0.9822485207100592
2019-03-18T23:16:11.574505: step 418, loss 0.163031, accuracy 0.941406, precision 0.9585798816568047, recall 0.9529411764705882
2019-03-18T23:16:12.757344: step 419, loss 0.142052, accuracy 0.941406, precision 0.9700598802395209, recall 0.9418604651162791
2019-03-18T23:16:13.942177: step 420, loss 0.160059, accuracy 0.9375, precision 0.94375, recall 0.9556962025316456
2019-03-18T23:16:15.245694: step 421, loss 0.118454, accuracy 0.96875, precision 0.9940119760479041, recall 0.9595375722543352
2019-03-18T23:16:16.772619: step 422, loss 0.104998, accuracy 0.972656, precision 0.9722222222222222, recall 0.9887005649717514
2019-03-18T23:16:18.149934: step 423, loss 0.128686, accuracy 0.960938, precision 0.9634146341463414, recall 0.9753086419753086
2019-03-18T23:16:19.543211: step 424, loss 0.131557, accuracy 0.941406, precision 0.95, recall 0.9559748427672956
2019-03-18T23:16:20.776497: step 425, loss 0.130389, accuracy 0.964844, precision 0.9700598802395209, recall 0.9759036144578314
2019-03-18T23:16:21.889521: step 426, loss 0.114564, accuracy 0.964844, precision 0.9691358024691358, recall 0.9751552795031055
2019-03-18T23:16:23.371582: step 427, loss 0.121318, accuracy 0.953125, precision 0.9719101123595506, recall 0.9611111111111111
2019-03-18T23:16:24.732455: step 428, loss 0.11018, accuracy 0.957031, precision 0.9696969696969697, recall 0.963855421686747
2019-03-18T23:16:25.947210: step 429, loss 0.144578, accuracy 0.941406, precision 0.9760479041916168, recall 0.9367816091954023
2019-03-18T23:16:27.089678: step 430, loss 0.121057, accuracy 0.960938, precision 0.9835164835164835, recall 0.9623655913978495
2019-03-18T23:16:28.436649: step 431, loss 0.123964, accuracy 0.960938, precision 0.9691358024691358, recall 0.9691358024691358
2019-03-18T23:16:29.926244: step 432, loss 0.133371, accuracy 0.960938, precision 0.9811320754716981, recall 0.9570552147239264
2019-03-18T23:16:31.225366: step 433, loss 0.151013, accuracy 0.9375, precision 0.9597701149425287, recall 0.9488636363636364
2019-03-18T23:16:32.451091: step 434, loss 0.131112, accuracy 0.960938, precision 0.9772727272727273, recall 0.9662921348314607
2019-03-18T23:16:33.526280: step 435, loss 0.125988, accuracy 0.964844, precision 0.9741935483870968, recall 0.967948717948718
2019-03-18T23:16:34.944000: step 436, loss 0.147535, accuracy 0.941406, precision 0.9528795811518325, recall 0.9680851063829787
2019-03-18T23:16:36.473913: step 437, loss 0.158893, accuracy 0.941406, precision 0.9745222929936306, recall 0.9329268292682927
2019-03-18T23:16:38.407853: step 438, loss 0.141109, accuracy 0.953125, precision 0.9738562091503268, recall 0.9490445859872612
2019-03-18T23:16:40.344190: step 439, loss 0.154059, accuracy 0.945312, precision 0.9590643274853801, recall 0.9590643274853801
2019-03-18T23:16:41.754580: step 440, loss 0.165679, accuracy 0.9375, precision 0.930635838150289, recall 0.9757575757575757
2019-03-18T23:16:43.494557: step 441, loss 0.122408, accuracy 0.945312, precision 0.9553072625698324, recall 0.9661016949152542
2019-03-18T23:16:44.936766: step 442, loss 0.137084, accuracy 0.953125, precision 0.9554140127388535, recall 0.967741935483871
2019-03-18T23:16:46.325057: step 443, loss 0.0947091, accuracy 0.96875, precision 0.9651162790697675, recall 0.9880952380952381
2019-03-18T23:16:46.936422: step 444, loss 0.166535, accuracy 0.918919, precision 0.9583333333333334, recall 0.92
2019-03-18T23:16:48.291800: step 445, loss 0.14807, accuracy 0.941406, precision 0.9819277108433735, recall 0.9314285714285714
2019-03-18T23:16:49.395850: step 446, loss 0.0937307, accuracy 0.96875, precision 0.9824561403508771, recall 0.9710982658959537
2019-03-18T23:16:50.888860: step 447, loss 0.108732, accuracy 0.972656, precision 0.9761904761904762, recall 0.9820359281437125
2019-03-18T23:16:52.568396: step 448, loss 0.119059, accuracy 0.957031, precision 0.9629629629629629, recall 0.968944099378882
2019-03-18T23:16:54.133214: step 449, loss 0.148066, accuracy 0.945312, precision 0.9554140127388535, recall 0.9554140127388535
2019-03-18T23:16:55.440721: step 450, loss 0.120952, accuracy 0.949219, precision 0.9672131147540983, recall 0.9619565217391305
2019-03-18T23:16:57.045053: step 451, loss 0.150586, accuracy 0.957031, precision 0.9607843137254902, recall 0.9671052631578947
2019-03-18T23:16:58.405438: step 452, loss 0.135415, accuracy 0.957031, precision 0.9823529411764705, recall 0.9542857142857143
2019-03-18T23:16:59.894004: step 453, loss 0.130185, accuracy 0.972656, precision 0.9605263157894737, recall 0.9931972789115646
2019-03-18T23:17:01.331265: step 454, loss 0.0938081, accuracy 0.980469, precision 0.9881656804733728, recall 0.9823529411764705
2019-03-18T23:17:02.578931: step 455, loss 0.139656, accuracy 0.945312, precision 0.9488636363636364, recall 0.9709302325581395
2019-03-18T23:17:03.808813: step 456, loss 0.11409, accuracy 0.957031, precision 0.9593023255813954, recall 0.9763313609467456
2019-03-18T23:17:05.074685: step 457, loss 0.125971, accuracy 0.953125, precision 0.9585798816568047, recall 0.9700598802395209
2019-03-18T23:17:06.315685: step 458, loss 0.134827, accuracy 0.949219, precision 0.9824561403508771, recall 0.9438202247191011
2019-03-18T23:17:07.573941: step 459, loss 0.10542, accuracy 0.976562, precision 0.9811320754716981, recall 0.9811320754716981
2019-03-18T23:17:08.697498: step 460, loss 0.129612, accuracy 0.957031, precision 0.9691358024691358, recall 0.9631901840490797
2019-03-18T23:17:09.800727: step 461, loss 0.117607, accuracy 0.949219, precision 0.9754601226993865, recall 0.9464285714285714
2019-03-18T23:17:11.069851: step 462, loss 0.10223, accuracy 0.980469, precision 0.9939759036144579, recall 0.9763313609467456
2019-03-18T23:17:12.534012: step 463, loss 0.105601, accuracy 0.972656, precision 0.9702380952380952, recall 0.9878787878787879
2019-03-18T23:17:13.832583: step 464, loss 0.170106, accuracy 0.941406, precision 0.9428571428571428, recall 0.9705882352941176
2019-03-18T23:17:14.866389: step 465, loss 0.122636, accuracy 0.957031, precision 0.9683544303797469, recall 0.9622641509433962
2019-03-18T23:17:16.218819: step 466, loss 0.112715, accuracy 0.980469, precision 0.9696969696969697, recall 1.0
2019-03-18T23:17:17.395761: step 467, loss 0.126556, accuracy 0.957031, precision 0.9757575757575757, recall 0.9583333333333334
2019-03-18T23:17:18.690890: step 468, loss 0.120213, accuracy 0.957031, precision 0.9781420765027322, recall 0.9623655913978495
2019-03-18T23:17:20.166946: step 469, loss 0.114005, accuracy 0.957031, precision 0.9590643274853801, recall 0.9761904761904762
2019-03-18T23:17:21.517791: step 470, loss 0.126143, accuracy 0.949219, precision 0.9811320754716981, recall 0.9397590361445783
2019-03-18T23:17:22.613901: step 471, loss 0.134341, accuracy 0.941406, precision 0.9565217391304348, recall 0.9506172839506173
2019-03-18T23:17:23.672095: step 472, loss 0.101614, accuracy 0.976562, precision 0.9876543209876543, recall 0.975609756097561
2019-03-18T23:17:24.933723: step 473, loss 0.126527, accuracy 0.953125, precision 0.9693251533742331, recall 0.9575757575757575
2019-03-18T23:17:25.954253: step 474, loss 0.104695, accuracy 0.960938, precision 0.9661016949152542, recall 0.9771428571428571
2019-03-18T23:17:27.079323: step 475, loss 0.112859, accuracy 0.96875, precision 0.9757575757575757, recall 0.9757575757575757
2019-03-18T23:17:28.489138: step 476, loss 0.115307, accuracy 0.957031, precision 0.976878612716763, recall 0.9602272727272727
2019-03-18T23:17:29.940261: step 477, loss 0.140864, accuracy 0.949219, precision 0.9580838323353293, recall 0.963855421686747
2019-03-18T23:17:30.957542: step 478, loss 0.112938, accuracy 0.972656, precision 0.968944099378882, recall 0.9873417721518988
2019-03-18T23:17:32.165843: step 479, loss 0.110154, accuracy 0.960938, precision 0.9590643274853801, recall 0.9820359281437125
2019-03-18T23:17:33.435451: step 480, loss 0.128319, accuracy 0.941406, precision 0.9542857142857143, recall 0.9597701149425287
2019-03-18T23:17:34.081724: step 481, loss 0.0672233, accuracy 1, precision 1.0, recall 1.0
2019-03-18T23:17:35.309444: step 482, loss 0.101631, accuracy 0.976562, precision 0.9877300613496932, recall 0.9757575757575757
2019-03-18T23:17:36.486346: step 483, loss 0.112494, accuracy 0.960938, precision 0.9771428571428571, recall 0.9661016949152542
2019-03-18T23:17:37.519103: step 484, loss 0.146734, accuracy 0.941406, precision 0.9696969696969697, recall 0.9411764705882353
2019-03-18T23:17:38.579265: step 485, loss 0.122549, accuracy 0.972656, precision 0.9876543209876543, recall 0.9696969696969697
2019-03-18T23:17:39.785071: step 486, loss 0.128519, accuracy 0.941406, precision 0.95625, recall 0.9503105590062112
2019-03-18T23:17:41.023788: step 487, loss 0.153032, accuracy 0.945312, precision 0.948051948051948, recall 0.9605263157894737
2019-03-18T23:17:42.316878: step 488, loss 0.0996092, accuracy 0.957031, precision 0.9404761904761905, recall 0.9937106918238994
2019-03-18T23:17:43.701258: step 489, loss 0.107562, accuracy 0.957031, precision 0.9325153374233128, recall 1.0
2019-03-18T23:17:45.747820: step 490, loss 0.114117, accuracy 0.957031, precision 0.9565217391304348, recall 0.9746835443037974
2019-03-18T23:17:47.405900: step 491, loss 0.132701, accuracy 0.957031, precision 0.967032967032967, recall 0.9723756906077348
2019-03-18T23:17:49.054006: step 492, loss 0.0811077, accuracy 0.984375, precision 0.9825581395348837, recall 0.9941176470588236
2019-03-18T23:17:50.709115: step 493, loss 0.101936, accuracy 0.949219, precision 0.9723756906077348, recall 0.9565217391304348
2019-03-18T23:17:52.491353: step 494, loss 0.128848, accuracy 0.941406, precision 0.9754601226993865, recall 0.9352941176470588
2019-03-18T23:17:53.932502: step 495, loss 0.100048, accuracy 0.976562, precision 0.9764705882352941, recall 0.9880952380952381
2019-03-18T23:17:55.486349: step 496, loss 0.106294, accuracy 0.957031, precision 0.9883040935672515, recall 0.949438202247191
2019-03-18T23:17:57.044187: step 497, loss 0.125487, accuracy 0.957031, precision 0.9819277108433735, recall 0.9532163742690059
2019-03-18T23:17:58.503288: step 498, loss 0.116555, accuracy 0.949219, precision 0.9657142857142857, recall 0.9602272727272727
2019-03-18T23:17:59.760927: step 499, loss 0.13128, accuracy 0.957031, precision 0.9757575757575757, recall 0.9583333333333334
2019-03-18T23:18:00.912850: step 500, loss 0.118052, accuracy 0.9375, precision 0.9375, recall 0.9615384615384616

Evaluation:
[[1391  211]
 [ 214  497]]
2019-03-18T23:18:02.970352: step 500, loss 0.461983, accuracy 0.816256, precision 0.8682896379525593, recall 0.8666666666666667

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968444\checkpoints\model-500


Process finished with exit code 0
