"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
Italian English
GloVe model loaded
Pretrained Embedding: GloVe
Italian: False
Loading data...
11566
Max Document length: 37
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:98: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 1
Train/Dev split: 9253/2313
2019-03-18 23:06:04.340639: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
35
34
33
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:485: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968369

2019-03-18T23:06:20.339077: step 1, loss 8.62522, accuracy 0.347656, precision 0.017964071856287425, recall 0.5
2019-03-18T23:06:22.314798: step 2, loss 2.73119, accuracy 0.53125, precision 0.6368715083798883, recall 0.6745562130177515
2019-03-18T23:06:24.041199: step 3, loss 5.53984, accuracy 0.597656, precision 0.9743589743589743, recall 0.6055776892430279
2019-03-18T23:06:25.411525: step 4, loss 6.01678, accuracy 0.628906, precision 0.9937888198757764, recall 0.6299212598425197
2019-03-18T23:06:26.871625: step 5, loss 5.1641, accuracy 0.625, precision 0.9570552147239264, recall 0.636734693877551
2019-03-18T23:06:28.213037: step 6, loss 3.07759, accuracy 0.605469, precision 0.896969696969697, recall 0.6379310344827587
2019-03-18T23:06:29.543482: step 7, loss 1.94207, accuracy 0.605469, precision 0.7100591715976331, recall 0.6976744186046512
2019-03-18T23:06:30.787164: step 8, loss 2.94866, accuracy 0.507812, precision 0.45555555555555555, recall 0.7454545454545455
2019-03-18T23:06:32.019864: step 9, loss 3.28606, accuracy 0.457031, precision 0.29545454545454547, recall 0.7761194029850746
2019-03-18T23:06:33.241601: step 10, loss 2.90599, accuracy 0.476562, precision 0.3674698795180723, recall 0.6777777777777778
2019-03-18T23:06:34.447377: step 11, loss 2.5034, accuracy 0.515625, precision 0.4520547945205479, recall 0.6
2019-03-18T23:06:35.598302: step 12, loss 2.09444, accuracy 0.628906, precision 0.6763005780346821, recall 0.75
2019-03-18T23:06:36.766182: step 13, loss 2.359, accuracy 0.609375, precision 0.7654320987654321, recall 0.6666666666666666
2019-03-18T23:06:37.937053: step 14, loss 2.22037, accuracy 0.683594, precision 0.8563218390804598, recall 0.7268292682926829
2019-03-18T23:06:39.079002: step 15, loss 2.46504, accuracy 0.6875, precision 0.9058823529411765, recall 0.7064220183486238
2019-03-18T23:06:40.214966: step 16, loss 2.25586, accuracy 0.691406, precision 0.896969696969697, recall 0.7047619047619048
2019-03-18T23:06:41.349937: step 17, loss 2.01685, accuracy 0.671875, precision 0.8705882352941177, recall 0.7047619047619048
2019-03-18T23:06:42.461961: step 18, loss 1.7354, accuracy 0.660156, precision 0.8170731707317073, recall 0.7015706806282722
2019-03-18T23:06:43.589947: step 19, loss 1.70465, accuracy 0.660156, precision 0.7300613496932515, recall 0.7345679012345679
2019-03-18T23:06:44.668068: step 20, loss 1.9022, accuracy 0.628906, precision 0.6428571428571429, recall 0.7552447552447552
2019-03-18T23:06:45.887812: step 21, loss 1.78243, accuracy 0.640625, precision 0.6646706586826348, recall 0.7551020408163265
2019-03-18T23:06:47.031750: step 22, loss 2.27763, accuracy 0.589844, precision 0.535031847133758, recall 0.7241379310344828
2019-03-18T23:06:48.230546: step 23, loss 1.48177, accuracy 0.679688, precision 0.6337209302325582, recall 0.8515625
2019-03-18T23:06:49.289716: step 24, loss 1.35731, accuracy 0.6875, precision 0.740506329113924, recall 0.75
2019-03-18T23:06:50.346891: step 25, loss 1.46543, accuracy 0.683594, precision 0.7564102564102564, recall 0.7329192546583851
2019-03-18T23:06:51.427008: step 26, loss 1.60255, accuracy 0.691406, precision 0.7679558011049724, recall 0.7897727272727273
2019-03-18T23:06:52.523076: step 27, loss 1.54415, accuracy 0.695312, precision 0.8034682080924855, recall 0.7595628415300546
2019-03-18T23:06:53.617153: step 28, loss 1.42631, accuracy 0.707031, precision 0.8011695906432749, recall 0.7696629213483146
2019-03-18T23:06:54.745138: step 29, loss 1.77286, accuracy 0.707031, precision 0.8159509202453987, recall 0.7471910112359551
2019-03-18T23:06:55.871131: step 30, loss 1.47094, accuracy 0.679688, precision 0.8385093167701864, recall 0.7068062827225131
2019-03-18T23:06:56.968200: step 31, loss 1.3666, accuracy 0.679688, precision 0.8132530120481928, recall 0.7258064516129032
2019-03-18T23:06:57.895720: step 32, loss 1.41544, accuracy 0.71875, precision 0.8433734939759037, recall 0.7526881720430108
2019-03-18T23:06:58.939930: step 33, loss 1.09526, accuracy 0.699219, precision 0.7411764705882353, recall 0.7924528301886793
2019-03-18T23:07:00.158673: step 34, loss 1.56775, accuracy 0.664062, precision 0.7017543859649122, recall 0.7741935483870968
2019-03-18T23:07:01.265715: step 35, loss 1.46766, accuracy 0.675781, precision 0.6931818181818182, recall 0.8079470198675497
2019-03-18T23:07:02.340842: step 36, loss 1.23832, accuracy 0.703125, precision 0.703030303030303, recall 0.8111888111888111
2019-03-18T23:07:02.960187: step 37, loss 1.80085, accuracy 0.594595, precision 0.7272727272727273, recall 0.64
2019-03-18T23:07:03.974478: step 38, loss 1.30058, accuracy 0.691406, precision 0.7607361963190185, recall 0.7560975609756098
2019-03-18T23:07:05.035642: step 39, loss 1.11691, accuracy 0.726562, precision 0.7682926829268293, recall 0.7974683544303798
2019-03-18T23:07:06.207510: step 40, loss 1.18863, accuracy 0.707031, precision 0.7687861271676301, recall 0.7916666666666666
2019-03-18T23:07:07.260695: step 41, loss 1.22657, accuracy 0.703125, precision 0.8271604938271605, recall 0.7362637362637363
2019-03-18T23:07:08.345797: step 42, loss 1.35929, accuracy 0.695312, precision 0.8023952095808383, recall 0.7486033519553073
2019-03-18T23:07:09.491734: step 43, loss 1.13877, accuracy 0.71875, precision 0.7962962962962963, recall 0.7678571428571429
2019-03-18T23:07:10.546914: step 44, loss 1.1795, accuracy 0.703125, precision 0.7961783439490446, recall 0.7396449704142012
2019-03-18T23:07:11.597108: step 45, loss 1.04625, accuracy 0.6875, precision 0.7337278106508875, recall 0.779874213836478
2019-03-18T23:07:12.658273: step 46, loss 1.16651, accuracy 0.691406, precision 0.7425149700598802, recall 0.775
2019-03-18T23:07:13.625688: step 47, loss 0.946603, accuracy 0.757812, precision 0.8571428571428571, recall 0.7912087912087912
2019-03-18T23:07:14.809524: step 48, loss 1.05425, accuracy 0.722656, precision 0.7469879518072289, recall 0.8104575163398693
2019-03-18T23:07:15.880661: step 49, loss 0.805928, accuracy 0.78125, precision 0.8191489361702128, recall 0.875
2019-03-18T23:07:17.091426: step 50, loss 1.12641, accuracy 0.722656, precision 0.8, recall 0.7664670658682635
2019-03-18T23:07:18.291220: step 51, loss 1.02119, accuracy 0.726562, precision 0.7784090909090909, recall 0.8154761904761905
2019-03-18T23:07:19.376321: step 52, loss 0.96434, accuracy 0.722656, precision 0.8197674418604651, recall 0.7790055248618785
2019-03-18T23:07:20.491341: step 53, loss 0.953183, accuracy 0.707031, precision 0.7833333333333333, recall 0.7966101694915254
2019-03-18T23:07:21.687146: step 54, loss 1.122, accuracy 0.703125, precision 0.864516129032258, recall 0.708994708994709
2019-03-18T23:07:22.861010: step 55, loss 0.793978, accuracy 0.777344, precision 0.893491124260355, recall 0.7947368421052632
2019-03-18T23:07:23.989993: step 56, loss 0.797233, accuracy 0.785156, precision 0.9122807017543859, recall 0.7959183673469388
2019-03-18T23:07:25.145903: step 57, loss 0.892833, accuracy 0.753906, precision 0.8109756097560976, recall 0.806060606060606
2019-03-18T23:07:26.248956: step 58, loss 0.852174, accuracy 0.746094, precision 0.774390243902439, recall 0.8193548387096774
2019-03-18T23:07:27.391902: step 59, loss 0.787941, accuracy 0.734375, precision 0.7261904761904762, recall 0.8472222222222222
2019-03-18T23:07:28.476007: step 60, loss 0.75438, accuracy 0.738281, precision 0.7654320987654321, recall 0.8104575163398693
2019-03-18T23:07:29.779522: step 61, loss 0.792038, accuracy 0.71875, precision 0.75, recall 0.8164556962025317
2019-03-18T23:07:31.113956: step 62, loss 0.753842, accuracy 0.710938, precision 0.7640449438202247, recall 0.8095238095238095
2019-03-18T23:07:32.541142: step 63, loss 0.970848, accuracy 0.734375, precision 0.782608695652174, recall 0.7924528301886793
2019-03-18T23:07:34.047118: step 64, loss 0.75604, accuracy 0.746094, precision 0.9019607843137255, recall 0.7340425531914894
2019-03-18T23:07:35.531154: step 65, loss 0.880732, accuracy 0.75, precision 0.8908045977011494, recall 0.775
2019-03-18T23:07:36.945374: step 66, loss 0.776214, accuracy 0.730469, precision 0.8152866242038217, recall 0.7619047619047619
2019-03-18T23:07:38.536124: step 67, loss 0.671935, accuracy 0.816406, precision 0.8874172185430463, recall 0.8170731707317073
2019-03-18T23:07:40.059054: step 68, loss 0.874948, accuracy 0.742188, precision 0.7710843373493976, recall 0.8205128205128205
2019-03-18T23:07:41.708646: step 69, loss 0.697066, accuracy 0.761719, precision 0.8192771084337349, recall 0.8143712574850299
2019-03-18T23:07:43.093946: step 70, loss 0.711103, accuracy 0.777344, precision 0.7857142857142857, recall 0.8627450980392157
2019-03-18T23:07:44.470266: step 71, loss 0.873391, accuracy 0.710938, precision 0.7289156626506024, recall 0.8066666666666666
2019-03-18T23:07:45.911415: step 72, loss 0.928728, accuracy 0.710938, precision 0.7337278106508875, recall 0.8104575163398693
2019-03-18T23:07:47.422378: step 73, loss 0.767476, accuracy 0.75, precision 0.8054054054054054, recall 0.8418079096045198
2019-03-18T23:07:47.980885: step 74, loss 0.641592, accuracy 0.756757, precision 0.7894736842105263, recall 0.75
2019-03-18T23:07:49.484867: step 75, loss 0.748124, accuracy 0.765625, precision 0.8793103448275862, recall 0.796875
2019-03-18T23:07:50.807332: step 76, loss 0.740743, accuracy 0.761719, precision 0.9064327485380117, recall 0.775
2019-03-18T23:07:52.177671: step 77, loss 0.777287, accuracy 0.777344, precision 0.8895348837209303, recall 0.8010471204188482
2019-03-18T23:07:53.637770: step 78, loss 0.541124, accuracy 0.804688, precision 0.8987341772151899, recall 0.8068181818181818
2019-03-18T23:07:55.203586: step 79, loss 0.788387, accuracy 0.742188, precision 0.7888198757763976, recall 0.7987421383647799
2019-03-18T23:07:56.655705: step 80, loss 0.708742, accuracy 0.761719, precision 0.7413793103448276, recall 0.8896551724137931
2019-03-18T23:07:58.120793: step 81, loss 0.608419, accuracy 0.765625, precision 0.7791411042944786, recall 0.8410596026490066
2019-03-18T23:07:59.600835: step 82, loss 0.858245, accuracy 0.742188, precision 0.7604790419161677, recall 0.8300653594771242
2019-03-18T23:08:01.089858: step 83, loss 0.756713, accuracy 0.742188, precision 0.7514792899408284, recall 0.8410596026490066
2019-03-18T23:08:02.619768: step 84, loss 0.602638, accuracy 0.769531, precision 0.8186813186813187, recall 0.8514285714285714
2019-03-18T23:08:03.987114: step 85, loss 0.576919, accuracy 0.800781, precision 0.8735632183908046, recall 0.8397790055248618
2019-03-18T23:08:05.428262: step 86, loss 0.59226, accuracy 0.808594, precision 0.9127906976744186, recall 0.8219895287958116
2019-03-18T23:08:06.762696: step 87, loss 0.68859, accuracy 0.78125, precision 0.8982035928143712, recall 0.7936507936507936
2019-03-18T23:08:08.099128: step 88, loss 0.612874, accuracy 0.800781, precision 0.9146341463414634, recall 0.8021390374331551
2019-03-18T23:08:09.501384: step 89, loss 0.563958, accuracy 0.808594, precision 0.8963414634146342, recall 0.8212290502793296
2019-03-18T23:08:10.829829: step 90, loss 0.598684, accuracy 0.773438, precision 0.8580246913580247, recall 0.7988505747126436
2019-03-18T23:08:12.133346: step 91, loss 0.638593, accuracy 0.757812, precision 0.774390243902439, recall 0.8355263157894737
2019-03-18T23:08:13.565519: step 92, loss 0.540705, accuracy 0.808594, precision 0.7916666666666666, recall 0.9047619047619048
2019-03-18T23:08:14.977745: step 93, loss 0.577091, accuracy 0.773438, precision 0.7421383647798742, recall 0.8740740740740741
2019-03-18T23:08:16.302206: step 94, loss 0.49973, accuracy 0.828125, precision 0.8505747126436781, recall 0.891566265060241
2019-03-18T23:08:17.663569: step 95, loss 0.495374, accuracy 0.804688, precision 0.891566265060241, recall 0.8222222222222222
2019-03-18T23:08:19.163561: step 96, loss 0.669359, accuracy 0.800781, precision 0.8959537572254336, recall 0.824468085106383
2019-03-18T23:08:20.711425: step 97, loss 0.739189, accuracy 0.742188, precision 0.8805031446540881, recall 0.7486631016042781
2019-03-18T23:08:22.252312: step 98, loss 0.587022, accuracy 0.78125, precision 0.8518518518518519, recall 0.8117647058823529
2019-03-18T23:08:23.806154: step 99, loss 0.630764, accuracy 0.789062, precision 0.8647058823529412, recall 0.8258426966292135
2019-03-18T23:08:25.268249: step 100, loss 0.66438, accuracy 0.789062, precision 0.8622754491017964, recall 0.8228571428571428
2019-03-18T23:08:26.844036: step 101, loss 0.609492, accuracy 0.792969, precision 0.8275862068965517, recall 0.8622754491017964
2019-03-18T23:08:28.530531: step 102, loss 0.667489, accuracy 0.765625, precision 0.8095238095238095, recall 0.8292682926829268
2019-03-18T23:08:30.261903: step 103, loss 0.75171, accuracy 0.746094, precision 0.8011695906432749, recall 0.8154761904761905
2019-03-18T23:08:31.694078: step 104, loss 0.63342, accuracy 0.796875, precision 0.8802395209580839, recall 0.8212290502793296
2019-03-18T23:08:33.300783: step 105, loss 0.579962, accuracy 0.785156, precision 0.8562091503267973, recall 0.7987804878048781
2019-03-18T23:08:35.149843: step 106, loss 0.595666, accuracy 0.773438, precision 0.8376623376623377, recall 0.7962962962962963
2019-03-18T23:08:36.711669: step 107, loss 0.557423, accuracy 0.785156, precision 0.844311377245509, recall 0.8294117647058824
2019-03-18T23:08:38.201690: step 108, loss 0.518649, accuracy 0.792969, precision 0.7844311377245509, recall 0.8851351351351351
2019-03-18T23:08:39.778474: step 109, loss 0.60504, accuracy 0.757812, precision 0.806060606060606, recall 0.8159509202453987
2019-03-18T23:08:41.215634: step 110, loss 0.558085, accuracy 0.78125, precision 0.8554216867469879, recall 0.8160919540229885
2019-03-18T23:08:42.041427: step 111, loss 0.317941, accuracy 0.918919, precision 1.0, recall 0.8888888888888888
2019-03-18T23:08:43.782774: step 112, loss 0.498245, accuracy 0.808594, precision 0.8742138364779874, recall 0.8273809523809523
2019-03-18T23:08:45.500185: step 113, loss 0.634832, accuracy 0.777344, precision 0.808641975308642, recall 0.8343949044585988
2019-03-18T23:08:47.122849: step 114, loss 0.496656, accuracy 0.820312, precision 0.8372093023255814, recall 0.8888888888888888
2019-03-18T23:08:48.869182: step 115, loss 0.556422, accuracy 0.78125, precision 0.8376623376623377, recall 0.80625
2019-03-18T23:08:50.496835: step 116, loss 0.481455, accuracy 0.824219, precision 0.8888888888888888, recall 0.8648648648648649
2019-03-18T23:08:52.028739: step 117, loss 0.569396, accuracy 0.804688, precision 0.8838709677419355, recall 0.8106508875739645
2019-03-18T23:08:53.451937: step 118, loss 0.522277, accuracy 0.796875, precision 0.8333333333333334, recall 0.8630952380952381
2019-03-18T23:08:55.213231: step 119, loss 0.380539, accuracy 0.847656, precision 0.8780487804878049, recall 0.8834355828220859
2019-03-18T23:08:56.794006: step 120, loss 0.505889, accuracy 0.84375, precision 0.9177215189873418, recall 0.8430232558139535
2019-03-18T23:08:58.212216: step 121, loss 0.525887, accuracy 0.804688, precision 0.86875, recall 0.8273809523809523
2019-03-18T23:08:59.779030: step 122, loss 0.403177, accuracy 0.855469, precision 0.8860759493670886, recall 0.8805031446540881
2019-03-18T23:09:01.193251: step 123, loss 0.478877, accuracy 0.808594, precision 0.8275862068965517, recall 0.8834355828220859
2019-03-18T23:09:02.640384: step 124, loss 0.490243, accuracy 0.804688, precision 0.7965116279069767, recall 0.9013157894736842
2019-03-18T23:09:04.275016: step 125, loss 0.502223, accuracy 0.800781, precision 0.8265895953757225, recall 0.8719512195121951
2019-03-18T23:09:05.735114: step 126, loss 0.469237, accuracy 0.824219, precision 0.8881987577639752, recall 0.8411764705882353
2019-03-18T23:09:07.321874: step 127, loss 0.366398, accuracy 0.832031, precision 0.9212121212121213, recall 0.8351648351648352
2019-03-18T23:09:09.079178: step 128, loss 0.401666, accuracy 0.828125, precision 0.930635838150289, recall 0.8341968911917098
2019-03-18T23:09:10.487415: step 129, loss 0.464552, accuracy 0.839844, precision 0.9261363636363636, recall 0.8534031413612565
2019-03-18T23:09:11.902665: step 130, loss 0.454582, accuracy 0.824219, precision 0.8787878787878788, recall 0.8529411764705882
2019-03-18T23:09:13.261035: step 131, loss 0.388168, accuracy 0.828125, precision 0.8511904761904762, recall 0.8827160493827161
2019-03-18T23:09:14.772041: step 132, loss 0.48372, accuracy 0.8125, precision 0.8383233532934131, recall 0.8695652173913043
2019-03-18T23:09:16.311927: step 133, loss 0.444141, accuracy 0.804688, precision 0.8529411764705882, recall 0.8529411764705882
2019-03-18T23:09:17.788980: step 134, loss 0.503083, accuracy 0.8125, precision 0.8511904761904762, recall 0.8614457831325302
2019-03-18T23:09:19.339837: step 135, loss 0.386655, accuracy 0.824219, precision 0.8511904761904762, recall 0.8773006134969326
2019-03-18T23:09:20.751603: step 136, loss 0.418516, accuracy 0.8125, precision 0.8258426966292135, recall 0.8963414634146342
2019-03-18T23:09:22.295988: step 137, loss 0.411872, accuracy 0.847656, precision 0.8855421686746988, recall 0.8802395209580839
2019-03-18T23:09:23.689314: step 138, loss 0.414167, accuracy 0.84375, precision 0.9090909090909091, recall 0.8695652173913043
2019-03-18T23:09:25.151408: step 139, loss 0.453191, accuracy 0.835938, precision 0.9034090909090909, recall 0.8641304347826086
2019-03-18T23:09:26.801526: step 140, loss 0.497702, accuracy 0.835938, precision 0.9281437125748503, recall 0.8378378378378378
2019-03-18T23:09:28.422196: step 141, loss 0.472972, accuracy 0.792969, precision 0.891566265060241, recall 0.8087431693989071
2019-03-18T23:09:29.921190: step 142, loss 0.448788, accuracy 0.84375, precision 0.8901734104046243, recall 0.88
2019-03-18T23:09:31.374309: step 143, loss 0.365239, accuracy 0.832031, precision 0.8971428571428571, recall 0.8626373626373627
2019-03-18T23:09:32.645924: step 144, loss 0.568346, accuracy 0.808594, precision 0.8376623376623377, recall 0.8431372549019608
2019-03-18T23:09:34.142923: step 145, loss 0.386035, accuracy 0.820312, precision 0.8466257668711656, recall 0.8679245283018868
2019-03-18T23:09:35.671840: step 146, loss 0.428409, accuracy 0.835938, precision 0.8535031847133758, recall 0.8758169934640523
2019-03-18T23:09:37.160378: step 147, loss 0.449075, accuracy 0.804688, precision 0.8641975308641975, recall 0.8333333333333334
2019-03-18T23:09:37.879517: step 148, loss 0.420076, accuracy 0.864865, precision 0.9130434782608695, recall 0.875
2019-03-18T23:09:39.456304: step 149, loss 0.44175, accuracy 0.832031, precision 0.8285714285714286, recall 0.9177215189873418
2019-03-18T23:09:40.930384: step 150, loss 0.37134, accuracy 0.820312, precision 0.847953216374269, recall 0.8787878787878788
2019-03-18T23:09:42.500190: step 151, loss 0.427114, accuracy 0.8125, precision 0.8625, recall 0.8414634146341463
2019-03-18T23:09:43.994786: step 152, loss 0.311019, accuracy 0.84375, precision 0.8736263736263736, recall 0.9034090909090909
2019-03-18T23:09:45.622495: step 153, loss 0.343375, accuracy 0.878906, precision 0.9333333333333333, recall 0.8850574712643678
2019-03-18T23:09:47.238696: step 154, loss 0.415258, accuracy 0.808594, precision 0.8734939759036144, recall 0.838150289017341
2019-03-18T23:09:48.813997: step 155, loss 0.350914, accuracy 0.851562, precision 0.9060773480662984, recall 0.8864864864864865
2019-03-18T23:09:50.268112: step 156, loss 0.44217, accuracy 0.820312, precision 0.9038461538461539, recall 0.8197674418604651
2019-03-18T23:09:51.780128: step 157, loss 0.369861, accuracy 0.84375, precision 0.9235294117647059, recall 0.8532608695652174
2019-03-18T23:09:53.600317: step 158, loss 0.4341, accuracy 0.824219, precision 0.8682634730538922, recall 0.8630952380952381
2019-03-18T23:09:55.059951: step 159, loss 0.374284, accuracy 0.84375, precision 0.8944099378881988, recall 0.8622754491017964
2019-03-18T23:09:56.683104: step 160, loss 0.430927, accuracy 0.796875, precision 0.8136645962732919, recall 0.8562091503267973
2019-03-18T23:09:58.339254: step 161, loss 0.463085, accuracy 0.789062, precision 0.8192090395480226, recall 0.8682634730538922
2019-03-18T23:09:59.822870: step 162, loss 0.398614, accuracy 0.8125, precision 0.7852760736196319, recall 0.9078014184397163
2019-03-18T23:10:01.366744: step 163, loss 0.424886, accuracy 0.816406, precision 0.8588235294117647, recall 0.863905325443787
2019-03-18T23:10:02.868760: step 164, loss 0.304108, accuracy 0.875, precision 0.9074074074074074, recall 0.8963414634146342
2019-03-18T23:10:04.241113: step 165, loss 0.371049, accuracy 0.832031, precision 0.9329268292682927, recall 0.827027027027027
2019-03-18T23:10:05.797526: step 166, loss 0.380008, accuracy 0.839844, precision 0.9084967320261438, recall 0.8373493975903614
2019-03-18T23:10:07.441188: step 167, loss 0.32473, accuracy 0.871094, precision 0.9137931034482759, recall 0.8983050847457628
2019-03-18T23:10:08.892340: step 168, loss 0.438775, accuracy 0.824219, precision 0.9112426035502958, recall 0.8369565217391305
2019-03-18T23:10:10.355431: step 169, loss 0.435397, accuracy 0.808594, precision 0.8841463414634146, recall 0.8285714285714286
2019-03-18T23:10:11.878915: step 170, loss 0.316774, accuracy 0.863281, precision 0.9005847953216374, recall 0.8953488372093024
2019-03-18T23:10:13.651759: step 171, loss 0.377232, accuracy 0.859375, precision 0.8539325842696629, recall 0.9382716049382716
2019-03-18T23:10:15.108375: step 172, loss 0.393632, accuracy 0.835938, precision 0.8424242424242424, recall 0.896774193548387
2019-03-18T23:10:16.562512: step 173, loss 0.338216, accuracy 0.867188, precision 0.8773006134969326, recall 0.910828025477707
2019-03-18T23:10:17.936841: step 174, loss 0.405064, accuracy 0.851562, precision 0.9119496855345912, recall 0.8579881656804734
2019-03-18T23:10:19.374997: step 175, loss 0.458691, accuracy 0.816406, precision 0.9085365853658537, recall 0.8232044198895028
2019-03-18T23:10:20.949813: step 176, loss 0.424733, accuracy 0.804688, precision 0.8827160493827161, recall 0.8218390804597702
2019-03-18T23:10:22.403929: step 177, loss 0.34929, accuracy 0.867188, precision 0.9166666666666666, recall 0.8850574712643678
2019-03-18T23:10:23.971250: step 178, loss 0.282849, accuracy 0.871094, precision 0.8895705521472392, recall 0.90625
2019-03-18T23:10:25.432343: step 179, loss 0.245046, accuracy 0.886719, precision 0.9316770186335404, recall 0.8928571428571429
2019-03-18T23:10:26.955352: step 180, loss 0.342404, accuracy 0.871094, precision 0.9166666666666666, recall 0.8901734104046243
2019-03-18T23:10:28.514186: step 181, loss 0.353668, accuracy 0.859375, precision 0.893491124260355, recall 0.893491124260355
2019-03-18T23:10:29.994768: step 182, loss 0.395902, accuracy 0.847656, precision 0.8902439024390244, recall 0.874251497005988
2019-03-18T23:10:31.714266: step 183, loss 0.326754, accuracy 0.839844, precision 0.8538011695906432, recall 0.9012345679012346
2019-03-18T23:10:33.228220: step 184, loss 0.327115, accuracy 0.847656, precision 0.8908045977011494, recall 0.8857142857142857
2019-03-18T23:10:34.061992: step 185, loss 0.253144, accuracy 0.918919, precision 0.9523809523809523, recall 0.9090909090909091
2019-03-18T23:10:35.505155: step 186, loss 0.323828, accuracy 0.859375, precision 0.8789808917197452, recall 0.8903225806451613
2019-03-18T23:10:36.925911: step 187, loss 0.276293, accuracy 0.875, precision 0.9161676646706587, recall 0.8947368421052632
2019-03-18T23:10:38.248376: step 188, loss 0.339903, accuracy 0.835938, precision 0.8928571428571429, recall 0.8620689655172413
2019-03-18T23:10:39.751420: step 189, loss 0.318557, accuracy 0.878906, precision 0.9056603773584906, recall 0.9
2019-03-18T23:10:41.390555: step 190, loss 0.371422, accuracy 0.835938, precision 0.8726114649681529, recall 0.8616352201257862
2019-03-18T23:10:43.041700: step 191, loss 0.288632, accuracy 0.882812, precision 0.8988095238095238, recall 0.9207317073170732
2019-03-18T23:10:44.632471: step 192, loss 0.27692, accuracy 0.898438, precision 0.95, recall 0.9095744680851063
2019-03-18T23:10:46.355408: step 193, loss 0.417808, accuracy 0.800781, precision 0.8404907975460123, recall 0.845679012345679
2019-03-18T23:10:48.226090: step 194, loss 0.304004, accuracy 0.882812, precision 0.9320987654320988, recall 0.888235294117647
2019-03-18T23:10:49.903607: step 195, loss 0.286759, accuracy 0.871094, precision 0.9182389937106918, recall 0.8795180722891566
2019-03-18T23:10:51.490367: step 196, loss 0.280532, accuracy 0.894531, precision 0.9044943820224719, recall 0.9415204678362573
2019-03-18T23:10:53.126504: step 197, loss 0.287723, accuracy 0.863281, precision 0.8853503184713376, recall 0.8910256410256411
2019-03-18T23:10:55.020445: step 198, loss 0.26105, accuracy 0.910156, precision 0.950920245398773, recall 0.9117647058823529
2019-03-18T23:10:56.769801: step 199, loss 0.280035, accuracy 0.882812, precision 0.937888198757764, recall 0.8830409356725146
2019-03-18T23:10:58.373514: step 200, loss 0.282571, accuracy 0.882812, precision 0.8950617283950617, recall 0.9177215189873418
2019-03-18T23:11:00.042058: step 201, loss 0.272948, accuracy 0.894531, precision 0.9299363057324841, recall 0.9012345679012346
2019-03-18T23:11:01.564987: step 202, loss 0.285954, accuracy 0.863281, precision 0.8895705521472392, recall 0.8950617283950617
2019-03-18T23:11:02.833616: step 203, loss 0.295936, accuracy 0.882812, precision 0.9069767441860465, recall 0.9176470588235294
2019-03-18T23:11:04.396440: step 204, loss 0.328514, accuracy 0.863281, precision 0.896551724137931, recall 0.9017341040462428
2019-03-18T23:11:05.893443: step 205, loss 0.320521, accuracy 0.855469, precision 0.898876404494382, recall 0.8938547486033519
2019-03-18T23:11:07.508160: step 206, loss 0.317704, accuracy 0.890625, precision 0.9289940828402367, recall 0.9075144508670521
2019-03-18T23:11:09.037583: step 207, loss 0.32209, accuracy 0.839844, precision 0.9244186046511628, recall 0.8502673796791443
2019-03-18T23:11:10.446817: step 208, loss 0.33811, accuracy 0.824219, precision 0.9011627906976745, recall 0.8469945355191257
2019-03-18T23:11:11.882998: step 209, loss 0.281841, accuracy 0.882812, precision 0.888235294117647, recall 0.9320987654320988
2019-03-18T23:11:13.617918: step 210, loss 0.304596, accuracy 0.867188, precision 0.896969696969697, recall 0.896969696969697
2019-03-18T23:11:15.222635: step 211, loss 0.317518, accuracy 0.855469, precision 0.9006211180124224, recall 0.8734939759036144
2019-03-18T23:11:16.881199: step 212, loss 0.265522, accuracy 0.890625, precision 0.9239766081871345, recall 0.9132947976878613
2019-03-18T23:11:18.524318: step 213, loss 0.241464, accuracy 0.890625, precision 0.9395604395604396, recall 0.9095744680851063
2019-03-18T23:11:20.141016: step 214, loss 0.30301, accuracy 0.878906, precision 0.9171597633136095, recall 0.9011627906976745
2019-03-18T23:11:21.577234: step 215, loss 0.300757, accuracy 0.871094, precision 0.9101796407185628, recall 0.8941176470588236
2019-03-18T23:11:23.036453: step 216, loss 0.317474, accuracy 0.851562, precision 0.9011627906976745, recall 0.8806818181818182
2019-03-18T23:11:24.744398: step 217, loss 0.3769, accuracy 0.84375, precision 0.8902439024390244, recall 0.8690476190476191
2019-03-18T23:11:26.196573: step 218, loss 0.366161, accuracy 0.832031, precision 0.8855421686746988, recall 0.8596491228070176
2019-03-18T23:11:27.772361: step 219, loss 0.243986, accuracy 0.890625, precision 0.9333333333333333, recall 0.9005847953216374
2019-03-18T23:11:29.303294: step 220, loss 0.277343, accuracy 0.882812, precision 0.9122807017543859, recall 0.9122807017543859
2019-03-18T23:11:30.981010: step 221, loss 0.318321, accuracy 0.875, precision 0.9226190476190477, recall 0.8908045977011494
2019-03-18T23:11:31.669680: step 222, loss 0.460657, accuracy 0.783784, precision 0.8695652173913043, recall 0.8
2019-03-18T23:11:33.294914: step 223, loss 0.247394, accuracy 0.914062, precision 0.9235294117647059, recall 0.9457831325301205
2019-03-18T23:11:34.939521: step 224, loss 0.277822, accuracy 0.886719, precision 0.9024390243902439, recall 0.9192546583850931
2019-03-18T23:11:36.429047: step 225, loss 0.235315, accuracy 0.898438, precision 0.930635838150289, recall 0.92
2019-03-18T23:11:37.881674: step 226, loss 0.292936, accuracy 0.871094, precision 0.9121621621621622, recall 0.8709677419354839
2019-03-18T23:11:39.355736: step 227, loss 0.262541, accuracy 0.875, precision 0.8685714285714285, recall 0.9440993788819876
2019-03-18T23:11:40.960499: step 228, loss 0.240392, accuracy 0.890625, precision 0.9272727272727272, recall 0.9053254437869822
2019-03-18T23:11:42.358387: step 229, loss 0.281274, accuracy 0.863281, precision 0.8903225806451613, recall 0.8846153846153846
2019-03-18T23:11:43.893285: step 230, loss 0.299127, accuracy 0.863281, precision 0.9022988505747126, recall 0.8971428571428571
2019-03-18T23:11:45.427214: step 231, loss 0.261657, accuracy 0.898438, precision 0.96, recall 0.8983957219251337
2019-03-18T23:11:46.827546: step 232, loss 0.26945, accuracy 0.875, precision 0.898936170212766, recall 0.9285714285714286
2019-03-18T23:11:48.520608: step 233, loss 0.242595, accuracy 0.902344, precision 0.9602649006622517, recall 0.8841463414634146
2019-03-18T23:11:49.916909: step 234, loss 0.263727, accuracy 0.898438, precision 0.950920245398773, recall 0.8959537572254336
2019-03-18T23:11:51.313698: step 235, loss 0.317438, accuracy 0.878906, precision 0.9261744966442953, recall 0.8734177215189873
2019-03-18T23:11:52.763336: step 236, loss 0.317946, accuracy 0.851562, precision 0.8342541436464088, recall 0.949685534591195
2019-03-18T23:11:54.304727: step 237, loss 0.266347, accuracy 0.898438, precision 0.9074074074074074, recall 0.930379746835443
2019-03-18T23:11:55.858090: step 238, loss 0.241262, accuracy 0.90625, precision 0.9074074074074074, recall 0.9423076923076923
2019-03-18T23:11:57.487734: step 239, loss 0.232966, accuracy 0.914062, precision 0.9523809523809523, recall 0.9195402298850575
2019-03-18T23:11:59.065077: step 240, loss 0.227399, accuracy 0.921875, precision 0.9685534591194969, recall 0.9112426035502958
2019-03-18T23:12:00.822937: step 241, loss 0.261674, accuracy 0.898438, precision 0.9277777777777778, recall 0.9277777777777778
2019-03-18T23:12:02.206242: step 242, loss 0.338251, accuracy 0.839844, precision 0.9084507042253521, recall 0.821656050955414
2019-03-18T23:12:03.744140: step 243, loss 0.247321, accuracy 0.894531, precision 0.925, recall 0.9079754601226994
2019-03-18T23:12:05.244174: step 244, loss 0.243933, accuracy 0.894531, precision 0.9333333333333333, recall 0.9058823529411765
2019-03-18T23:12:06.608605: step 245, loss 0.244113, accuracy 0.886719, precision 0.8983050847457628, recall 0.9352941176470588
2019-03-18T23:12:07.872790: step 246, loss 0.234552, accuracy 0.902344, precision 0.9386503067484663, recall 0.9107142857142857
2019-03-18T23:12:09.432134: step 247, loss 0.199695, accuracy 0.921875, precision 0.9476744186046512, recall 0.9367816091954023
2019-03-18T23:12:10.640903: step 248, loss 0.27736, accuracy 0.882812, precision 0.9005524861878453, recall 0.9314285714285714
2019-03-18T23:12:12.091538: step 249, loss 0.270605, accuracy 0.894531, precision 0.90625, recall 0.9235668789808917
2019-03-18T23:12:13.416067: step 250, loss 0.251057, accuracy 0.910156, precision 0.9454545454545454, recall 0.9176470588235294

Evaluation:
[[1304  298]
 [ 598  113]]
2019-03-18T23:12:16.843141: step 250, loss 1.10585, accuracy 0.612624, precision 0.8139825218476904, recall 0.6855941114616193

2019-03-18 23:12:14.518709: W tensorflow/core/framework/allocator.cc:122] Allocation of 102697200 exceeds 10% of system memory.
2019-03-18 23:12:14.538498: W tensorflow/core/framework/allocator.cc:122] Allocation of 39080448 exceeds 10% of system memory.
2019-03-18 23:12:14.538497: W tensorflow/core/framework/allocator.cc:122] Allocation of 41448960 exceeds 10% of system memory.
2019-03-18 23:12:14.538497: W tensorflow/core/framework/allocator.cc:122] Allocation of 40264704 exceeds 10% of system memory.
Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968369\checkpoints\model-250

2019-03-18T23:12:19.286669: step 251, loss 0.221825, accuracy 0.90625, precision 0.9135802469135802, recall 0.9367088607594937
2019-03-18T23:12:20.824631: step 252, loss 0.276896, accuracy 0.859375, precision 0.8777777777777778, recall 0.9186046511627907
2019-03-18T23:12:22.507156: step 253, loss 0.293163, accuracy 0.894531, precision 0.9565217391304348, recall 0.8850574712643678
2019-03-18T23:12:23.967768: step 254, loss 0.225785, accuracy 0.933594, precision 0.9672131147540983, recall 0.9414893617021277
2019-03-18T23:12:25.568488: step 255, loss 0.212752, accuracy 0.90625, precision 0.9404761904761905, recall 0.9186046511627907
2019-03-18T23:12:27.171715: step 256, loss 0.256745, accuracy 0.902344, precision 0.9518072289156626, recall 0.9028571428571428
2019-03-18T23:12:28.662733: step 257, loss 0.303843, accuracy 0.894531, precision 0.927710843373494, recall 0.9112426035502958
2019-03-18T23:12:30.136793: step 258, loss 0.237764, accuracy 0.890625, precision 0.9166666666666666, recall 0.9166666666666666
2019-03-18T23:12:30.910724: step 259, loss 0.148539, accuracy 0.972973, precision 0.967741935483871, recall 1.0
2019-03-18T23:12:32.377314: step 260, loss 0.27121, accuracy 0.871094, precision 0.9447852760736196, recall 0.8651685393258427
2019-03-18T23:12:33.886789: step 261, loss 0.226052, accuracy 0.921875, precision 0.9342105263157895, recall 0.9342105263157895
2019-03-18T23:12:35.328494: step 262, loss 0.266307, accuracy 0.871094, precision 0.935672514619883, recall 0.8791208791208791
2019-03-18T23:12:36.698831: step 263, loss 0.207704, accuracy 0.917969, precision 0.9397590361445783, recall 0.9341317365269461
2019-03-18T23:12:38.253677: step 264, loss 0.202626, accuracy 0.921875, precision 0.935672514619883, recall 0.9467455621301775
2019-03-18T23:12:39.766652: step 265, loss 0.199378, accuracy 0.933594, precision 0.9664804469273743, recall 0.9402173913043478
2019-03-18T23:12:41.286231: step 266, loss 0.196524, accuracy 0.914062, precision 0.9629629629629629, recall 0.9069767441860465
2019-03-18T23:12:42.681502: step 267, loss 0.267653, accuracy 0.867188, precision 0.9308176100628931, recall 0.8654970760233918
2019-03-18T23:12:44.278768: step 268, loss 0.283943, accuracy 0.859375, precision 0.8959537572254336, recall 0.8959537572254336
2019-03-18T23:12:45.898441: step 269, loss 0.211885, accuracy 0.914062, precision 0.9212121212121213, recall 0.9440993788819876
2019-03-18T23:12:47.463259: step 270, loss 0.218626, accuracy 0.921875, precision 0.9363057324840764, recall 0.9363057324840764
2019-03-18T23:12:49.077017: step 271, loss 0.244309, accuracy 0.886719, precision 0.9190751445086706, recall 0.9137931034482759
2019-03-18T23:12:50.523153: step 272, loss 0.221043, accuracy 0.910156, precision 0.9141104294478528, recall 0.9430379746835443
2019-03-18T23:12:52.057567: step 273, loss 0.205743, accuracy 0.929688, precision 0.9570552147239264, recall 0.9341317365269461
2019-03-18T23:12:53.561065: step 274, loss 0.2702, accuracy 0.894531, precision 0.9202453987730062, recall 0.9146341463414634
2019-03-18T23:12:55.087011: step 275, loss 0.2396, accuracy 0.914062, precision 0.935672514619883, recall 0.935672514619883
2019-03-18T23:12:56.556197: step 276, loss 0.200113, accuracy 0.910156, precision 0.9322033898305084, recall 0.9375
2019-03-18T23:12:58.003842: step 277, loss 0.211167, accuracy 0.925781, precision 0.9244186046511628, recall 0.9636363636363636
2019-03-18T23:12:59.565669: step 278, loss 0.228358, accuracy 0.910156, precision 0.9526627218934911, recall 0.9147727272727273
2019-03-18T23:13:01.250192: step 279, loss 0.240468, accuracy 0.890625, precision 0.9252873563218391, recall 0.9147727272727273
2019-03-18T23:13:02.709293: step 280, loss 0.22635, accuracy 0.882812, precision 0.9358974358974359, recall 0.8795180722891566
2019-03-18T23:13:04.399399: step 281, loss 0.156687, accuracy 0.9375, precision 0.9542857142857143, recall 0.9542857142857143
2019-03-18T23:13:05.854022: step 282, loss 0.189753, accuracy 0.925781, precision 0.9625, recall 0.9221556886227545
2019-03-18T23:13:07.271235: step 283, loss 0.271839, accuracy 0.894531, precision 0.9314285714285714, recall 0.9157303370786517
2019-03-18T23:13:08.937295: step 284, loss 0.236605, accuracy 0.914062, precision 0.9272727272727272, recall 0.9386503067484663
2019-03-18T23:13:10.481679: step 285, loss 0.195029, accuracy 0.917969, precision 0.94, recall 0.9215686274509803
2019-03-18T23:13:11.963770: step 286, loss 0.204594, accuracy 0.914062, precision 0.9186046511627907, recall 0.9518072289156626
2019-03-18T23:13:13.459283: step 287, loss 0.215039, accuracy 0.902344, precision 0.9240506329113924, recall 0.9182389937106918
2019-03-18T23:13:14.855611: step 288, loss 0.184944, accuracy 0.933594, precision 0.9770114942528736, recall 0.9289617486338798
2019-03-18T23:13:16.441373: step 289, loss 0.227151, accuracy 0.882812, precision 0.9171974522292994, recall 0.8944099378881988
2019-03-18T23:13:17.914436: step 290, loss 0.17881, accuracy 0.925781, precision 0.9329608938547486, recall 0.9597701149425287
2019-03-18T23:13:19.485240: step 291, loss 0.188899, accuracy 0.933594, precision 0.9540229885057471, recall 0.9485714285714286
2019-03-18T23:13:20.958812: step 292, loss 0.226385, accuracy 0.898438, precision 0.9221556886227545, recall 0.9221556886227545
2019-03-18T23:13:22.422900: step 293, loss 0.198797, accuracy 0.921875, precision 0.9461077844311377, recall 0.9349112426035503
2019-03-18T23:13:23.837635: step 294, loss 0.221949, accuracy 0.914062, precision 0.9588235294117647, recall 0.9157303370786517
2019-03-18T23:13:25.168080: step 295, loss 0.230965, accuracy 0.890625, precision 0.9285714285714286, recall 0.9069767441860465
2019-03-18T23:13:26.054759: step 296, loss 0.265342, accuracy 0.918919, precision 0.9545454545454546, recall 0.9130434782608695
2019-03-18T23:13:27.494426: step 297, loss 0.208325, accuracy 0.917969, precision 0.9139072847682119, recall 0.9452054794520548
2019-03-18T23:13:28.935086: step 298, loss 0.21878, accuracy 0.90625, precision 0.8978494623655914, recall 0.9709302325581395
2019-03-18T23:13:30.335369: step 299, loss 0.208184, accuracy 0.933594, precision 0.9488636363636364, recall 0.9542857142857143
2019-03-18T23:13:31.969046: step 300, loss 0.215883, accuracy 0.902344, precision 0.9364161849710982, recall 0.9204545454545454
2019-03-18T23:13:33.471031: step 301, loss 0.25405, accuracy 0.878906, precision 0.9197530864197531, recall 0.8922155688622755
2019-03-18T23:13:35.011455: step 302, loss 0.178318, accuracy 0.921875, precision 0.9880239520958084, recall 0.9016393442622951
2019-03-18T23:13:36.609184: step 303, loss 0.261319, accuracy 0.890625, precision 0.9171974522292994, recall 0.9056603773584906
2019-03-18T23:13:38.119177: step 304, loss 0.237033, accuracy 0.890625, precision 0.9447852760736196, recall 0.8901734104046243
2019-03-18T23:13:39.639115: step 305, loss 0.187202, accuracy 0.929688, precision 0.9540229885057471, recall 0.9431818181818182
2019-03-18T23:13:41.137709: step 306, loss 0.198236, accuracy 0.910156, precision 0.9371428571428572, recall 0.9318181818181818
2019-03-18T23:13:42.690559: step 307, loss 0.228764, accuracy 0.914062, precision 0.9230769230769231, recall 0.9454545454545454
2019-03-18T23:13:44.209501: step 308, loss 0.174186, accuracy 0.933594, precision 0.9518072289156626, recall 0.9461077844311377
2019-03-18T23:13:45.755884: step 309, loss 0.16181, accuracy 0.945312, precision 0.9657142857142857, recall 0.9548022598870056
2019-03-18T23:13:47.158648: step 310, loss 0.220387, accuracy 0.902344, precision 0.936046511627907, recall 0.92
2019-03-18T23:13:48.678152: step 311, loss 0.228585, accuracy 0.921875, precision 0.9556962025316456, recall 0.9207317073170732
2019-03-18T23:13:50.216042: step 312, loss 0.211198, accuracy 0.917969, precision 0.9298245614035088, recall 0.9464285714285714
2019-03-18T23:13:51.666167: step 313, loss 0.218448, accuracy 0.914062, precision 0.9453551912568307, recall 0.9351351351351351
2019-03-18T23:13:53.360697: step 314, loss 0.17568, accuracy 0.917969, precision 0.9709302325581395, recall 0.912568306010929
2019-03-18T23:13:54.771925: step 315, loss 0.237333, accuracy 0.867188, precision 0.9075144508670521, recall 0.8971428571428571
2019-03-18T23:13:56.338318: step 316, loss 0.159229, accuracy 0.925781, precision 0.9573170731707317, recall 0.9289940828402367
2019-03-18T23:13:57.837338: step 317, loss 0.219128, accuracy 0.910156, precision 0.9354838709677419, recall 0.9177215189873418
2019-03-18T23:13:59.218646: step 318, loss 0.216226, accuracy 0.894531, precision 0.9239766081871345, recall 0.9186046511627907
2019-03-18T23:14:00.748558: step 319, loss 0.207122, accuracy 0.914062, precision 0.9426751592356688, recall 0.9192546583850931
2019-03-18T23:14:02.261547: step 320, loss 0.181496, accuracy 0.925781, precision 0.9192546583850931, recall 0.961038961038961
2019-03-18T23:14:03.870303: step 321, loss 0.213812, accuracy 0.925781, precision 0.9186046511627907, recall 0.9693251533742331
2019-03-18T23:14:06.191712: step 322, loss 0.192477, accuracy 0.921875, precision 0.95, recall 0.926829268292683
2019-03-18T23:14:08.066701: step 323, loss 0.249482, accuracy 0.898438, precision 0.9151515151515152, recall 0.9263803680981595
2019-03-18T23:14:10.074445: step 324, loss 0.163014, accuracy 0.941406, precision 0.9695121951219512, recall 0.9408284023668639
2019-03-18T23:14:12.241269: step 325, loss 0.203754, accuracy 0.917969, precision 0.9696969696969697, recall 0.9090909090909091
2019-03-18T23:14:14.137802: step 326, loss 0.182424, accuracy 0.921875, precision 0.9444444444444444, recall 0.9329268292682927
2019-03-18T23:14:15.840348: step 327, loss 0.195775, accuracy 0.929688, precision 0.954248366013072, recall 0.9299363057324841
2019-03-18T23:14:17.571771: step 328, loss 0.187839, accuracy 0.9375, precision 0.9523809523809523, recall 0.9523809523809523
2019-03-18T23:14:19.433955: step 329, loss 0.212848, accuracy 0.90625, precision 0.9642857142857143, recall 0.9
2019-03-18T23:14:21.301502: step 330, loss 0.202839, accuracy 0.921875, precision 0.9397590361445783, recall 0.9397590361445783
2019-03-18T23:14:22.831414: step 331, loss 0.185469, accuracy 0.914062, precision 0.92, recall 0.9526627218934911
2019-03-18T23:14:24.877945: step 332, loss 0.193772, accuracy 0.925781, precision 0.9308176100628931, recall 0.9487179487179487
2019-03-18T23:14:25.674816: step 333, loss 0.23093, accuracy 0.918919, precision 0.9583333333333334, recall 0.92
2019-03-18T23:14:27.604660: step 334, loss 0.188991, accuracy 0.910156, precision 0.9512195121951219, recall 0.9122807017543859
2019-03-18T23:14:29.291158: step 335, loss 0.175944, accuracy 0.933594, precision 0.96045197740113, recall 0.9444444444444444
2019-03-18T23:14:31.192072: step 336, loss 0.151349, accuracy 0.941406, precision 0.9753086419753086, recall 0.9349112426035503
2019-03-18T23:14:33.001238: step 337, loss 0.198112, accuracy 0.925781, precision 0.9461077844311377, recall 0.9404761904761905
2019-03-18T23:14:34.716655: step 338, loss 0.140672, accuracy 0.945312, precision 0.9585798816568047, recall 0.9585798816568047
2019-03-18T23:14:36.493906: step 339, loss 0.16218, accuracy 0.933594, precision 0.9555555555555556, recall 0.9502762430939227
2019-03-18T23:14:38.014847: step 340, loss 0.207001, accuracy 0.902344, precision 0.9285714285714286, recall 0.910828025477707
2019-03-18T23:14:39.511841: step 341, loss 0.165283, accuracy 0.949219, precision 0.9685534591194969, recall 0.9506172839506173
2019-03-18T23:14:40.936035: step 342, loss 0.182223, accuracy 0.917969, precision 0.9433962264150944, recall 0.9259259259259259
2019-03-18T23:14:42.503846: step 343, loss 0.180623, accuracy 0.921875, precision 0.9636363636363636, recall 0.9190751445086706
2019-03-18T23:14:43.823320: step 344, loss 0.162073, accuracy 0.949219, precision 0.9702380952380952, recall 0.9532163742690059
2019-03-18T23:14:45.193660: step 345, loss 0.166607, accuracy 0.917969, precision 0.91875, recall 0.9483870967741935
2019-03-18T23:14:46.530232: step 346, loss 0.149906, accuracy 0.957031, precision 0.9542857142857143, recall 0.9823529411764705
2019-03-18T23:14:48.270581: step 347, loss 0.192422, accuracy 0.917969, precision 0.9393939393939394, recall 0.9337349397590361
2019-03-18T23:14:49.739654: step 348, loss 0.189479, accuracy 0.90625, precision 0.9285714285714286, recall 0.9285714285714286
2019-03-18T23:14:51.535855: step 349, loss 0.171478, accuracy 0.917969, precision 0.9476744186046512, recall 0.9314285714285714
2019-03-18T23:14:53.424808: step 350, loss 0.192554, accuracy 0.910156, precision 0.9404761904761905, recall 0.9239766081871345
2019-03-18T23:14:54.739295: step 351, loss 0.168821, accuracy 0.941406, precision 0.9659863945578231, recall 0.9342105263157895
2019-03-18T23:14:56.390882: step 352, loss 0.176516, accuracy 0.925781, precision 0.94, recall 0.9337748344370861
2019-03-18T23:14:57.841007: step 353, loss 0.180576, accuracy 0.929688, precision 0.930635838150289, recall 0.9640718562874252
2019-03-18T23:14:59.621249: step 354, loss 0.194653, accuracy 0.910156, precision 0.9034090909090909, recall 0.9636363636363636
2019-03-18T23:15:01.243451: step 355, loss 0.153512, accuracy 0.953125, precision 0.9580838323353293, recall 0.9696969696969697
2019-03-18T23:15:02.801329: step 356, loss 0.143238, accuracy 0.953125, precision 0.9625, recall 0.9625
2019-03-18T23:15:04.473860: step 357, loss 0.202522, accuracy 0.929688, precision 0.9573170731707317, recall 0.9345238095238095
2019-03-18T23:15:05.975358: step 358, loss 0.205786, accuracy 0.917969, precision 0.9430379746835443, recall 0.9254658385093167
2019-03-18T23:15:07.606000: step 359, loss 0.174338, accuracy 0.925781, precision 0.96, recall 0.9333333333333333
2019-03-18T23:15:09.252600: step 360, loss 0.144457, accuracy 0.945312, precision 0.9606741573033708, recall 0.9606741573033708
2019-03-18T23:15:10.841354: step 361, loss 0.190105, accuracy 0.90625, precision 0.9415204678362573, recall 0.92
2019-03-18T23:15:12.568782: step 362, loss 0.159268, accuracy 0.925781, precision 0.937888198757764, recall 0.94375
2019-03-18T23:15:14.167513: step 363, loss 0.171712, accuracy 0.941406, precision 0.937888198757764, recall 0.967948717948718
2019-03-18T23:15:15.592701: step 364, loss 0.205571, accuracy 0.929688, precision 0.9613259668508287, recall 0.9405405405405406
2019-03-18T23:15:17.206390: step 365, loss 0.170571, accuracy 0.941406, precision 0.9651162790697675, recall 0.9485714285714286
2019-03-18T23:15:18.728322: step 366, loss 0.164055, accuracy 0.933594, precision 0.9401197604790419, recall 0.9573170731707317
2019-03-18T23:15:20.315081: step 367, loss 0.175875, accuracy 0.925781, precision 0.9719101123595506, recall 0.9251336898395722
2019-03-18T23:15:21.955698: step 368, loss 0.221621, accuracy 0.921875, precision 0.9542857142857143, recall 0.9329608938547486
2019-03-18T23:15:23.620250: step 369, loss 0.141072, accuracy 0.945312, precision 0.9636363636363636, recall 0.9520958083832335
2019-03-18T23:15:24.319382: step 370, loss 0.1589, accuracy 1, precision 1.0, recall 1.0
2019-03-18T23:15:26.050755: step 371, loss 0.121076, accuracy 0.960938, precision 0.96, recall 0.9824561403508771
2019-03-18T23:15:27.686385: step 372, loss 0.164264, accuracy 0.9375, precision 0.9597701149425287, recall 0.9488636363636364
2019-03-18T23:15:29.357919: step 373, loss 0.122792, accuracy 0.960938, precision 0.9820359281437125, recall 0.9590643274853801
2019-03-18T23:15:30.941843: step 374, loss 0.158645, accuracy 0.933594, precision 0.9388888888888889, recall 0.9657142857142857
2019-03-18T23:15:32.719095: step 375, loss 0.151621, accuracy 0.9375, precision 0.9597701149425287, recall 0.9488636363636364
2019-03-18T23:15:34.260493: step 376, loss 0.161165, accuracy 0.945312, precision 0.9553072625698324, recall 0.9661016949152542
2019-03-18T23:15:35.845258: step 377, loss 0.13346, accuracy 0.964844, precision 0.9885057471264368, recall 0.9608938547486033
2019-03-18T23:15:37.584610: step 378, loss 0.134985, accuracy 0.964844, precision 0.9772727272727273, recall 0.9717514124293786
2019-03-18T23:15:39.219242: step 379, loss 0.179861, accuracy 0.925781, precision 0.9754601226993865, recall 0.9137931034482759
2019-03-18T23:15:40.717240: step 380, loss 0.160506, accuracy 0.925781, precision 0.9595375722543352, recall 0.9325842696629213
2019-03-18T23:15:42.209252: step 381, loss 0.12696, accuracy 0.945312, precision 0.9636363636363636, recall 0.9520958083832335
2019-03-18T23:15:43.784045: step 382, loss 0.165003, accuracy 0.929688, precision 0.9515151515151515, recall 0.9401197604790419
2019-03-18T23:15:45.532372: step 383, loss 0.14469, accuracy 0.953125, precision 0.9642857142857143, recall 0.9642857142857143
2019-03-18T23:15:47.094199: step 384, loss 0.13497, accuracy 0.957031, precision 0.9693251533742331, recall 0.9634146341463414
2019-03-18T23:15:48.718859: step 385, loss 0.177469, accuracy 0.925781, precision 0.9316770186335404, recall 0.9493670886075949
2019-03-18T23:15:50.185970: step 386, loss 0.14661, accuracy 0.933594, precision 0.95625, recall 0.9386503067484663
2019-03-18T23:15:51.761758: step 387, loss 0.157987, accuracy 0.945312, precision 0.9320987654320988, recall 0.9805194805194806
2019-03-18T23:15:53.267734: step 388, loss 0.176959, accuracy 0.921875, precision 0.9177215189873418, recall 0.9539473684210527
2019-03-18T23:15:54.826568: step 389, loss 0.135991, accuracy 0.953125, precision 0.9590643274853801, recall 0.9704142011834319
2019-03-18T23:15:56.374433: step 390, loss 0.107985, accuracy 0.972656, precision 0.968944099378882, recall 0.9873417721518988
2019-03-18T23:15:57.801619: step 391, loss 0.166163, accuracy 0.9375, precision 0.9554140127388535, recall 0.9433962264150944
2019-03-18T23:15:59.559921: step 392, loss 0.176203, accuracy 0.929688, precision 0.9696969696969697, recall 0.9248554913294798
2019-03-18T23:16:01.154241: step 393, loss 0.17139, accuracy 0.941406, precision 0.9882352941176471, recall 0.9281767955801105
2019-03-18T23:16:02.586414: step 394, loss 0.175612, accuracy 0.914062, precision 0.9573170731707317, recall 0.9127906976744186
2019-03-18T23:16:04.088405: step 395, loss 0.148697, accuracy 0.933594, precision 0.9470588235294117, recall 0.9526627218934911
2019-03-18T23:16:05.741982: step 396, loss 0.140318, accuracy 0.941406, precision 0.9693251533742331, recall 0.9404761904761905
2019-03-18T23:16:07.249511: step 397, loss 0.192271, accuracy 0.917969, precision 0.9085714285714286, recall 0.9695121951219512
2019-03-18T23:16:08.835780: step 398, loss 0.167422, accuracy 0.917969, precision 0.9217877094972067, recall 0.9593023255813954
2019-03-18T23:16:10.466464: step 399, loss 0.167218, accuracy 0.929688, precision 0.9464285714285714, recall 0.9464285714285714
2019-03-18T23:16:12.038265: step 400, loss 0.128231, accuracy 0.953125, precision 0.96875, recall 0.9567901234567902
2019-03-18T23:16:13.565184: step 401, loss 0.180741, accuracy 0.921875, precision 0.9615384615384616, recall 0.9146341463414634
2019-03-18T23:16:15.121027: step 402, loss 0.154055, accuracy 0.9375, precision 0.9532163742690059, recall 0.9532163742690059
2019-03-18T23:16:16.871350: step 403, loss 0.136912, accuracy 0.9375, precision 0.9636363636363636, recall 0.9408284023668639
2019-03-18T23:16:18.452127: step 404, loss 0.181798, accuracy 0.917969, precision 0.9627329192546584, recall 0.9117647058823529
2019-03-18T23:16:20.019517: step 405, loss 0.176215, accuracy 0.9375, precision 0.9545454545454546, recall 0.9423076923076923
2019-03-18T23:16:21.627223: step 406, loss 0.200581, accuracy 0.917969, precision 0.9333333333333333, recall 0.9390243902439024
2019-03-18T23:16:22.423117: step 407, loss 0.191289, accuracy 0.918919, precision 0.95, recall 0.9047619047619048
2019-03-18T23:16:23.857285: step 408, loss 0.165941, accuracy 0.941406, precision 0.9390243902439024, recall 0.9685534591194969
2019-03-18T23:16:25.344818: step 409, loss 0.168885, accuracy 0.933594, precision 0.9502762430939227, recall 0.9555555555555556
2019-03-18T23:16:26.953044: step 410, loss 0.153082, accuracy 0.9375, precision 0.935064935064935, recall 0.96
2019-03-18T23:16:28.557326: step 411, loss 0.140398, accuracy 0.96875, precision 0.9570552147239264, recall 0.9936305732484076
2019-03-18T23:16:30.212478: step 412, loss 0.135828, accuracy 0.945312, precision 0.949685534591195, recall 0.9617834394904459
2019-03-18T23:16:32.035202: step 413, loss 0.131873, accuracy 0.949219, precision 0.9611111111111111, recall 0.9664804469273743
2019-03-18T23:16:33.629006: step 414, loss 0.143959, accuracy 0.929688, precision 0.9636363636363636, recall 0.9298245614035088
2019-03-18T23:16:35.251180: step 415, loss 0.199089, accuracy 0.929688, precision 0.9874213836477987, recall 0.9075144508670521
2019-03-18T23:16:36.739229: step 416, loss 0.146015, accuracy 0.945312, precision 0.9647058823529412, recall 0.9534883720930233
2019-03-18T23:16:38.914502: step 417, loss 0.157219, accuracy 0.945312, precision 0.9695121951219512, recall 0.9464285714285714
2019-03-18T23:16:41.538001: step 418, loss 0.161741, accuracy 0.921875, precision 0.9556962025316456, recall 0.9207317073170732
2019-03-18T23:16:43.776864: step 419, loss 0.115112, accuracy 0.964844, precision 0.9558011049723757, recall 0.9942528735632183
2019-03-18T23:16:45.581044: step 420, loss 0.13181, accuracy 0.960938, precision 0.9832402234636871, recall 0.9617486338797814
2019-03-18T23:16:47.232630: step 421, loss 0.138159, accuracy 0.945312, precision 0.9502762430939227, recall 0.9717514124293786
2019-03-18T23:16:48.978963: step 422, loss 0.121952, accuracy 0.957031, precision 0.9583333333333334, recall 0.9757575757575757
2019-03-18T23:16:50.836999: step 423, loss 0.124844, accuracy 0.945312, precision 0.9461077844311377, recall 0.9693251533742331
2019-03-18T23:16:52.834689: step 424, loss 0.160165, accuracy 0.933594, precision 0.9476744186046512, recall 0.9532163742690059
2019-03-18T23:16:54.866256: step 425, loss 0.12264, accuracy 0.957031, precision 0.963855421686747, recall 0.9696969696969697
2019-03-18T23:16:56.703965: step 426, loss 0.121858, accuracy 0.949219, precision 0.9523809523809523, recall 0.9696969696969697
2019-03-18T23:16:58.573989: step 427, loss 0.137614, accuracy 0.949219, precision 0.9875, recall 0.9349112426035503
2019-03-18T23:17:00.113499: step 428, loss 0.173462, accuracy 0.914062, precision 0.9691358024691358, recall 0.9022988505747126
2019-03-18T23:17:01.751143: step 429, loss 0.169276, accuracy 0.933594, precision 0.9461077844311377, recall 0.9518072289156626
2019-03-18T23:17:03.395756: step 430, loss 0.10579, accuracy 0.964844, precision 0.9693251533742331, recall 0.9753086419753086
2019-03-18T23:17:05.018834: step 431, loss 0.142474, accuracy 0.949219, precision 0.976878612716763, recall 0.949438202247191
2019-03-18T23:17:06.583968: step 432, loss 0.143555, accuracy 0.953125, precision 0.9358974358974359, recall 0.9864864864864865
2019-03-18T23:17:08.070662: step 433, loss 0.167653, accuracy 0.945312, precision 0.9691358024691358, recall 0.9457831325301205
2019-03-18T23:17:09.598090: step 434, loss 0.134047, accuracy 0.953125, precision 0.9580838323353293, recall 0.9696969696969697
2019-03-18T23:17:11.157617: step 435, loss 0.133272, accuracy 0.949219, precision 0.9473684210526315, recall 0.9759036144578314
2019-03-18T23:17:12.711537: step 436, loss 0.137332, accuracy 0.941406, precision 0.9590643274853801, recall 0.9534883720930233
2019-03-18T23:17:14.248491: step 437, loss 0.14417, accuracy 0.957031, precision 0.9870967741935484, recall 0.9444444444444444
2019-03-18T23:17:15.747565: step 438, loss 0.143291, accuracy 0.945312, precision 0.9570552147239264, recall 0.9570552147239264
2019-03-18T23:17:17.241134: step 439, loss 0.178784, accuracy 0.9375, precision 0.9515151515151515, recall 0.9515151515151515
2019-03-18T23:17:18.780650: step 440, loss 0.117838, accuracy 0.953125, precision 0.9831460674157303, recall 0.9510869565217391
2019-03-18T23:17:20.395886: step 441, loss 0.150498, accuracy 0.941406, precision 0.9887640449438202, recall 0.9312169312169312
2019-03-18T23:17:22.039397: step 442, loss 0.164224, accuracy 0.9375, precision 0.968944099378882, recall 0.9341317365269461
2019-03-18T23:17:23.492575: step 443, loss 0.132348, accuracy 0.941406, precision 0.9691358024691358, recall 0.9401197604790419
2019-03-18T23:17:24.331334: step 444, loss 0.171219, accuracy 0.918919, precision 0.8947368421052632, recall 0.9444444444444444
2019-03-18T23:17:25.595488: step 445, loss 0.119303, accuracy 0.953125, precision 0.937888198757764, recall 0.9869281045751634
2019-03-18T23:17:27.078326: step 446, loss 0.126946, accuracy 0.933594, precision 0.936046511627907, recall 0.9640718562874252
2019-03-18T23:17:28.568926: step 447, loss 0.132775, accuracy 0.960938, precision 0.9702380952380952, recall 0.9702380952380952
2019-03-18T23:17:30.295312: step 448, loss 0.124473, accuracy 0.957031, precision 0.9390243902439024, recall 0.9935483870967742
2019-03-18T23:17:31.872119: step 449, loss 0.122098, accuracy 0.953125, precision 0.976878612716763, recall 0.9548022598870056
2019-03-18T23:17:33.381595: step 450, loss 0.125384, accuracy 0.96875, precision 0.9888888888888889, recall 0.967391304347826
2019-03-18T23:17:34.916493: step 451, loss 0.119638, accuracy 0.960938, precision 0.9748427672955975, recall 0.9627329192546584
2019-03-18T23:17:36.423514: step 452, loss 0.159657, accuracy 0.949219, precision 0.9814814814814815, recall 0.9408284023668639
2019-03-18T23:17:39.459940: step 453, loss 0.114681, accuracy 0.96875, precision 0.9880239520958084, recall 0.9649122807017544
2019-03-18T23:17:40.913083: step 454, loss 0.145372, accuracy 0.949219, precision 0.975609756097561, recall 0.9467455621301775
2019-03-18T23:17:42.361758: step 455, loss 0.100847, accuracy 0.964844, precision 0.9745222929936306, recall 0.9683544303797469
2019-03-18T23:17:44.125643: step 456, loss 0.131867, accuracy 0.964844, precision 0.9753086419753086, recall 0.9693251533742331
2019-03-18T23:17:46.558653: step 457, loss 0.155583, accuracy 0.929688, precision 0.9235294117647059, recall 0.9691358024691358
2019-03-18T23:17:48.658553: step 458, loss 0.172302, accuracy 0.945312, precision 0.950920245398773, recall 0.9627329192546584
2019-03-18T23:17:50.582454: step 459, loss 0.137895, accuracy 0.953125, precision 0.96, recall 0.9710982658959537
2019-03-18T23:17:52.719743: step 460, loss 0.146048, accuracy 0.949219, precision 0.959731543624161, recall 0.9533333333333334
2019-03-18T23:17:54.601714: step 461, loss 0.114726, accuracy 0.953125, precision 0.9822485207100592, recall 0.9485714285714286
2019-03-18T23:17:56.277236: step 462, loss 0.154457, accuracy 0.941406, precision 0.9595375722543352, recall 0.9540229885057471
2019-03-18T23:17:58.179155: step 463, loss 0.120046, accuracy 0.957031, precision 0.9748427672955975, recall 0.9567901234567902
2019-03-18T23:17:59.490650: step 464, loss 0.123137, accuracy 0.964844, precision 0.9881656804733728, recall 0.9597701149425287
2019-03-18T23:18:00.815110: step 465, loss 0.127506, accuracy 0.976562, precision 0.9939759036144579, recall 0.9705882352941176
2019-03-18T23:18:02.302136: step 466, loss 0.131995, accuracy 0.945312, precision 0.9771428571428571, recall 0.9447513812154696
2019-03-18T23:18:03.483979: step 467, loss 0.115962, accuracy 0.953125, precision 0.95, recall 0.9743589743589743
2019-03-18T23:18:04.426461: step 468, loss 0.124498, accuracy 0.964844, precision 0.967948717948718, recall 0.9741935483870968
2019-03-18T23:18:05.268743: step 469, loss 0.111845, accuracy 0.964844, precision 0.9627329192546584, recall 0.9810126582278481
2019-03-18T23:18:06.189283: step 470, loss 0.161455, accuracy 0.933594, precision 0.9467455621301775, recall 0.9523809523809523
2019-03-18T23:18:07.155784: step 471, loss 0.110869, accuracy 0.960938, precision 0.9512195121951219, recall 0.9873417721518988
2019-03-18T23:18:08.210964: step 472, loss 0.126596, accuracy 0.957031, precision 0.9774011299435028, recall 0.9611111111111111
2019-03-18T23:18:09.273965: step 473, loss 0.107446, accuracy 0.964844, precision 0.9813664596273292, recall 0.9634146341463414
2019-03-18T23:18:10.390550: step 474, loss 0.156248, accuracy 0.921875, precision 0.943502824858757, recall 0.943502824858757
2019-03-18T23:18:11.346162: step 475, loss 0.123817, accuracy 0.957031, precision 0.9826589595375722, recall 0.9550561797752809
2019-03-18T23:18:12.278485: step 476, loss 0.102484, accuracy 0.972656, precision 0.9887005649717514, recall 0.9722222222222222
2019-03-18T23:18:13.362675: step 477, loss 0.113798, accuracy 0.957031, precision 0.9770114942528736, recall 0.96045197740113
2019-03-18T23:18:14.268737: step 478, loss 0.127824, accuracy 0.953125, precision 0.96, recall 0.9710982658959537
2019-03-18T23:18:15.614716: step 479, loss 0.104073, accuracy 0.964844, precision 0.9748427672955975, recall 0.96875
2019-03-18T23:18:16.488403: step 480, loss 0.0925839, accuracy 0.980469, precision 0.9878048780487805, recall 0.9818181818181818
2019-03-18T23:18:16.837733: step 481, loss 0.174525, accuracy 0.918919, precision 0.9285714285714286, recall 0.9629629629629629
2019-03-18T23:18:17.687483: step 482, loss 0.114878, accuracy 0.960938, precision 0.9657142857142857, recall 0.976878612716763
2019-03-18T23:18:18.548750: step 483, loss 0.0977941, accuracy 0.960938, precision 0.9707602339181286, recall 0.9707602339181286
2019-03-18T23:18:19.370715: step 484, loss 0.100868, accuracy 0.972656, precision 0.993103448275862, recall 0.96
2019-03-18T23:18:20.143464: step 485, loss 0.106507, accuracy 0.964844, precision 0.9657142857142857, recall 0.9825581395348837
2019-03-18T23:18:20.988051: step 486, loss 0.096306, accuracy 0.976562, precision 0.9738562091503268, recall 0.9867549668874173
2019-03-18T23:18:21.791489: step 487, loss 0.10113, accuracy 0.972656, precision 0.9695121951219512, recall 0.9875776397515528
2019-03-18T23:18:22.563460: step 488, loss 0.135014, accuracy 0.941406, precision 0.9512195121951219, recall 0.9570552147239264
2019-03-18T23:18:23.334078: step 489, loss 0.115623, accuracy 0.960938, precision 0.9693251533742331, recall 0.9693251533742331
2019-03-18T23:18:24.060636: step 490, loss 0.108871, accuracy 0.953125, precision 0.9873417721518988, recall 0.9397590361445783
2019-03-18T23:18:24.817637: step 491, loss 0.0875379, accuracy 0.980469, precision 0.9934640522875817, recall 0.9743589743589743
2019-03-18T23:18:25.564640: step 492, loss 0.0966578, accuracy 0.964844, precision 0.9548387096774194, recall 0.9866666666666667
2019-03-18T23:18:26.292207: step 493, loss 0.104507, accuracy 0.972656, precision 0.9712643678160919, recall 0.9883040935672515
2019-03-18T23:18:27.050736: step 494, loss 0.12275, accuracy 0.964844, precision 0.9550561797752809, recall 0.9941520467836257
2019-03-18T23:18:27.775309: step 495, loss 0.116638, accuracy 0.964844, precision 0.976878612716763, recall 0.9712643678160919
2019-03-18T23:18:28.511343: step 496, loss 0.113521, accuracy 0.960938, precision 0.9759036144578314, recall 0.9642857142857143
2019-03-18T23:18:29.312710: step 497, loss 0.0873795, accuracy 0.976562, precision 0.9787234042553191, recall 0.989247311827957
2019-03-18T23:18:30.007852: step 498, loss 0.155974, accuracy 0.945312, precision 0.963855421686747, recall 0.9523809523809523
2019-03-18T23:18:30.759515: step 499, loss 0.14032, accuracy 0.945312, precision 0.976878612716763, recall 0.9441340782122905
2019-03-18 23:18:31.497306: W tensorflow/core/framework/allocator.cc:122] Allocation of 102697200 exceeds 10% of system memory.
2019-03-18T23:18:31.494061: step 500, loss 0.143569, accuracy 0.925781, precision 0.9771428571428571, recall 0.9193548387096774

Evaluation:
[[1260  342]
 [ 574  137]]
2019-03-18T23:18:32.799620: step 500, loss 1.28362, accuracy 0.603978, precision 0.7865168539325843, recall 0.6870229007633588

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1552968369\checkpoints\model-500


Process finished with exit code 0
