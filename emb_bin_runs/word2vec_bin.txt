Loading data...
10251
Max Document length: 2407
Vocabulary Size: 24319
Train/Dev split: 9226/1025
Writing to /home/ubuntu/Project/runs/1550682642

2019-02-20T17:11:24.674119: step 1, loss 3.27608, accuracy 0.25, precision 1.0, recall 0.07692307692307693
2019-02-20T17:11:24.901616: step 2, loss 2.05749, accuracy 0.5, precision 0.0, recall 0.0
2019-02-20T17:11:25.107406: step 3, loss 1.20191, accuracy 0.4375, precision nan, recall 0.0
2019-02-20T17:11:25.318835: step 4, loss 0.442056, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:25.528430: step 5, loss 2.11609, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:11:25.719356: step 6, loss 0.0995065, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:25.914555: step 7, loss 0.563151, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:26.107327: step 8, loss 1.74335, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:26.293798: step 9, loss 3.31452, accuracy 0.6875, precision 0.0, recall nan
2019-02-20T17:11:26.486543: step 10, loss 1.56324, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:11:26.680552: step 11, loss 1.32933, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:26.870127: step 12, loss 1.08956, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:27.065008: step 13, loss 1.11492, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:27.259072: step 14, loss 0.00195664, accuracy 1, precision nan, recall nan
2019-02-20T17:11:27.451874: step 15, loss 0.526553, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:27.648196: step 16, loss 0.48086, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:27.841199: step 17, loss 1.01318, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:28.034612: step 18, loss 0.301043, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:28.225083: step 19, loss 0.0400018, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:11:28.420132: step 20, loss 1.45582, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:11:28.613681: step 21, loss 2.33057, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:11:28.807442: step 22, loss 1.55582, accuracy 0.75, precision 0.3333333333333333, recall 0.3333333333333333
2019-02-20T17:11:29.003189: step 23, loss 0.230938, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:11:29.195445: step 24, loss 0.516948, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:29.391667: step 25, loss 0.358321, accuracy 0.8125, precision 1.0, recall 0.25
2019-02-20T17:11:29.584735: step 26, loss 1.75907, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:11:29.775698: step 27, loss 0.531172, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:11:29.975703: step 28, loss 0.365664, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:11:30.163763: step 29, loss 0.164327, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:11:30.351837: step 30, loss 0.0632466, accuracy 1, precision nan, recall nan
2019-02-20T17:11:30.543257: step 31, loss 0.290698, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:30.735087: step 32, loss 1.1212, accuracy 0.75, precision 0.75, recall 0.5
2019-02-20T17:11:30.930813: step 33, loss 0.0263499, accuracy 1, precision nan, recall nan
2019-02-20T17:11:31.119527: step 34, loss 1.99728, accuracy 0.8125, precision 0.4, recall 1.0
2019-02-20T17:11:31.309658: step 35, loss 0.14042, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:31.504826: step 36, loss 1.12371, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:31.702214: step 37, loss 1.76037, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:11:31.893992: step 38, loss 0.248126, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:32.083898: step 39, loss 1.20983, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:32.270664: step 40, loss 0.56787, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:32.459040: step 41, loss 0.25, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:32.654505: step 42, loss 3.05909, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:11:32.851923: step 43, loss 0.851898, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:33.048390: step 44, loss 1.036, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:11:33.241418: step 45, loss 0.721713, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:11:33.433740: step 46, loss 0.486294, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:11:33.625293: step 47, loss 1.8789, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:33.822844: step 48, loss 0.796795, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:11:34.016000: step 49, loss 0.538825, accuracy 0.8125, precision nan, recall 0.0
2019-02-20T17:11:34.209795: step 50, loss 0.158539, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:34.404475: step 51, loss 1.10814, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:34.597809: step 52, loss 0.388487, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:11:34.792613: step 53, loss 1.4268, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:11:34.988826: step 54, loss 0.178024, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:11:35.178945: step 55, loss 0.603198, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:11:35.367327: step 56, loss 0.135183, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:35.559168: step 57, loss 0.724533, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:35.755373: step 58, loss 0.367468, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:11:35.944708: step 59, loss 0.603058, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:36.140168: step 60, loss 0.640596, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:36.334515: step 61, loss 0.227815, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:11:36.532844: step 62, loss 1.12954, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:11:36.725808: step 63, loss 0.758841, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:36.919872: step 64, loss 0.934731, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:11:37.110501: step 65, loss 1.04244, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:37.298364: step 66, loss 0.00203693, accuracy 1, precision nan, recall nan
2019-02-20T17:11:37.496241: step 67, loss 0.778755, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:37.692100: step 68, loss 0.01385, accuracy 1, precision nan, recall nan
2019-02-20T17:11:37.886716: step 69, loss 0.0279493, accuracy 1, precision nan, recall nan
2019-02-20T17:11:38.081726: step 70, loss 1.96425, accuracy 0.625, precision 0.0, recall 0.0
2019-02-20T17:11:38.275382: step 71, loss 0.379609, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:38.469800: step 72, loss 0.0125128, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:11:38.659477: step 73, loss 0.0652008, accuracy 1, precision nan, recall nan
2019-02-20T17:11:38.849883: step 74, loss 0.917358, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:39.045406: step 75, loss 0.0762431, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:11:39.238746: step 76, loss 0.0427051, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:11:39.431420: step 77, loss 1.24145, accuracy 0.75, precision 0.25, recall 0.5
2019-02-20T17:11:39.623880: step 78, loss 0.800582, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:11:39.816223: step 79, loss 0.733162, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:40.010222: step 80, loss 0.444561, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:40.198779: step 81, loss 0.255486, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:40.395221: step 82, loss 1.38207, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:40.588827: step 83, loss 0.536607, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:40.783280: step 84, loss 0.0615831, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:11:40.969485: step 85, loss 0.0324023, accuracy 1, precision nan, recall nan
2019-02-20T17:11:41.160476: step 86, loss 1.22132, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:11:41.357365: step 87, loss 0.707026, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:41.549795: step 88, loss 0.367848, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:11:41.738457: step 89, loss 0.951573, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:41.925779: step 90, loss 0.677194, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:11:42.118906: step 91, loss 1.22859, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:42.309441: step 92, loss 0.0552383, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:42.504301: step 93, loss 2.77475, accuracy 0.5, precision 0.16666666666666666, recall 0.25
2019-02-20T17:11:42.700459: step 94, loss 0.584625, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:42.891789: step 95, loss 0.351691, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:43.084177: step 96, loss 0.445621, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:43.276412: step 97, loss 0.331815, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:43.470750: step 98, loss 0.566686, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:43.662543: step 99, loss 1.18246, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:43.851157: step 100, loss 0.933419, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:44.041493: step 101, loss 0.185611, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:11:44.239938: step 102, loss 0.841803, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:44.432264: step 103, loss 2.6045, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:11:44.627625: step 104, loss 1.028, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:44.817085: step 105, loss 0.331819, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:11:45.011461: step 106, loss 1.44355, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:11:45.204279: step 107, loss 1.62527, accuracy 0.75, precision 0.3333333333333333, recall 0.3333333333333333
2019-02-20T17:11:45.398264: step 108, loss 0.102877, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:11:45.590439: step 109, loss 0.088551, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:11:45.779938: step 110, loss 1.54197, accuracy 0.625, precision 0.0, recall 0.0
2019-02-20T17:11:45.971779: step 111, loss 0.155196, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:11:46.162495: step 112, loss 0.665601, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:46.356652: step 113, loss 0.7232, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:46.547025: step 114, loss 0.5476, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:11:46.738973: step 115, loss 0.486368, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:46.938299: step 116, loss 0.46059, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:11:47.129964: step 117, loss 0.135584, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:47.323394: step 118, loss 1.51784, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:11:47.512606: step 119, loss 0.671618, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:47.702589: step 120, loss 0.442294, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:47.893869: step 121, loss 1.93035, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:48.088680: step 122, loss 0.724499, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:48.279576: step 123, loss 0.928791, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:11:48.468422: step 124, loss 0.190036, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:48.660894: step 125, loss 0.030501, accuracy 1, precision nan, recall nan
2019-02-20T17:11:48.853660: step 126, loss 1.02782, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:11:49.049422: step 127, loss 0.426132, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:11:49.242385: step 128, loss 0.969813, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:49.439241: step 129, loss 0.72512, accuracy 0.6875, precision 0.3333333333333333, recall 0.25
2019-02-20T17:11:49.631209: step 130, loss 0.204947, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:11:49.826914: step 131, loss 0.938204, accuracy 0.75, precision 0.3333333333333333, recall 0.3333333333333333
2019-02-20T17:11:50.018684: step 132, loss 1.06824, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:50.213404: step 133, loss 1.60555, accuracy 0.75, precision 0.5, recall 0.5
2019-02-20T17:11:50.402372: step 134, loss 0.57381, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:50.600034: step 135, loss 1.02361, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:50.787359: step 136, loss 0.221768, accuracy 0.875, precision 1.0, recall 0.6
2019-02-20T17:11:50.980613: step 137, loss 0.139348, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:11:51.169456: step 138, loss 0.997852, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:11:51.359062: step 139, loss 0.181556, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:51.554562: step 140, loss 0.502391, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:51.747971: step 141, loss 0.450474, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:11:51.943269: step 142, loss 0.3915, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:11:52.130728: step 143, loss 0.260014, accuracy 0.8125, precision 1.0, recall 0.25
2019-02-20T17:11:52.325916: step 144, loss 0.431468, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:11:52.523624: step 145, loss 0.706436, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:52.713885: step 146, loss 0.147398, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:52.907718: step 147, loss 1.14316, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:11:53.103617: step 148, loss 0.870319, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:53.293141: step 149, loss 0.449941, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:53.491343: step 150, loss 0.29371, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:53.680900: step 151, loss 0.282806, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:11:53.878732: step 152, loss 0.663152, accuracy 0.8125, precision 1.0, recall 0.25
2019-02-20T17:11:54.073318: step 153, loss 0.947204, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:54.264334: step 154, loss 0.00972913, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:11:54.460004: step 155, loss 0.715049, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:11:54.657322: step 156, loss 0.100026, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:54.848774: step 157, loss 0.30608, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:55.037542: step 158, loss 0.666212, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:11:55.233557: step 159, loss 0.448478, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:55.426622: step 160, loss 0.490749, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:55.619785: step 161, loss 0.0679142, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:11:55.810788: step 162, loss 0.0859688, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:11:56.003847: step 163, loss 0.524978, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:11:56.195864: step 164, loss 0.145288, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:11:56.387703: step 165, loss 1.88253, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:11:56.587034: step 166, loss 0.0921219, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:56.773032: step 167, loss 0.12566, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:56.968041: step 168, loss 0.164528, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:11:57.159574: step 169, loss 0.412589, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:57.352452: step 170, loss 0.0148117, accuracy 1, precision nan, recall nan
2019-02-20T17:11:57.546417: step 171, loss 0.155576, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:11:57.744522: step 172, loss 1.27818, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:57.936004: step 173, loss 1.34961, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:58.133212: step 174, loss 1.12968, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:11:58.331274: step 175, loss 1.03706, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:58.524935: step 176, loss 1.04641, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:11:58.715776: step 177, loss 0.151807, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:11:58.906131: step 178, loss 1.47374, accuracy 0.75, precision 0.25, recall 0.5
2019-02-20T17:11:59.101331: step 179, loss 0.178833, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:11:59.297455: step 180, loss 0.709347, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:11:59.494546: step 181, loss 1.73367, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:11:59.690221: step 182, loss 0.772439, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:11:59.888096: step 183, loss 0.142248, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:00.079027: step 184, loss 0.671898, accuracy 0.8125, precision 0.5, recall 0.6666666666666666
2019-02-20T17:12:00.272545: step 185, loss 0.765148, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:00.459625: step 186, loss 0.355899, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:00.654134: step 187, loss 0.91696, accuracy 0.75, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:00.849863: step 188, loss 0.676077, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:01.046753: step 189, loss 1.85818, accuracy 0.5, precision 0.0, recall 0.0
2019-02-20T17:12:01.243927: step 190, loss 0.511759, accuracy 0.8125, precision nan, recall 0.0
2019-02-20T17:12:01.436578: step 191, loss 0.435272, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:01.629210: step 192, loss 0.332678, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:01.819095: step 193, loss 1.38096, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:02.015986: step 194, loss 0.00796164, accuracy 1, precision nan, recall nan
2019-02-20T17:12:02.213337: step 195, loss 0.28444, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:02.409366: step 196, loss 0.761102, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:02.609406: step 197, loss 0.753565, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:02.801714: step 198, loss 0.398343, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:02.998425: step 199, loss 1.25334, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:03.198137: step 200, loss 1.20857, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:12:03.386983: step 201, loss 0.655725, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:03.582709: step 202, loss 0.462342, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:03.772298: step 203, loss 1.03307, accuracy 0.75, precision 0.0, recall nan
2019-02-20T17:12:03.969530: step 204, loss 0.605578, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:04.158717: step 205, loss 0.0362515, accuracy 1, precision nan, recall nan
2019-02-20T17:12:04.355183: step 206, loss 0.160136, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:04.548309: step 207, loss 0.330986, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:04.737552: step 208, loss 1.31592, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:04.929613: step 209, loss 0.372361, accuracy 0.8125, precision 1.0, recall 0.25
2019-02-20T17:12:05.124560: step 210, loss 0.0701091, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:05.320490: step 211, loss 1.04153, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:12:05.515204: step 212, loss 0.759707, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:05.713719: step 213, loss 1.49438, accuracy 0.75, precision 0.2, recall 1.0
2019-02-20T17:12:05.904131: step 214, loss 0.37178, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:12:06.092092: step 215, loss 0.628182, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:12:06.285527: step 216, loss 0.899943, accuracy 0.75, precision 0.5, recall 0.25
2019-02-20T17:12:06.478045: step 217, loss 0.188581, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:06.676058: step 218, loss 0.110679, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:06.875598: step 219, loss 0.656823, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:07.071869: step 220, loss 0.291813, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:12:07.270508: step 221, loss 0.455846, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:07.468056: step 222, loss 0.524035, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:07.663379: step 223, loss 1.02341, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:12:07.859973: step 224, loss 0.00857604, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:08.054086: step 225, loss 0.440946, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:08.250066: step 226, loss 0.822097, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:12:08.444099: step 227, loss 0.483793, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:08.631853: step 228, loss 0.305111, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:08.826475: step 229, loss 0.0087558, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:09.024182: step 230, loss 0.768705, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:09.221250: step 231, loss 0.01622, accuracy 1, precision nan, recall nan
2019-02-20T17:12:09.412525: step 232, loss 0.688974, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:09.602775: step 233, loss 0.358224, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:09.795650: step 234, loss 0.760008, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:09.986333: step 235, loss 0.168174, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:10.179834: step 236, loss 0.74651, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:10.375069: step 237, loss 0.119904, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:12:10.570615: step 238, loss 1.55452, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:10.762372: step 239, loss 0.00653752, accuracy 1, precision nan, recall nan
2019-02-20T17:12:10.960357: step 240, loss 1.46696, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:11.154400: step 241, loss 1.16836, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:12:11.347402: step 242, loss 0.539899, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:11.537082: step 243, loss 0.481881, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:11.730187: step 244, loss 0.642663, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:11.925518: step 245, loss 0.213975, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:12.123229: step 246, loss 0.771543, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:12:12.318057: step 247, loss 0.0659501, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:12.512590: step 248, loss 0.509709, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:12.707350: step 249, loss 0.398299, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:12.903145: step 250, loss 1.12678, accuracy 0.75, precision 0.0, recall 0.0

Evaluation:
[[ 12  83]
 [  0 930]]
2019-02-20T17:12:17.085044: step 250, loss 0.397769, accuracy 0.919024, precision 0.12631578947368421, recall 1.0

Saved model checkpoint to /home/ubuntu/Project/runs/1550682642/checkpoints/model-250

2019-02-20T17:12:17.393530: step 251, loss 0.604065, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:17.584841: step 252, loss 0.946932, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:17.780123: step 253, loss 0.147778, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:17.971914: step 254, loss 0.288644, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:18.160184: step 255, loss 0.502477, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:18.359070: step 256, loss 0.731762, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:18.549476: step 257, loss 0.115821, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:18.746647: step 258, loss 0.0147881, accuracy 1, precision nan, recall nan
2019-02-20T17:12:18.942598: step 259, loss 0.140856, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:12:19.138201: step 260, loss 1.54156, accuracy 0.75, precision 0.3333333333333333, recall 0.3333333333333333
2019-02-20T17:12:19.333128: step 261, loss 0.00607119, accuracy 1, precision nan, recall nan
2019-02-20T17:12:19.530110: step 262, loss 1.4541, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:19.721548: step 263, loss 0.041374, accuracy 1, precision nan, recall nan
2019-02-20T17:12:19.914675: step 264, loss 0.259316, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:20.107871: step 265, loss 0.627914, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:12:20.305166: step 266, loss 0.00631631, accuracy 1, precision nan, recall nan
2019-02-20T17:12:20.499946: step 267, loss 0.32368, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:20.697792: step 268, loss 0.287715, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:20.893442: step 269, loss 0.860468, accuracy 0.75, precision 0.25, recall 0.5
2019-02-20T17:12:21.092104: step 270, loss 0.488072, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:21.287335: step 271, loss 0.115171, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:21.478566: step 272, loss 0.428155, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:21.674831: step 273, loss 0.392492, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:21.870107: step 274, loss 1.30069, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:22.065746: step 275, loss 1.74883, accuracy 0.75, precision 0.0, recall nan
2019-02-20T17:12:22.256839: step 276, loss 0.111813, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:22.444353: step 277, loss 0.756896, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:12:22.642576: step 278, loss 0.746872, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:12:22.837239: step 279, loss 1.15229, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:12:23.026441: step 280, loss 0.255414, accuracy 0.875, precision 1.0, recall 0.5
2019-02-20T17:12:23.220115: step 281, loss 0.845924, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:12:23.414569: step 282, loss 0.452706, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:23.604330: step 283, loss 0.176423, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:12:23.794448: step 284, loss 1.06639, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:23.991026: step 285, loss 1.64271, accuracy 0.625, precision 0.0, recall 0.0
2019-02-20T17:12:24.185386: step 286, loss 0.113297, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:24.382178: step 287, loss 1.51352, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:24.583887: step 288, loss 0.440674, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:24.778261: step 289, loss 1.67088, accuracy 0.6875, precision 0.3333333333333333, recall 0.25
2019-02-20T17:12:24.975875: step 290, loss 0.223633, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:25.175578: step 291, loss 0.565621, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:25.371292: step 292, loss 0.507713, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:25.568233: step 293, loss 0.543144, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:25.769000: step 294, loss 0.17232, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:25.960515: step 295, loss 1.02877, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:26.155153: step 296, loss 0.250019, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:26.344704: step 297, loss 0.137601, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:26.539909: step 298, loss 0.964391, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:26.732952: step 299, loss 0.1252, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:26.923256: step 300, loss 0.410486, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:27.116112: step 301, loss 0.640708, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:27.313867: step 302, loss 0.593765, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:27.511650: step 303, loss 0.0147018, accuracy 1, precision nan, recall nan
2019-02-20T17:12:27.706829: step 304, loss 0.600073, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:27.899955: step 305, loss 0.499386, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:28.093402: step 306, loss 0.429786, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:28.286423: step 307, loss 0.340928, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:12:28.482355: step 308, loss 0.0294075, accuracy 1, precision nan, recall nan
2019-02-20T17:12:28.675130: step 309, loss 1.00267, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:28.872297: step 310, loss 0.458489, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:29.068326: step 311, loss 0.0522127, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:29.263856: step 312, loss 1.37147, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:29.458871: step 313, loss 0.0050524, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:29.655040: step 314, loss 0.455216, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:12:29.846070: step 315, loss 0.124361, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:30.043190: step 316, loss 0.448296, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:30.232440: step 317, loss 0.157393, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:30.429039: step 318, loss 0.441057, accuracy 0.8125, precision nan, recall 0.0
2019-02-20T17:12:30.623268: step 319, loss 0.43741, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:30.823716: step 320, loss 1.4489, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:12:31.017579: step 321, loss 0.558496, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:31.213071: step 322, loss 0.428517, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:31.403767: step 323, loss 0.107139, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:31.598393: step 324, loss 1.16446, accuracy 0.75, precision 0.25, recall 0.5
2019-02-20T17:12:31.796049: step 325, loss 0.884436, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:31.992460: step 326, loss 0.307832, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:32.185132: step 327, loss 0.199806, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:32.378729: step 328, loss 0.685698, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:32.574094: step 329, loss 0.0640707, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:32.770307: step 330, loss 0.101502, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:32.962431: step 331, loss 1.09776, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:33.163482: step 332, loss 1.40504, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:33.358607: step 333, loss 0.186303, accuracy 0.875, precision 1.0, recall 0.5
2019-02-20T17:12:33.555918: step 334, loss 0.149699, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:33.747817: step 335, loss 0.564103, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:33.939117: step 336, loss 0.393775, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:12:34.134261: step 337, loss 0.544671, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:34.325349: step 338, loss 0.215726, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:34.514163: step 339, loss 0.317363, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:34.705524: step 340, loss 0.448735, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:34.901231: step 341, loss 0.67757, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:12:35.092293: step 342, loss 0.985767, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:35.285224: step 343, loss 0.361753, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:35.476900: step 344, loss 0.0205204, accuracy 1, precision nan, recall nan
2019-02-20T17:12:35.675786: step 345, loss 1.40949, accuracy 0.6875, precision 0.3333333333333333, recall 0.25
2019-02-20T17:12:35.872025: step 346, loss 0.604831, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:36.068256: step 347, loss 0.0722882, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:36.267469: step 348, loss 0.737038, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:12:36.460295: step 349, loss 1.16093, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:12:36.651674: step 350, loss 0.0914878, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:36.847779: step 351, loss 0.00609724, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:37.042983: step 352, loss 0.187633, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:37.237880: step 353, loss 0.0297968, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:37.433975: step 354, loss 1.42212, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:12:37.632140: step 355, loss 0.28131, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:12:37.833308: step 356, loss 0.354977, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:38.031784: step 357, loss 0.0552129, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:38.231108: step 358, loss 1.08491, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:38.427718: step 359, loss 0.0719323, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:38.615534: step 360, loss 0.131633, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:38.813332: step 361, loss 0.234797, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:39.010197: step 362, loss 1.11039, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:39.201260: step 363, loss 0.298465, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:39.396074: step 364, loss 0.287146, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:39.593859: step 365, loss 0.0764909, accuracy 1, precision nan, recall nan
2019-02-20T17:12:39.791949: step 366, loss 0.114751, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:39.988374: step 367, loss 0.0516213, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:40.182193: step 368, loss 1.64, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:40.371225: step 369, loss 0.00937062, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:40.565594: step 370, loss 0.195075, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:12:40.759465: step 371, loss 0.0652475, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:40.956057: step 372, loss 0.359876, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:41.149009: step 373, loss 0.0250769, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:41.349697: step 374, loss 0.477642, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:41.543174: step 375, loss 0.0154825, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:41.734663: step 376, loss 0.324097, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:41.924932: step 377, loss 1.18384, accuracy 0.625, precision 0.0, recall 0.0
2019-02-20T17:12:42.119253: step 378, loss 0.0678746, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:42.310921: step 379, loss 0.36098, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:12:42.507725: step 380, loss 0.78816, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:42.694986: step 381, loss 0.0205597, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:42.889406: step 382, loss 0.662686, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:12:43.081674: step 383, loss 0.152628, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:43.278005: step 384, loss 1.17526, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:12:43.473129: step 385, loss 0.312446, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:43.665585: step 386, loss 0.115627, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:43.861336: step 387, loss 0.849879, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:44.050228: step 388, loss 1.33543, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:44.248555: step 389, loss 0.798409, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:44.443987: step 390, loss 1.12498, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:44.636878: step 391, loss 0.405531, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:44.831226: step 392, loss 1.00821, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:45.028357: step 393, loss 0.251091, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:45.223464: step 394, loss 0.272581, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:45.419745: step 395, loss 0.571974, accuracy 0.8125, precision 0.6666666666666666, recall 0.5
2019-02-20T17:12:45.614840: step 396, loss 0.809978, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:12:45.807990: step 397, loss 0.284947, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:45.998692: step 398, loss 0.559818, accuracy 0.75, precision 0.6666666666666666, recall 0.4
2019-02-20T17:12:46.194038: step 399, loss 0.121912, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:46.388621: step 400, loss 0.184642, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:12:46.584140: step 401, loss 1.03352, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:46.776822: step 402, loss 0.492989, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:46.967997: step 403, loss 0.0028333, accuracy 1, precision nan, recall nan
2019-02-20T17:12:47.164990: step 404, loss 0.360621, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:47.363718: step 405, loss 0.499541, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:47.560708: step 406, loss 1.32775, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:47.752195: step 407, loss 0.0839236, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:47.951764: step 408, loss 1.17273, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:48.150478: step 409, loss 0.579308, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:48.349477: step 410, loss 0.0858701, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:48.545927: step 411, loss 0.105846, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:48.738398: step 412, loss 0.343496, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:48.929802: step 413, loss 0.0336321, accuracy 1, precision nan, recall nan
2019-02-20T17:12:49.123463: step 414, loss 0.181389, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:49.315972: step 415, loss 0.16596, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:49.507481: step 416, loss 0.368611, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:49.702353: step 417, loss 0.275968, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:12:49.892676: step 418, loss 0.453855, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:12:50.084754: step 419, loss 0.551012, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:12:50.277277: step 420, loss 0.915342, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:50.470643: step 421, loss 0.525404, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:50.664181: step 422, loss 0.885108, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:50.859674: step 423, loss 0.031774, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:51.055286: step 424, loss 1.13056, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:51.250790: step 425, loss 0.540833, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:51.444671: step 426, loss 0.278028, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:51.640481: step 427, loss 0.00932307, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:51.833287: step 428, loss 0.0819191, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:52.032258: step 429, loss 0.0538101, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:52.219433: step 430, loss 1.06646, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:52.412158: step 431, loss 0.245164, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:52.609542: step 432, loss 0.0314763, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:52.805333: step 433, loss 0.524606, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:52.998025: step 434, loss 0.320333, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:12:53.191449: step 435, loss 0.035942, accuracy 1, precision nan, recall nan
2019-02-20T17:12:53.381521: step 436, loss 0.422821, accuracy 0.75, precision 0.3333333333333333, recall 0.3333333333333333
2019-02-20T17:12:53.575604: step 437, loss 0.4176, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:12:53.771894: step 438, loss 0.263461, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:12:53.963787: step 439, loss 0.0776519, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:54.157693: step 440, loss 0.163857, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:54.352701: step 441, loss 0.270271, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:54.541580: step 442, loss 0.0282259, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:54.739568: step 443, loss 0.334672, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:54.935538: step 444, loss 1.54448, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:55.131416: step 445, loss 0.509417, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:12:55.329623: step 446, loss 0.206893, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:55.525317: step 447, loss 0.298274, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:55.723533: step 448, loss 0.423395, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:55.914548: step 449, loss 0.249491, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:56.108817: step 450, loss 0.587207, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:56.307344: step 451, loss 0.500026, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:12:56.496828: step 452, loss 1.02673, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:12:56.690415: step 453, loss 0.131233, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:56.883071: step 454, loss 1.2703, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:12:57.077221: step 455, loss 0.70317, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:12:57.272067: step 456, loss 0.132582, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:57.470021: step 457, loss 0.385842, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:12:57.664441: step 458, loss 0.625829, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:57.857439: step 459, loss 0.392374, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:12:58.051872: step 460, loss 0.133257, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:12:58.251912: step 461, loss 0.634911, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:12:58.446520: step 462, loss 0.758213, accuracy 0.75, precision 0.3333333333333333, recall 0.3333333333333333
2019-02-20T17:12:58.637803: step 463, loss 0.00578073, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:12:58.829347: step 464, loss 0.784341, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:12:59.018513: step 465, loss 0.0491941, accuracy 1, precision nan, recall nan
2019-02-20T17:12:59.213751: step 466, loss 0.303955, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:59.407274: step 467, loss 0.153834, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:12:59.597825: step 468, loss 0.445311, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:12:59.793778: step 469, loss 0.177495, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:12:59.984190: step 470, loss 0.218801, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:00.176264: step 471, loss 0.748602, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:00.366037: step 472, loss 0.520557, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:00.559588: step 473, loss 0.0752045, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:00.750474: step 474, loss 0.592616, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:00.940202: step 475, loss 0.0241568, accuracy 1, precision nan, recall nan
2019-02-20T17:13:01.132723: step 476, loss 0.648259, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:01.326625: step 477, loss 0.133053, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:13:01.522765: step 478, loss 0.422489, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:01.715509: step 479, loss 0.0237125, accuracy 1, precision nan, recall nan
2019-02-20T17:13:01.911612: step 480, loss 0.337417, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:13:02.106094: step 481, loss 0.0137563, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:02.298094: step 482, loss 0.533449, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:02.496466: step 483, loss 0.364078, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:02.693104: step 484, loss 0.0476761, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:02.887409: step 485, loss 0.252615, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:13:03.081616: step 486, loss 0.151321, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:03.273436: step 487, loss 0.435807, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:03.469869: step 488, loss 0.14647, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:13:03.659273: step 489, loss 0.64606, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:03.856148: step 490, loss 0.629599, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:04.046651: step 491, loss 0.71881, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:04.241867: step 492, loss 0.904468, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:04.436619: step 493, loss 0.00510013, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:04.629562: step 494, loss 0.671681, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:04.825123: step 495, loss 0.208326, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:05.019604: step 496, loss 0.608318, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:13:05.218711: step 497, loss 0.332349, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:05.409279: step 498, loss 1.27612, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:13:05.607596: step 499, loss 0.513031, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:05.803303: step 500, loss 0.164413, accuracy 0.9375, precision nan, recall 0.0

Evaluation:
[[ 60  35]
 [ 18 912]]
2019-02-20T17:13:07.563178: step 500, loss 0.165954, accuracy 0.948293, precision 0.631578947368421, recall 0.7692307692307693

Saved model checkpoint to /home/ubuntu/Project/runs/1550682642/checkpoints/model-500

2019-02-20T17:13:07.854975: step 501, loss 0.529208, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:08.049824: step 502, loss 0.73178, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:13:08.245608: step 503, loss 0.266681, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:08.443919: step 504, loss 0.403962, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:13:08.633862: step 505, loss 0.665653, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:13:08.826882: step 506, loss 0.0157257, accuracy 1, precision nan, recall nan
2019-02-20T17:13:09.021500: step 507, loss 0.0060513, accuracy 1, precision nan, recall nan
2019-02-20T17:13:09.219333: step 508, loss 0.366009, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:13:09.411525: step 509, loss 0.123558, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:13:09.603697: step 510, loss 0.840736, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:09.802028: step 511, loss 0.0590291, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:09.999216: step 512, loss 0.501908, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:13:10.191739: step 513, loss 0.85604, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:10.388457: step 514, loss 0.77948, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:10.581875: step 515, loss 0.692144, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:13:10.773465: step 516, loss 0.996883, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:10.968206: step 517, loss 0.351335, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:11.163771: step 518, loss 0.0527573, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:11.359876: step 519, loss 0.241593, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:13:11.554064: step 520, loss 0.0465437, accuracy 1, precision nan, recall nan
2019-02-20T17:13:11.745118: step 521, loss 0.0791445, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:11.940420: step 522, loss 0.364163, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:12.133730: step 523, loss 0.847355, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:13:12.329457: step 524, loss 0.0341706, accuracy 1, precision nan, recall nan
2019-02-20T17:13:12.523689: step 525, loss 0.472575, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:12.715322: step 526, loss 0.310511, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:12.909568: step 527, loss 1.14051, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:13.103681: step 528, loss 0.244544, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:13.298735: step 529, loss 0.245145, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:13.489784: step 530, loss 0.499372, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:13.686143: step 531, loss 0.259638, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:13.882073: step 532, loss 0.133137, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:13:14.075267: step 533, loss 0.46756, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:13:14.267101: step 534, loss 0.288312, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:14.460440: step 535, loss 0.317063, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:14.654064: step 536, loss 0.537646, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:14.848735: step 537, loss 0.237676, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:15.043927: step 538, loss 0.143271, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:15.235928: step 539, loss 0.032534, accuracy 1, precision nan, recall nan
2019-02-20T17:13:15.434895: step 540, loss 0.101294, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:15.629865: step 541, loss 0.0880708, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:15.820951: step 542, loss 0.411641, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:16.014767: step 543, loss 0.173011, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:16.210712: step 544, loss 0.0436245, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:16.405288: step 545, loss 0.496851, accuracy 0.8125, precision nan, recall 0.0
2019-02-20T17:13:16.599663: step 546, loss 0.0279425, accuracy 1, precision nan, recall nan
2019-02-20T17:13:16.793578: step 547, loss 0.040753, accuracy 1, precision nan, recall nan
2019-02-20T17:13:16.984560: step 548, loss 0.783728, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:17.178875: step 549, loss 0.0857361, accuracy 1, precision nan, recall nan
2019-02-20T17:13:17.369929: step 550, loss 0.661487, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:17.557125: step 551, loss 0.0247902, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:17.747098: step 552, loss 0.116479, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:13:17.941048: step 553, loss 0.15241, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:18.136818: step 554, loss 0.711953, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:13:18.333283: step 555, loss 0.0550603, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:18.526001: step 556, loss 0.0184446, accuracy 1, precision nan, recall nan
2019-02-20T17:13:18.720756: step 557, loss 0.0320785, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:18.915459: step 558, loss 0.177123, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:13:19.109309: step 559, loss 0.584317, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:13:19.303980: step 560, loss 0.108421, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:19.500048: step 561, loss 0.125268, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:19.697039: step 562, loss 0.0198978, accuracy 1, precision nan, recall nan
2019-02-20T17:13:19.893099: step 563, loss 0.784722, accuracy 0.8125, precision 0.6666666666666666, recall 0.5
2019-02-20T17:13:20.088045: step 564, loss 0.256352, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:13:20.283234: step 565, loss 1.36639, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:20.471310: step 566, loss 0.0340045, accuracy 1, precision nan, recall nan
2019-02-20T17:13:20.666281: step 567, loss 0.304206, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:13:20.860683: step 568, loss 0.0547933, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:21.055677: step 569, loss 0.499531, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:21.250260: step 570, loss 0.693823, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:13:21.442818: step 571, loss 0.667177, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:13:21.635464: step 572, loss 0.573677, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:21.829871: step 573, loss 0.636928, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:13:22.022929: step 574, loss 0.0682924, accuracy 1, precision nan, recall nan
2019-02-20T17:13:22.218781: step 575, loss 0.64927, accuracy 0.875, precision 0.75, recall 0.75
2019-02-20T17:13:22.410432: step 576, loss 0.783543, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:13:22.688996: step 577, loss 0.789064, accuracy 0.8, precision 0.0, recall 0.0
2019-02-20T17:13:22.887108: step 578, loss 0.326692, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:13:23.081089: step 579, loss 0.432598, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:23.279435: step 580, loss 0.177403, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:23.471708: step 581, loss 0.0161529, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:23.665904: step 582, loss 0.0111656, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:23.859834: step 583, loss 0.25977, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:24.058506: step 584, loss 0.0192151, accuracy 1, precision nan, recall nan
2019-02-20T17:13:24.252820: step 585, loss 0.0115598, accuracy 1, precision nan, recall nan
2019-02-20T17:13:24.449862: step 586, loss 0.114776, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:13:24.644326: step 587, loss 0.019911, accuracy 1, precision nan, recall nan
2019-02-20T17:13:24.842490: step 588, loss 0.0813711, accuracy 1, precision nan, recall nan
2019-02-20T17:13:25.040699: step 589, loss 0.296901, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:25.234578: step 590, loss 0.00238388, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:25.430585: step 591, loss 1.41167, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:13:25.629057: step 592, loss 0.100076, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:25.822341: step 593, loss 0.367895, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:26.014565: step 594, loss 0.0330351, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:26.209429: step 595, loss 0.0181496, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:26.404618: step 596, loss 0.183897, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:26.600862: step 597, loss 0.189759, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:13:26.795491: step 598, loss 1.243, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:13:26.992026: step 599, loss 0.730784, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:27.180787: step 600, loss 0.0565744, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:27.370357: step 601, loss 0.104717, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:27.563647: step 602, loss 0.871986, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:27.751590: step 603, loss 0.230139, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:13:27.946209: step 604, loss 0.124224, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:28.135118: step 605, loss 0.0506963, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:28.329759: step 606, loss 0.363773, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:28.520478: step 607, loss 0.598413, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:28.718253: step 608, loss 0.247559, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:13:28.909101: step 609, loss 1.05135, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:13:29.104002: step 610, loss 0.0170375, accuracy 1, precision nan, recall nan
2019-02-20T17:13:29.294759: step 611, loss 0.710425, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:13:29.485111: step 612, loss 0.420141, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:29.675987: step 613, loss 0.18678, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:29.870301: step 614, loss 0.0244153, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:30.062222: step 615, loss 0.843038, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:13:30.255505: step 616, loss 0.0508469, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:30.446311: step 617, loss 0.117938, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:30.641814: step 618, loss 0.442857, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:30.831731: step 619, loss 0.267626, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:31.026810: step 620, loss 0.237423, accuracy 0.8125, precision nan, recall 0.0
2019-02-20T17:13:31.213867: step 621, loss 0.125118, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:31.411501: step 622, loss 0.0989508, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:13:31.608444: step 623, loss 0.289047, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:31.801335: step 624, loss 0.181504, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:31.992234: step 625, loss 0.422138, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:32.185103: step 626, loss 0.235122, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:32.380558: step 627, loss 0.203996, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:32.575164: step 628, loss 0.00694558, accuracy 1, precision nan, recall nan
2019-02-20T17:13:32.769675: step 629, loss 0.219765, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:32.960029: step 630, loss 0.0172193, accuracy 1, precision nan, recall nan
2019-02-20T17:13:33.155754: step 631, loss 0.892953, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:13:33.352980: step 632, loss 0.0389588, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:33.546034: step 633, loss 0.103466, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:13:33.741255: step 634, loss 0.0215203, accuracy 1, precision nan, recall nan
2019-02-20T17:13:33.938101: step 635, loss 0.0413666, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:34.127584: step 636, loss 0.60914, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:13:34.323106: step 637, loss 0.245111, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:34.515670: step 638, loss 0.00550115, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:34.710632: step 639, loss 0.337895, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:34.906819: step 640, loss 0.0240451, accuracy 1, precision nan, recall nan
2019-02-20T17:13:35.099125: step 641, loss 0.00844407, accuracy 1, precision nan, recall nan
2019-02-20T17:13:35.290497: step 642, loss 0.388663, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:35.485942: step 643, loss 0.248001, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:35.683371: step 644, loss 0.144837, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:35.874138: step 645, loss 0.140933, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:36.069521: step 646, loss 0.716802, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:13:36.258325: step 647, loss 0.508663, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:36.450662: step 648, loss 0.774949, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:13:36.641687: step 649, loss 0.011711, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:36.837268: step 650, loss 0.147884, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:37.028422: step 651, loss 0.348369, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:13:37.215928: step 652, loss 0.0750657, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:37.408405: step 653, loss 0.166768, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:13:37.597278: step 654, loss 0.235074, accuracy 0.875, precision 1.0, recall 0.5
2019-02-20T17:13:37.787166: step 655, loss 0.167215, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:37.981077: step 656, loss 0.209592, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:38.176411: step 657, loss 0.180835, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:13:38.368239: step 658, loss 0.302187, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:13:38.564519: step 659, loss 0.257181, accuracy 0.875, precision 1.0, recall 0.5
2019-02-20T17:13:38.763921: step 660, loss 0.691938, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:13:38.957553: step 661, loss 0.0152002, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:39.155474: step 662, loss 0.0752441, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:13:39.344562: step 663, loss 0.0205208, accuracy 1, precision nan, recall nan
2019-02-20T17:13:39.532302: step 664, loss 0.0343164, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:39.725926: step 665, loss 0.359169, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:39.913529: step 666, loss 0.507829, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:13:40.109136: step 667, loss 0.211812, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:13:40.300872: step 668, loss 0.0110102, accuracy 1, precision nan, recall nan
2019-02-20T17:13:40.493719: step 669, loss 0.100653, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:40.687844: step 670, loss 0.576541, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:40.881650: step 671, loss 0.659724, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:41.070037: step 672, loss 0.387052, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:41.257285: step 673, loss 0.0622642, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:41.456404: step 674, loss 0.00530834, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:41.652386: step 675, loss 0.0416292, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:41.842039: step 676, loss 0.593665, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:13:42.040142: step 677, loss 0.650073, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:42.229849: step 678, loss 0.711524, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:42.424865: step 679, loss 0.147986, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:42.618471: step 680, loss 0.126975, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:13:42.817610: step 681, loss 0.585054, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:43.016319: step 682, loss 0.291659, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:43.210621: step 683, loss 0.0893494, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:43.399794: step 684, loss 0.00214571, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:43.592726: step 685, loss 0.158433, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:43.786598: step 686, loss 0.0264527, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:43.975137: step 687, loss 0.0323731, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:44.170537: step 688, loss 0.975616, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:44.370912: step 689, loss 0.133996, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:13:44.562414: step 690, loss 0.411606, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:44.753949: step 691, loss 0.580008, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:44.946416: step 692, loss 0.347596, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:13:45.139345: step 693, loss 0.768006, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:13:45.331819: step 694, loss 0.506409, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:45.525058: step 695, loss 0.184915, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:45.714742: step 696, loss 0.054828, accuracy 1, precision nan, recall nan
2019-02-20T17:13:45.909877: step 697, loss 0.0143024, accuracy 1, precision nan, recall nan
2019-02-20T17:13:46.103376: step 698, loss 0.0336974, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:46.300683: step 699, loss 0.0373558, accuracy 1, precision nan, recall nan
2019-02-20T17:13:46.494015: step 700, loss 0.0926227, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:46.690086: step 701, loss 0.26682, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:46.890716: step 702, loss 0.0189643, accuracy 1, precision nan, recall nan
2019-02-20T17:13:47.090350: step 703, loss 0.0339687, accuracy 1, precision nan, recall nan
2019-02-20T17:13:47.281673: step 704, loss 0.14145, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:47.475459: step 705, loss 0.0981119, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:47.671736: step 706, loss 0.623123, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:47.862919: step 707, loss 0.00455073, accuracy 1, precision nan, recall nan
2019-02-20T17:13:48.051103: step 708, loss 1.00601, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:13:48.239849: step 709, loss 0.442014, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:48.431419: step 710, loss 0.126224, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:13:48.624099: step 711, loss 0.271814, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:48.817568: step 712, loss 0.226813, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:49.012281: step 713, loss 0.76232, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:13:49.206899: step 714, loss 0.00878116, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:49.399083: step 715, loss 0.509022, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:13:49.586064: step 716, loss 0.0871217, accuracy 1, precision nan, recall nan
2019-02-20T17:13:49.782116: step 717, loss 0.308383, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:13:49.969522: step 718, loss 0.0154682, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:50.157842: step 719, loss 0.617353, accuracy 0.6875, precision 0.0, recall 0.0
2019-02-20T17:13:50.348965: step 720, loss 0.525881, accuracy 0.75, precision 1.0, recall 0.2
2019-02-20T17:13:50.542958: step 721, loss 0.354444, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:13:50.734506: step 722, loss 0.164764, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:50.926685: step 723, loss 0.0211131, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:51.125625: step 724, loss 0.127222, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:51.322288: step 725, loss 0.539538, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:51.515779: step 726, loss 0.156161, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:51.708546: step 727, loss 0.0129047, accuracy 1, precision nan, recall nan
2019-02-20T17:13:51.901710: step 728, loss 0.0913351, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:52.098264: step 729, loss 0.150524, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:13:52.295819: step 730, loss 0.977737, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:52.491329: step 731, loss 0.283145, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:13:52.686239: step 732, loss 0.933018, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:52.876799: step 733, loss 0.429732, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:13:53.068941: step 734, loss 0.0634732, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:53.264149: step 735, loss 0.465032, accuracy 0.8125, precision 0.6, recall 0.75
2019-02-20T17:13:53.456362: step 736, loss 0.0262911, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:53.647340: step 737, loss 0.416732, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:13:53.843140: step 738, loss 0.503067, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:13:54.034268: step 739, loss 0.0725392, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:54.224973: step 740, loss 0.0852633, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:54.422191: step 741, loss 0.112885, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:54.614360: step 742, loss 0.156936, accuracy 0.9375, precision 1.0, recall 0.8
2019-02-20T17:13:54.808288: step 743, loss 0.0374802, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:55.000305: step 744, loss 0.0647194, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:55.191432: step 745, loss 0.319489, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:13:55.385572: step 746, loss 0.0346531, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:55.577835: step 747, loss 0.0122401, accuracy 1, precision nan, recall nan
2019-02-20T17:13:55.767258: step 748, loss 0.0339546, accuracy 1, precision nan, recall nan
2019-02-20T17:13:55.957561: step 749, loss 0.372063, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:13:56.153696: step 750, loss 0.196297, accuracy 0.9375, precision 0.0, recall nan

Evaluation:
[[ 23  72]
 [  1 929]]
2019-02-20T17:13:57.926173: step 750, loss 0.234848, accuracy 0.92878, precision 0.24210526315789474, recall 0.9583333333333334

Saved model checkpoint to /home/ubuntu/Project/runs/1550682642/checkpoints/model-750

2019-02-20T17:13:58.214683: step 751, loss 0.493181, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:58.413494: step 752, loss 0.498095, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:58.610051: step 753, loss 0.0551999, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:58.803229: step 754, loss 0.162695, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:13:58.995043: step 755, loss 1.28703, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:13:59.191726: step 756, loss 0.352287, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:13:59.383437: step 757, loss 0.76807, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:13:59.578902: step 758, loss 0.0883724, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:13:59.773085: step 759, loss 0.49611, accuracy 0.8125, precision nan, recall 0.0
2019-02-20T17:13:59.969419: step 760, loss 0.0546072, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:00.159528: step 761, loss 0.481081, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:00.353224: step 762, loss 0.197983, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:14:00.545501: step 763, loss 0.0145559, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:00.737738: step 764, loss 0.412996, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:14:00.930841: step 765, loss 0.199033, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:01.121952: step 766, loss 0.101308, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:01.315655: step 767, loss 0.460459, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:14:01.504138: step 768, loss 0.0692205, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:01.695810: step 769, loss 0.0321524, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:01.892164: step 770, loss 0.171923, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:14:02.092199: step 771, loss 0.169727, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:02.287958: step 772, loss 0.825969, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:02.482371: step 773, loss 0.397049, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:02.674901: step 774, loss 0.007832, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:02.867615: step 775, loss 0.741572, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:03.063799: step 776, loss 0.107926, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:03.258438: step 777, loss 0.0791357, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:03.454725: step 778, loss 0.00769111, accuracy 1, precision nan, recall nan
2019-02-20T17:14:03.648519: step 779, loss 0.116647, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:03.846492: step 780, loss 0.148528, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:04.039493: step 781, loss 0.118059, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:04.243255: step 782, loss 0.346607, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:04.437915: step 783, loss 0.106441, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:04.627731: step 784, loss 0.0649035, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:04.822100: step 785, loss 0.661873, accuracy 0.8125, precision 0.4, recall 1.0
2019-02-20T17:14:05.015142: step 786, loss 0.22994, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:05.205933: step 787, loss 0.0238502, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:05.399367: step 788, loss 0.407475, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:14:05.594998: step 789, loss 0.061253, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:05.787748: step 790, loss 0.435894, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:05.982546: step 791, loss 0.154013, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:14:06.179401: step 792, loss 0.205322, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:06.374020: step 793, loss 0.17526, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:06.568704: step 794, loss 0.289955, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:14:06.762552: step 795, loss 0.18251, accuracy 0.875, precision 0.8, recall 0.8
2019-02-20T17:14:06.958707: step 796, loss 0.134912, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:07.157029: step 797, loss 0.9358, accuracy 0.8125, precision nan, recall 0.0
2019-02-20T17:14:07.354398: step 798, loss 0.462718, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:07.548486: step 799, loss 0.105035, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:07.739710: step 800, loss 0.130062, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:07.934331: step 801, loss 0.943022, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:14:08.127045: step 802, loss 0.248701, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:08.321629: step 803, loss 0.0676938, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:08.518361: step 804, loss 0.178148, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:08.711642: step 805, loss 0.0934904, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:08.906012: step 806, loss 0.338381, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:09.102289: step 807, loss 0.532518, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:14:09.290598: step 808, loss 0.0342078, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:09.485312: step 809, loss 0.497675, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:14:09.676095: step 810, loss 0.0385574, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:09.868165: step 811, loss 0.604227, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:10.061980: step 812, loss 0.165183, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:10.257302: step 813, loss 0.046464, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:10.448324: step 814, loss 0.578007, accuracy 0.875, precision 1.0, recall 0.5
2019-02-20T17:14:10.635128: step 815, loss 0.440013, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:14:10.830043: step 816, loss 0.381078, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:11.022491: step 817, loss 0.204338, accuracy 0.9375, precision 1.0, recall 0.75
2019-02-20T17:14:11.216123: step 818, loss 0.226368, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:11.412122: step 819, loss 0.0847791, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:11.607680: step 820, loss 0.135068, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:11.803610: step 821, loss 0.0342353, accuracy 1, precision nan, recall nan
2019-02-20T17:14:11.997339: step 822, loss 0.0419222, accuracy 1, precision nan, recall nan
2019-02-20T17:14:12.192541: step 823, loss 0.320827, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:12.382439: step 824, loss 0.136305, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:12.578946: step 825, loss 0.233107, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:12.773417: step 826, loss 0.16082, accuracy 0.875, precision 1.0, recall 0.6
2019-02-20T17:14:12.970076: step 827, loss 0.124898, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:13.164334: step 828, loss 0.287161, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:13.355344: step 829, loss 0.117489, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:13.546211: step 830, loss 0.614934, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:13.737631: step 831, loss 0.432451, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:14:13.932641: step 832, loss 0.0146509, accuracy 1, precision nan, recall nan
2019-02-20T17:14:14.122999: step 833, loss 0.046821, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:14.314688: step 834, loss 0.119858, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:14.507796: step 835, loss 0.0689995, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:14.695065: step 836, loss 0.399198, accuracy 0.8125, precision 0.75, recall 0.6
2019-02-20T17:14:14.889782: step 837, loss 0.203311, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:15.079604: step 838, loss 0.150118, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:14:15.274539: step 839, loss 0.0619329, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:14:15.472220: step 840, loss 0.0955037, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:15.666786: step 841, loss 0.0376029, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:15.860287: step 842, loss 0.312275, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:14:16.051625: step 843, loss 0.156642, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:16.243598: step 844, loss 0.293277, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:16.431630: step 845, loss 0.415284, accuracy 0.8125, precision 0.5, recall 0.6666666666666666
2019-02-20T17:14:16.623794: step 846, loss 0.0691837, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:16.818977: step 847, loss 0.442331, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:14:17.006911: step 848, loss 0.155979, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:17.195020: step 849, loss 0.470418, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:17.385796: step 850, loss 0.26212, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:17.584497: step 851, loss 0.0597639, accuracy 1, precision nan, recall nan
2019-02-20T17:14:17.776850: step 852, loss 0.765873, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:17.972095: step 853, loss 0.0163241, accuracy 1, precision nan, recall nan
2019-02-20T17:14:18.162383: step 854, loss 0.233084, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:18.355980: step 855, loss 0.0108004, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:18.549856: step 856, loss 0.0216974, accuracy 1, precision nan, recall nan
2019-02-20T17:14:18.742203: step 857, loss 0.632808, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:14:18.937086: step 858, loss 0.424825, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:14:19.129348: step 859, loss 0.117871, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:19.322638: step 860, loss 0.777139, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:19.518602: step 861, loss 0.4195, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:19.715112: step 862, loss 0.0616425, accuracy 1, precision nan, recall nan
2019-02-20T17:14:19.903720: step 863, loss 0.451376, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:20.097735: step 864, loss 0.039309, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:20.291898: step 865, loss 0.0937753, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:20.486739: step 866, loss 0.0145008, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:20.683944: step 867, loss 0.056708, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:20.877509: step 868, loss 0.177739, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:21.068239: step 869, loss 0.0376559, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:21.264807: step 870, loss 0.110539, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:21.459614: step 871, loss 0.158439, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:14:21.653609: step 872, loss 0.214839, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:14:21.845732: step 873, loss 0.0813367, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:22.034365: step 874, loss 0.167667, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:22.226626: step 875, loss 0.381836, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:14:22.416982: step 876, loss 0.578705, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:14:22.608850: step 877, loss 0.422693, accuracy 0.9375, precision 1.0, recall 0.8
2019-02-20T17:14:22.805269: step 878, loss 0.112774, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:23.000418: step 879, loss 0.139519, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:23.192382: step 880, loss 0.421703, accuracy 0.8125, precision 0.6666666666666666, recall 0.5
2019-02-20T17:14:23.380486: step 881, loss 0.0963285, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:23.574313: step 882, loss 0.14776, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:23.767445: step 883, loss 0.0242378, accuracy 1, precision nan, recall nan
2019-02-20T17:14:23.962402: step 884, loss 0.0225645, accuracy 1, precision nan, recall nan
2019-02-20T17:14:24.155791: step 885, loss 0.19749, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:24.350476: step 886, loss 0.0561552, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:24.544300: step 887, loss 0.00550161, accuracy 1, precision nan, recall nan
2019-02-20T17:14:24.741630: step 888, loss 0.382426, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:24.936567: step 889, loss 0.0364556, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:25.131558: step 890, loss 0.201319, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:14:25.325285: step 891, loss 0.393073, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:25.520274: step 892, loss 0.0105551, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:25.715932: step 893, loss 0.419309, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:25.908131: step 894, loss 0.0308362, accuracy 1, precision nan, recall nan
2019-02-20T17:14:26.106888: step 895, loss 0.153042, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:26.303329: step 896, loss 0.0388417, accuracy 1, precision nan, recall nan
2019-02-20T17:14:26.494059: step 897, loss 0.00933069, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:26.685497: step 898, loss 0.0109932, accuracy 1, precision nan, recall nan
2019-02-20T17:14:26.881160: step 899, loss 0.217327, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:14:27.074458: step 900, loss 0.519261, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:14:27.272648: step 901, loss 0.214821, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:27.472279: step 902, loss 0.03532, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:27.670243: step 903, loss 0.0469604, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:27.863533: step 904, loss 0.0363071, accuracy 1, precision nan, recall nan
2019-02-20T17:14:28.056342: step 905, loss 0.0167875, accuracy 1, precision nan, recall nan
2019-02-20T17:14:28.252512: step 906, loss 0.461702, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:28.439479: step 907, loss 0.324187, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:14:28.631654: step 908, loss 0.0958884, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:28.822502: step 909, loss 0.627526, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:29.015907: step 910, loss 0.0679351, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:29.207507: step 911, loss 0.371139, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:29.401109: step 912, loss 0.140879, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:29.594125: step 913, loss 0.0734397, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:29.788568: step 914, loss 0.08657, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:29.987130: step 915, loss 0.0425399, accuracy 1, precision nan, recall nan
2019-02-20T17:14:30.179852: step 916, loss 0.218898, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:30.372296: step 917, loss 0.132179, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:14:30.566637: step 918, loss 1.07168, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:14:30.762278: step 919, loss 0.0108303, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:30.956580: step 920, loss 0.26917, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:31.144785: step 921, loss 0.389734, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:14:31.335736: step 922, loss 0.233936, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:31.531350: step 923, loss 0.101239, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:31.721986: step 924, loss 0.38671, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:14:31.918915: step 925, loss 0.153596, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:32.111251: step 926, loss 0.645428, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:32.310675: step 927, loss 0.00871884, accuracy 1, precision nan, recall nan
2019-02-20T17:14:32.506066: step 928, loss 0.018946, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:32.696351: step 929, loss 0.0456245, accuracy 1, precision nan, recall nan
2019-02-20T17:14:32.891865: step 930, loss 0.785955, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:14:33.084957: step 931, loss 0.0453416, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:33.278285: step 932, loss 0.779954, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:33.471172: step 933, loss 0.132905, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:33.664405: step 934, loss 1.07892, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:14:33.856303: step 935, loss 0.0600982, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:34.056730: step 936, loss 0.23324, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:34.245693: step 937, loss 0.484729, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:34.435366: step 938, loss 0.095105, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:34.628340: step 939, loss 0.107973, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:34.819761: step 940, loss 0.0276259, accuracy 1, precision nan, recall nan
2019-02-20T17:14:35.012202: step 941, loss 0.430064, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:35.202345: step 942, loss 0.749735, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:14:35.397219: step 943, loss 0.260533, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:35.592597: step 944, loss 0.19864, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:35.786110: step 945, loss 0.446321, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:14:35.981875: step 946, loss 0.840072, accuracy 0.8125, precision 0.4, recall 1.0
2019-02-20T17:14:36.179435: step 947, loss 0.0483084, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:36.374550: step 948, loss 0.457909, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:14:36.567710: step 949, loss 0.0104302, accuracy 1, precision nan, recall nan
2019-02-20T17:14:36.761954: step 950, loss 0.453942, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:36.956509: step 951, loss 0.130662, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:37.152664: step 952, loss 0.608513, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:14:37.346398: step 953, loss 0.201129, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:37.537640: step 954, loss 0.0478033, accuracy 1, precision nan, recall nan
2019-02-20T17:14:37.726642: step 955, loss 0.18186, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:14:37.919016: step 956, loss 0.0760212, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:38.115747: step 957, loss 0.243787, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:14:38.307376: step 958, loss 0.5127, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:38.499465: step 959, loss 0.0850795, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:38.693136: step 960, loss 0.0986592, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:38.879611: step 961, loss 0.228103, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:39.072038: step 962, loss 0.064032, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:39.262755: step 963, loss 0.0127577, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:39.455749: step 964, loss 0.42875, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:39.648077: step 965, loss 0.207267, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:14:39.838895: step 966, loss 0.148602, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:40.030406: step 967, loss 0.0390461, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:40.225965: step 968, loss 0.020736, accuracy 1, precision nan, recall nan
2019-02-20T17:14:40.418833: step 969, loss 0.389036, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:14:40.609286: step 970, loss 0.0121327, accuracy 1, precision nan, recall nan
2019-02-20T17:14:40.800679: step 971, loss 0.146104, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:40.995748: step 972, loss 0.437234, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:41.186394: step 973, loss 0.353262, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:14:41.380744: step 974, loss 0.00199087, accuracy 1, precision nan, recall nan
2019-02-20T17:14:41.576882: step 975, loss 0.163034, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:14:41.766073: step 976, loss 0.0177248, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:41.964605: step 977, loss 0.506258, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:42.160964: step 978, loss 0.377085, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:42.357111: step 979, loss 0.415514, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:42.546610: step 980, loss 0.402035, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:42.739883: step 981, loss 0.134776, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:42.937203: step 982, loss 0.0214999, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:43.131281: step 983, loss 0.264769, accuracy 0.9375, precision 1.0, recall 0.75
2019-02-20T17:14:43.326805: step 984, loss 0.341932, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:43.521254: step 985, loss 0.43061, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:14:43.711461: step 986, loss 0.24763, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:43.901977: step 987, loss 0.114837, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:44.101441: step 988, loss 0.162935, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:44.296862: step 989, loss 0.622197, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:14:44.491527: step 990, loss 0.209129, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:44.682059: step 991, loss 0.505591, accuracy 0.8125, precision 0.5, recall 0.6666666666666666
2019-02-20T17:14:44.872036: step 992, loss 0.2906, accuracy 0.875, precision 0.75, recall 0.75
2019-02-20T17:14:45.070458: step 993, loss 0.0123236, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:45.262921: step 994, loss 0.31267, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:14:45.454952: step 995, loss 0.0794769, accuracy 1, precision nan, recall nan
2019-02-20T17:14:45.644759: step 996, loss 0.0866946, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:45.840867: step 997, loss 0.125481, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:46.033046: step 998, loss 0.0276339, accuracy 1, precision nan, recall nan
2019-02-20T17:14:46.222659: step 999, loss 0.0497288, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:46.421142: step 1000, loss 0.567145, accuracy 0.875, precision 0.5, recall 1.0

Evaluation:
[[ 52  43]
 [  9 921]]
2019-02-20T17:14:48.185790: step 1000, loss 0.158305, accuracy 0.949268, precision 0.5473684210526316, recall 0.8524590163934426

Saved model checkpoint to /home/ubuntu/Project/runs/1550682642/checkpoints/model-1000

2019-02-20T17:14:48.474902: step 1001, loss 0.031952, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:48.673029: step 1002, loss 0.306894, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:48.868144: step 1003, loss 0.0711858, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:49.058220: step 1004, loss 0.492564, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:49.245824: step 1005, loss 0.396536, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:14:49.443131: step 1006, loss 0.249909, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:49.635347: step 1007, loss 0.0219814, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:49.833327: step 1008, loss 0.0810463, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:50.028893: step 1009, loss 0.511517, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:50.219051: step 1010, loss 0.209567, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:14:50.411506: step 1011, loss 0.0132098, accuracy 1, precision nan, recall nan
2019-02-20T17:14:50.606794: step 1012, loss 0.0601559, accuracy 1, precision nan, recall nan
2019-02-20T17:14:50.802178: step 1013, loss 0.126469, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:14:50.996872: step 1014, loss 0.0914516, accuracy 1, precision nan, recall nan
2019-02-20T17:14:51.190175: step 1015, loss 0.167941, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:14:51.382450: step 1016, loss 0.467577, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:51.575451: step 1017, loss 0.165457, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:51.771898: step 1018, loss 0.396103, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:51.962466: step 1019, loss 0.0206151, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:52.156972: step 1020, loss 0.668415, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:14:52.345188: step 1021, loss 0.130923, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:52.539781: step 1022, loss 0.151112, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:52.732799: step 1023, loss 0.134517, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:52.928073: step 1024, loss 0.350249, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:14:53.122624: step 1025, loss 0.0638212, accuracy 1, precision nan, recall nan
2019-02-20T17:14:53.320102: step 1026, loss 0.344301, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:53.507636: step 1027, loss 0.2087, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:14:53.701247: step 1028, loss 0.102158, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:53.894319: step 1029, loss 0.0521048, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:54.083704: step 1030, loss 0.153273, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:54.273499: step 1031, loss 0.787789, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:54.470818: step 1032, loss 0.342395, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:54.669530: step 1033, loss 1.09177, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:14:54.862632: step 1034, loss 0.044794, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:55.060340: step 1035, loss 0.196048, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:55.257206: step 1036, loss 0.199933, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:14:55.450976: step 1037, loss 0.0757363, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:55.643409: step 1038, loss 0.167543, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:14:55.839783: step 1039, loss 0.115439, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:56.034740: step 1040, loss 0.309912, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:14:56.226950: step 1041, loss 0.464082, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:14:56.415096: step 1042, loss 0.174241, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:14:56.606653: step 1043, loss 0.274495, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:14:56.799975: step 1044, loss 0.0836792, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:56.993664: step 1045, loss 0.544795, accuracy 0.75, precision nan, recall 0.0
2019-02-20T17:14:57.190462: step 1046, loss 0.0901351, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:57.387461: step 1047, loss 0.354856, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:57.577583: step 1048, loss 0.0247516, accuracy 1, precision nan, recall nan
2019-02-20T17:14:57.766944: step 1049, loss 0.0807971, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:57.961501: step 1050, loss 0.262311, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:14:58.155140: step 1051, loss 0.0405317, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:14:58.344984: step 1052, loss 0.363859, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:14:58.538389: step 1053, loss 0.0603144, accuracy 1, precision nan, recall nan
2019-02-20T17:14:58.734239: step 1054, loss 0.0173421, accuracy 1, precision nan, recall nan
2019-02-20T17:14:58.927544: step 1055, loss 0.25401, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:14:59.122681: step 1056, loss 0.033884, accuracy 1, precision nan, recall nan
2019-02-20T17:14:59.320241: step 1057, loss 0.374272, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:14:59.514032: step 1058, loss 0.200082, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:14:59.706784: step 1059, loss 0.0108669, accuracy 1, precision nan, recall nan
2019-02-20T17:14:59.902099: step 1060, loss 0.123219, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:00.102232: step 1061, loss 0.0563665, accuracy 1, precision nan, recall nan
2019-02-20T17:15:00.292963: step 1062, loss 0.122846, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:00.490592: step 1063, loss 0.444181, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:00.684808: step 1064, loss 0.0339172, accuracy 1, precision nan, recall nan
2019-02-20T17:15:00.875276: step 1065, loss 0.208438, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:01.067804: step 1066, loss 0.812337, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:15:01.261127: step 1067, loss 0.194777, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:01.453820: step 1068, loss 0.00446679, accuracy 1, precision nan, recall nan
2019-02-20T17:15:01.646326: step 1069, loss 0.320323, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:01.837111: step 1070, loss 0.488739, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:02.029312: step 1071, loss 0.141259, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:02.222050: step 1072, loss 0.590195, accuracy 0.75, precision 0.5, recall 0.5
2019-02-20T17:15:02.416237: step 1073, loss 0.346073, accuracy 0.8125, precision 0.4, recall 1.0
2019-02-20T17:15:02.611171: step 1074, loss 0.0710218, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:02.809409: step 1075, loss 0.467995, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:15:03.002026: step 1076, loss 0.287987, accuracy 0.875, precision 1.0, recall 0.6
2019-02-20T17:15:03.193870: step 1077, loss 0.115703, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:03.388389: step 1078, loss 0.213897, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:15:03.581510: step 1079, loss 0.311158, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:03.773010: step 1080, loss 0.320331, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:15:03.965262: step 1081, loss 0.323698, accuracy 0.8125, precision 1.0, recall 0.4
2019-02-20T17:15:04.162444: step 1082, loss 0.24094, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:04.354940: step 1083, loss 0.454993, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:04.548852: step 1084, loss 0.0188488, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:04.742412: step 1085, loss 0.162099, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:04.936589: step 1086, loss 0.0512677, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:05.131355: step 1087, loss 0.00712158, accuracy 1, precision nan, recall nan
2019-02-20T17:15:05.322439: step 1088, loss 0.149871, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:05.513117: step 1089, loss 0.691141, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:05.707088: step 1090, loss 0.340836, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:05.901443: step 1091, loss 0.0181292, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:06.097221: step 1092, loss 0.747065, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:06.292959: step 1093, loss 0.00777473, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:06.488158: step 1094, loss 0.215974, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:06.680056: step 1095, loss 0.0414888, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:06.871925: step 1096, loss 0.0588341, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:07.067955: step 1097, loss 0.0205198, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:07.264382: step 1098, loss 0.150281, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:07.455891: step 1099, loss 0.0255527, accuracy 1, precision nan, recall nan
2019-02-20T17:15:07.644724: step 1100, loss 0.313089, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:15:07.833718: step 1101, loss 0.054946, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:08.029612: step 1102, loss 0.0325696, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:08.223089: step 1103, loss 0.0375624, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:08.417706: step 1104, loss 0.0665663, accuracy 1, precision nan, recall nan
2019-02-20T17:15:08.614845: step 1105, loss 0.126872, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:08.805096: step 1106, loss 0.324503, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:15:08.997075: step 1107, loss 0.29031, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:09.193683: step 1108, loss 0.0726422, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:09.387473: step 1109, loss 0.0697282, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:09.583473: step 1110, loss 0.034947, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:09.776771: step 1111, loss 1.01139, accuracy 0.625, precision 0.0, recall 0.0
2019-02-20T17:15:09.968535: step 1112, loss 0.130305, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:10.161333: step 1113, loss 0.214475, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:15:10.356728: step 1114, loss 0.0242241, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:10.557495: step 1115, loss 0.100979, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:10.753319: step 1116, loss 0.00617285, accuracy 1, precision nan, recall nan
2019-02-20T17:15:10.947716: step 1117, loss 0.82269, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:15:11.142871: step 1118, loss 0.0145215, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:11.333938: step 1119, loss 0.022119, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:11.525727: step 1120, loss 0.416037, accuracy 0.9375, precision 0.8, recall 1.0
2019-02-20T17:15:11.719107: step 1121, loss 0.145217, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:11.914781: step 1122, loss 0.0263114, accuracy 1, precision nan, recall nan
2019-02-20T17:15:12.110394: step 1123, loss 0.031216, accuracy 1, precision nan, recall nan
2019-02-20T17:15:12.303683: step 1124, loss 0.0182954, accuracy 1, precision nan, recall nan
2019-02-20T17:15:12.497771: step 1125, loss 0.118032, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:12.694036: step 1126, loss 0.276493, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:12.890828: step 1127, loss 0.276104, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:13.091369: step 1128, loss 0.527575, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:13.289404: step 1129, loss 0.20561, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:13.482788: step 1130, loss 0.29975, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:13.671004: step 1131, loss 0.21992, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:13.864222: step 1132, loss 0.227302, accuracy 0.875, precision 0.6, recall 1.0
2019-02-20T17:15:14.055192: step 1133, loss 0.258181, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:14.253374: step 1134, loss 0.197338, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:14.449220: step 1135, loss 0.0974053, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:14.642227: step 1136, loss 0.235693, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:14.839390: step 1137, loss 0.0695672, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:15.033759: step 1138, loss 0.223662, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:15:15.226574: step 1139, loss 0.228901, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:15:15.422765: step 1140, loss 0.0394322, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:15.620229: step 1141, loss 0.15165, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:15:15.813951: step 1142, loss 0.0895525, accuracy 1, precision nan, recall nan
2019-02-20T17:15:16.011044: step 1143, loss 0.188352, accuracy 0.9375, precision 1.0, recall 0.75
2019-02-20T17:15:16.203648: step 1144, loss 0.5069, accuracy 0.8125, precision 0.6666666666666666, recall 0.5
2019-02-20T17:15:16.401107: step 1145, loss 0.0418947, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:16.594356: step 1146, loss 0.0772805, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:16.786610: step 1147, loss 0.430499, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:16.978876: step 1148, loss 0.0358216, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:17.167682: step 1149, loss 0.814951, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:15:17.366511: step 1150, loss 0.0709494, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:17.561819: step 1151, loss 0.0961539, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:17.750757: step 1152, loss 0.151805, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:17.941659: step 1153, loss 0.115304, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:18.093277: step 1154, loss 0.0214321, accuracy 1, precision nan, recall nan
2019-02-20T17:15:18.286382: step 1155, loss 0.104663, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:18.477667: step 1156, loss 0.154212, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:18.677406: step 1157, loss 0.208221, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:18.869588: step 1158, loss 0.0991292, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:19.061327: step 1159, loss 0.0550442, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:19.252507: step 1160, loss 0.0162232, accuracy 1, precision nan, recall nan
2019-02-20T17:15:19.445120: step 1161, loss 0.253946, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:19.637405: step 1162, loss 0.208369, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:19.830797: step 1163, loss 0.0601241, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:20.023928: step 1164, loss 0.309837, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:20.214400: step 1165, loss 0.411374, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:20.406591: step 1166, loss 0.0284887, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:20.595514: step 1167, loss 0.216596, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:20.787194: step 1168, loss 0.0332538, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:20.979243: step 1169, loss 0.096306, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:21.174750: step 1170, loss 0.629205, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:15:21.366651: step 1171, loss 0.280949, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:15:21.563378: step 1172, loss 0.118058, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:21.758943: step 1173, loss 0.286747, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:21.952266: step 1174, loss 0.0438412, accuracy 1, precision nan, recall nan
2019-02-20T17:15:22.141990: step 1175, loss 0.101417, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:22.337843: step 1176, loss 0.0358211, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:22.534804: step 1177, loss 0.0252729, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:22.729141: step 1178, loss 0.00355446, accuracy 1, precision nan, recall nan
2019-02-20T17:15:22.924372: step 1179, loss 0.00510562, accuracy 1, precision nan, recall nan
2019-02-20T17:15:23.116343: step 1180, loss 0.00577856, accuracy 1, precision nan, recall nan
2019-02-20T17:15:23.311105: step 1181, loss 0.463525, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:23.505543: step 1182, loss 0.576783, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:23.696442: step 1183, loss 0.145928, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:23.891981: step 1184, loss 0.0121406, accuracy 1, precision nan, recall nan
2019-02-20T17:15:24.083095: step 1185, loss 0.084419, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:15:24.274786: step 1186, loss 0.232761, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:24.470553: step 1187, loss 0.0739641, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:24.661805: step 1188, loss 0.0202362, accuracy 1, precision nan, recall nan
2019-02-20T17:15:24.861049: step 1189, loss 0.0238572, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:25.054220: step 1190, loss 0.00417005, accuracy 1, precision nan, recall nan
2019-02-20T17:15:25.245342: step 1191, loss 0.01853, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:25.437992: step 1192, loss 0.0317201, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:25.631752: step 1193, loss 0.0294752, accuracy 1, precision nan, recall nan
2019-02-20T17:15:25.829341: step 1194, loss 0.0263412, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:26.018912: step 1195, loss 0.0205349, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:26.213812: step 1196, loss 0.024768, accuracy 1, precision nan, recall nan
2019-02-20T17:15:26.407664: step 1197, loss 0.331479, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:26.602555: step 1198, loss 0.0138178, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:26.794275: step 1199, loss 0.0160964, accuracy 1, precision nan, recall nan
2019-02-20T17:15:26.986749: step 1200, loss 0.518525, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:15:27.183517: step 1201, loss 0.347393, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:15:27.370456: step 1202, loss 0.0268936, accuracy 1, precision nan, recall nan
2019-02-20T17:15:27.565640: step 1203, loss 0.383131, accuracy 0.8125, precision 0.6666666666666666, recall 0.5
2019-02-20T17:15:27.757690: step 1204, loss 0.0590574, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:27.950543: step 1205, loss 0.0128587, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:28.147743: step 1206, loss 0.0934354, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:28.342335: step 1207, loss 0.0202899, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:28.537986: step 1208, loss 0.0139262, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:28.733407: step 1209, loss 0.00122113, accuracy 1, precision nan, recall nan
2019-02-20T17:15:28.924592: step 1210, loss 0.336529, accuracy 0.8125, precision 0.5, recall 0.3333333333333333
2019-02-20T17:15:29.122814: step 1211, loss 0.738126, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:15:29.315734: step 1212, loss 0.339845, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:29.507568: step 1213, loss 0.000806453, accuracy 1, precision nan, recall nan
2019-02-20T17:15:29.704193: step 1214, loss 0.264005, accuracy 0.9375, precision 0.8333333333333334, recall 1.0
2019-02-20T17:15:29.899581: step 1215, loss 0.0768538, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:30.102753: step 1216, loss 0.00527481, accuracy 1, precision nan, recall nan
2019-02-20T17:15:30.289834: step 1217, loss 0.00245576, accuracy 1, precision nan, recall nan
2019-02-20T17:15:30.487276: step 1218, loss 0.107555, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:30.679493: step 1219, loss 0.0396591, accuracy 1, precision nan, recall nan
2019-02-20T17:15:30.872108: step 1220, loss 0.422646, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:31.071606: step 1221, loss 0.260468, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:31.263892: step 1222, loss 0.0433967, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:31.451360: step 1223, loss 0.212314, accuracy 0.9375, precision 1.0, recall 0.75
2019-02-20T17:15:31.646735: step 1224, loss 0.0490131, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:31.837134: step 1225, loss 0.0883046, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:32.032380: step 1226, loss 0.470929, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:32.219975: step 1227, loss 0.0593783, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:32.409602: step 1228, loss 0.0985909, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:32.598904: step 1229, loss 0.497259, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:32.792042: step 1230, loss 0.17385, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:15:32.983294: step 1231, loss 0.0527504, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:33.172247: step 1232, loss 0.0842897, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:33.366719: step 1233, loss 0.0741088, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:33.561867: step 1234, loss 0.0600953, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:33.758112: step 1235, loss 0.0713515, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:33.952343: step 1236, loss 0.220573, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:15:34.143069: step 1237, loss 0.0885114, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:34.334348: step 1238, loss 0.195451, accuracy 0.9375, precision 0.8, recall 1.0
2019-02-20T17:15:34.528558: step 1239, loss 0.0456859, accuracy 1, precision nan, recall nan
2019-02-20T17:15:34.721891: step 1240, loss 0.178612, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:15:34.916050: step 1241, loss 0.0169727, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:35.110775: step 1242, loss 0.23128, accuracy 0.9375, precision 0.8333333333333334, recall 1.0
2019-02-20T17:15:35.308984: step 1243, loss 0.237027, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:35.504218: step 1244, loss 0.462409, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:35.698049: step 1245, loss 0.0822876, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:35.888039: step 1246, loss 0.154636, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:36.081297: step 1247, loss 0.246453, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:36.280331: step 1248, loss 0.0770229, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:36.476954: step 1249, loss 0.127139, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:36.673451: step 1250, loss 0.00934063, accuracy 1, precision nan, recall nan

Evaluation:
[[ 54  41]
 [ 14 916]]
2019-02-20T17:15:38.454213: step 1250, loss 0.15906, accuracy 0.946341, precision 0.5684210526315789, recall 0.7941176470588235

Saved model checkpoint to /home/ubuntu/Project/runs/1550682642/checkpoints/model-1250

2019-02-20T17:15:38.738186: step 1251, loss 0.259303, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:38.930017: step 1252, loss 0.230807, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:15:39.123389: step 1253, loss 0.0638837, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:39.316414: step 1254, loss 0.022326, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:39.510124: step 1255, loss 0.508307, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:39.701833: step 1256, loss 0.17999, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:15:39.896205: step 1257, loss 0.286911, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:40.087743: step 1258, loss 0.076836, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:40.279722: step 1259, loss 0.204821, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:40.466337: step 1260, loss 0.0486159, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:40.664414: step 1261, loss 0.0317896, accuracy 1, precision nan, recall nan
2019-02-20T17:15:40.857862: step 1262, loss 0.412572, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:41.049948: step 1263, loss 0.638923, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:15:41.247957: step 1264, loss 0.228285, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:41.445676: step 1265, loss 0.00826007, accuracy 1, precision nan, recall nan
2019-02-20T17:15:41.635923: step 1266, loss 0.201857, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:41.825115: step 1267, loss 0.0986744, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:42.019902: step 1268, loss 0.0124535, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:42.215665: step 1269, loss 0.192422, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:42.409491: step 1270, loss 0.144915, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:15:42.602332: step 1271, loss 0.161284, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:42.791780: step 1272, loss 0.301418, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:42.986639: step 1273, loss 0.0215063, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:43.181359: step 1274, loss 0.302648, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:43.379614: step 1275, loss 0.023189, accuracy 1, precision nan, recall nan
2019-02-20T17:15:43.570446: step 1276, loss 0.178671, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:43.768214: step 1277, loss 0.0276731, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:43.962934: step 1278, loss 0.191395, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:44.158004: step 1279, loss 0.300839, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:44.351118: step 1280, loss 0.0548976, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:44.543198: step 1281, loss 0.218378, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:44.736206: step 1282, loss 0.0526591, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:44.923317: step 1283, loss 0.0479683, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:45.119612: step 1284, loss 0.285633, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:45.310774: step 1285, loss 0.0881603, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:45.506423: step 1286, loss 0.155249, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:15:45.701414: step 1287, loss 0.0719808, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:15:45.897047: step 1288, loss 0.0636036, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:46.091386: step 1289, loss 0.138344, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:46.283698: step 1290, loss 0.297899, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:46.479473: step 1291, loss 0.318667, accuracy 0.8125, precision 0.0, recall nan
2019-02-20T17:15:46.671299: step 1292, loss 0.0112895, accuracy 1, precision nan, recall nan
2019-02-20T17:15:46.865917: step 1293, loss 0.288503, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:15:47.061014: step 1294, loss 0.0713983, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:15:47.255931: step 1295, loss 0.28569, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:47.446798: step 1296, loss 0.140236, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:15:47.641441: step 1297, loss 0.117878, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:47.838634: step 1298, loss 0.0881236, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:48.031879: step 1299, loss 0.113019, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:48.228459: step 1300, loss 0.0868881, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:48.420982: step 1301, loss 0.0502715, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:48.614776: step 1302, loss 0.363847, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:48.811596: step 1303, loss 0.012835, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:49.004047: step 1304, loss 0.0338679, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:49.191102: step 1305, loss 0.369219, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:49.384272: step 1306, loss 0.198297, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:49.580109: step 1307, loss 0.419644, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:15:49.775675: step 1308, loss 0.284988, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:49.972069: step 1309, loss 0.0459729, accuracy 1, precision nan, recall nan
2019-02-20T17:15:50.165160: step 1310, loss 0.443728, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:50.357814: step 1311, loss 0.0197489, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:50.548251: step 1312, loss 0.100447, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:50.741037: step 1313, loss 0.513608, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:50.931353: step 1314, loss 0.156145, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:51.127761: step 1315, loss 0.233217, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:51.319654: step 1316, loss 0.0150809, accuracy 1, precision nan, recall nan
2019-02-20T17:15:51.509916: step 1317, loss 0.0654309, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:51.705821: step 1318, loss 0.0115905, accuracy 1, precision nan, recall nan
2019-02-20T17:15:51.901458: step 1319, loss 0.291127, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:52.091890: step 1320, loss 0.0892938, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:52.281592: step 1321, loss 0.13643, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:52.477195: step 1322, loss 0.132674, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:52.669895: step 1323, loss 0.011082, accuracy 1, precision nan, recall nan
2019-02-20T17:15:52.861747: step 1324, loss 0.215479, accuracy 0.875, precision 0.8, recall 0.8
2019-02-20T17:15:53.055177: step 1325, loss 0.205297, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:15:53.252405: step 1326, loss 0.0837548, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:53.445547: step 1327, loss 0.128393, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:15:53.634379: step 1328, loss 0.170735, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:53.833838: step 1329, loss 0.0463558, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:54.028584: step 1330, loss 0.235368, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:54.221015: step 1331, loss 0.0357443, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:54.413090: step 1332, loss 0.15936, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:54.612047: step 1333, loss 0.251309, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:15:54.808388: step 1334, loss 0.5585, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:15:54.998895: step 1335, loss 0.0204231, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:55.193512: step 1336, loss 0.00812392, accuracy 1, precision nan, recall nan
2019-02-20T17:15:55.386436: step 1337, loss 0.176503, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:55.573805: step 1338, loss 0.134414, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:15:55.772223: step 1339, loss 0.0655021, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:55.968607: step 1340, loss 0.101341, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:56.163338: step 1341, loss 0.150117, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:56.359920: step 1342, loss 0.223681, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:15:56.554960: step 1343, loss 0.246489, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:56.741724: step 1344, loss 0.0223876, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:56.934898: step 1345, loss 0.0600232, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:57.127468: step 1346, loss 0.0221887, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:57.320253: step 1347, loss 0.290464, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:15:57.515284: step 1348, loss 0.226667, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:15:57.707079: step 1349, loss 0.0190734, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:57.902636: step 1350, loss 0.4947, accuracy 0.75, precision 0.0, recall 0.0
2019-02-20T17:15:58.094832: step 1351, loss 0.220669, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:15:58.287870: step 1352, loss 0.233846, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:15:58.482316: step 1353, loss 0.0448146, accuracy 1, precision nan, recall nan
2019-02-20T17:15:58.674312: step 1354, loss 0.0213758, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:58.868479: step 1355, loss 0.0595127, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:59.065028: step 1356, loss 0.0185449, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:15:59.255472: step 1357, loss 0.0672928, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:59.452695: step 1358, loss 0.346655, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:15:59.646421: step 1359, loss 0.123796, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:15:59.841735: step 1360, loss 0.73781, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:16:00.034142: step 1361, loss 0.143815, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:00.224458: step 1362, loss 0.35392, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:16:00.419828: step 1363, loss 0.179036, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:00.614729: step 1364, loss 0.279438, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:16:00.805270: step 1365, loss 0.0200259, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:01.002380: step 1366, loss 0.0229499, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:01.198927: step 1367, loss 0.218467, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:16:01.389356: step 1368, loss 0.308487, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:01.580332: step 1369, loss 0.102227, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:01.773314: step 1370, loss 0.12304, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:16:01.963830: step 1371, loss 0.242755, accuracy 0.9375, precision 1.0, recall 0.75
2019-02-20T17:16:02.150254: step 1372, loss 0.432792, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:16:02.344807: step 1373, loss 0.156946, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:02.541819: step 1374, loss 0.189263, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:16:02.737710: step 1375, loss 0.217613, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:16:02.929796: step 1376, loss 0.0579485, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:03.118890: step 1377, loss 0.241644, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:03.310604: step 1378, loss 0.0983843, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:03.503377: step 1379, loss 0.306813, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:16:03.697494: step 1380, loss 0.0281972, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:03.887976: step 1381, loss 0.119846, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:04.081429: step 1382, loss 0.118606, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:16:04.280815: step 1383, loss 0.0390435, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:04.470875: step 1384, loss 0.274458, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:16:04.660770: step 1385, loss 0.077101, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:04.855159: step 1386, loss 0.0568844, accuracy 1, precision nan, recall nan
2019-02-20T17:16:05.048144: step 1387, loss 0.556025, accuracy 0.75, precision 0.2, recall 1.0
2019-02-20T17:16:05.245574: step 1388, loss 0.0290598, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:05.439257: step 1389, loss 0.166697, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:05.632905: step 1390, loss 0.0862041, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:16:05.827648: step 1391, loss 0.309134, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:16:06.021212: step 1392, loss 0.0347827, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:06.218913: step 1393, loss 0.282727, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:06.412758: step 1394, loss 0.170007, accuracy 0.875, precision 1.0, recall 0.3333333333333333
2019-02-20T17:16:06.608689: step 1395, loss 0.581217, accuracy 0.875, precision 1.0, recall 0.5
2019-02-20T17:16:06.806343: step 1396, loss 0.32685, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:16:06.998112: step 1397, loss 0.350227, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:16:07.195502: step 1398, loss 0.34797, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:16:07.389768: step 1399, loss 0.225201, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:07.586565: step 1400, loss 0.0278432, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:07.781673: step 1401, loss 0.0317507, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:07.976526: step 1402, loss 0.00199624, accuracy 1, precision nan, recall nan
2019-02-20T17:16:08.171040: step 1403, loss 0.00313238, accuracy 1, precision nan, recall nan
2019-02-20T17:16:08.363428: step 1404, loss 0.243124, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:16:08.557673: step 1405, loss 0.0532627, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:08.747693: step 1406, loss 0.169227, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:08.938469: step 1407, loss 0.149573, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:09.131342: step 1408, loss 0.106781, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:09.325934: step 1409, loss 0.126953, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:09.517774: step 1410, loss 0.100085, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:09.710550: step 1411, loss 0.327898, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:09.906754: step 1412, loss 0.281258, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:10.101088: step 1413, loss 0.0248692, accuracy 1, precision nan, recall nan
2019-02-20T17:16:10.294351: step 1414, loss 0.00537147, accuracy 1, precision nan, recall nan
2019-02-20T17:16:10.486677: step 1415, loss 0.175702, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:16:10.680709: step 1416, loss 0.0193002, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:10.872937: step 1417, loss 0.0891325, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:11.069151: step 1418, loss 0.300394, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:16:11.261603: step 1419, loss 0.226104, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:16:11.450920: step 1420, loss 0.0972857, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:11.637914: step 1421, loss 0.622059, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:16:11.832049: step 1422, loss 0.0311023, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:12.028884: step 1423, loss 0.0557299, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:16:12.222432: step 1424, loss 0.0193301, accuracy 1, precision nan, recall nan
2019-02-20T17:16:12.413694: step 1425, loss 0.353006, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:16:12.601243: step 1426, loss 0.10181, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:12.794892: step 1427, loss 0.197079, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:16:12.988854: step 1428, loss 0.170095, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:13.187600: step 1429, loss 0.113814, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:16:13.383678: step 1430, loss 0.355009, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:16:13.576041: step 1431, loss 0.0807106, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:13.767224: step 1432, loss 0.132975, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:13.963417: step 1433, loss 0.773006, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:16:14.158768: step 1434, loss 0.156732, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:16:14.349652: step 1435, loss 0.170151, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:16:14.539355: step 1436, loss 0.331398, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:16:14.731353: step 1437, loss 0.131404, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:16:14.926114: step 1438, loss 0.477527, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:16:15.117744: step 1439, loss 0.073482, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:15.311355: step 1440, loss 0.0211408, accuracy 1, precision nan, recall nan
2019-02-20T17:16:15.503335: step 1441, loss 0.0945748, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:16:15.699201: step 1442, loss 0.164378, accuracy 0.875, precision 0.5, recall 0.5
2019-02-20T17:16:15.897938: step 1443, loss 0.0391354, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:16.094781: step 1444, loss 0.481916, accuracy 0.8125, precision 0.5, recall 0.6666666666666666
2019-02-20T17:16:16.285785: step 1445, loss 0.0831778, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:16:16.474758: step 1446, loss 0.450891, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:16.669108: step 1447, loss 0.51065, accuracy 0.875, precision 0.5, recall 1.0
2019-02-20T17:16:16.861494: step 1448, loss 0.202284, accuracy 0.875, precision nan, recall 0.0
2019-02-20T17:16:17.051006: step 1449, loss 0.0389825, accuracy 1, precision nan, recall nan
2019-02-20T17:16:17.245069: step 1450, loss 0.103819, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:17.444483: step 1451, loss 0.195277, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:17.643441: step 1452, loss 0.250777, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:16:17.833500: step 1453, loss 0.193715, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:16:18.030356: step 1454, loss 0.0320317, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:18.225305: step 1455, loss 0.295558, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:18.417680: step 1456, loss 0.00880108, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:18.614773: step 1457, loss 0.0131865, accuracy 1, precision nan, recall nan
2019-02-20T17:16:18.806765: step 1458, loss 0.00347364, accuracy 1, precision nan, recall nan
2019-02-20T17:16:18.996711: step 1459, loss 0.480385, accuracy 0.8125, precision 0.0, recall 0.0
2019-02-20T17:16:19.195250: step 1460, loss 0.613559, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:16:19.388113: step 1461, loss 0.0844941, accuracy 0.9375, precision 1.0, recall 0.5
2019-02-20T17:16:19.583516: step 1462, loss 0.0305857, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:19.778213: step 1463, loss 0.427276, accuracy 0.75, precision 0.0, recall nan
2019-02-20T17:16:19.971047: step 1464, loss 0.221981, accuracy 0.875, precision 0.0, recall nan
2019-02-20T17:16:20.165288: step 1465, loss 0.0989943, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:16:20.362262: step 1466, loss 0.0206892, accuracy 1, precision nan, recall nan
2019-02-20T17:16:20.558943: step 1467, loss 0.270326, accuracy 0.9375, precision 0.75, recall 1.0
2019-02-20T17:16:20.748821: step 1468, loss 0.145875, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:16:20.941071: step 1469, loss 0.229676, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:16:21.137815: step 1470, loss 0.338524, accuracy 0.875, precision 1.0, recall 0.5
2019-02-20T17:16:21.332976: step 1471, loss 0.19027, accuracy 0.8125, precision 0.25, recall 1.0
2019-02-20T17:16:21.531322: step 1472, loss 0.23386, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:16:21.727441: step 1473, loss 0.0620532, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:21.917537: step 1474, loss 0.0319537, accuracy 1, precision nan, recall nan
2019-02-20T17:16:22.111688: step 1475, loss 0.277536, accuracy 0.875, precision 0.75, recall 0.75
2019-02-20T17:16:22.307509: step 1476, loss 0.0782929, accuracy 1, precision nan, recall nan
2019-02-20T17:16:22.502567: step 1477, loss 0.145158, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:22.696573: step 1478, loss 0.0644808, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:16:22.886122: step 1479, loss 0.0471445, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:23.083751: step 1480, loss 0.430182, accuracy 0.875, precision 0.6666666666666666, recall 0.6666666666666666
2019-02-20T17:16:23.279239: step 1481, loss 0.105983, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:23.477252: step 1482, loss 0.133182, accuracy 0.9375, precision 1.0, recall 0.6666666666666666
2019-02-20T17:16:23.666784: step 1483, loss 0.343198, accuracy 0.9375, precision nan, recall 0.0
2019-02-20T17:16:23.865058: step 1484, loss 0.589439, accuracy 0.75, precision 0.3333333333333333, recall 0.3333333333333333
2019-02-20T17:16:24.060780: step 1485, loss 0.0641384, accuracy 1, precision 1.0, recall 1.0
2019-02-20T17:16:24.251494: step 1486, loss 0.156478, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:24.442806: step 1487, loss 0.29718, accuracy 0.8125, precision 0.3333333333333333, recall 0.5
2019-02-20T17:16:24.638370: step 1488, loss 0.420817, accuracy 0.875, precision 0.3333333333333333, recall 1.0
2019-02-20T17:16:24.832973: step 1489, loss 0.0799374, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:25.026312: step 1490, loss 0.126905, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:16:25.217521: step 1491, loss 0.0198012, accuracy 1, precision nan, recall nan
2019-02-20T17:16:25.415969: step 1492, loss 0.00286667, accuracy 1, precision nan, recall nan
2019-02-20T17:16:25.608031: step 1493, loss 0.00757942, accuracy 1, precision nan, recall nan
2019-02-20T17:16:25.797360: step 1494, loss 0.0449099, accuracy 1, precision nan, recall nan
2019-02-20T17:16:25.989888: step 1495, loss 0.163549, accuracy 0.9375, precision 0.0, recall nan
2019-02-20T17:16:26.180503: step 1496, loss 0.874777, accuracy 0.75, precision 0.0, recall nan
2019-02-20T17:16:26.378212: step 1497, loss 0.245266, accuracy 0.9375, precision 0.6666666666666666, recall 1.0
2019-02-20T17:16:26.571956: step 1498, loss 0.311151, accuracy 0.9375, precision 0.5, recall 1.0
2019-02-20T17:16:26.768462: step 1499, loss 0.382, accuracy 0.875, precision 0.0, recall 0.0
2019-02-20T17:16:26.960580: step 1500, loss 0.0358808, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[ 56  39]
 [  8 922]]
2019-02-20T17:16:28.734041: step 1500, loss 0.139021, accuracy 0.954146, precision 0.5894736842105263, recall 0.875

Saved model checkpoint to /home/ubuntu/Project/runs/1550682642/checkpoints/model-1500

