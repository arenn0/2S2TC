"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
GloVe model loaded
Pretrained Embedding: GloVe
Italian: False
Loading data...
11566
Max Document length: 36
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 1
Train/Dev split: 10410/1156
2019-03-03 13:05:42.570724: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:475: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551639943

2019-03-03T13:05:45.557285: step 1, loss 1.21581, accuracy 0.628906, precision 0.8370786516853933, recall 0.6930232558139535
2019-03-03T13:05:46.009104: step 2, loss 1.15304, accuracy 0.621094, precision 0.7976190476190477, recall 0.6802030456852792
2019-03-03T13:05:46.447335: step 3, loss 1.18014, accuracy 0.617188, precision 0.7052631578947368, recall 0.7613636363636364
2019-03-03T13:05:46.868258: step 4, loss 0.961246, accuracy 0.691406, precision 0.7752808988764045, recall 0.7796610169491526
2019-03-03T13:05:47.288122: step 5, loss 0.890734, accuracy 0.699219, precision 0.7942857142857143, recall 0.7722222222222223
2019-03-03T13:05:47.718986: step 6, loss 1.01128, accuracy 0.71875, precision 0.8143712574850299, recall 0.768361581920904
2019-03-03T13:05:48.212668: step 7, loss 1.02152, accuracy 0.679688, precision 0.8125, recall 0.7142857142857143
2019-03-03T13:05:48.635564: step 8, loss 0.617323, accuracy 0.773438, precision 0.8248587570621468, recall 0.8439306358381503
2019-03-03T13:05:49.055433: step 9, loss 0.951911, accuracy 0.675781, precision 0.7371428571428571, recall 0.7771084337349398
2019-03-03T13:05:49.502747: step 10, loss 0.932381, accuracy 0.644531, precision 0.7025316455696202, recall 0.7161290322580646
2019-03-03T13:05:49.955169: step 11, loss 0.737172, accuracy 0.734375, precision 0.770949720670391, recall 0.8363636363636363
2019-03-03T13:05:50.425136: step 12, loss 0.923298, accuracy 0.714844, precision 0.791907514450867, recall 0.7873563218390804
2019-03-03T13:05:50.844360: step 13, loss 0.810112, accuracy 0.730469, precision 0.8372093023255814, recall 0.7783783783783784
2019-03-03T13:05:51.268246: step 14, loss 0.733274, accuracy 0.757812, precision 0.8333333333333334, recall 0.7831325301204819
2019-03-03T13:05:51.742484: step 15, loss 0.772374, accuracy 0.746094, precision 0.8630952380952381, recall 0.7754010695187166
2019-03-03T13:05:52.406451: step 16, loss 0.762273, accuracy 0.734375, precision 0.8383233532934131, recall 0.7734806629834254
2019-03-03T13:05:54.618746: step 17, loss 0.91323, accuracy 0.710938, precision 0.8223684210526315, recall 0.7267441860465116
2019-03-03T13:05:56.833801: step 18, loss 0.762097, accuracy 0.746094, precision 0.7833333333333333, recall 0.844311377245509
2019-03-03T13:05:58.443162: step 19, loss 0.775342, accuracy 0.695312, precision 0.735632183908046, recall 0.8
2019-03-03T13:06:00.253095: step 20, loss 0.717261, accuracy 0.726562, precision 0.8074534161490683, recall 0.7692307692307693
2019-03-03T13:06:01.591358: step 21, loss 0.83644, accuracy 0.714844, precision 0.7891566265060241, recall 0.7751479289940828
2019-03-03T13:06:02.795972: step 22, loss 0.687549, accuracy 0.769531, precision 0.8372093023255814, recall 0.8228571428571428
2019-03-03T13:06:03.850150: step 23, loss 0.687477, accuracy 0.757812, precision 0.8136645962732919, recall 0.803680981595092
2019-03-03T13:06:04.773681: step 24, loss 1.02575, accuracy 0.691406, precision 0.7607361963190185, recall 0.7560975609756098
2019-03-03T13:06:05.723164: step 25, loss 0.850899, accuracy 0.714844, precision 0.8106508875739645, recall 0.7696629213483146
2019-03-03T13:06:06.581891: step 26, loss 0.66394, accuracy 0.78125, precision 0.8047337278106509, recall 0.8553459119496856
2019-03-03T13:06:07.375337: step 27, loss 0.739032, accuracy 0.738281, precision 0.8352272727272727, recall 0.7945945945945946
2019-03-03T13:06:08.190156: step 28, loss 0.638989, accuracy 0.769531, precision 0.8203592814371258, recall 0.8253012048192772
2019-03-03T13:06:08.894248: step 29, loss 0.808278, accuracy 0.726562, precision 0.8048780487804879, recall 0.7764705882352941
2019-03-03T13:06:09.606329: step 30, loss 0.795357, accuracy 0.707031, precision 0.7878787878787878, recall 0.7647058823529411
2019-03-03T13:06:10.424708: step 31, loss 0.587951, accuracy 0.761719, precision 0.8035714285714286, recall 0.8282208588957055
2019-03-03T13:06:11.077976: step 32, loss 0.730321, accuracy 0.753906, precision 0.7898089171974523, recall 0.8051948051948052
2019-03-03T13:06:11.788480: step 33, loss 0.808298, accuracy 0.710938, precision 0.7804878048780488, recall 0.7710843373493976
2019-03-03T13:06:12.510066: step 34, loss 0.738127, accuracy 0.730469, precision 0.7869822485207101, recall 0.8012048192771084
2019-03-03T13:06:13.150352: step 35, loss 0.635546, accuracy 0.726562, precision 0.810126582278481, recall 0.7619047619047619
2019-03-03T13:06:13.840507: step 36, loss 0.663467, accuracy 0.773438, precision 0.8314606741573034, recall 0.8409090909090909
2019-03-03T13:06:14.478742: step 37, loss 0.786301, accuracy 0.71875, precision 0.8421052631578947, recall 0.7619047619047619
2019-03-03T13:06:15.086630: step 38, loss 0.528847, accuracy 0.765625, precision 0.8342245989304813, recall 0.8432432432432433
2019-03-03T13:06:15.736401: step 39, loss 0.492713, accuracy 0.820312, precision 0.8787878787878788, recall 0.847953216374269
2019-03-03T13:06:16.382200: step 40, loss 0.480739, accuracy 0.789062, precision 0.8707865168539326, recall 0.8333333333333334
2019-03-03T13:06:16.807617: step 41, loss 0.783276, accuracy 0.694118, precision 0.7981651376146789, recall 0.7435897435897436
2019-03-03T13:06:17.375138: step 42, loss 0.514737, accuracy 0.789062, precision 0.8670520231213873, recall 0.8287292817679558
2019-03-03T13:06:17.984413: step 43, loss 0.620642, accuracy 0.742188, precision 0.8378378378378378, recall 0.7469879518072289
2019-03-03T13:06:18.619232: step 44, loss 0.536963, accuracy 0.777344, precision 0.7816091954022989, recall 0.8774193548387097
2019-03-03T13:06:19.167605: step 45, loss 0.471796, accuracy 0.785156, precision 0.8098159509202454, recall 0.8461538461538461
2019-03-03T13:06:19.730766: step 46, loss 0.540608, accuracy 0.792969, precision 0.8212290502793296, recall 0.875
2019-03-03T13:06:20.293266: step 47, loss 0.426794, accuracy 0.84375, precision 0.875, recall 0.8953488372093024
2019-03-03T13:06:20.914661: step 48, loss 0.598564, accuracy 0.773438, precision 0.8809523809523809, recall 0.7956989247311828
2019-03-03T13:06:21.435803: step 49, loss 0.430632, accuracy 0.824219, precision 0.8862275449101796, recall 0.8505747126436781
2019-03-03T13:06:22.012765: step 50, loss 0.50897, accuracy 0.808594, precision 0.9141104294478528, recall 0.8097826086956522
2019-03-03T13:06:22.545993: step 51, loss 0.522287, accuracy 0.777344, precision 0.8719512195121951, recall 0.7988826815642458
2019-03-03T13:06:23.063182: step 52, loss 0.483016, accuracy 0.777344, precision 0.825, recall 0.8198757763975155
2019-03-03T13:06:23.626219: step 53, loss 0.458121, accuracy 0.816406, precision 0.8514285714285714, recall 0.8764705882352941
2019-03-03T13:06:24.234653: step 54, loss 0.447156, accuracy 0.820312, precision 0.8412698412698413, recall 0.9085714285714286
2019-03-03T13:06:24.752394: step 55, loss 0.452852, accuracy 0.785156, precision 0.844311377245509, recall 0.8294117647058824
2019-03-03T13:06:25.258036: step 56, loss 0.408343, accuracy 0.847656, precision 0.9096385542168675, recall 0.8628571428571429
2019-03-03T13:06:25.879882: step 57, loss 0.418181, accuracy 0.84375, precision 0.9107142857142857, recall 0.8595505617977528
2019-03-03T13:06:26.393543: step 58, loss 0.508298, accuracy 0.820312, precision 0.8875739644970414, recall 0.847457627118644
2019-03-03T13:06:26.926618: step 59, loss 0.419706, accuracy 0.855469, precision 0.9235294117647059, recall 0.8674033149171271
2019-03-03T13:06:27.422313: step 60, loss 0.465205, accuracy 0.8125, precision 0.850828729281768, recall 0.88
2019-03-03T13:06:27.953913: step 61, loss 0.49328, accuracy 0.785156, precision 0.8855421686746988, recall 0.8032786885245902
2019-03-03T13:06:28.459072: step 62, loss 0.555741, accuracy 0.78125, precision 0.8971428571428571, recall 0.8051282051282052
2019-03-03T13:06:28.987676: step 63, loss 0.47214, accuracy 0.804688, precision 0.8837209302325582, recall 0.8351648351648352
2019-03-03T13:06:29.483888: step 64, loss 0.453961, accuracy 0.832031, precision 0.9230769230769231, recall 0.8387096774193549
2019-03-03T13:06:30.014491: step 65, loss 0.51594, accuracy 0.796875, precision 0.8588235294117647, recall 0.8390804597701149
2019-03-03T13:06:30.538666: step 66, loss 0.491324, accuracy 0.792969, precision 0.8068181818181818, recall 0.8819875776397516
2019-03-03T13:06:31.066784: step 67, loss 0.518555, accuracy 0.785156, precision 0.8055555555555556, recall 0.8787878787878788
2019-03-03T13:06:31.574846: step 68, loss 0.539449, accuracy 0.757812, precision 0.7865853658536586, recall 0.8269230769230769
2019-03-03T13:06:32.061073: step 69, loss 0.529141, accuracy 0.785156, precision 0.8441558441558441, recall 0.8074534161490683
2019-03-03T13:06:32.540812: step 70, loss 0.532159, accuracy 0.773438, precision 0.8780487804878049, recall 0.7912087912087912
2019-03-03T13:06:33.062446: step 71, loss 0.391647, accuracy 0.839844, precision 0.9090909090909091, recall 0.8648648648648649
2019-03-03T13:06:33.563177: step 72, loss 0.405394, accuracy 0.820312, precision 0.8926553672316384, recall 0.8540540540540541
2019-03-03T13:06:34.100074: step 73, loss 0.421924, accuracy 0.8125, precision 0.9112426035502958, recall 0.8235294117647058
2019-03-03T13:06:34.610876: step 74, loss 0.466431, accuracy 0.828125, precision 0.9069767441860465, recall 0.8478260869565217
2019-03-03T13:06:35.110074: step 75, loss 0.372754, accuracy 0.839844, precision 0.9117647058823529, recall 0.856353591160221
2019-03-03T13:06:35.644901: step 76, loss 0.539286, accuracy 0.773438, precision 0.8502994011976048, recall 0.8114285714285714
2019-03-03T13:06:36.121222: step 77, loss 0.425698, accuracy 0.792969, precision 0.8553459119496856, recall 0.8192771084337349
2019-03-03T13:06:36.584458: step 78, loss 0.422272, accuracy 0.808594, precision 0.8024691358024691, recall 0.8843537414965986
2019-03-03T13:06:37.090576: step 79, loss 0.517667, accuracy 0.757812, precision 0.7485029940119761, recall 0.8620689655172413
2019-03-03T13:06:37.593740: step 80, loss 0.425019, accuracy 0.792969, precision 0.8505747126436781, recall 0.8457142857142858
2019-03-03T13:06:38.054040: step 81, loss 0.391484, accuracy 0.816406, precision 0.8895705521472392, recall 0.8333333333333334
2019-03-03T13:06:38.403122: step 82, loss 0.43174, accuracy 0.823529, precision 0.8898305084745762, recall 0.860655737704918
2019-03-03T13:06:38.855919: step 83, loss 0.380907, accuracy 0.847656, precision 0.9161676646706587, recall 0.8595505617977528
2019-03-03T13:06:39.400303: step 84, loss 0.355699, accuracy 0.851562, precision 0.9132947976878613, recall 0.8729281767955801
2019-03-03T13:06:39.878026: step 85, loss 0.369758, accuracy 0.84375, precision 0.893491124260355, recall 0.8728323699421965
2019-03-03T13:06:40.341064: step 86, loss 0.462772, accuracy 0.800781, precision 0.8908045977011494, recall 0.8288770053475936
2019-03-03T13:06:40.802849: step 87, loss 0.353036, accuracy 0.867188, precision 0.9289940828402367, recall 0.8770949720670391
2019-03-03T13:06:41.337417: step 88, loss 0.33489, accuracy 0.875, precision 0.9261363636363636, recall 0.8956043956043956
2019-03-03T13:06:41.793236: step 89, loss 0.326679, accuracy 0.835938, precision 0.875, recall 0.8850574712643678
2019-03-03T13:06:42.238486: step 90, loss 0.314286, accuracy 0.851562, precision 0.8685714285714285, recall 0.9101796407185628
2019-03-03T13:06:42.696289: step 91, loss 0.358427, accuracy 0.839844, precision 0.9146341463414634, recall 0.847457627118644
2019-03-03T13:06:43.189993: step 92, loss 0.332159, accuracy 0.863281, precision 0.9017341040462428, recall 0.896551724137931
2019-03-03T13:06:43.727069: step 93, loss 0.327376, accuracy 0.875, precision 0.9012345679012346, recall 0.9012345679012346
2019-03-03T13:06:44.190855: step 94, loss 0.31008, accuracy 0.875, precision 0.9325842696629213, recall 0.8924731182795699
2019-03-03T13:06:44.658647: step 95, loss 0.346299, accuracy 0.847656, precision 0.9005524861878453, recall 0.8858695652173914
2019-03-03T13:06:45.146345: step 96, loss 0.314377, accuracy 0.855469, precision 0.9011627906976745, recall 0.8857142857142857
2019-03-03T13:06:45.642018: step 97, loss 0.39174, accuracy 0.839844, precision 0.901840490797546, recall 0.8546511627906976
2019-03-03T13:06:46.139762: step 98, loss 0.414318, accuracy 0.804688, precision 0.8589743589743589, recall 0.8271604938271605
2019-03-03T13:06:46.608012: step 99, loss 0.377283, accuracy 0.824219, precision 0.8827160493827161, recall 0.8461538461538461
2019-03-03T13:06:47.125629: step 100, loss 0.312752, accuracy 0.867188, precision 0.8837209302325582, recall 0.9156626506024096
2019-03-03T13:06:47.590435: step 101, loss 0.396592, accuracy 0.832031, precision 0.9090909090909091, recall 0.8426966292134831
2019-03-03T13:06:48.044229: step 102, loss 0.351872, accuracy 0.832031, precision 0.9096774193548387, recall 0.8294117647058824
2019-03-03T13:06:48.493537: step 103, loss 0.304945, accuracy 0.859375, precision 0.8846153846153846, recall 0.9147727272727273
2019-03-03T13:06:48.961891: step 104, loss 0.367461, accuracy 0.867188, precision 0.9491525423728814, recall 0.8704663212435233
2019-03-03T13:06:49.471039: step 105, loss 0.397364, accuracy 0.851562, precision 0.8787878787878788, recall 0.8895705521472392
2019-03-03T13:06:49.917868: step 106, loss 0.386111, accuracy 0.839844, precision 0.864406779661017, recall 0.9
2019-03-03T13:06:50.430497: step 107, loss 0.373262, accuracy 0.839844, precision 0.8982035928143712, recall 0.8620689655172413
2019-03-03T13:06:50.894260: step 108, loss 0.33833, accuracy 0.835938, precision 0.8579545454545454, recall 0.8988095238095238
2019-03-03T13:06:51.406895: step 109, loss 0.285246, accuracy 0.882812, precision 0.9, recall 0.9310344827586207
2019-03-03T13:06:51.867518: step 110, loss 0.348369, accuracy 0.835938, precision 0.9141104294478528, recall 0.8418079096045198
2019-03-03T13:06:52.375640: step 111, loss 0.317403, accuracy 0.859375, precision 0.93125, recall 0.8563218390804598
2019-03-03T13:06:52.847912: step 112, loss 0.371949, accuracy 0.824219, precision 0.8611111111111112, recall 0.8857142857142857
2019-03-03T13:06:53.348589: step 113, loss 0.348071, accuracy 0.851562, precision 0.93125, recall 0.8465909090909091
2019-03-03T13:06:53.815369: step 114, loss 0.313032, accuracy 0.84375, precision 0.9079754601226994, recall 0.8554913294797688
2019-03-03T13:06:54.317667: step 115, loss 0.328727, accuracy 0.847656, precision 0.8888888888888888, recall 0.8837209302325582
2019-03-03T13:06:54.782444: step 116, loss 0.335595, accuracy 0.824219, precision 0.8622754491017964, recall 0.8674698795180723
2019-03-03T13:06:55.272135: step 117, loss 0.306719, accuracy 0.871094, precision 0.9117647058823529, recall 0.8959537572254336
2019-03-03T13:06:55.725987: step 118, loss 0.236316, accuracy 0.910156, precision 0.9273743016759777, recall 0.9431818181818182
2019-03-03T13:06:56.188744: step 119, loss 0.361806, accuracy 0.835938, precision 0.8636363636363636, recall 0.8941176470588236
2019-03-03T13:06:56.665457: step 120, loss 0.332581, accuracy 0.855469, precision 0.901840490797546, recall 0.875
2019-03-03T13:06:57.154198: step 121, loss 0.431792, accuracy 0.820312, precision 0.8963414634146342, recall 0.8352272727272727
2019-03-03T13:06:57.592867: step 122, loss 0.36524, accuracy 0.855469, precision 0.9096774193548387, recall 0.8597560975609756
2019-03-03T13:06:57.938478: step 123, loss 0.316133, accuracy 0.841176, precision 0.897196261682243, recall 0.8571428571428571
2019-03-03T13:06:58.411214: step 124, loss 0.310084, accuracy 0.875, precision 0.9298245614035088, recall 0.888268156424581
2019-03-03T13:06:58.937340: step 125, loss 0.251784, accuracy 0.898438, precision 0.9285714285714286, recall 0.9176470588235294
2019-03-03T13:06:59.409585: step 126, loss 0.252059, accuracy 0.875, precision 0.9030303030303031, recall 0.9030303030303031
2019-03-03T13:06:59.880243: step 127, loss 0.286468, accuracy 0.863281, precision 0.8670520231213873, recall 0.9259259259259259
2019-03-03T13:07:00.409015: step 128, loss 0.296752, accuracy 0.878906, precision 0.9090909090909091, recall 0.9142857142857143
2019-03-03T13:07:00.999942: step 129, loss 0.263038, accuracy 0.882812, precision 0.94375, recall 0.877906976744186
2019-03-03T13:07:01.524682: step 130, loss 0.250867, accuracy 0.894531, precision 0.9190751445086706, recall 0.9244186046511628
2019-03-03T13:07:01.982067: step 131, loss 0.327728, accuracy 0.867188, precision 0.9272727272727272, recall 0.8742857142857143
2019-03-03T13:07:02.509641: step 132, loss 0.282094, accuracy 0.875, precision 0.9005847953216374, recall 0.9112426035502958
2019-03-03T13:07:02.996539: step 133, loss 0.332986, accuracy 0.859375, precision 0.9457831325301205, recall 0.8532608695652174
2019-03-03T13:07:03.466814: step 134, loss 0.284938, accuracy 0.875, precision 0.9135802469135802, recall 0.891566265060241
2019-03-03T13:07:03.920600: step 135, loss 0.326585, accuracy 0.871094, precision 0.9333333333333333, recall 0.8888888888888888
2019-03-03T13:07:04.414280: step 136, loss 0.316364, accuracy 0.855469, precision 0.8988095238095238, recall 0.8830409356725146
2019-03-03T13:07:04.896531: step 137, loss 0.207536, accuracy 0.925781, precision 0.9532163742690059, recall 0.9367816091954023
2019-03-03T13:07:05.352337: step 138, loss 0.278865, accuracy 0.882812, precision 0.9320987654320988, recall 0.888235294117647
2019-03-03T13:07:05.847493: step 139, loss 0.331335, accuracy 0.859375, precision 0.9025974025974026, recall 0.86875
2019-03-03T13:07:06.385148: step 140, loss 0.264721, accuracy 0.894531, precision 0.9181286549707602, recall 0.9235294117647059
2019-03-03T13:07:06.908318: step 141, loss 0.300268, accuracy 0.847656, precision 0.8764705882352941, recall 0.8922155688622755
2019-03-03T13:07:07.380648: step 142, loss 0.295152, accuracy 0.839844, precision 0.8588957055214724, recall 0.8860759493670886
2019-03-03T13:07:07.897786: step 143, loss 0.312215, accuracy 0.867188, precision 0.9190751445086706, recall 0.888268156424581
2019-03-03T13:07:08.359553: step 144, loss 0.263777, accuracy 0.882812, precision 0.907103825136612, recall 0.9273743016759777
2019-03-03T13:07:08.857223: step 145, loss 0.319433, accuracy 0.875, precision 0.9065934065934066, recall 0.9166666666666666
2019-03-03T13:07:09.308523: step 146, loss 0.259485, accuracy 0.890625, precision 0.9454545454545454, recall 0.8914285714285715
2019-03-03T13:07:09.810716: step 147, loss 0.293785, accuracy 0.871094, precision 0.9142857142857143, recall 0.898876404494382
2019-03-03T13:07:10.279991: step 148, loss 0.302161, accuracy 0.867188, precision 0.9106145251396648, recall 0.9005524861878453
2019-03-03T13:07:10.760744: step 149, loss 0.318746, accuracy 0.867188, precision 0.9352941176470588, recall 0.8736263736263736
2019-03-03T13:07:11.241461: step 150, loss 0.244465, accuracy 0.902344, precision 0.9608938547486033, recall 0.9052631578947369
2019-03-03T13:07:11.701744: step 151, loss 0.224998, accuracy 0.898438, precision 0.9318181818181818, recall 0.9213483146067416
2019-03-03T13:07:12.194425: step 152, loss 0.283024, accuracy 0.867188, precision 0.9209039548022598, recall 0.8907103825136612
2019-03-03T13:07:12.694031: step 153, loss 0.304917, accuracy 0.863281, precision 0.90625, recall 0.8787878787878788
2019-03-03T13:07:13.135869: step 154, loss 0.256557, accuracy 0.871094, precision 0.9085714285714286, recall 0.9034090909090909
2019-03-03T13:07:13.576723: step 155, loss 0.265891, accuracy 0.882812, precision 0.9119496855345912, recall 0.9006211180124224
2019-03-03T13:07:14.071002: step 156, loss 0.297883, accuracy 0.875, precision 0.9012345679012346, recall 0.9012345679012346
2019-03-03T13:07:14.567933: step 157, loss 0.319763, accuracy 0.859375, precision 0.8922155688622755, recall 0.8922155688622755
2019-03-03T13:07:15.042105: step 158, loss 0.272825, accuracy 0.867188, precision 0.8941176470588236, recall 0.9047619047619048
2019-03-03T13:07:15.524835: step 159, loss 0.291302, accuracy 0.855469, precision 0.9302325581395349, recall 0.8648648648648649
2019-03-03T13:07:16.067813: step 160, loss 0.314641, accuracy 0.867188, precision 0.9078947368421053, recall 0.8734177215189873
2019-03-03T13:07:16.570471: step 161, loss 0.291505, accuracy 0.871094, precision 0.8850574712643678, recall 0.9221556886227545
2019-03-03T13:07:17.024765: step 162, loss 0.249014, accuracy 0.894531, precision 0.9482758620689655, recall 0.9016393442622951
2019-03-03T13:07:17.472117: step 163, loss 0.283441, accuracy 0.894531, precision 0.9390243902439024, recall 0.9005847953216374
2019-03-03T13:07:17.821208: step 164, loss 0.295264, accuracy 0.9, precision 0.9252336448598131, recall 0.9166666666666666
2019-03-03T13:07:18.336344: step 165, loss 0.216156, accuracy 0.90625, precision 0.9101796407185628, recall 0.9440993788819876
2019-03-03T13:07:18.855057: step 166, loss 0.192578, accuracy 0.925781, precision 0.9476744186046512, recall 0.9421965317919075
2019-03-03T13:07:19.320337: step 167, loss 0.156416, accuracy 0.960938, precision 0.9879518072289156, recall 0.9534883720930233
2019-03-03T13:07:19.812043: step 168, loss 0.242931, accuracy 0.871094, precision 0.9106145251396648, recall 0.9055555555555556
2019-03-03T13:07:20.326668: step 169, loss 0.231703, accuracy 0.902344, precision 0.930635838150289, recall 0.9252873563218391
2019-03-03T13:07:20.813383: step 170, loss 0.256067, accuracy 0.886719, precision 0.9333333333333333, recall 0.8805031446540881
2019-03-03T13:07:21.307091: step 171, loss 0.302821, accuracy 0.855469, precision 0.861878453038674, recall 0.9285714285714286
2019-03-03T13:07:21.807752: step 172, loss 0.231984, accuracy 0.890625, precision 0.9171597633136095, recall 0.9171597633136095
2019-03-03T13:07:22.304935: step 173, loss 0.238326, accuracy 0.886719, precision 0.9142857142857143, recall 0.9195402298850575
2019-03-03T13:07:22.774701: step 174, loss 0.213547, accuracy 0.917969, precision 0.9285714285714286, recall 0.9454545454545454
2019-03-03T13:07:23.226043: step 175, loss 0.233, accuracy 0.886719, precision 0.9349112426035503, recall 0.8977272727272727
2019-03-03T13:07:23.669830: step 176, loss 0.252812, accuracy 0.90625, precision 0.9171597633136095, recall 0.9393939393939394
2019-03-03T13:07:24.177983: step 177, loss 0.201325, accuracy 0.902344, precision 0.9281767955801105, recall 0.9333333333333333
2019-03-03T13:07:24.625743: step 178, loss 0.255498, accuracy 0.894531, precision 0.9556962025316456, recall 0.8830409356725146
2019-03-03T13:07:25.207182: step 179, loss 0.233229, accuracy 0.882812, precision 0.9629629629629629, recall 0.8666666666666667
2019-03-03T13:07:25.833533: step 180, loss 0.221293, accuracy 0.925781, precision 0.9770114942528736, recall 0.918918918918919
2019-03-03T13:07:26.513243: step 181, loss 0.217759, accuracy 0.910156, precision 0.9308176100628931, recall 0.925
2019-03-03T13:07:27.119157: step 182, loss 0.213943, accuracy 0.925781, precision 0.9397590361445783, recall 0.9454545454545454
2019-03-03T13:07:27.685644: step 183, loss 0.185108, accuracy 0.9375, precision 0.9493670886075949, recall 0.9493670886075949
2019-03-03T13:07:28.307009: step 184, loss 0.250477, accuracy 0.90625, precision 0.9075144508670521, recall 0.9515151515151515
2019-03-03T13:07:28.859098: step 185, loss 0.270831, accuracy 0.886719, precision 0.9058823529411765, recall 0.9221556886227545
2019-03-03T13:07:29.432128: step 186, loss 0.245, accuracy 0.878906, precision 0.8837209302325582, recall 0.9325153374233128
2019-03-03T13:07:29.997636: step 187, loss 0.22657, accuracy 0.882812, precision 0.9147727272727273, recall 0.9147727272727273
2019-03-03T13:07:30.567146: step 188, loss 0.222531, accuracy 0.914062, precision 0.9426751592356688, recall 0.9192546583850931
2019-03-03T13:07:31.132870: step 189, loss 0.234763, accuracy 0.902344, precision 0.9341317365269461, recall 0.9176470588235294
2019-03-03T13:07:31.709329: step 190, loss 0.236422, accuracy 0.890625, precision 0.9285714285714286, recall 0.89375
2019-03-03T13:07:32.274333: step 191, loss 0.266221, accuracy 0.871094, precision 0.9119496855345912, recall 0.8841463414634146
2019-03-03T13:07:32.833870: step 192, loss 0.227867, accuracy 0.917969, precision 0.9476744186046512, recall 0.9314285714285714
2019-03-03T13:07:33.354024: step 193, loss 0.209568, accuracy 0.910156, precision 0.9540229885057471, recall 0.9171270718232044
2019-03-03T13:07:33.859674: step 194, loss 0.209527, accuracy 0.925781, precision 0.9320987654320988, recall 0.949685534591195
2019-03-03T13:07:34.474386: step 195, loss 0.235559, accuracy 0.90625, precision 0.927710843373494, recall 0.927710843373494
2019-03-03T13:07:35.014957: step 196, loss 0.251607, accuracy 0.894531, precision 0.9057591623036649, recall 0.9505494505494505
2019-03-03T13:07:35.521272: step 197, loss 0.203557, accuracy 0.929688, precision 0.9385474860335196, recall 0.96
2019-03-03T13:07:36.044871: step 198, loss 0.288527, accuracy 0.867188, precision 0.9084967320261438, recall 0.8742138364779874
2019-03-03T13:07:36.559036: step 199, loss 0.240148, accuracy 0.917969, precision 0.9451219512195121, recall 0.9281437125748503
2019-03-03T13:07:37.067675: step 200, loss 0.218886, accuracy 0.894531, precision 0.9476744186046512, recall 0.9005524861878453
2019-03-03T13:07:37.621703: step 201, loss 0.204844, accuracy 0.925781, precision 0.967391304347826, recall 0.9319371727748691
2019-03-03T13:07:38.131887: step 202, loss 0.227805, accuracy 0.921875, precision 0.9497206703910615, recall 0.9392265193370166
2019-03-03T13:07:38.651507: step 203, loss 0.25945, accuracy 0.898438, precision 0.9488636363636364, recall 0.907608695652174
2019-03-03T13:07:39.183594: step 204, loss 0.202559, accuracy 0.921875, precision 0.9529411764705882, recall 0.9310344827586207
2019-03-03T13:07:39.600136: step 205, loss 0.142353, accuracy 0.935294, precision 0.9322033898305084, recall 0.9734513274336283
2019-03-03T13:07:40.104290: step 206, loss 0.176932, accuracy 0.9375, precision 0.9397590361445783, recall 0.9629629629629629
2019-03-03T13:07:40.617935: step 207, loss 0.224879, accuracy 0.914062, precision 0.9197860962566845, recall 0.9608938547486033
2019-03-03T13:07:41.109622: step 208, loss 0.18244, accuracy 0.925781, precision 0.9741935483870968, recall 0.9096385542168675
2019-03-03T13:07:41.609286: step 209, loss 0.187277, accuracy 0.921875, precision 0.9590643274853801, recall 0.9265536723163842
2019-03-03T13:07:42.237404: step 210, loss 0.163752, accuracy 0.921875, precision 0.9415204678362573, recall 0.9415204678362573
2019-03-03T13:07:42.802947: step 211, loss 0.208953, accuracy 0.914062, precision 0.9354838709677419, recall 0.9235668789808917
2019-03-03T13:07:43.338824: step 212, loss 0.166505, accuracy 0.921875, precision 0.96, recall 0.9281767955801105
2019-03-03T13:07:43.876414: step 213, loss 0.216057, accuracy 0.914062, precision 0.9483870967741935, recall 0.9130434782608695
2019-03-03T13:07:44.494275: step 214, loss 0.21308, accuracy 0.914062, precision 0.9371428571428572, recall 0.9371428571428572
2019-03-03T13:07:45.013398: step 215, loss 0.202931, accuracy 0.921875, precision 0.9226519337016574, recall 0.9653179190751445
2019-03-03T13:07:45.498121: step 216, loss 0.185084, accuracy 0.921875, precision 0.9532163742690059, recall 0.9314285714285714
2019-03-03T13:07:45.975394: step 217, loss 0.1807, accuracy 0.945312, precision 0.9467455621301775, recall 0.9696969696969697
2019-03-03T13:07:46.466610: step 218, loss 0.161156, accuracy 0.941406, precision 0.9467455621301775, recall 0.963855421686747
2019-03-03T13:07:46.924384: step 219, loss 0.145555, accuracy 0.960938, precision 0.978021978021978, recall 0.967391304347826
2019-03-03T13:07:47.471949: step 220, loss 0.200048, accuracy 0.917969, precision 0.9827586206896551, recall 0.9047619047619048
2019-03-03T13:07:47.966813: step 221, loss 0.178245, accuracy 0.910156, precision 0.9705882352941176, recall 0.9016393442622951
2019-03-03T13:07:48.598639: step 222, loss 0.168309, accuracy 0.925781, precision 0.9529411764705882, recall 0.9364161849710982
2019-03-03T13:07:49.124229: step 223, loss 0.216727, accuracy 0.914062, precision 0.9700598802395209, recall 0.9050279329608939
2019-03-03T13:07:49.635862: step 224, loss 0.193832, accuracy 0.902344, precision 0.9488636363636364, recall 0.912568306010929
2019-03-03T13:07:50.198904: step 225, loss 0.204187, accuracy 0.921875, precision 0.9382022471910112, recall 0.9488636363636364
2019-03-03T13:07:50.692560: step 226, loss 0.22259, accuracy 0.914062, precision 0.9629629629629629, recall 0.9069767441860465
2019-03-03T13:07:51.163280: step 227, loss 0.194579, accuracy 0.925781, precision 0.9263803680981595, recall 0.9556962025316456
2019-03-03T13:07:51.699313: step 228, loss 0.192007, accuracy 0.9375, precision 0.9595375722543352, recall 0.9485714285714286
2019-03-03T13:07:52.254849: step 229, loss 0.181667, accuracy 0.925781, precision 0.9518072289156626, recall 0.9349112426035503
2019-03-03T13:07:52.795759: step 230, loss 0.172948, accuracy 0.933594, precision 0.936046511627907, recall 0.9640718562874252
2019-03-03T13:07:53.264032: step 231, loss 0.1864, accuracy 0.929688, precision 0.9388888888888889, recall 0.9602272727272727
2019-03-03T13:07:53.723334: step 232, loss 0.209544, accuracy 0.910156, precision 0.9482758620689655, recall 0.9217877094972067
2019-03-03T13:07:54.234568: step 233, loss 0.209273, accuracy 0.914062, precision 0.9565217391304348, recall 0.9112426035502958
2019-03-03T13:07:54.775666: step 234, loss 0.189458, accuracy 0.929688, precision 0.976878612716763, recall 0.9234972677595629
2019-03-03T13:07:55.340690: step 235, loss 0.213537, accuracy 0.921875, precision 0.9627329192546584, recall 0.9171597633136095
2019-03-03T13:07:55.913426: step 236, loss 0.157598, accuracy 0.953125, precision 0.9807692307692307, recall 0.9444444444444444
2019-03-03T13:07:56.470447: step 237, loss 0.205452, accuracy 0.929688, precision 0.9580838323353293, recall 0.935672514619883
2019-03-03T13:07:56.963270: step 238, loss 0.215752, accuracy 0.910156, precision 0.9156626506024096, recall 0.9440993788819876
2019-03-03T13:07:57.478892: step 239, loss 0.199465, accuracy 0.929688, precision 0.9447852760736196, recall 0.9447852760736196
2019-03-03T13:07:57.978556: step 240, loss 0.151197, accuracy 0.945312, precision 0.9518072289156626, recall 0.9634146341463414
2019-03-03T13:07:58.499810: step 241, loss 0.193006, accuracy 0.902344, precision 0.930635838150289, recall 0.9252873563218391
2019-03-03T13:07:58.978925: step 242, loss 0.208432, accuracy 0.917969, precision 0.9457831325301205, recall 0.9289940828402367
2019-03-03T13:07:59.469126: step 243, loss 0.195722, accuracy 0.921875, precision 0.9634146341463414, recall 0.9186046511627907
2019-03-03T13:07:59.977790: step 244, loss 0.202711, accuracy 0.929688, precision 0.9285714285714286, recall 0.9629629629629629
2019-03-03T13:08:00.471006: step 245, loss 0.221948, accuracy 0.902344, precision 0.943502824858757, recall 0.9175824175824175
2019-03-03T13:08:00.826057: step 246, loss 0.221597, accuracy 0.905882, precision 0.9482758620689655, recall 0.9166666666666666
2019-03-03T13:08:01.286956: step 247, loss 0.14848, accuracy 0.941406, precision 0.9651162790697675, recall 0.9485714285714286
2019-03-03T13:08:01.827026: step 248, loss 0.139426, accuracy 0.960938, precision 0.9763313609467456, recall 0.9649122807017544
2019-03-03T13:08:02.328316: step 249, loss 0.0991629, accuracy 0.972656, precision 0.9831460674157303, recall 0.9776536312849162
2019-03-03T13:08:02.810570: step 250, loss 0.160586, accuracy 0.949219, precision 0.9760479041916168, recall 0.9476744186046512

Evaluation:
[[686  64]
 [120 286]]
2019-03-03T13:08:03.327858: step 250, loss 0.369043, accuracy 0.84083, precision 0.9146666666666666, recall 0.8511166253101737

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551639943\checkpoints\model-250

2019-03-03T13:08:04.120203: step 251, loss 0.156731, accuracy 0.941406, precision 0.9712643678160919, recall 0.9441340782122905
2019-03-03T13:08:04.628870: step 252, loss 0.184518, accuracy 0.914062, precision 0.9298245614035088, recall 0.9408284023668639
2019-03-03T13:08:05.129247: step 253, loss 0.169584, accuracy 0.917969, precision 0.9473684210526315, recall 0.9310344827586207
2019-03-03T13:08:05.604936: step 254, loss 0.130737, accuracy 0.964844, precision 0.9661016949152542, recall 0.9827586206896551
2019-03-03T13:08:06.108898: step 255, loss 0.152963, accuracy 0.957031, precision 0.9567901234567902, recall 0.9748427672955975
2019-03-03T13:08:06.573193: step 256, loss 0.168727, accuracy 0.945312, precision 0.94375, recall 0.967948717948718
2019-03-03T13:08:07.073793: step 257, loss 0.156532, accuracy 0.929688, precision 0.9702380952380952, recall 0.9261363636363636
2019-03-03T13:08:07.548036: step 258, loss 0.134202, accuracy 0.953125, precision 0.9617834394904459, recall 0.9617834394904459
2019-03-03T13:08:08.064692: step 259, loss 0.133559, accuracy 0.945312, precision 0.9700598802395209, recall 0.9473684210526315
2019-03-03T13:08:08.543069: step 260, loss 0.127753, accuracy 0.953125, precision 0.9479768786127167, recall 0.9820359281437125
2019-03-03T13:08:09.009351: step 261, loss 0.165509, accuracy 0.925781, precision 0.9588235294117647, recall 0.9314285714285714
2019-03-03T13:08:09.470142: step 262, loss 0.157918, accuracy 0.945312, precision 0.96045197740113, recall 0.96045197740113
2019-03-03T13:08:09.959233: step 263, loss 0.184695, accuracy 0.917969, precision 0.9320987654320988, recall 0.937888198757764
2019-03-03T13:08:10.410034: step 264, loss 0.146675, accuracy 0.941406, precision 0.9649122807017544, recall 0.9482758620689655
2019-03-03T13:08:10.860831: step 265, loss 0.14874, accuracy 0.9375, precision 0.9710982658959537, recall 0.9385474860335196
2019-03-03T13:08:11.335864: step 266, loss 0.141044, accuracy 0.9375, precision 0.9523809523809523, recall 0.9523809523809523
2019-03-03T13:08:11.823076: step 267, loss 0.13428, accuracy 0.945312, precision 0.967741935483871, recall 0.9433962264150944
2019-03-03T13:08:12.317000: step 268, loss 0.137827, accuracy 0.945312, precision 0.968944099378882, recall 0.9454545454545454
2019-03-03T13:08:12.847581: step 269, loss 0.154961, accuracy 0.945312, precision 0.9421965317919075, recall 0.9760479041916168
2019-03-03T13:08:13.343257: step 270, loss 0.17393, accuracy 0.949219, precision 0.9625, recall 0.9565217391304348
2019-03-03T13:08:13.853912: step 271, loss 0.130116, accuracy 0.960938, precision 0.9502762430939227, recall 0.9942196531791907
2019-03-03T13:08:14.316553: step 272, loss 0.13925, accuracy 0.949219, precision 0.9595375722543352, recall 0.9651162790697675
2019-03-03T13:08:14.776830: step 273, loss 0.124688, accuracy 0.960938, precision 0.9763313609467456, recall 0.9649122807017544
2019-03-03T13:08:15.227335: step 274, loss 0.120189, accuracy 0.957031, precision 0.9485714285714286, recall 0.9880952380952381
2019-03-03T13:08:15.715057: step 275, loss 0.146083, accuracy 0.9375, precision 0.9476744186046512, recall 0.9588235294117647
2019-03-03T13:08:16.164905: step 276, loss 0.147752, accuracy 0.953125, precision 0.9518072289156626, recall 0.9753086419753086
2019-03-03T13:08:16.634281: step 277, loss 0.161274, accuracy 0.925781, precision 0.9828571428571429, recall 0.9148936170212766
2019-03-03T13:08:17.164374: step 278, loss 0.13734, accuracy 0.941406, precision 0.9779005524861878, recall 0.9414893617021277
2019-03-03T13:08:17.723880: step 279, loss 0.145842, accuracy 0.953125, precision 0.987012987012987, recall 0.9382716049382716
2019-03-03T13:08:18.232543: step 280, loss 0.127733, accuracy 0.953125, precision 0.9666666666666667, recall 0.9666666666666667
2019-03-03T13:08:18.713745: step 281, loss 0.187369, accuracy 0.9375, precision 0.9817073170731707, recall 0.9252873563218391
2019-03-03T13:08:19.188006: step 282, loss 0.120488, accuracy 0.953125, precision 0.9585798816568047, recall 0.9700598802395209
2019-03-03T13:08:19.690664: step 283, loss 0.168609, accuracy 0.925781, precision 0.9358974358974359, recall 0.9419354838709677
2019-03-03T13:08:20.174883: step 284, loss 0.160739, accuracy 0.949219, precision 0.9653179190751445, recall 0.9597701149425287
2019-03-03T13:08:20.633659: step 285, loss 0.144778, accuracy 0.953125, precision 0.9479768786127167, recall 0.9820359281437125
2019-03-03T13:08:21.121866: step 286, loss 0.192144, accuracy 0.925781, precision 0.9464285714285714, recall 0.9408284023668639
2019-03-03T13:08:21.507687: step 287, loss 0.131381, accuracy 0.952941, precision 0.9495798319327731, recall 0.9826086956521739
2019-03-03T13:08:21.982933: step 288, loss 0.133389, accuracy 0.941406, precision 0.9627329192546584, recall 0.9451219512195121
2019-03-03T13:08:22.491107: step 289, loss 0.149145, accuracy 0.9375, precision 0.9513513513513514, recall 0.9617486338797814
2019-03-03T13:08:22.930991: step 290, loss 0.0699107, accuracy 0.976562, precision 0.9821428571428571, recall 0.9821428571428571
2019-03-03T13:08:23.438655: step 291, loss 0.0978351, accuracy 0.96875, precision 0.9879518072289156, recall 0.9647058823529412
2019-03-03T13:08:23.880876: step 292, loss 0.131955, accuracy 0.929688, precision 0.9617486338797814, recall 0.9411764705882353
2019-03-03T13:08:24.381515: step 293, loss 0.122711, accuracy 0.953125, precision 0.9882352941176471, recall 0.9438202247191011
2019-03-03T13:08:24.826834: step 294, loss 0.127378, accuracy 0.953125, precision 0.9698795180722891, recall 0.9583333333333334
2019-03-03T13:08:25.319515: step 295, loss 0.144605, accuracy 0.945312, precision 0.994413407821229, recall 0.9319371727748691
2019-03-03T13:08:25.790801: step 296, loss 0.127018, accuracy 0.945312, precision 0.967741935483871, recall 0.9433962264150944
2019-03-03T13:08:26.246968: step 297, loss 0.131816, accuracy 0.945312, precision 0.9760479041916168, recall 0.9421965317919075
2019-03-03T13:08:26.704812: step 298, loss 0.128736, accuracy 0.941406, precision 0.9702380952380952, recall 0.9421965317919075
2019-03-03T13:08:27.192985: step 299, loss 0.0979863, accuracy 0.960938, precision 0.9722222222222222, recall 0.9722222222222222
2019-03-03T13:08:27.638801: step 300, loss 0.0975369, accuracy 0.964844, precision 0.9625, recall 0.9808917197452229
2019-03-03T13:08:28.107095: step 301, loss 0.14691, accuracy 0.941406, precision 0.9322033898305084, recall 0.9821428571428571
2019-03-03T13:08:28.566850: step 302, loss 0.100427, accuracy 0.964844, precision 0.9631901840490797, recall 0.98125
2019-03-03T13:08:29.032512: step 303, loss 0.116343, accuracy 0.960938, precision 0.9832402234636871, recall 0.9617486338797814
2019-03-03T13:08:29.503764: step 304, loss 0.106689, accuracy 0.972656, precision 0.976878612716763, recall 0.9825581395348837
2019-03-03T13:08:29.985005: step 305, loss 0.115507, accuracy 0.960938, precision 0.9723756906077348, recall 0.9723756906077348
2019-03-03T13:08:30.472232: step 306, loss 0.131797, accuracy 0.957031, precision 0.9817073170731707, recall 0.9526627218934911
2019-03-03T13:08:30.949467: step 307, loss 0.126781, accuracy 0.960938, precision 0.9700598802395209, recall 0.9700598802395209
2019-03-03T13:08:31.437166: step 308, loss 0.112687, accuracy 0.953125, precision 0.976878612716763, recall 0.9548022598870056
2019-03-03T13:08:31.901145: step 309, loss 0.0959054, accuracy 0.960938, precision 0.9723756906077348, recall 0.9723756906077348
2019-03-03T13:08:32.412838: step 310, loss 0.103693, accuracy 0.964844, precision 0.9941520467836257, recall 0.9550561797752809
2019-03-03T13:08:32.876630: step 311, loss 0.158268, accuracy 0.941406, precision 0.9664804469273743, recall 0.9505494505494505
2019-03-03T13:08:33.352790: step 312, loss 0.15547, accuracy 0.933594, precision 0.9551282051282052, recall 0.9371069182389937
2019-03-03T13:08:33.855446: step 313, loss 0.122507, accuracy 0.957031, precision 0.9820359281437125, recall 0.9534883720930233
2019-03-03T13:08:34.309258: step 314, loss 0.128879, accuracy 0.953125, precision 0.9704142011834319, recall 0.9590643274853801
2019-03-03T13:08:34.755573: step 315, loss 0.143575, accuracy 0.949219, precision 0.9570552147239264, recall 0.9629629629629629
2019-03-03T13:08:35.242274: step 316, loss 0.117201, accuracy 0.949219, precision 0.9461077844311377, recall 0.9753086419753086
2019-03-03T13:08:35.723517: step 317, loss 0.116884, accuracy 0.953125, precision 0.9751552795031055, recall 0.9515151515151515
2019-03-03T13:08:36.186280: step 318, loss 0.0918886, accuracy 0.972656, precision 0.9751552795031055, recall 0.98125
2019-03-03T13:08:36.638044: step 319, loss 0.133393, accuracy 0.957031, precision 0.9588235294117647, recall 0.9760479041916168
2019-03-03T13:08:37.161159: step 320, loss 0.12662, accuracy 0.945312, precision 0.9358974358974359, recall 0.9733333333333334
2019-03-03T13:08:37.607021: step 321, loss 0.128598, accuracy 0.957031, precision 0.9542857142857143, recall 0.9823529411764705
2019-03-03T13:08:38.064770: step 322, loss 0.100362, accuracy 0.957031, precision 0.9602272727272727, recall 0.976878612716763
2019-03-03T13:08:38.506981: step 323, loss 0.156002, accuracy 0.945312, precision 0.9411764705882353, recall 0.975609756097561
2019-03-03T13:08:38.999684: step 324, loss 0.140149, accuracy 0.9375, precision 0.96875, recall 0.9337349397590361
2019-03-03T13:08:39.476408: step 325, loss 0.118337, accuracy 0.953125, precision 0.9813664596273292, recall 0.9461077844311377
2019-03-03T13:08:39.982568: step 326, loss 0.101649, accuracy 0.964844, precision 0.9940828402366864, recall 0.9545454545454546
2019-03-03T13:08:40.436382: step 327, loss 0.144679, accuracy 0.949219, precision 0.9640718562874252, recall 0.9583333333333334
2019-03-03T13:08:40.819760: step 328, loss 0.128593, accuracy 0.941176, precision 0.9583333333333334, recall 0.9583333333333334
2019-03-03T13:08:41.343909: step 329, loss 0.106825, accuracy 0.96875, precision 0.9696969696969697, recall 0.9815950920245399
2019-03-03T13:08:41.803679: step 330, loss 0.113219, accuracy 0.960938, precision 0.9825581395348837, recall 0.9602272727272727
2019-03-03T13:08:42.250496: step 331, loss 0.0921859, accuracy 0.976562, precision 0.9763313609467456, recall 0.9880239520958084
2019-03-03T13:08:42.702795: step 332, loss 0.0703573, accuracy 0.988281, precision 1.0, recall 0.9823529411764705
2019-03-03T13:08:43.217118: step 333, loss 0.100716, accuracy 0.953125, precision 0.9540229885057471, recall 0.9764705882352941
2019-03-03T13:08:43.718792: step 334, loss 0.0821234, accuracy 0.980469, precision 0.9763313609467456, recall 0.9939759036144579
2019-03-03T13:08:44.245787: step 335, loss 0.10093, accuracy 0.972656, precision 0.9714285714285714, recall 0.9883720930232558
2019-03-03T13:08:44.710568: step 336, loss 0.114645, accuracy 0.964844, precision 0.9935483870967742, recall 0.9506172839506173
2019-03-03T13:08:45.229658: step 337, loss 0.125478, accuracy 0.953125, precision 0.9565217391304348, recall 0.9685534591194969
2019-03-03T13:08:45.711369: step 338, loss 0.109401, accuracy 0.957031, precision 0.9590643274853801, recall 0.9761904761904762
2019-03-03T13:08:46.176635: step 339, loss 0.0799287, accuracy 0.976562, precision 0.984375, recall 0.984375
2019-03-03T13:08:46.640920: step 340, loss 0.0957057, accuracy 0.964844, precision 0.9661016949152542, recall 0.9827586206896551
2019-03-03T13:08:47.157154: step 341, loss 0.123595, accuracy 0.949219, precision 0.9764705882352941, recall 0.9485714285714286
2019-03-03T13:08:47.631415: step 342, loss 0.0934044, accuracy 0.96875, precision 0.9822485207100592, recall 0.9707602339181286
2019-03-03T13:08:48.101157: step 343, loss 0.131851, accuracy 0.949219, precision 0.9763313609467456, recall 0.9482758620689655
2019-03-03T13:08:48.562971: step 344, loss 0.108335, accuracy 0.960938, precision 0.9814814814814815, recall 0.9578313253012049
2019-03-03T13:08:49.062236: step 345, loss 0.108163, accuracy 0.957031, precision 0.9725274725274725, recall 0.9672131147540983
2019-03-03T13:08:49.511032: step 346, loss 0.11076, accuracy 0.964844, precision 0.9817073170731707, recall 0.9640718562874252
2019-03-03T13:08:50.006243: step 347, loss 0.102854, accuracy 0.960938, precision 0.9695121951219512, recall 0.9695121951219512
2019-03-03T13:08:50.495599: step 348, loss 0.0905994, accuracy 0.96875, precision 0.9700598802395209, recall 0.9818181818181818
2019-03-03T13:08:51.040144: step 349, loss 0.0824218, accuracy 0.96875, precision 0.9835164835164835, recall 0.9728260869565217
2019-03-03T13:08:51.539839: step 350, loss 0.107273, accuracy 0.957031, precision 0.9695121951219512, recall 0.9636363636363636
2019-03-03T13:08:52.012140: step 351, loss 0.0913813, accuracy 0.976562, precision 0.9691358024691358, recall 0.9936708860759493
2019-03-03T13:08:52.478368: step 352, loss 0.115525, accuracy 0.957031, precision 0.9555555555555556, recall 0.9828571428571429
2019-03-03T13:08:53.008460: step 353, loss 0.0779698, accuracy 0.976562, precision 0.9753086419753086, recall 0.9875
2019-03-03T13:08:53.504644: step 354, loss 0.119971, accuracy 0.949219, precision 0.9550561797752809, recall 0.9714285714285714
2019-03-03T13:08:53.974916: step 355, loss 0.0839911, accuracy 0.972656, precision 0.9710982658959537, recall 0.9882352941176471
2019-03-03T13:08:54.445179: step 356, loss 0.110158, accuracy 0.953125, precision 0.9883720930232558, recall 0.9444444444444444
2019-03-03T13:08:54.952856: step 357, loss 0.0867531, accuracy 0.972656, precision 0.9876543209876543, recall 0.9696969696969697
2019-03-03T13:08:55.415642: step 358, loss 0.10062, accuracy 0.96875, precision 0.9872611464968153, recall 0.9627329192546584
2019-03-03T13:08:55.891443: step 359, loss 0.121108, accuracy 0.953125, precision 0.9874213836477987, recall 0.9401197604790419
2019-03-03T13:08:56.373122: step 360, loss 0.0973846, accuracy 0.96875, precision 0.9888888888888889, recall 0.967391304347826
2019-03-03T13:08:56.853340: step 361, loss 0.106711, accuracy 0.96875, precision 0.9880952380952381, recall 0.9651162790697675
2019-03-03T13:08:57.303206: step 362, loss 0.0596868, accuracy 0.992188, precision 1.0, recall 0.9883040935672515
2019-03-03T13:08:57.769003: step 363, loss 0.105513, accuracy 0.957031, precision 0.9644970414201184, recall 0.9702380952380952
2019-03-03T13:08:58.263680: step 364, loss 0.135857, accuracy 0.9375, precision 0.9314285714285714, recall 0.9760479041916168
2019-03-03T13:08:58.741914: step 365, loss 0.1204, accuracy 0.953125, precision 0.9440993788819876, recall 0.9806451612903225
2019-03-03T13:08:59.208243: step 366, loss 0.130927, accuracy 0.941406, precision 0.9526627218934911, recall 0.9583333333333334
2019-03-03T13:08:59.646061: step 367, loss 0.111007, accuracy 0.945312, precision 0.950920245398773, recall 0.9627329192546584
2019-03-03T13:09:00.091657: step 368, loss 0.127109, accuracy 0.953125, precision 0.9817073170731707, recall 0.9470588235294117
2019-03-03T13:09:00.470655: step 369, loss 0.0734322, accuracy 0.982353, precision 0.9752066115702479, recall 1.0
2019-03-03T13:09:00.944968: step 370, loss 0.0843596, accuracy 0.976562, precision 0.98125, recall 0.98125
2019-03-03T13:09:01.431250: step 371, loss 0.0700226, accuracy 0.992188, precision 0.9942528735632183, recall 0.9942528735632183
2019-03-03T13:09:01.885024: step 372, loss 0.0945348, accuracy 0.964844, precision 0.9936305732484076, recall 0.9512195121951219
2019-03-03T13:09:02.387698: step 373, loss 0.110843, accuracy 0.960938, precision 0.9876543209876543, recall 0.9523809523809523
2019-03-03T13:09:02.882374: step 374, loss 0.0727143, accuracy 0.96875, precision 0.9777777777777777, recall 0.9777777777777777
2019-03-03T13:09:03.359639: step 375, loss 0.0890769, accuracy 0.972656, precision 0.9710982658959537, recall 0.9882352941176471
2019-03-03T13:09:03.887760: step 376, loss 0.0628623, accuracy 0.988281, precision 0.9878048780487805, recall 0.9938650306748467
2019-03-03T13:09:04.410363: step 377, loss 0.0648346, accuracy 0.976562, precision 0.9774011299435028, recall 0.9885714285714285
2019-03-03T13:09:04.879618: step 378, loss 0.0792885, accuracy 0.980469, precision 1.0, recall 0.9707602339181286
2019-03-03T13:09:05.346907: step 379, loss 0.0919696, accuracy 0.964844, precision 0.9700598802395209, recall 0.9759036144578314
2019-03-03T13:09:05.833876: step 380, loss 0.0695992, accuracy 0.988281, precision 0.9939759036144579, recall 0.9880239520958084
2019-03-03T13:09:06.324585: step 381, loss 0.0741265, accuracy 0.976562, precision 0.9817073170731707, recall 0.9817073170731707
2019-03-03T13:09:06.776375: step 382, loss 0.0685078, accuracy 0.972656, precision 0.9642857142857143, recall 0.9938650306748467
2019-03-03T13:09:07.240143: step 383, loss 0.0757699, accuracy 0.976562, precision 0.9873417721518988, recall 0.975
2019-03-03T13:09:07.720335: step 384, loss 0.0710663, accuracy 0.984375, precision 0.9884393063583815, recall 0.9884393063583815
2019-03-03T13:09:08.288813: step 385, loss 0.0962063, accuracy 0.96875, precision 0.9731182795698925, recall 0.9836956521739131
2019-03-03T13:09:08.742601: step 386, loss 0.0958035, accuracy 0.957031, precision 0.9482758620689655, recall 0.9880239520958084
2019-03-03T13:09:09.205363: step 387, loss 0.0620224, accuracy 0.992188, precision 0.9943181818181818, recall 0.9943181818181818
2019-03-03T13:09:09.685590: step 388, loss 0.0845788, accuracy 0.972656, precision 0.9775280898876404, recall 0.9830508474576272
2019-03-03T13:09:10.154863: step 389, loss 0.0840607, accuracy 0.96875, precision 0.9709302325581395, recall 0.9823529411764705
2019-03-03T13:09:10.639587: step 390, loss 0.0717497, accuracy 0.972656, precision 1.0, recall 0.9629629629629629
2019-03-03T13:09:11.151219: step 391, loss 0.1004, accuracy 0.964844, precision 0.9835164835164835, recall 0.9675675675675676
2019-03-03T13:09:11.633945: step 392, loss 0.0510734, accuracy 0.996094, precision 1.0, recall 0.9940828402366864
2019-03-03T13:09:12.103225: step 393, loss 0.0494325, accuracy 0.992188, precision 1.0, recall 0.9880952380952381
2019-03-03T13:09:12.555143: step 394, loss 0.0667384, accuracy 0.980469, precision 0.9767441860465116, recall 0.9940828402366864
2019-03-03T13:09:12.994003: step 395, loss 0.0666465, accuracy 0.984375, precision 0.9815950920245399, recall 0.9937888198757764
2019-03-03T13:09:13.529592: step 396, loss 0.0912677, accuracy 0.96875, precision 0.9760479041916168, recall 0.9760479041916168
2019-03-03T13:09:14.036748: step 397, loss 0.103571, accuracy 0.96875, precision 0.9700598802395209, recall 0.9818181818181818
2019-03-03T13:09:14.505516: step 398, loss 0.0832404, accuracy 0.96875, precision 0.9760479041916168, recall 0.9760479041916168
2019-03-03T13:09:14.964824: step 399, loss 0.0503281, accuracy 0.988281, precision 0.9940119760479041, recall 0.9880952380952381
2019-03-03T13:09:15.464032: step 400, loss 0.0802665, accuracy 0.972656, precision 0.9819277108433735, recall 0.9760479041916168
2019-03-03T13:09:15.912832: step 401, loss 0.0677192, accuracy 0.984375, precision 0.9936708860759493, recall 0.98125
2019-03-03T13:09:16.371594: step 402, loss 0.0673032, accuracy 0.984375, precision 0.9830508474576272, recall 0.9942857142857143
2019-03-03T13:09:16.829334: step 403, loss 0.0650778, accuracy 0.976562, precision 0.9939024390243902, recall 0.9702380952380952
2019-03-03T13:09:17.332959: step 404, loss 0.0792037, accuracy 0.972656, precision 0.9882352941176471, recall 0.9710982658959537
2019-03-03T13:09:17.790800: step 405, loss 0.0697919, accuracy 0.96875, precision 0.9763313609467456, recall 0.9763313609467456
2019-03-03T13:09:18.241158: step 406, loss 0.0733199, accuracy 0.984375, precision 0.9761904761904762, recall 1.0
2019-03-03T13:09:18.745793: step 407, loss 0.0697132, accuracy 0.972656, precision 0.9873417721518988, recall 0.968944099378882
2019-03-03T13:09:19.242337: step 408, loss 0.0764762, accuracy 0.964844, precision 0.9887640449438202, recall 0.9617486338797814
2019-03-03T13:09:19.698641: step 409, loss 0.117927, accuracy 0.953125, precision 0.9588235294117647, recall 0.9702380952380952
2019-03-03T13:09:20.041746: step 410, loss 0.0772487, accuracy 0.964706, precision 0.9818181818181818, recall 0.9642857142857143
2019-03-03T13:09:20.504310: step 411, loss 0.0677636, accuracy 0.984375, precision 0.9881656804733728, recall 0.9881656804733728
2019-03-03T13:09:21.012948: step 412, loss 0.0756509, accuracy 0.980469, precision 0.9888888888888889, recall 0.9834254143646409
2019-03-03T13:09:21.494660: step 413, loss 0.0883564, accuracy 0.972656, precision 0.9617486338797814, recall 1.0
2019-03-03T13:09:21.952604: step 414, loss 0.080945, accuracy 0.96875, precision 0.9764705882352941, recall 0.9764705882352941
2019-03-03T13:09:22.411974: step 415, loss 0.0627287, accuracy 0.996094, precision 0.99375, recall 1.0
2019-03-03T13:09:22.897373: step 416, loss 0.0623169, accuracy 0.988281, precision 0.9827586206896551, recall 1.0
2019-03-03T13:09:23.390181: step 417, loss 0.0529796, accuracy 0.984375, precision 0.978021978021978, recall 1.0
2019-03-03T13:09:23.903802: step 418, loss 0.0668705, accuracy 0.976562, precision 0.9818181818181818, recall 0.9818181818181818
2019-03-03T13:09:24.398514: step 419, loss 0.0823024, accuracy 0.96875, precision 0.9936708860759493, recall 0.9573170731707317
2019-03-03T13:09:24.899686: step 420, loss 0.0602382, accuracy 0.980469, precision 0.9866666666666667, recall 0.9801324503311258
2019-03-03T13:09:25.369426: step 421, loss 0.0673133, accuracy 0.96875, precision 0.9886363636363636, recall 0.9666666666666667
2019-03-03T13:09:25.839682: step 422, loss 0.0803984, accuracy 0.964844, precision 0.9696969696969697, recall 0.975609756097561
2019-03-03T13:09:26.305469: step 423, loss 0.0491327, accuracy 0.988281, precision 0.9941860465116279, recall 0.9884393063583815
2019-03-03T13:09:26.822456: step 424, loss 0.0890415, accuracy 0.972656, precision 0.9806451612903225, recall 0.9743589743589743
2019-03-03T13:09:27.282265: step 425, loss 0.0685191, accuracy 0.976562, precision 0.9715909090909091, recall 0.9941860465116279
2019-03-03T13:09:27.755513: step 426, loss 0.039923, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:09:28.224254: step 427, loss 0.0763998, accuracy 0.972656, precision 0.9707602339181286, recall 0.9880952380952381
2019-03-03T13:09:28.734249: step 428, loss 0.062571, accuracy 0.976562, precision 0.9887005649717514, recall 0.9776536312849162
2019-03-03T13:09:29.209512: step 429, loss 0.0557993, accuracy 0.984375, precision 0.9821428571428571, recall 0.9939759036144579
2019-03-03T13:09:29.674138: step 430, loss 0.0654488, accuracy 0.980469, precision 0.9722222222222222, recall 1.0
2019-03-03T13:09:30.173802: step 431, loss 0.0759193, accuracy 0.960938, precision 0.968944099378882, recall 0.968944099378882
2019-03-03T13:09:30.670986: step 432, loss 0.0989769, accuracy 0.96875, precision 0.9775280898876404, recall 0.9775280898876404
2019-03-03T13:09:31.137739: step 433, loss 0.0520481, accuracy 0.980469, precision 0.9940828402366864, recall 0.9767441860465116
2019-03-03T13:09:31.598167: step 434, loss 0.0655403, accuracy 0.980469, precision 0.9937888198757764, recall 0.975609756097561
2019-03-03T13:09:32.080910: step 435, loss 0.0677509, accuracy 0.976562, precision 0.9719101123595506, recall 0.9942528735632183
2019-03-03T13:09:32.562633: step 436, loss 0.0678207, accuracy 0.976562, precision 0.9938271604938271, recall 0.9698795180722891
2019-03-03T13:09:33.023428: step 437, loss 0.0427336, accuracy 0.992188, precision 1.0, recall 0.9886363636363636
2019-03-03T13:09:33.468750: step 438, loss 0.0877239, accuracy 0.957031, precision 0.9726775956284153, recall 0.967391304347826
2019-03-03T13:09:33.917560: step 439, loss 0.0808579, accuracy 0.976562, precision 0.9760479041916168, recall 0.9878787878787879
2019-03-03T13:09:34.400832: step 440, loss 0.0757806, accuracy 0.980469, precision 0.9883720930232558, recall 0.9826589595375722
2019-03-03T13:09:34.879524: step 441, loss 0.0650408, accuracy 0.976562, precision 0.9877300613496932, recall 0.9757575757575757
2019-03-03T13:09:35.415093: step 442, loss 0.0471656, accuracy 0.988281, precision 0.9937106918238994, recall 0.9875
2019-03-03T13:09:35.870810: step 443, loss 0.0590072, accuracy 0.980469, precision 0.9745222929936306, recall 0.9935064935064936
2019-03-03T13:09:36.394968: step 444, loss 0.0737575, accuracy 0.960938, precision 0.96, recall 0.9824561403508771
2019-03-03T13:09:36.868566: step 445, loss 0.0910257, accuracy 0.972656, precision 0.9725274725274725, recall 0.9888268156424581
2019-03-03T13:09:37.362755: step 446, loss 0.0608443, accuracy 0.976562, precision 0.9774011299435028, recall 0.9885714285714285
2019-03-03T13:09:37.828597: step 447, loss 0.0794517, accuracy 0.972656, precision 0.975609756097561, recall 0.9815950920245399
2019-03-03T13:09:38.363684: step 448, loss 0.0824885, accuracy 0.96875, precision 0.9753086419753086, recall 0.9753086419753086
2019-03-03T13:09:38.848347: step 449, loss 0.0664344, accuracy 0.96875, precision 0.9655172413793104, recall 0.9882352941176471
2019-03-03T13:09:39.337551: step 450, loss 0.102637, accuracy 0.957031, precision 0.974025974025974, recall 0.9554140127388535
2019-03-03T13:09:39.704086: step 451, loss 0.0470217, accuracy 0.976471, precision 1.0, recall 0.9661016949152542
2019-03-03T13:09:40.186682: step 452, loss 0.0600598, accuracy 0.976562, precision 0.9939759036144579, recall 0.9705882352941176
2019-03-03T13:09:40.682872: step 453, loss 0.0508016, accuracy 0.992188, precision 1.0, recall 0.9880239520958084
2019-03-03T13:09:41.140680: step 454, loss 0.0527527, accuracy 0.980469, precision 0.9818181818181818, recall 0.9878048780487805
2019-03-03T13:09:41.587486: step 455, loss 0.0519076, accuracy 0.988281, precision 1.0, recall 0.9814814814814815
2019-03-03T13:09:42.053931: step 456, loss 0.0435952, accuracy 0.988281, precision 0.994475138121547, recall 0.989010989010989
2019-03-03T13:09:42.544039: step 457, loss 0.0340545, accuracy 0.992188, precision 0.9891891891891892, recall 1.0
2019-03-03T13:09:42.985394: step 458, loss 0.0626854, accuracy 0.976562, precision 0.9827586206896551, recall 0.9827586206896551
2019-03-03T13:09:43.497025: step 459, loss 0.0367723, accuracy 0.996094, precision 1.0, recall 0.9940119760479041
2019-03-03T13:09:43.946811: step 460, loss 0.0388272, accuracy 0.992188, precision 0.9940119760479041, recall 0.9940119760479041
2019-03-03T13:09:44.454488: step 461, loss 0.0545414, accuracy 0.976562, precision 0.9726775956284153, recall 0.994413407821229
2019-03-03T13:09:44.914247: step 462, loss 0.0659662, accuracy 0.980469, precision 0.9766081871345029, recall 0.9940476190476191
2019-03-03T13:09:45.408522: step 463, loss 0.0540018, accuracy 0.980469, precision 0.9815950920245399, recall 0.9876543209876543
2019-03-03T13:09:45.865321: step 464, loss 0.0504453, accuracy 0.992188, precision 0.9884393063583815, recall 1.0
2019-03-03T13:09:46.378959: step 465, loss 0.0698512, accuracy 0.964844, precision 0.9764705882352941, recall 0.9707602339181286
2019-03-03T13:09:46.845529: step 466, loss 0.073212, accuracy 0.96875, precision 0.9757575757575757, recall 0.9757575757575757
2019-03-03T13:09:47.313293: step 467, loss 0.0645068, accuracy 0.984375, precision 0.9938271604938271, recall 0.9817073170731707
2019-03-03T13:09:47.797026: step 468, loss 0.0609591, accuracy 0.984375, precision 0.9883040935672515, recall 0.9883040935672515
2019-03-03T13:09:48.377474: step 469, loss 0.0554247, accuracy 0.992188, precision 1.0, recall 0.9877300613496932
2019-03-03T13:09:48.854231: step 470, loss 0.045395, accuracy 0.984375, precision 1.0, recall 0.9763313609467456
2019-03-03T13:09:49.331922: step 471, loss 0.0394353, accuracy 0.992188, precision 0.9938271604938271, recall 0.9938271604938271
2019-03-03T13:09:49.809644: step 472, loss 0.0712771, accuracy 0.980469, precision 0.9880952380952381, recall 0.9822485207100592
2019-03-03T13:09:50.343737: step 473, loss 0.0365363, accuracy 0.996094, precision 0.9941176470588236, recall 1.0
2019-03-03T13:09:50.805544: step 474, loss 0.0471858, accuracy 0.976562, precision 0.9767441860465116, recall 0.9882352941176471
2019-03-03T13:09:51.261146: step 475, loss 0.0672488, accuracy 0.980469, precision 0.9941860465116279, recall 0.9771428571428571
2019-03-03T13:09:51.727931: step 476, loss 0.0580052, accuracy 0.988281, precision 0.9884393063583815, recall 0.9941860465116279
2019-03-03T13:09:52.222621: step 477, loss 0.0399677, accuracy 0.996094, precision 0.9936305732484076, recall 1.0
2019-03-03T13:09:52.673912: step 478, loss 0.0570931, accuracy 0.980469, precision 0.9879518072289156, recall 0.9820359281437125
2019-03-03T13:09:53.132212: step 479, loss 0.0576923, accuracy 0.992188, precision 0.9882352941176471, recall 1.0
2019-03-03T13:09:53.588500: step 480, loss 0.0578784, accuracy 0.976562, precision 0.9771428571428571, recall 0.9884393063583815
2019-03-03T13:09:54.079189: step 481, loss 0.0389903, accuracy 0.996094, precision 0.9941520467836257, recall 1.0
2019-03-03T13:09:54.537257: step 482, loss 0.0643231, accuracy 0.972656, precision 0.9754601226993865, recall 0.9814814814814815
2019-03-03T13:09:55.017472: step 483, loss 0.079726, accuracy 0.972656, precision 0.9702380952380952, recall 0.9878787878787879
2019-03-03T13:09:55.494713: step 484, loss 0.0749996, accuracy 0.96875, precision 0.9754601226993865, recall 0.9754601226993865
2019-03-03T13:09:55.956641: step 485, loss 0.0561155, accuracy 0.984375, precision 0.9939393939393939, recall 0.9820359281437125
2019-03-03T13:09:56.446347: step 486, loss 0.0571803, accuracy 0.984375, precision 1.0, recall 0.978021978021978
2019-03-03T13:09:56.942022: step 487, loss 0.0585112, accuracy 0.980469, precision 0.9880239520958084, recall 0.9821428571428571
2019-03-03T13:09:57.433247: step 488, loss 0.0759651, accuracy 0.960938, precision 0.9760479041916168, recall 0.9644970414201184
2019-03-03T13:09:57.899509: step 489, loss 0.0649485, accuracy 0.980469, precision 0.9886363636363636, recall 0.9830508474576272
2019-03-03T13:09:58.387204: step 490, loss 0.0678002, accuracy 0.980469, precision 0.9779005524861878, recall 0.9943820224719101
2019-03-03T13:09:58.842503: step 491, loss 0.0701486, accuracy 0.96875, precision 0.9661016949152542, recall 0.9884393063583815
2019-03-03T13:09:59.179147: step 492, loss 0.0827828, accuracy 0.964706, precision 0.954954954954955, recall 0.9906542056074766
2019-03-03T13:09:59.682789: step 493, loss 0.0345826, accuracy 0.992188, precision 0.994413407821229, recall 0.994413407821229
2019-03-03T13:10:00.201403: step 494, loss 0.0452599, accuracy 0.976562, precision 0.9726775956284153, recall 0.994413407821229
2019-03-03T13:10:00.691602: step 495, loss 0.0437528, accuracy 0.992188, precision 1.0, recall 0.9877300613496932
2019-03-03T13:10:01.141196: step 496, loss 0.0857705, accuracy 0.96875, precision 0.9772727272727273, recall 0.9772727272727273
2019-03-03T13:10:01.599225: step 497, loss 0.0393393, accuracy 0.988281, precision 0.9823529411764705, recall 1.0
2019-03-03T13:10:02.071951: step 498, loss 0.0467465, accuracy 0.988281, precision 1.0, recall 0.9829545454545454
2019-03-03T13:10:02.589079: step 499, loss 0.0652326, accuracy 0.976562, precision 1.0, recall 0.9664804469273743
2019-03-03T13:10:03.088994: step 500, loss 0.0728082, accuracy 0.976562, precision 0.9757575757575757, recall 0.9877300613496932

Evaluation:
[[686  64]
 [109 297]]
2019-03-03T13:10:03.502903: step 500, loss 0.460129, accuracy 0.850346, precision 0.9146666666666666, recall 0.8628930817610063

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551639943\checkpoints\model-500


Process finished with exit code 0
