"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
Pretrained Embedding: Doc2Vec
Italian: True
Loading data...
4681
Max Document length: 81
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 7231
Train/Dev split: 4213/468
2019-03-03 14:16:09.311389: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:475: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551644202

2019-03-03T14:16:44.089784: step 1, loss 1.93435, accuracy 0.558594, precision 0.45263157894736844, recall 0.41346153846153844
2019-03-03T14:16:44.567022: step 2, loss 1.61934, accuracy 0.660156, precision 0.35714285714285715, recall 0.47619047619047616
2019-03-03T14:16:45.028649: step 3, loss 1.75623, accuracy 0.632812, precision 0.5212765957446809, recall 0.5
2019-03-03T14:16:45.488783: step 4, loss 1.82615, accuracy 0.636719, precision 0.5102040816326531, recall 0.5263157894736842
2019-03-03T14:16:45.953676: step 5, loss 1.51837, accuracy 0.644531, precision 0.5925925925925926, recall 0.4528301886792453
2019-03-03T14:16:46.420761: step 6, loss 1.37998, accuracy 0.714844, precision 0.6136363636363636, recall 0.5806451612903226
2019-03-03T14:16:46.894494: step 7, loss 1.24544, accuracy 0.757812, precision 0.4880952380952381, recall 0.6833333333333333
2019-03-03T14:16:47.379199: step 8, loss 1.74764, accuracy 0.699219, precision 0.4725274725274725, recall 0.5972222222222222
2019-03-03T14:16:47.846971: step 9, loss 1.17368, accuracy 0.742188, precision 0.5555555555555556, recall 0.6578947368421053
2019-03-03T14:16:48.318224: step 10, loss 1.19949, accuracy 0.714844, precision 0.5959595959595959, recall 0.6413043478260869
2019-03-03T14:16:48.796944: step 11, loss 1.2032, accuracy 0.742188, precision 0.7777777777777778, recall 0.6363636363636364
2019-03-03T14:16:49.271674: step 12, loss 1.35751, accuracy 0.707031, precision 0.7450980392156863, recall 0.608
2019-03-03T14:16:49.726464: step 13, loss 0.993845, accuracy 0.804688, precision 0.7523809523809524, recall 0.7669902912621359
2019-03-03T14:16:51.285532: step 14, loss 0.934096, accuracy 0.761719, precision 0.7586206896551724, recall 0.6226415094339622
2019-03-03T14:16:53.291190: step 15, loss 1.03564, accuracy 0.777344, precision 0.6601941747572816, recall 0.7555555555555555
2019-03-03T14:16:55.708033: step 16, loss 1.13916, accuracy 0.734375, precision 0.5909090909090909, recall 0.6190476190476191
2019-03-03T14:16:56.711612: step 17, loss 0.727949, accuracy 0.803419, precision 0.5625, recall 0.6666666666666666
2019-03-03T14:16:58.872256: step 18, loss 0.72769, accuracy 0.828125, precision 0.7159090909090909, recall 0.7682926829268293
2019-03-03T14:17:01.052813: step 19, loss 0.848366, accuracy 0.785156, precision 0.5617977528089888, recall 0.7575757575757576
2019-03-03T14:17:03.182016: step 20, loss 1.02109, accuracy 0.753906, precision 0.6176470588235294, recall 0.7241379310344828
2019-03-03T14:17:05.214751: step 21, loss 0.666687, accuracy 0.804688, precision 0.7474747474747475, recall 0.7474747474747475
2019-03-03T14:17:07.132299: step 22, loss 0.908333, accuracy 0.785156, precision 0.794392523364486, recall 0.7203389830508474
2019-03-03T14:17:09.010568: step 23, loss 1.1166, accuracy 0.746094, precision 0.826530612244898, recall 0.627906976744186
2019-03-03T14:17:11.136555: step 24, loss 0.783696, accuracy 0.785156, precision 0.8105263157894737, recall 0.6754385964912281
2019-03-03T14:17:13.377224: step 25, loss 0.713297, accuracy 0.8125, precision 0.7553191489361702, recall 0.7395833333333334
2019-03-03T14:17:15.489001: step 26, loss 0.667017, accuracy 0.828125, precision 0.6931818181818182, recall 0.782051282051282
2019-03-03T14:17:17.589652: step 27, loss 0.559608, accuracy 0.847656, precision 0.7625, recall 0.7530864197530864
2019-03-03T14:17:19.703667: step 28, loss 0.892692, accuracy 0.796875, precision 0.6292134831460674, recall 0.7466666666666667
2019-03-03T14:17:21.024768: step 29, loss 0.676175, accuracy 0.804688, precision 0.5851063829787234, recall 0.8333333333333334
2019-03-03T14:17:21.703952: step 30, loss 0.613429, accuracy 0.851562, precision 0.6704545454545454, recall 0.8676470588235294
2019-03-03T14:17:22.448981: step 31, loss 0.609238, accuracy 0.832031, precision 0.7415730337078652, recall 0.7674418604651163
2019-03-03T14:17:23.193988: step 32, loss 0.72049, accuracy 0.796875, precision 0.7469879518072289, recall 0.6666666666666666
2019-03-03T14:17:23.914216: step 33, loss 0.856703, accuracy 0.785156, precision 0.711340206185567, recall 0.71875
2019-03-03T14:17:24.246328: step 34, loss 0.981908, accuracy 0.769231, precision 0.725, recall 0.6444444444444445
2019-03-03T14:17:24.901893: step 35, loss 0.641522, accuracy 0.792969, precision 0.8240740740740741, recall 0.7235772357723578
2019-03-03T14:17:25.542200: step 36, loss 0.504192, accuracy 0.816406, precision 0.8314606741573034, recall 0.6981132075471698
2019-03-03T14:17:26.229929: step 37, loss 0.667435, accuracy 0.792969, precision 0.7027027027027027, recall 0.7959183673469388
2019-03-03T14:17:26.871215: step 38, loss 0.470399, accuracy 0.851562, precision 0.7722772277227723, recall 0.8387096774193549
2019-03-03T14:17:27.513525: step 39, loss 0.493705, accuracy 0.839844, precision 0.71875, recall 0.8313253012048193
2019-03-03T14:17:28.187235: step 40, loss 0.62033, accuracy 0.832031, precision 0.7340425531914894, recall 0.7931034482758621
2019-03-03T14:17:28.819546: step 41, loss 0.555851, accuracy 0.855469, precision 0.76, recall 0.8539325842696629
2019-03-03T14:17:29.403988: step 42, loss 0.48666, accuracy 0.835938, precision 0.7160493827160493, recall 0.7532467532467533
2019-03-03T14:17:29.994408: step 43, loss 0.603983, accuracy 0.789062, precision 0.686046511627907, recall 0.686046511627907
2019-03-03T14:17:30.575883: step 44, loss 0.611143, accuracy 0.824219, precision 0.8024691358024691, recall 0.6914893617021277
2019-03-03T14:17:31.178801: step 45, loss 0.322656, accuracy 0.882812, precision 0.8607594936708861, recall 0.7816091954022989
2019-03-03T14:17:31.742293: step 46, loss 0.54869, accuracy 0.816406, precision 0.725, recall 0.6987951807228916
2019-03-03T14:17:32.331718: step 47, loss 0.650312, accuracy 0.804688, precision 0.6842105263157895, recall 0.7647058823529411
2019-03-03T14:17:32.920144: step 48, loss 0.447006, accuracy 0.84375, precision 0.7843137254901961, recall 0.8163265306122449
2019-03-03T14:17:33.495606: step 49, loss 0.467944, accuracy 0.820312, precision 0.7216494845360825, recall 0.7865168539325843
2019-03-03T14:17:34.086027: step 50, loss 0.451817, accuracy 0.84375, precision 0.6987951807228916, recall 0.7945205479452054
2019-03-03T14:17:34.383232: step 51, loss 0.487135, accuracy 0.820513, precision 0.7027027027027027, recall 0.7222222222222222
2019-03-03T14:17:34.939744: step 52, loss 0.24868, accuracy 0.914062, precision 0.8837209302325582, recall 0.8636363636363636
2019-03-03T14:17:35.505232: step 53, loss 0.495058, accuracy 0.851562, precision 0.8181818181818182, recall 0.7659574468085106
2019-03-03T14:17:36.077220: step 54, loss 0.472565, accuracy 0.832031, precision 0.7523809523809524, recall 0.8229166666666666
2019-03-03T14:17:36.640713: step 55, loss 0.487365, accuracy 0.847656, precision 0.7864077669902912, recall 0.826530612244898
2019-03-03T14:17:37.181269: step 56, loss 0.443385, accuracy 0.886719, precision 0.8372093023255814, recall 0.8275862068965517
2019-03-03T14:17:37.738571: step 57, loss 0.41024, accuracy 0.828125, precision 0.81, recall 0.7641509433962265
2019-03-03T14:17:38.322237: step 58, loss 0.390026, accuracy 0.898438, precision 0.8266666666666667, recall 0.8266666666666667
2019-03-03T14:17:38.880772: step 59, loss 0.42931, accuracy 0.859375, precision 0.7931034482758621, recall 0.7931034482758621
2019-03-03T14:17:39.432806: step 60, loss 0.351337, accuracy 0.875, precision 0.8152173913043478, recall 0.8333333333333334
2019-03-03T14:17:39.986326: step 61, loss 0.361298, accuracy 0.878906, precision 0.8, recall 0.9072164948453608
2019-03-03T14:17:40.548866: step 62, loss 0.589275, accuracy 0.824219, precision 0.7564102564102564, recall 0.6941176470588235
2019-03-03T14:17:41.085432: step 63, loss 0.457661, accuracy 0.855469, precision 0.8369565217391305, recall 0.7777777777777778
2019-03-03T14:17:41.605082: step 64, loss 0.546439, accuracy 0.863281, precision 0.782608695652174, recall 0.8275862068965517
2019-03-03T14:17:42.137995: step 65, loss 0.301047, accuracy 0.898438, precision 0.8409090909090909, recall 0.8604651162790697
2019-03-03T14:17:42.685071: step 66, loss 0.325181, accuracy 0.902344, precision 0.8369565217391305, recall 0.8850574712643678
2019-03-03T14:17:43.274500: step 67, loss 0.403776, accuracy 0.859375, precision 0.73, recall 0.8902439024390244
2019-03-03T14:17:43.543780: step 68, loss 0.447757, accuracy 0.871795, precision 0.717391304347826, recall 0.9428571428571428
2019-03-03T14:17:44.115251: step 69, loss 0.333769, accuracy 0.875, precision 0.7763157894736842, recall 0.7972972972972973
2019-03-03T14:17:44.695759: step 70, loss 0.31487, accuracy 0.882812, precision 0.9139784946236559, recall 0.794392523364486
2019-03-03T14:17:45.269225: step 71, loss 0.486307, accuracy 0.808594, precision 0.825, recall 0.6534653465346535
2019-03-03T14:17:45.815759: step 72, loss 0.421737, accuracy 0.851562, precision 0.8541666666666666, recall 0.7735849056603774
2019-03-03T14:17:46.406192: step 73, loss 0.392418, accuracy 0.863281, precision 0.8181818181818182, recall 0.7912087912087912
2019-03-03T14:17:46.968722: step 74, loss 0.32842, accuracy 0.882812, precision 0.7916666666666666, recall 0.8837209302325582
2019-03-03T14:17:47.527271: step 75, loss 0.376136, accuracy 0.855469, precision 0.7634408602150538, recall 0.8255813953488372
2019-03-03T14:17:48.106114: step 76, loss 0.320627, accuracy 0.882812, precision 0.8018867924528302, recall 0.9042553191489362
2019-03-03T14:17:48.653133: step 77, loss 0.366122, accuracy 0.875, precision 0.7708333333333334, recall 0.8809523809523809
2019-03-03T14:17:49.195959: step 78, loss 0.206162, accuracy 0.921875, precision 0.8640776699029126, recall 0.9368421052631579
2019-03-03T14:17:49.741524: step 79, loss 0.286178, accuracy 0.894531, precision 0.8131868131868132, recall 0.8809523809523809
2019-03-03T14:17:50.306524: step 80, loss 0.378207, accuracy 0.875, precision 0.9090909090909091, recall 0.7964601769911505
2019-03-03T14:17:50.838103: step 81, loss 0.309808, accuracy 0.886719, precision 0.8804347826086957, recall 0.8181818181818182
2019-03-03T14:17:51.398164: step 82, loss 0.432921, accuracy 0.855469, precision 0.9080459770114943, recall 0.7314814814814815
2019-03-03T14:17:51.942224: step 83, loss 0.350172, accuracy 0.875, precision 0.75, recall 0.868421052631579
2019-03-03T14:17:52.492752: step 84, loss 0.398698, accuracy 0.851562, precision 0.7701149425287356, recall 0.788235294117647
2019-03-03T14:17:52.759059: step 85, loss 0.339896, accuracy 0.82906, precision 0.8367346938775511, recall 0.7735849056603774
2019-03-03T14:17:53.300612: step 86, loss 0.272918, accuracy 0.890625, precision 0.8160919540229885, recall 0.8554216867469879
2019-03-03T14:17:53.819802: step 87, loss 0.369414, accuracy 0.859375, precision 0.7411764705882353, recall 0.8181818181818182
2019-03-03T14:17:54.357699: step 88, loss 0.376595, accuracy 0.863281, precision 0.71, recall 0.922077922077922
2019-03-03T14:17:54.869840: step 89, loss 0.368924, accuracy 0.871094, precision 0.8155339805825242, recall 0.8571428571428571
2019-03-03T14:17:55.399436: step 90, loss 0.239667, accuracy 0.882812, precision 0.8333333333333334, recall 0.8333333333333334
2019-03-03T14:17:55.951950: step 91, loss 0.30823, accuracy 0.886719, precision 0.8709677419354839, recall 0.826530612244898
2019-03-03T14:17:56.478437: step 92, loss 0.216551, accuracy 0.929688, precision 0.9354838709677419, recall 0.8787878787878788
2019-03-03T14:17:57.011012: step 93, loss 0.316247, accuracy 0.871094, precision 0.8666666666666667, recall 0.7878787878787878
2019-03-03T14:17:57.507685: step 94, loss 0.425032, accuracy 0.871094, precision 0.8737864077669902, recall 0.8181818181818182
2019-03-03T14:17:58.041261: step 95, loss 0.301167, accuracy 0.875, precision 0.8461538461538461, recall 0.7674418604651163
2019-03-03T14:17:58.558105: step 96, loss 0.222882, accuracy 0.90625, precision 0.8181818181818182, recall 0.863013698630137
2019-03-03T14:17:59.059275: step 97, loss 0.295693, accuracy 0.863281, precision 0.7549019607843137, recall 0.8850574712643678
2019-03-03T14:17:59.551466: step 98, loss 0.370958, accuracy 0.875, precision 0.7857142857142857, recall 0.875
2019-03-03T14:18:00.082042: step 99, loss 0.218708, accuracy 0.898438, precision 0.8222222222222222, recall 0.8809523809523809
2019-03-03T14:18:00.575723: step 100, loss 0.290238, accuracy 0.882812, precision 0.7920792079207921, recall 0.898876404494382
2019-03-03T14:18:01.073899: step 101, loss 0.246348, accuracy 0.890625, precision 0.8478260869565217, recall 0.8478260869565217
2019-03-03T14:18:01.321238: step 102, loss 0.208726, accuracy 0.905983, precision 0.9210526315789473, recall 0.813953488372093
2019-03-03T14:18:01.821898: step 103, loss 0.217617, accuracy 0.914062, precision 0.9, recall 0.8823529411764706
2019-03-03T14:18:02.325552: step 104, loss 0.26466, accuracy 0.859375, precision 0.8105263157894737, recall 0.8105263157894737
2019-03-03T14:18:02.820229: step 105, loss 0.217764, accuracy 0.910156, precision 0.9090909090909091, recall 0.8421052631578947
2019-03-03T14:18:03.323882: step 106, loss 0.258469, accuracy 0.902344, precision 0.8947368421052632, recall 0.85
2019-03-03T14:18:03.837747: step 107, loss 0.221112, accuracy 0.882812, precision 0.8152173913043478, recall 0.8522727272727273
2019-03-03T14:18:04.332527: step 108, loss 0.266357, accuracy 0.894531, precision 0.8148148148148148, recall 0.9263157894736842
2019-03-03T14:18:04.807261: step 109, loss 0.240091, accuracy 0.914062, precision 0.8555555555555555, recall 0.8953488372093024
2019-03-03T14:18:05.292963: step 110, loss 0.305274, accuracy 0.882812, precision 0.8350515463917526, recall 0.8526315789473684
2019-03-03T14:18:05.809028: step 111, loss 0.216424, accuracy 0.898438, precision 0.8762886597938144, recall 0.8585858585858586
2019-03-03T14:18:06.286777: step 112, loss 0.267157, accuracy 0.890625, precision 0.8735632183908046, recall 0.8172043010752689
2019-03-03T14:18:06.770771: step 113, loss 0.20099, accuracy 0.90625, precision 0.8352941176470589, recall 0.8765432098765432
2019-03-03T14:18:07.264959: step 114, loss 0.237875, accuracy 0.925781, precision 0.92, recall 0.8932038834951457
2019-03-03T14:18:07.780604: step 115, loss 0.219733, accuracy 0.914062, precision 0.8488372093023255, recall 0.8902439024390244
2019-03-03T14:18:08.264817: step 116, loss 0.249315, accuracy 0.921875, precision 0.8777777777777778, recall 0.8977272727272727
2019-03-03T14:18:08.749526: step 117, loss 0.257239, accuracy 0.898438, precision 0.7727272727272727, recall 0.918918918918919
2019-03-03T14:18:09.241211: step 118, loss 0.283577, accuracy 0.894531, precision 0.8072289156626506, recall 0.8589743589743589
2019-03-03T14:18:09.485558: step 119, loss 0.193155, accuracy 0.905983, precision 0.8205128205128205, recall 0.8888888888888888
2019-03-03T14:18:09.968741: step 120, loss 0.201962, accuracy 0.9375, precision 0.9052631578947369, recall 0.9247311827956989
2019-03-03T14:18:10.446969: step 121, loss 0.254427, accuracy 0.886719, precision 0.8526315789473684, recall 0.84375
2019-03-03T14:18:10.954122: step 122, loss 0.335169, accuracy 0.875, precision 0.8888888888888888, recall 0.8073394495412844
2019-03-03T14:18:11.455782: step 123, loss 0.187276, accuracy 0.925781, precision 0.8947368421052632, recall 0.9042553191489362
2019-03-03T14:18:11.940891: step 124, loss 0.257476, accuracy 0.90625, precision 0.8620689655172413, recall 0.8620689655172413
2019-03-03T14:18:12.420770: step 125, loss 0.183402, accuracy 0.917969, precision 0.8775510204081632, recall 0.9052631578947369
2019-03-03T14:18:12.928411: step 126, loss 0.194146, accuracy 0.914062, precision 0.8105263157894737, recall 0.9506172839506173
2019-03-03T14:18:13.429091: step 127, loss 0.180857, accuracy 0.917969, precision 0.85, recall 0.9340659340659341
2019-03-03T14:18:13.938833: step 128, loss 0.189366, accuracy 0.914062, precision 0.8372093023255814, recall 0.9
2019-03-03T14:18:14.428527: step 129, loss 0.232304, accuracy 0.886719, precision 0.8222222222222222, recall 0.8505747126436781
2019-03-03T14:18:14.917220: step 130, loss 0.239037, accuracy 0.914062, precision 0.9368421052631579, recall 0.8476190476190476
2019-03-03T14:18:15.406912: step 131, loss 0.214336, accuracy 0.917969, precision 0.9230769230769231, recall 0.8275862068965517
2019-03-03T14:18:15.882640: step 132, loss 0.272952, accuracy 0.894531, precision 0.8555555555555555, recall 0.8461538461538461
2019-03-03T14:18:16.382325: step 133, loss 0.162774, accuracy 0.945312, precision 0.9263157894736842, recall 0.9263157894736842
2019-03-03T14:18:16.867524: step 134, loss 0.180463, accuracy 0.929688, precision 0.8604651162790697, recall 0.925
2019-03-03T14:18:17.374183: step 135, loss 0.220717, accuracy 0.902344, precision 0.8131868131868132, recall 0.9024390243902439
2019-03-03T14:18:17.607564: step 136, loss 0.170615, accuracy 0.923077, precision 0.8666666666666667, recall 0.9285714285714286
2019-03-03T14:18:18.097829: step 137, loss 0.19383, accuracy 0.921875, precision 0.8588235294117647, recall 0.9012345679012346
2019-03-03T14:18:18.578544: step 138, loss 0.151249, accuracy 0.949219, precision 0.9285714285714286, recall 0.9381443298969072
2019-03-03T14:18:19.095678: step 139, loss 0.150615, accuracy 0.9375, precision 0.9247311827956989, recall 0.9052631578947369
2019-03-03T14:18:19.594739: step 140, loss 0.169409, accuracy 0.941406, precision 0.9148936170212766, recall 0.9247311827956989
2019-03-03T14:18:20.288884: step 141, loss 0.172473, accuracy 0.929688, precision 0.898876404494382, recall 0.898876404494382
2019-03-03T14:18:21.030901: step 142, loss 0.186268, accuracy 0.90625, precision 0.8666666666666667, recall 0.8666666666666667
2019-03-03T14:18:21.803358: step 143, loss 0.209268, accuracy 0.902344, precision 0.8536585365853658, recall 0.8433734939759037
2019-03-03T14:18:22.557858: step 144, loss 0.131283, accuracy 0.953125, precision 0.9156626506024096, recall 0.9382716049382716
2019-03-03T14:18:23.271985: step 145, loss 0.197275, accuracy 0.90625, precision 0.8247422680412371, recall 0.9195402298850575
2019-03-03T14:18:23.976611: step 146, loss 0.201833, accuracy 0.925781, precision 0.8631578947368421, recall 0.9318181818181818
2019-03-03T14:18:24.677735: step 147, loss 0.214506, accuracy 0.921875, precision 0.8620689655172413, recall 0.9036144578313253
2019-03-03T14:18:25.383848: step 148, loss 0.193581, accuracy 0.941406, precision 0.9318181818181818, recall 0.9010989010989011
2019-03-03T14:18:26.050074: step 149, loss 0.160963, accuracy 0.921875, precision 0.8712871287128713, recall 0.9263157894736842
2019-03-03T14:18:26.717732: step 150, loss 0.249439, accuracy 0.894531, precision 0.9047619047619048, recall 0.8
2019-03-03T14:18:27.405882: step 151, loss 0.228959, accuracy 0.90625, precision 0.9183673469387755, recall 0.8490566037735849
2019-03-03T14:18:28.106028: step 152, loss 0.183619, accuracy 0.949219, precision 0.9636363636363636, recall 0.9217391304347826
2019-03-03T14:18:28.428189: step 153, loss 0.166254, accuracy 0.931624, precision 0.9130434782608695, recall 0.9130434782608695
2019-03-03T14:18:29.056512: step 154, loss 0.21297, accuracy 0.917969, precision 0.8476190476190476, recall 0.9468085106382979
2019-03-03T14:18:29.694802: step 155, loss 0.196209, accuracy 0.933594, precision 0.8452380952380952, recall 0.9466666666666667
2019-03-03T14:18:30.297576: step 156, loss 0.16363, accuracy 0.929688, precision 0.9, recall 0.9183673469387755
2019-03-03T14:18:30.898984: step 157, loss 0.255113, accuracy 0.925781, precision 0.8712871287128713, recall 0.9361702127659575
2019-03-03T14:18:31.521322: step 158, loss 0.16515, accuracy 0.921875, precision 0.8426966292134831, recall 0.9259259259259259
2019-03-03T14:18:32.109748: step 159, loss 0.214827, accuracy 0.921875, precision 0.8947368421052632, recall 0.8947368421052632
2019-03-03T14:18:32.705771: step 160, loss 0.20339, accuracy 0.929688, precision 0.9333333333333333, recall 0.875
2019-03-03T14:18:33.315147: step 161, loss 0.172684, accuracy 0.9375, precision 0.9512195121951219, recall 0.8666666666666667
2019-03-03T14:18:33.891612: step 162, loss 0.16282, accuracy 0.933594, precision 0.9431818181818182, recall 0.8736842105263158
2019-03-03T14:18:34.466505: step 163, loss 0.144001, accuracy 0.941406, precision 0.9213483146067416, recall 0.9111111111111111
2019-03-03T14:18:35.024530: step 164, loss 0.193503, accuracy 0.921875, precision 0.8613861386138614, recall 0.9354838709677419
2019-03-03T14:18:35.598994: step 165, loss 0.249265, accuracy 0.910156, precision 0.8241758241758241, recall 0.9146341463414634
2019-03-03T14:18:36.146626: step 166, loss 0.161512, accuracy 0.941406, precision 0.8873239436619719, recall 0.9
2019-03-03T14:18:36.711634: step 167, loss 0.1476, accuracy 0.933594, precision 0.8777777777777778, recall 0.9294117647058824
2019-03-03T14:18:37.263152: step 168, loss 0.230434, accuracy 0.898438, precision 0.8207547169811321, recall 0.925531914893617
2019-03-03T14:18:37.837615: step 169, loss 0.248357, accuracy 0.902344, precision 0.8383838383838383, recall 0.9021739130434783
2019-03-03T14:18:38.105898: step 170, loss 0.0886273, accuracy 0.974359, precision 0.9487179487179487, recall 0.9736842105263158
2019-03-03T14:18:38.661429: step 171, loss 0.173516, accuracy 0.925781, precision 0.9, recall 0.8901098901098901
2019-03-03T14:18:39.203980: step 172, loss 0.143481, accuracy 0.957031, precision 0.9555555555555556, recall 0.9247311827956989
2019-03-03T14:18:39.748533: step 173, loss 0.171176, accuracy 0.9375, precision 0.9655172413793104, recall 0.865979381443299
2019-03-03T14:18:40.285598: step 174, loss 0.173562, accuracy 0.917969, precision 0.9578947368421052, recall 0.8425925925925926
2019-03-03T14:18:40.840113: step 175, loss 0.176765, accuracy 0.925781, precision 0.8953488372093024, recall 0.8850574712643678
2019-03-03T14:18:41.381666: step 176, loss 0.140564, accuracy 0.949219, precision 0.9052631578947369, recall 0.9555555555555556
2019-03-03T14:18:41.928130: step 177, loss 0.171331, accuracy 0.941406, precision 0.8876404494382022, recall 0.9404761904761905
2019-03-03T14:18:42.460788: step 178, loss 0.156541, accuracy 0.933594, precision 0.9047619047619048, recall 0.9313725490196079
2019-03-03T14:18:42.983895: step 179, loss 0.229435, accuracy 0.894531, precision 0.8152173913043478, recall 0.8823529411764706
2019-03-03T14:18:43.525448: step 180, loss 0.124835, accuracy 0.949219, precision 0.9195402298850575, recall 0.9302325581395349
2019-03-03T14:18:44.046054: step 181, loss 0.191822, accuracy 0.902344, precision 0.8505747126436781, recall 0.8604651162790697
2019-03-03T14:18:44.598577: step 182, loss 0.221044, accuracy 0.910156, precision 0.8823529411764706, recall 0.8910891089108911
2019-03-03T14:18:45.133148: step 183, loss 0.176113, accuracy 0.929688, precision 0.9058823529411765, recall 0.8850574712643678
2019-03-03T14:18:45.642809: step 184, loss 0.190897, accuracy 0.921875, precision 0.8543689320388349, recall 0.946236559139785
2019-03-03T14:18:46.158431: step 185, loss 0.173842, accuracy 0.945312, precision 0.9247311827956989, recall 0.9247311827956989
2019-03-03T14:18:46.680037: step 186, loss 0.153, accuracy 0.941406, precision 0.967032967032967, recall 0.88
2019-03-03T14:18:46.932054: step 187, loss 0.229098, accuracy 0.923077, precision 0.9302325581395349, recall 0.8695652173913043
2019-03-03T14:18:47.504979: step 188, loss 0.177149, accuracy 0.925781, precision 0.8969072164948454, recall 0.90625
2019-03-03T14:18:48.044537: step 189, loss 0.14033, accuracy 0.933594, precision 0.8773584905660378, recall 0.9587628865979382
2019-03-03T14:18:48.571636: step 190, loss 0.120299, accuracy 0.953125, precision 0.9230769230769231, recall 0.9438202247191011
2019-03-03T14:18:49.085263: step 191, loss 0.163206, accuracy 0.914062, precision 0.8333333333333334, recall 0.9146341463414634
2019-03-03T14:18:49.561994: step 192, loss 0.15446, accuracy 0.941406, precision 0.9042553191489362, recall 0.9340659340659341
2019-03-03T14:18:50.063784: step 193, loss 0.15038, accuracy 0.945312, precision 0.9, recall 0.9230769230769231
2019-03-03T14:18:50.587383: step 194, loss 0.0985921, accuracy 0.964844, precision 0.9468085106382979, recall 0.956989247311828
2019-03-03T14:18:51.105017: step 195, loss 0.184506, accuracy 0.917969, precision 0.875, recall 0.9032258064516129
2019-03-03T14:18:51.598697: step 196, loss 0.158861, accuracy 0.949219, precision 0.9479166666666666, recall 0.9191919191919192
2019-03-03T14:18:52.095751: step 197, loss 0.128186, accuracy 0.9375, precision 0.9069767441860465, recall 0.9069767441860465
2019-03-03T14:18:52.591413: step 198, loss 0.188103, accuracy 0.917969, precision 0.9090909090909091, recall 0.8602150537634409
2019-03-03T14:18:53.089083: step 199, loss 0.128615, accuracy 0.949219, precision 0.9666666666666667, recall 0.8969072164948454
2019-03-03T14:18:53.576778: step 200, loss 0.162041, accuracy 0.929688, precision 0.89, recall 0.9270833333333334
2019-03-03T14:18:54.072453: step 201, loss 0.236549, accuracy 0.914062, precision 0.8736842105263158, recall 0.8924731182795699
2019-03-03T14:18:54.553680: step 202, loss 0.165729, accuracy 0.921875, precision 0.8313253012048193, recall 0.92
2019-03-03T14:18:55.042373: step 203, loss 0.175526, accuracy 0.9375, precision 0.8804347826086957, recall 0.9418604651162791
2019-03-03T14:18:55.292910: step 204, loss 0.14546, accuracy 0.923077, precision 0.9047619047619048, recall 0.8837209302325582
2019-03-03T14:18:55.778644: step 205, loss 0.152798, accuracy 0.953125, precision 0.9494949494949495, recall 0.9306930693069307
2019-03-03T14:18:56.263349: step 206, loss 0.125156, accuracy 0.957031, precision 0.9473684210526315, recall 0.9375
2019-03-03T14:18:56.764015: step 207, loss 0.162751, accuracy 0.957031, precision 0.9487179487179487, recall 0.9135802469135802
2019-03-03T14:18:57.261685: step 208, loss 0.142217, accuracy 0.933594, precision 0.9354838709677419, recall 0.8877551020408163
2019-03-03T14:18:57.748405: step 209, loss 0.106979, accuracy 0.957031, precision 0.9310344827586207, recall 0.9418604651162791
2019-03-03T14:18:58.242129: step 210, loss 0.185021, accuracy 0.925781, precision 0.898989898989899, recall 0.9081632653061225
2019-03-03T14:18:58.745783: step 211, loss 0.100674, accuracy 0.972656, precision 0.9534883720930233, recall 0.9647058823529412
2019-03-03T14:18:59.240792: step 212, loss 0.148874, accuracy 0.9375, precision 0.8631578947368421, recall 0.9647058823529412
2019-03-03T14:18:59.713793: step 213, loss 0.194241, accuracy 0.921875, precision 0.8586956521739131, recall 0.9186046511627907
2019-03-03T14:19:00.215461: step 214, loss 0.116279, accuracy 0.949219, precision 0.9456521739130435, recall 0.9157894736842105
2019-03-03T14:19:00.722363: step 215, loss 0.105509, accuracy 0.957031, precision 0.9215686274509803, recall 0.9690721649484536
2019-03-03T14:19:01.219542: step 216, loss 0.152058, accuracy 0.929688, precision 0.9176470588235294, recall 0.8764044943820225
2019-03-03T14:19:01.702877: step 217, loss 0.139414, accuracy 0.941406, precision 0.9393939393939394, recall 0.9117647058823529
2019-03-03T14:19:02.191591: step 218, loss 0.152586, accuracy 0.9375, precision 0.9032258064516129, recall 0.9230769230769231
2019-03-03T14:19:02.697747: step 219, loss 0.0827239, accuracy 0.964844, precision 0.9896907216494846, recall 0.9230769230769231
2019-03-03T14:19:03.190458: step 220, loss 0.129574, accuracy 0.953125, precision 0.8902439024390244, recall 0.9605263157894737
2019-03-03T14:19:03.421839: step 221, loss 0.181318, accuracy 0.923077, precision 0.9130434782608695, recall 0.8936170212765957
2019-03-03T14:19:03.903551: step 222, loss 0.150709, accuracy 0.953125, precision 0.9431818181818182, recall 0.9222222222222223
2019-03-03T14:19:04.395238: step 223, loss 0.0947721, accuracy 0.957031, precision 0.9340659340659341, recall 0.9444444444444444
2019-03-03T14:19:04.900395: step 224, loss 0.10432, accuracy 0.960938, precision 0.9120879120879121, recall 0.9764705882352941
2019-03-03T14:19:05.386101: step 225, loss 0.112495, accuracy 0.960938, precision 0.9390243902439024, recall 0.9390243902439024
2019-03-03T14:19:05.859510: step 226, loss 0.124323, accuracy 0.953125, precision 0.8941176470588236, recall 0.9620253164556962
2019-03-03T14:19:06.344514: step 227, loss 0.15908, accuracy 0.929688, precision 0.8375, recall 0.9305555555555556
2019-03-03T14:19:06.855143: step 228, loss 0.118828, accuracy 0.945312, precision 0.9139784946236559, recall 0.9340659340659341
2019-03-03T14:19:07.354776: step 229, loss 0.136215, accuracy 0.9375, precision 0.9080459770114943, recall 0.9080459770114943
2019-03-03T14:19:08.072855: step 230, loss 0.130163, accuracy 0.964844, precision 0.9809523809523809, recall 0.9363636363636364
2019-03-03T14:19:08.852771: step 231, loss 0.193898, accuracy 0.914062, precision 0.8735632183908046, recall 0.8735632183908046
2019-03-03T14:19:09.591794: step 232, loss 0.128355, accuracy 0.949219, precision 0.9489795918367347, recall 0.9207920792079208
2019-03-03T14:19:10.344781: step 233, loss 0.110135, accuracy 0.964844, precision 0.9714285714285714, recall 0.9444444444444444
2019-03-03T14:19:11.087309: step 234, loss 0.118411, accuracy 0.957031, precision 0.9473684210526315, recall 0.9375
2019-03-03T14:19:11.803400: step 235, loss 0.151953, accuracy 0.9375, precision 0.9166666666666666, recall 0.9166666666666666
2019-03-03T14:19:12.512964: step 236, loss 0.0865235, accuracy 0.960938, precision 0.9183673469387755, recall 0.9782608695652174
2019-03-03T14:19:13.196204: step 237, loss 0.142468, accuracy 0.941406, precision 0.9381443298969072, recall 0.91
2019-03-03T14:19:13.522332: step 238, loss 0.102009, accuracy 0.957265, precision 0.9761904761904762, recall 0.9111111111111111
2019-03-03T14:19:14.179574: step 239, loss 0.114008, accuracy 0.96875, precision 0.9340659340659341, recall 0.9770114942528736
2019-03-03T14:19:14.859744: step 240, loss 0.114611, accuracy 0.957031, precision 0.9473684210526315, recall 0.9375
2019-03-03T14:19:15.499031: step 241, loss 0.0969401, accuracy 0.964844, precision 0.9036144578313253, recall 0.9868421052631579
2019-03-03T14:19:16.145507: step 242, loss 0.126218, accuracy 0.941406, precision 0.8807339449541285, recall 0.9795918367346939
2019-03-03T14:19:16.799757: step 243, loss 0.0853202, accuracy 0.976562, precision 0.9767441860465116, recall 0.9545454545454546
2019-03-03T14:19:17.415113: step 244, loss 0.122723, accuracy 0.949219, precision 0.9230769230769231, recall 0.9113924050632911
2019-03-03T14:19:18.026535: step 245, loss 0.124445, accuracy 0.960938, precision 0.9310344827586207, recall 0.9529411764705882
2019-03-03T14:19:18.673805: step 246, loss 0.101394, accuracy 0.96875, precision 0.96, recall 0.96
2019-03-03T14:19:19.282178: step 247, loss 0.106405, accuracy 0.957031, precision 0.978021978021978, recall 0.9081632653061225
2019-03-03T14:19:19.888557: step 248, loss 0.0998125, accuracy 0.957031, precision 0.9425287356321839, recall 0.9318181818181818
2019-03-03T14:19:20.486508: step 249, loss 0.126032, accuracy 0.960938, precision 0.9340659340659341, recall 0.9550561797752809
2019-03-03T14:19:21.080437: step 250, loss 0.140673, accuracy 0.945312, precision 0.926605504587156, recall 0.9439252336448598

Evaluation:
[[134  39]
 [ 22 273]]
2019-03-03T14:19:22.164056: step 250, loss 0.320166, accuracy 0.869658, precision 0.7745664739884393, recall 0.8589743589743589

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551644202\checkpoints\model-250

2019-03-03T14:19:22.983436: step 251, loss 0.133013, accuracy 0.953125, precision 0.9591836734693877, recall 0.9215686274509803
2019-03-03T14:19:23.535960: step 252, loss 0.103615, accuracy 0.957031, precision 0.941747572815534, recall 0.9509803921568627
2019-03-03T14:19:24.087486: step 253, loss 0.103468, accuracy 0.972656, precision 0.9166666666666666, recall 1.0
2019-03-03T14:19:24.670925: step 254, loss 0.1086, accuracy 0.953125, precision 0.8837209302325582, recall 0.9743589743589743
2019-03-03T14:19:24.941204: step 255, loss 0.0736296, accuracy 0.982906, precision 0.9761904761904762, recall 0.9761904761904762
2019-03-03T14:19:25.499225: step 256, loss 0.144793, accuracy 0.945312, precision 0.9186046511627907, recall 0.9186046511627907
2019-03-03T14:19:26.063218: step 257, loss 0.0781892, accuracy 0.96875, precision 0.9292929292929293, recall 0.989247311827957
2019-03-03T14:19:26.630701: step 258, loss 0.10878, accuracy 0.953125, precision 0.9473684210526315, recall 0.9278350515463918
2019-03-03T14:19:27.171275: step 259, loss 0.118854, accuracy 0.949219, precision 0.956989247311828, recall 0.9081632653061225
2019-03-03T14:19:27.709836: step 260, loss 0.135718, accuracy 0.949219, precision 0.9791666666666666, recall 0.8952380952380953
2019-03-03T14:19:28.244406: step 261, loss 0.0789633, accuracy 0.972656, precision 0.9518072289156626, recall 0.9634146341463414
2019-03-03T14:19:28.787972: step 262, loss 0.109493, accuracy 0.960938, precision 0.9, recall 0.972972972972973
2019-03-03T14:19:29.338501: step 263, loss 0.0830911, accuracy 0.96875, precision 0.9139784946236559, recall 1.0
2019-03-03T14:19:29.878078: step 264, loss 0.154535, accuracy 0.949219, precision 0.8735632183908046, recall 0.9743589743589743
2019-03-03T14:19:30.427117: step 265, loss 0.109661, accuracy 0.960938, precision 0.9166666666666666, recall 0.9777777777777777
2019-03-03T14:19:30.996104: step 266, loss 0.144703, accuracy 0.953125, precision 0.9175257731958762, recall 0.956989247311828
2019-03-03T14:19:31.567575: step 267, loss 0.139441, accuracy 0.945312, precision 0.978494623655914, recall 0.883495145631068
2019-03-03T14:19:32.115111: step 268, loss 0.123786, accuracy 0.949219, precision 0.925531914893617, recall 0.9354838709677419
2019-03-03T14:19:32.665152: step 269, loss 0.0994881, accuracy 0.96875, precision 0.99, recall 0.9339622641509434
2019-03-03T14:19:33.188260: step 270, loss 0.127122, accuracy 0.964844, precision 0.967741935483871, recall 0.9375
2019-03-03T14:19:33.705592: step 271, loss 0.104962, accuracy 0.960938, precision 0.9247311827956989, recall 0.9662921348314607
2019-03-03T14:19:33.952949: step 272, loss 0.0849193, accuracy 0.982906, precision 0.9761904761904762, recall 0.9761904761904762
2019-03-03T14:19:34.493015: step 273, loss 0.0942512, accuracy 0.949219, precision 0.9090909090909091, recall 0.970873786407767
2019-03-03T14:19:35.004157: step 274, loss 0.0857387, accuracy 0.964844, precision 0.97, recall 0.941747572815534
2019-03-03T14:19:35.515816: step 275, loss 0.0786822, accuracy 0.976562, precision 0.9578947368421052, recall 0.978494623655914
2019-03-03T14:19:36.032439: step 276, loss 0.0825713, accuracy 0.964844, precision 0.9545454545454546, recall 0.9438202247191011
2019-03-03T14:19:36.551048: step 277, loss 0.0671621, accuracy 0.976562, precision 0.9540229885057471, recall 0.9764705882352941
2019-03-03T14:19:37.072653: step 278, loss 0.106896, accuracy 0.957031, precision 0.9222222222222223, recall 0.9540229885057471
2019-03-03T14:19:37.568314: step 279, loss 0.0728305, accuracy 0.984375, precision 0.978021978021978, recall 0.978021978021978
2019-03-03T14:19:38.068486: step 280, loss 0.0833075, accuracy 0.96875, precision 0.9690721649484536, recall 0.9494949494949495
2019-03-03T14:19:38.579120: step 281, loss 0.121074, accuracy 0.949219, precision 0.896551724137931, recall 0.9512195121951219
2019-03-03T14:19:39.130683: step 282, loss 0.127533, accuracy 0.945312, precision 0.8913043478260869, recall 0.9534883720930233
2019-03-03T14:19:39.626357: step 283, loss 0.0871914, accuracy 0.972656, precision 0.945054945054945, recall 0.9772727272727273
2019-03-03T14:19:40.126021: step 284, loss 0.110312, accuracy 0.960938, precision 0.9456521739130435, recall 0.9456521739130435
2019-03-03T14:19:40.638123: step 285, loss 0.112158, accuracy 0.945312, precision 0.9306930693069307, recall 0.9306930693069307
2019-03-03T14:19:41.150264: step 286, loss 0.179804, accuracy 0.941406, precision 0.9565217391304348, recall 0.8888888888888888
2019-03-03T14:19:41.643947: step 287, loss 0.11547, accuracy 0.960938, precision 0.9642857142857143, recall 0.9204545454545454
2019-03-03T14:19:42.147599: step 288, loss 0.105215, accuracy 0.960938, precision 0.9047619047619048, recall 0.9743589743589743
2019-03-03T14:19:42.398442: step 289, loss 0.101529, accuracy 0.948718, precision 0.8974358974358975, recall 0.9459459459459459
2019-03-03T14:19:42.894121: step 290, loss 0.105916, accuracy 0.957031, precision 0.8901098901098901, recall 0.9878048780487805
2019-03-03T14:19:43.385818: step 291, loss 0.124483, accuracy 0.953125, precision 0.9157894736842105, recall 0.9560439560439561
2019-03-03T14:19:43.891451: step 292, loss 0.100039, accuracy 0.960938, precision 0.9514563106796117, recall 0.9514563106796117
2019-03-03T14:19:44.417196: step 293, loss 0.080818, accuracy 0.96875, precision 0.9405940594059405, recall 0.979381443298969
2019-03-03T14:19:44.904910: step 294, loss 0.09286, accuracy 0.957031, precision 0.956989247311828, recall 0.9270833333333334
2019-03-03T14:19:45.405570: step 295, loss 0.0946921, accuracy 0.972656, precision 0.945054945054945, recall 0.9772727272727273
2019-03-03T14:19:45.893266: step 296, loss 0.0614853, accuracy 0.976562, precision 0.9333333333333333, recall 1.0
2019-03-03T14:19:46.401906: step 297, loss 0.128114, accuracy 0.957031, precision 0.9577464788732394, recall 0.8947368421052632
2019-03-03T14:19:46.880136: step 298, loss 0.0928219, accuracy 0.976562, precision 0.9659090909090909, recall 0.9659090909090909
2019-03-03T14:19:47.361367: step 299, loss 0.0975227, accuracy 0.96875, precision 0.9340659340659341, recall 0.9770114942528736
2019-03-03T14:19:47.841085: step 300, loss 0.0548495, accuracy 0.988281, precision 0.9696969696969697, recall 1.0
2019-03-03T14:19:48.350720: step 301, loss 0.0861499, accuracy 0.964844, precision 0.9540229885057471, recall 0.9431818181818182
2019-03-03T14:19:48.834427: step 302, loss 0.0769844, accuracy 0.984375, precision 0.9693877551020408, recall 0.9895833333333334
2019-03-03T14:19:49.338589: step 303, loss 0.099677, accuracy 0.957031, precision 0.9439252336448598, recall 0.9528301886792453
2019-03-03T14:19:49.825804: step 304, loss 0.0922934, accuracy 0.960938, precision 0.935064935064935, recall 0.935064935064935
2019-03-03T14:19:50.343420: step 305, loss 0.10106, accuracy 0.957031, precision 0.9550561797752809, recall 0.9239130434782609
2019-03-03T14:19:50.578308: step 306, loss 0.0711072, accuracy 0.982906, precision 0.9795918367346939, recall 0.9795918367346939
2019-03-03T14:19:51.059022: step 307, loss 0.105372, accuracy 0.96875, precision 0.963855421686747, recall 0.9411764705882353
2019-03-03T14:19:51.555213: step 308, loss 0.0767495, accuracy 0.976562, precision 0.9591836734693877, recall 0.9791666666666666
2019-03-03T14:19:52.052063: step 309, loss 0.125029, accuracy 0.949219, precision 0.9504950495049505, recall 0.9230769230769231
2019-03-03T14:19:52.544793: step 310, loss 0.0766591, accuracy 0.976562, precision 0.968421052631579, recall 0.968421052631579
2019-03-03T14:19:53.026500: step 311, loss 0.0691224, accuracy 0.988281, precision 0.9666666666666667, recall 1.0
2019-03-03T14:19:53.503734: step 312, loss 0.0707465, accuracy 0.976562, precision 0.9746835443037974, recall 0.9506172839506173
2019-03-03T14:19:54.013373: step 313, loss 0.104547, accuracy 0.972656, precision 0.978021978021978, recall 0.9468085106382979
2019-03-03T14:19:54.534488: step 314, loss 0.0859043, accuracy 0.960938, precision 0.9294117647058824, recall 0.9518072289156626
2019-03-03T14:19:55.043133: step 315, loss 0.135155, accuracy 0.945312, precision 0.9, recall 0.9418604651162791
2019-03-03T14:19:55.529000: step 316, loss 0.084885, accuracy 0.964844, precision 0.927710843373494, recall 0.9625
2019-03-03T14:19:56.020825: step 317, loss 0.0685902, accuracy 0.984375, precision 0.9607843137254902, recall 1.0
2019-03-03T14:19:56.554215: step 318, loss 0.107341, accuracy 0.972656, precision 0.970873786407767, recall 0.9615384615384616
2019-03-03T14:19:57.045923: step 319, loss 0.106788, accuracy 0.957031, precision 0.9345794392523364, recall 0.9615384615384616
2019-03-03T14:19:57.525566: step 320, loss 0.133366, accuracy 0.925781, precision 0.9078947368421053, recall 0.8518518518518519
2019-03-03T14:19:58.028224: step 321, loss 0.0761781, accuracy 0.980469, precision 0.9895833333333334, recall 0.9595959595959596
2019-03-03T14:19:58.540350: step 322, loss 0.0659479, accuracy 0.984375, precision 0.9891304347826086, recall 0.9680851063829787
2019-03-03T14:19:58.775232: step 323, loss 0.118115, accuracy 0.957265, precision 0.9387755102040817, recall 0.9583333333333334
2019-03-03T14:19:59.259934: step 324, loss 0.0531055, accuracy 0.988281, precision 0.989010989010989, recall 0.9782608695652174
2019-03-03T14:19:59.740649: step 325, loss 0.0741608, accuracy 0.984375, precision 0.9791666666666666, recall 0.9791666666666666
2019-03-03T14:20:00.277214: step 326, loss 0.106939, accuracy 0.964844, precision 0.9361702127659575, recall 0.967032967032967
2019-03-03T14:20:00.756665: step 327, loss 0.105126, accuracy 0.957031, precision 0.9438202247191011, recall 0.9333333333333333
2019-03-03T14:20:01.240913: step 328, loss 0.0982504, accuracy 0.972656, precision 0.9702970297029703, recall 0.9607843137254902
2019-03-03T14:20:01.758813: step 329, loss 0.104477, accuracy 0.960938, precision 0.9512195121951219, recall 0.9285714285714286
2019-03-03T14:20:02.483892: step 330, loss 0.0852123, accuracy 0.96875, precision 0.9489795918367347, recall 0.96875
2019-03-03T14:20:03.240897: step 331, loss 0.125139, accuracy 0.953125, precision 0.9304347826086956, recall 0.963963963963964
2019-03-03T14:20:03.980640: step 332, loss 0.114914, accuracy 0.957031, precision 0.9318181818181818, recall 0.9425287356321839
2019-03-03T14:20:04.746103: step 333, loss 0.0722078, accuracy 0.972656, precision 0.946236559139785, recall 0.9777777777777777
2019-03-03T14:20:05.474176: step 334, loss 0.10172, accuracy 0.964844, precision 0.9418604651162791, recall 0.9529411764705882
2019-03-03T14:20:06.189263: step 335, loss 0.0733899, accuracy 0.984375, precision 0.978021978021978, recall 0.978021978021978
2019-03-03T14:20:06.898368: step 336, loss 0.111203, accuracy 0.960938, precision 0.967032967032967, recall 0.9263157894736842
2019-03-03T14:20:07.598496: step 337, loss 0.0961923, accuracy 0.96875, precision 0.9583333333333334, recall 0.9583333333333334
2019-03-03T14:20:08.318579: step 338, loss 0.121707, accuracy 0.957031, precision 0.9342105263157895, recall 0.922077922077922
2019-03-03T14:20:09.003760: step 339, loss 0.0708748, accuracy 0.972656, precision 0.945054945054945, recall 0.9772727272727273
2019-03-03T14:20:09.313468: step 340, loss 0.0913185, accuracy 0.965812, precision 0.9285714285714286, recall 0.975
2019-03-03T14:20:09.960730: step 341, loss 0.0912104, accuracy 0.972656, precision 0.945054945054945, recall 0.9772727272727273
2019-03-03T14:20:10.620966: step 342, loss 0.11266, accuracy 0.96875, precision 0.93, recall 0.9893617021276596
2019-03-03T14:20:11.261760: step 343, loss 0.070405, accuracy 0.976562, precision 0.9479166666666666, recall 0.9891304347826086
2019-03-03T14:20:11.870137: step 344, loss 0.0947199, accuracy 0.960938, precision 0.9333333333333333, recall 0.9545454545454546
2019-03-03T14:20:12.528611: step 345, loss 0.0708212, accuracy 0.976562, precision 0.9878048780487805, recall 0.9418604651162791
2019-03-03T14:20:13.125027: step 346, loss 0.0983128, accuracy 0.96875, precision 0.9770114942528736, recall 0.9340659340659341
2019-03-03T14:20:13.727417: step 347, loss 0.0889269, accuracy 0.976562, precision 0.9824561403508771, recall 0.9655172413793104
2019-03-03T14:20:14.343837: step 348, loss 0.0797917, accuracy 0.972656, precision 0.9764705882352941, recall 0.9431818181818182
2019-03-03T14:20:14.930243: step 349, loss 0.0837522, accuracy 0.96875, precision 0.9411764705882353, recall 0.9795918367346939
2019-03-03T14:20:15.507150: step 350, loss 0.0765096, accuracy 0.976562, precision 0.9468085106382979, recall 0.9888888888888889
2019-03-03T14:20:16.110043: step 351, loss 0.0910161, accuracy 0.96875, precision 0.9302325581395349, recall 0.975609756097561
2019-03-03T14:20:16.691242: step 352, loss 0.0940377, accuracy 0.964844, precision 0.9195402298850575, recall 0.975609756097561
2019-03-03T14:20:17.267631: step 353, loss 0.0614719, accuracy 0.984375, precision 0.9629629629629629, recall 0.9873417721518988
2019-03-03T14:20:17.835621: step 354, loss 0.0597279, accuracy 0.992188, precision 0.9795918367346939, recall 1.0
2019-03-03T14:20:18.417584: step 355, loss 0.0731178, accuracy 0.984375, precision 0.97, recall 0.9897959183673469
2019-03-03T14:20:18.985574: step 356, loss 0.0684203, accuracy 0.976562, precision 0.9545454545454546, recall 0.9767441860465116
2019-03-03T14:20:19.264830: step 357, loss 0.112673, accuracy 0.957265, precision 0.9487179487179487, recall 0.925
2019-03-03T14:20:19.822336: step 358, loss 0.0568764, accuracy 0.988281, precision 0.989247311827957, recall 0.9787234042553191
2019-03-03T14:20:20.389844: step 359, loss 0.0767438, accuracy 0.980469, precision 0.9767441860465116, recall 0.9655172413793104
2019-03-03T14:20:20.944368: step 360, loss 0.0529341, accuracy 0.984375, precision 0.9777777777777777, recall 0.9777777777777777
2019-03-03T14:20:21.491946: step 361, loss 0.0823923, accuracy 0.976562, precision 0.9651162790697675, recall 0.9651162790697675
2019-03-03T14:20:22.056799: step 362, loss 0.0855989, accuracy 0.960938, precision 0.9529411764705882, recall 0.9310344827586207
2019-03-03T14:20:22.613759: step 363, loss 0.103464, accuracy 0.960938, precision 0.9387755102040817, recall 0.9583333333333334
2019-03-03T14:20:23.167060: step 364, loss 0.0841192, accuracy 0.964844, precision 0.94, recall 0.9690721649484536
2019-03-03T14:20:23.690660: step 365, loss 0.0847525, accuracy 0.972656, precision 0.9156626506024096, recall 1.0
2019-03-03T14:20:24.240190: step 366, loss 0.0650297, accuracy 0.980469, precision 0.9431818181818182, recall 1.0
2019-03-03T14:20:24.768734: step 367, loss 0.0855401, accuracy 0.964844, precision 0.9431818181818182, recall 0.9540229885057471
2019-03-03T14:20:25.293349: step 368, loss 0.0800182, accuracy 0.976562, precision 0.9759036144578314, recall 0.9529411764705882
2019-03-03T14:20:25.847233: step 369, loss 0.0676262, accuracy 0.984375, precision 0.9714285714285714, recall 0.9902912621359223
2019-03-03T14:20:26.387300: step 370, loss 0.0435608, accuracy 0.984375, precision 0.970873786407767, recall 0.9900990099009901
2019-03-03T14:20:26.929474: step 371, loss 0.0977122, accuracy 0.960938, precision 0.9578947368421052, recall 0.9381443298969072
2019-03-03T14:20:27.461054: step 372, loss 0.0773254, accuracy 0.96875, precision 0.9375, recall 0.9782608695652174
2019-03-03T14:20:27.990150: step 373, loss 0.0666444, accuracy 0.976562, precision 0.9591836734693877, recall 0.9791666666666666
2019-03-03T14:20:28.233304: step 374, loss 0.124276, accuracy 0.923077, precision 0.8837209302325582, recall 0.9047619047619048
2019-03-03T14:20:28.747929: step 375, loss 0.0978793, accuracy 0.960938, precision 0.9381443298969072, recall 0.9578947368421052
2019-03-03T14:20:29.277453: step 376, loss 0.0449817, accuracy 0.992188, precision 0.9882352941176471, recall 0.9882352941176471
2019-03-03T14:20:29.804044: step 377, loss 0.066777, accuracy 0.976562, precision 0.9425287356321839, recall 0.9879518072289156
2019-03-03T14:20:30.309691: step 378, loss 0.0859966, accuracy 0.964844, precision 0.9468085106382979, recall 0.956989247311828
2019-03-03T14:20:30.805879: step 379, loss 0.0525163, accuracy 0.992188, precision 0.975, recall 1.0
2019-03-03T14:20:31.313017: step 380, loss 0.0639463, accuracy 0.976562, precision 0.9405940594059405, recall 1.0
2019-03-03T14:20:31.847588: step 381, loss 0.0862246, accuracy 0.976562, precision 0.97, recall 0.97
2019-03-03T14:20:32.377207: step 382, loss 0.0785859, accuracy 0.980469, precision 0.946236559139785, recall 1.0
2019-03-03T14:20:32.895824: step 383, loss 0.0835376, accuracy 0.96875, precision 0.9278350515463918, recall 0.989010989010989
2019-03-03T14:20:33.404464: step 384, loss 0.059942, accuracy 0.988281, precision 1.0, recall 0.9724770642201835
2019-03-03T14:20:33.919087: step 385, loss 0.0845703, accuracy 0.976562, precision 0.9753086419753086, recall 0.9518072289156626
2019-03-03T14:20:34.413765: step 386, loss 0.0996884, accuracy 0.964844, precision 0.9787234042553191, recall 0.9292929292929293
2019-03-03T14:20:34.911436: step 387, loss 0.0909895, accuracy 0.972656, precision 0.978021978021978, recall 0.9468085106382979
2019-03-03T14:20:35.418381: step 388, loss 0.0702901, accuracy 0.972656, precision 0.9555555555555556, recall 0.9662921348314607
2019-03-03T14:20:35.912061: step 389, loss 0.0888385, accuracy 0.964844, precision 0.9175257731958762, recall 0.9888888888888889
2019-03-03T14:20:36.395798: step 390, loss 0.0644402, accuracy 0.976562, precision 0.9431818181818182, recall 0.9880952380952381
2019-03-03T14:20:36.629693: step 391, loss 0.0932446, accuracy 0.957265, precision 0.9230769230769231, recall 0.9473684210526315
2019-03-03T14:20:37.114904: step 392, loss 0.0768894, accuracy 0.96875, precision 0.9347826086956522, recall 0.9772727272727273
2019-03-03T14:20:37.625607: step 393, loss 0.0493572, accuracy 0.992188, precision 0.9904761904761905, recall 0.9904761904761905
2019-03-03T14:20:38.104327: step 394, loss 0.0958896, accuracy 0.972656, precision 0.9888888888888889, recall 0.9368421052631579
2019-03-03T14:20:38.588054: step 395, loss 0.075771, accuracy 0.972656, precision 0.9770114942528736, recall 0.9444444444444444
2019-03-03T14:20:39.082238: step 396, loss 0.0716647, accuracy 0.96875, precision 0.9696969696969697, recall 0.9504950495049505
2019-03-03T14:20:39.595807: step 397, loss 0.0611981, accuracy 0.988281, precision 1.0, recall 0.968421052631579
2019-03-03T14:20:40.085887: step 398, loss 0.0660783, accuracy 0.964844, precision 0.9247311827956989, recall 0.9772727272727273
2019-03-03T14:20:40.579565: step 399, loss 0.0945129, accuracy 0.953125, precision 0.9230769230769231, recall 0.9438202247191011
2019-03-03T14:20:41.086211: step 400, loss 0.0508964, accuracy 0.988281, precision 0.9651162790697675, recall 1.0
2019-03-03T14:20:41.578895: step 401, loss 0.0828498, accuracy 0.964844, precision 0.8977272727272727, recall 1.0
2019-03-03T14:20:42.059117: step 402, loss 0.0974253, accuracy 0.960938, precision 0.9139784946236559, recall 0.9770114942528736
2019-03-03T14:20:42.544818: step 403, loss 0.080033, accuracy 0.972656, precision 0.9375, recall 0.989010989010989
2019-03-03T14:20:43.068321: step 404, loss 0.0643319, accuracy 0.976562, precision 0.9789473684210527, recall 0.9587628865979382
2019-03-03T14:20:43.548052: step 405, loss 0.0608579, accuracy 0.980469, precision 0.9873417721518988, recall 0.9512195121951219
2019-03-03T14:20:44.041734: step 406, loss 0.0929611, accuracy 0.972656, precision 0.9809523809523809, recall 0.9537037037037037
2019-03-03T14:20:44.523955: step 407, loss 0.10233, accuracy 0.957031, precision 0.9886363636363636, recall 0.8969072164948454
2019-03-03T14:20:44.770811: step 408, loss 0.101014, accuracy 0.948718, precision 0.975609756097561, recall 0.8888888888888888
2019-03-03T14:20:45.261903: step 409, loss 0.0428288, accuracy 0.988281, precision 0.9770114942528736, recall 0.9883720930232558
2019-03-03T14:20:45.754134: step 410, loss 0.0521269, accuracy 0.988281, precision 0.9651162790697675, recall 1.0
2019-03-03T14:20:46.249310: step 411, loss 0.0621909, accuracy 0.976562, precision 0.96, recall 0.9795918367346939
2019-03-03T14:20:46.745982: step 412, loss 0.0557509, accuracy 0.984375, precision 0.9607843137254902, recall 1.0
2019-03-03T14:20:47.242670: step 413, loss 0.0691966, accuracy 0.984375, precision 0.9711538461538461, recall 0.9901960784313726
2019-03-03T14:20:47.773266: step 414, loss 0.0736201, accuracy 0.976562, precision 0.9340659340659341, recall 1.0
2019-03-03T14:20:48.263940: step 415, loss 0.0996902, accuracy 0.964844, precision 0.9381443298969072, recall 0.9680851063829787
2019-03-03T14:20:48.766595: step 416, loss 0.0562156, accuracy 0.980469, precision 0.9711538461538461, recall 0.9805825242718447
2019-03-03T14:20:49.258291: step 417, loss 0.0579681, accuracy 0.984375, precision 0.9894736842105263, recall 0.9690721649484536
2019-03-03T14:20:49.745103: step 418, loss 0.0590268, accuracy 0.976562, precision 0.961038961038961, recall 0.961038961038961
2019-03-03T14:20:50.419299: step 419, loss 0.094235, accuracy 0.957031, precision 0.9615384615384616, recall 0.9036144578313253
2019-03-03T14:20:51.212182: step 420, loss 0.0787061, accuracy 0.972656, precision 0.98989898989899, recall 0.9423076923076923
2019-03-03T14:20:51.961192: step 421, loss 0.114852, accuracy 0.953125, precision 0.9534883720930233, recall 0.9111111111111111
2019-03-03T14:20:52.720229: step 422, loss 0.0944587, accuracy 0.960938, precision 0.9166666666666666, recall 0.9777777777777777
2019-03-03T14:20:53.467231: step 423, loss 0.0602238, accuracy 0.976562, precision 0.9456521739130435, recall 0.9886363636363636
2019-03-03T14:20:54.157894: step 424, loss 0.0937283, accuracy 0.960938, precision 0.8977272727272727, recall 0.9875
2019-03-03T14:20:54.526247: step 425, loss 0.0544396, accuracy 0.991453, precision 1.0, recall 0.9743589743589743
2019-03-03T14:20:55.222504: step 426, loss 0.0581623, accuracy 0.992188, precision 0.989010989010989, recall 0.989010989010989
2019-03-03T14:20:55.889232: step 427, loss 0.0668253, accuracy 0.972656, precision 0.9603960396039604, recall 0.97
2019-03-03T14:20:56.533510: step 428, loss 0.0938747, accuracy 0.972656, precision 0.96875, recall 0.9587628865979382
2019-03-03T14:20:57.195757: step 429, loss 0.0438252, accuracy 0.980469, precision 0.9560439560439561, recall 0.9886363636363636
2019-03-03T14:20:57.861998: step 430, loss 0.06615, accuracy 0.984375, precision 0.9761904761904762, recall 0.9761904761904762
2019-03-03T14:20:58.515762: step 431, loss 0.0637087, accuracy 0.976562, precision 0.9897959183673469, recall 0.9509803921568627
2019-03-03T14:20:59.260282: step 432, loss 0.0517059, accuracy 0.984375, precision 0.9875, recall 0.9634146341463414
2019-03-03T14:20:59.943455: step 433, loss 0.0589067, accuracy 0.984375, precision 0.9885057471264368, recall 0.9662921348314607
2019-03-03T14:21:00.627626: step 434, loss 0.0455246, accuracy 0.992188, precision 0.989247311827957, recall 0.989247311827957
2019-03-03T14:21:01.249962: step 435, loss 0.0632331, accuracy 0.988281, precision 0.9767441860465116, recall 0.9882352941176471
2019-03-03T14:21:01.817962: step 436, loss 0.0681297, accuracy 0.976562, precision 0.9347826086956522, recall 1.0
2019-03-03T14:21:02.536550: step 437, loss 0.0585801, accuracy 0.984375, precision 0.9777777777777777, recall 0.9777777777777777
2019-03-03T14:21:03.171876: step 438, loss 0.0522295, accuracy 0.984375, precision 0.9797979797979798, recall 0.9797979797979798
2019-03-03T14:21:03.800196: step 439, loss 0.0606733, accuracy 0.984375, precision 0.9787234042553191, recall 0.9787234042553191
2019-03-03T14:21:04.387135: step 440, loss 0.0806483, accuracy 0.976562, precision 0.9690721649484536, recall 0.9690721649484536
2019-03-03T14:21:04.992521: step 441, loss 0.0732984, accuracy 0.972656, precision 0.96, recall 0.9696969696969697
2019-03-03T14:21:05.278751: step 442, loss 0.0423699, accuracy 0.991453, precision 0.975609756097561, recall 1.0
2019-03-03T14:21:05.848228: step 443, loss 0.0406051, accuracy 0.988281, precision 0.9882352941176471, recall 0.9767441860465116
2019-03-03T14:21:06.426682: step 444, loss 0.0803607, accuracy 0.96875, precision 0.96, recall 0.96
2019-03-03T14:21:07.012131: step 445, loss 0.0637593, accuracy 0.980469, precision 0.9529411764705882, recall 0.9878048780487805
2019-03-03T14:21:07.587089: step 446, loss 0.054876, accuracy 0.984375, precision 0.9583333333333334, recall 1.0
2019-03-03T14:21:08.153084: step 447, loss 0.0586017, accuracy 0.988281, precision 0.9767441860465116, recall 0.9882352941176471
2019-03-03T14:21:08.747983: step 448, loss 0.0727112, accuracy 0.972656, precision 0.9540229885057471, recall 0.9651162790697675
2019-03-03T14:21:09.289532: step 449, loss 0.0779092, accuracy 0.96875, precision 0.9361702127659575, recall 0.9777777777777777
2019-03-03T14:21:09.833589: step 450, loss 0.0505368, accuracy 0.980469, precision 0.963855421686747, recall 0.975609756097561
2019-03-03T14:21:10.379128: step 451, loss 0.0607137, accuracy 0.980469, precision 0.9886363636363636, recall 0.9560439560439561
2019-03-03T14:21:10.933647: step 452, loss 0.0750988, accuracy 0.976562, precision 0.967391304347826, recall 0.967391304347826
2019-03-03T14:21:11.472206: step 453, loss 0.0826175, accuracy 0.964844, precision 0.9247311827956989, recall 0.9772727272727273
2019-03-03T14:21:12.023428: step 454, loss 0.0655097, accuracy 0.976562, precision 0.9565217391304348, recall 0.9777777777777777
2019-03-03T14:21:12.559503: step 455, loss 0.0595391, accuracy 0.984375, precision 0.9603960396039604, recall 1.0
2019-03-03T14:21:13.090085: step 456, loss 0.0753122, accuracy 0.96875, precision 0.9702970297029703, recall 0.9514563106796117
2019-03-03T14:21:13.622170: step 457, loss 0.049931, accuracy 0.980469, precision 0.979381443298969, recall 0.9693877551020408
2019-03-03T14:21:14.158173: step 458, loss 0.067774, accuracy 0.972656, precision 0.9789473684210527, recall 0.9489795918367347
2019-03-03T14:21:14.416483: step 459, loss 0.0390317, accuracy 0.982906, precision 1.0, recall 0.9574468085106383
2019-03-03T14:21:14.958035: step 460, loss 0.0683369, accuracy 0.976562, precision 0.9578947368421052, recall 0.978494623655914
2019-03-03T14:21:15.494109: step 461, loss 0.0670403, accuracy 0.980469, precision 0.9787234042553191, recall 0.968421052631579
2019-03-03T14:21:16.029213: step 462, loss 0.0606762, accuracy 0.992188, precision 0.9900990099009901, recall 0.9900990099009901
2019-03-03T14:21:16.566286: step 463, loss 0.0555644, accuracy 0.976562, precision 0.9795918367346939, recall 0.96
2019-03-03T14:21:17.090885: step 464, loss 0.0361316, accuracy 0.988281, precision 0.967391304347826, recall 1.0
2019-03-03T14:21:17.607503: step 465, loss 0.0286819, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:21:18.104681: step 466, loss 0.0538004, accuracy 0.972656, precision 0.9195402298850575, recall 1.0
2019-03-03T14:21:18.642245: step 467, loss 0.0554029, accuracy 0.976562, precision 0.9789473684210527, recall 0.9587628865979382
2019-03-03T14:21:19.146928: step 468, loss 0.0531441, accuracy 0.976562, precision 0.9444444444444444, recall 0.9883720930232558
2019-03-03T14:21:19.641637: step 469, loss 0.0716571, accuracy 0.972656, precision 0.9550561797752809, recall 0.9659090909090909
2019-03-03T14:21:20.147798: step 470, loss 0.0675578, accuracy 0.984375, precision 0.9885057471264368, recall 0.9662921348314607
2019-03-03T14:21:20.668409: step 471, loss 0.0482734, accuracy 0.988281, precision 1.0, recall 0.9655172413793104
2019-03-03T14:21:21.175055: step 472, loss 0.0681227, accuracy 0.976562, precision 0.9811320754716981, recall 0.9629629629629629
2019-03-03T14:21:21.688677: step 473, loss 0.0647243, accuracy 0.976562, precision 0.9578947368421052, recall 0.978494623655914
2019-03-03T14:21:22.188631: step 474, loss 0.0610682, accuracy 0.984375, precision 0.9821428571428571, recall 0.9821428571428571
2019-03-03T14:21:22.741215: step 475, loss 0.0657874, accuracy 0.96875, precision 0.9487179487179487, recall 0.9487179487179487
2019-03-03T14:21:22.996536: step 476, loss 0.0493098, accuracy 0.991453, precision 1.0, recall 0.9736842105263158
2019-03-03T14:21:23.512158: step 477, loss 0.0443962, accuracy 0.988281, precision 0.9895833333333334, recall 0.979381443298969
2019-03-03T14:21:24.017869: step 478, loss 0.0767547, accuracy 0.988281, precision 0.97, recall 1.0
2019-03-03T14:21:24.543973: step 479, loss 0.0519217, accuracy 0.980469, precision 0.946236559139785, recall 1.0
2019-03-03T14:21:25.027679: step 480, loss 0.070309, accuracy 0.964844, precision 0.945054945054945, recall 0.9555555555555556
2019-03-03T14:21:25.529338: step 481, loss 0.0397956, accuracy 0.984375, precision 0.9887640449438202, recall 0.967032967032967
2019-03-03T14:21:26.021023: step 482, loss 0.0598265, accuracy 0.984375, precision 0.9801980198019802, recall 0.9801980198019802
2019-03-03T14:21:26.529663: step 483, loss 0.0332346, accuracy 0.988281, precision 0.9787234042553191, recall 0.989247311827957
2019-03-03T14:21:27.015381: step 484, loss 0.0451638, accuracy 0.988281, precision 1.0, recall 0.963855421686747
2019-03-03T14:21:27.513115: step 485, loss 0.0632279, accuracy 0.980469, precision 0.9787234042553191, recall 0.968421052631579
2019-03-03T14:21:28.024273: step 486, loss 0.0589366, accuracy 0.984375, precision 0.9888888888888889, recall 0.967391304347826
2019-03-03T14:21:28.548381: step 487, loss 0.0605333, accuracy 0.976562, precision 0.9489795918367347, recall 0.9893617021276596
2019-03-03T14:21:29.058122: step 488, loss 0.0498536, accuracy 0.984375, precision 0.989010989010989, recall 0.967741935483871
2019-03-03T14:21:29.548320: step 489, loss 0.0525184, accuracy 0.984375, precision 0.9864864864864865, recall 0.9605263157894737
2019-03-03T14:21:30.034023: step 490, loss 0.0772496, accuracy 0.96875, precision 0.978494623655914, recall 0.9381443298969072
2019-03-03T14:21:30.544657: step 491, loss 0.0572698, accuracy 0.980469, precision 0.9591836734693877, recall 0.9894736842105263
2019-03-03T14:21:31.051302: step 492, loss 0.0595366, accuracy 0.976562, precision 0.9550561797752809, recall 0.9770114942528736
2019-03-03T14:21:31.295391: step 493, loss 0.0335934, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:21:31.831957: step 494, loss 0.0414167, accuracy 0.992188, precision 1.0, recall 0.978494623655914
2019-03-03T14:21:32.360544: step 495, loss 0.0545138, accuracy 0.980469, precision 0.956989247311828, recall 0.9888888888888889
2019-03-03T14:21:32.851232: step 496, loss 0.0434999, accuracy 0.980469, precision 0.978021978021978, recall 0.967391304347826
2019-03-03T14:21:33.341475: step 497, loss 0.0608223, accuracy 0.976562, precision 0.9454545454545454, recall 1.0
2019-03-03T14:21:33.836152: step 498, loss 0.0731303, accuracy 0.976562, precision 0.97, recall 0.97
2019-03-03T14:21:34.364739: step 499, loss 0.057659, accuracy 0.980469, precision 0.9787234042553191, recall 0.968421052631579
2019-03-03T14:21:34.868391: step 500, loss 0.035756, accuracy 0.996094, precision 1.0, recall 0.9893617021276596

Evaluation:
[[143  30]
 [ 32 263]]
2019-03-03T14:21:35.250389: step 500, loss 0.342991, accuracy 0.867521, precision 0.8265895953757225, recall 0.8171428571428572

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551644202\checkpoints\model-500


Process finished with exit code 0
