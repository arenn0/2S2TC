"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
No Pretrained Embedding
Italian: False
Loading data...
11566
Max Document length: 36
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 29102
Train/Dev split: 10410/1156
2019-03-03 15:04:46.662400: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_with_embeddings.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551647087

2019-03-03T15:04:48.593463: step 1, loss 2.19497, accuracy 0.628906, precision 0.8765432098765432, recall 0.6543778801843319
2019-03-03T15:04:48.835833: step 2, loss 1.95848, accuracy 0.507812, precision 0.5976331360946746, recall 0.6352201257861635
2019-03-03T15:04:49.074196: step 3, loss 1.91176, accuracy 0.554688, precision 0.6011560693641619, recall 0.697986577181208
2019-03-03T15:04:49.307574: step 4, loss 1.43483, accuracy 0.609375, precision 0.6740331491712708, recall 0.7484662576687117
2019-03-03T15:04:49.544941: step 5, loss 1.48936, accuracy 0.640625, precision 0.7455621301775148, recall 0.72
2019-03-03T15:04:49.779339: step 6, loss 1.47257, accuracy 0.597656, precision 0.7341040462427746, recall 0.6902173913043478
2019-03-03T15:04:50.019995: step 7, loss 1.99777, accuracy 0.597656, precision 0.7973856209150327, recall 0.6288659793814433
2019-03-03T15:04:50.261360: step 8, loss 1.60454, accuracy 0.605469, precision 0.7559523809523809, recall 0.679144385026738
2019-03-03T15:04:50.500757: step 9, loss 1.59153, accuracy 0.640625, precision 0.7514450867052023, recall 0.7262569832402235
2019-03-03T15:04:50.728656: step 10, loss 1.70388, accuracy 0.59375, precision 0.6956521739130435, recall 0.6706586826347305
2019-03-03T15:04:50.975995: step 11, loss 1.38388, accuracy 0.671875, precision 0.7411764705882353, recall 0.7590361445783133
2019-03-03T15:04:51.211366: step 12, loss 1.62661, accuracy 0.621094, precision 0.6826347305389222, recall 0.7215189873417721
2019-03-03T15:04:51.448837: step 13, loss 1.94126, accuracy 0.546875, precision 0.6331360946745562, recall 0.6645962732919255
2019-03-03T15:04:51.673726: step 14, loss 1.595, accuracy 0.617188, precision 0.6727272727272727, recall 0.7161290322580646
2019-03-03T15:04:51.899630: step 15, loss 1.74273, accuracy 0.613281, precision 0.6727272727272727, recall 0.7115384615384616
2019-03-03T15:04:52.126024: step 16, loss 1.49039, accuracy 0.664062, precision 0.7333333333333333, recall 0.7423312883435583
2019-03-03T15:04:52.359405: step 17, loss 1.32022, accuracy 0.699219, precision 0.7885714285714286, recall 0.7752808988764045
2019-03-03T15:04:52.590871: step 18, loss 1.68894, accuracy 0.636719, precision 0.7633136094674556, recall 0.7087912087912088
2019-03-03T15:04:52.835222: step 19, loss 1.60651, accuracy 0.621094, precision 0.7848101265822784, recall 0.6631016042780749
2019-03-03T15:04:53.084068: step 20, loss 1.38192, accuracy 0.667969, precision 0.774390243902439, recall 0.7257142857142858
2019-03-03T15:04:53.320437: step 21, loss 1.38881, accuracy 0.664062, precision 0.7401129943502824, recall 0.7660818713450293
2019-03-03T15:04:53.556807: step 22, loss 1.11497, accuracy 0.671875, precision 0.7309941520467836, recall 0.7668711656441718
2019-03-03T15:04:53.787226: step 23, loss 1.17086, accuracy 0.703125, precision 0.810126582278481, recall 0.735632183908046
2019-03-03T15:04:54.016577: step 24, loss 1.64622, accuracy 0.582031, precision 0.6390532544378699, recall 0.7012987012987013
2019-03-03T15:04:54.245962: step 25, loss 1.03427, accuracy 0.710938, precision 0.7657142857142857, recall 0.8023952095808383
2019-03-03T15:04:54.472356: step 26, loss 1.37418, accuracy 0.628906, precision 0.6728395061728395, recall 0.7218543046357616
2019-03-03T15:04:54.701744: step 27, loss 1.14751, accuracy 0.660156, precision 0.7062146892655368, recall 0.78125
2019-03-03T15:04:54.955096: step 28, loss 1.35475, accuracy 0.695312, precision 0.7719298245614035, recall 0.7719298245614035
2019-03-03T15:04:55.182382: step 29, loss 1.25528, accuracy 0.65625, precision 0.7848837209302325, recall 0.7258064516129032
2019-03-03T15:04:55.414813: step 30, loss 1.10395, accuracy 0.707031, precision 0.8682634730538922, recall 0.7323232323232324
2019-03-03T15:04:55.646168: step 31, loss 1.64445, accuracy 0.640625, precision 0.7621951219512195, recall 0.702247191011236
2019-03-03T15:04:55.877548: step 32, loss 1.08898, accuracy 0.667969, precision 0.8012048192771084, recall 0.7189189189189189
2019-03-03T15:04:56.106935: step 33, loss 0.854521, accuracy 0.777344, precision 0.847457627118644, recall 0.8333333333333334
2019-03-03T15:04:56.336323: step 34, loss 1.35244, accuracy 0.632812, precision 0.7848101265822784, recall 0.6739130434782609
2019-03-03T15:04:56.563715: step 35, loss 1.04857, accuracy 0.683594, precision 0.7305389221556886, recall 0.7721518987341772
2019-03-03T15:04:56.800283: step 36, loss 1.07178, accuracy 0.699219, precision 0.7314285714285714, recall 0.810126582278481
2019-03-03T15:04:57.040657: step 37, loss 1.42859, accuracy 0.628906, precision 0.6783625730994152, recall 0.7435897435897436
2019-03-03T15:04:57.267077: step 38, loss 1.2694, accuracy 0.632812, precision 0.7295597484276729, recall 0.6946107784431138
2019-03-03T15:04:57.513916: step 39, loss 1.22869, accuracy 0.671875, precision 0.7441860465116279, recall 0.7619047619047619
2019-03-03T15:04:57.749794: step 40, loss 1.2066, accuracy 0.675781, precision 0.7085714285714285, recall 0.7948717948717948
2019-03-03T15:04:57.934300: step 41, loss 0.999229, accuracy 0.711765, precision 0.7909090909090909, recall 0.7699115044247787
2019-03-03T15:04:58.163686: step 42, loss 0.868833, accuracy 0.757812, precision 0.845679012345679, recall 0.7873563218390804
2019-03-03T15:04:58.396074: step 43, loss 1.10952, accuracy 0.6875, precision 0.803680981595092, recall 0.7318435754189944
2019-03-03T15:04:58.632443: step 44, loss 0.98675, accuracy 0.730469, precision 0.8263473053892215, recall 0.7752808988764045
2019-03-03T15:04:58.874645: step 45, loss 1.04145, accuracy 0.683594, precision 0.7916666666666666, recall 0.7430167597765364
2019-03-03T15:04:59.110524: step 46, loss 0.89324, accuracy 0.710938, precision 0.8022598870056498, recall 0.7845303867403315
2019-03-03T15:04:59.336920: step 47, loss 0.857618, accuracy 0.710938, precision 0.8154761904761905, recall 0.7611111111111111
2019-03-03T15:04:59.570823: step 48, loss 1.02297, accuracy 0.707031, precision 0.7653631284916201, recall 0.8058823529411765
2019-03-03T15:04:59.800722: step 49, loss 0.941694, accuracy 0.714844, precision 0.7630057803468208, recall 0.8048780487804879
2019-03-03T15:05:00.037117: step 50, loss 0.949981, accuracy 0.722656, precision 0.7719298245614035, recall 0.8048780487804879
2019-03-03T15:05:00.270471: step 51, loss 0.917218, accuracy 0.71875, precision 0.7894736842105263, recall 0.7894736842105263
2019-03-03T15:05:00.498856: step 52, loss 0.930239, accuracy 0.746094, precision 0.8114285714285714, recall 0.8160919540229885
2019-03-03T15:05:00.732233: step 53, loss 0.947051, accuracy 0.707031, precision 0.8081395348837209, recall 0.7679558011049724
2019-03-03T15:05:00.967619: step 54, loss 1.12733, accuracy 0.679688, precision 0.8588957055214724, recall 0.7035175879396985
2019-03-03T15:05:01.191039: step 55, loss 0.859304, accuracy 0.71875, precision 0.7818181818181819, recall 0.7818181818181819
2019-03-03T15:05:01.417432: step 56, loss 0.976795, accuracy 0.691406, precision 0.7852760736196319, recall 0.7441860465116279
2019-03-03T15:05:01.640837: step 57, loss 0.832802, accuracy 0.730469, precision 0.7976878612716763, recall 0.8023255813953488
2019-03-03T15:05:01.890716: step 58, loss 0.974801, accuracy 0.683594, precision 0.7810650887573964, recall 0.75
2019-03-03T15:05:02.119107: step 59, loss 0.995738, accuracy 0.699219, precision 0.7964071856287425, recall 0.7556818181818182
2019-03-03T15:05:02.348043: step 60, loss 0.886705, accuracy 0.65625, precision 0.7261904761904762, recall 0.7439024390243902
2019-03-03T15:05:02.612317: step 61, loss 0.828137, accuracy 0.699219, precision 0.7724550898203593, recall 0.7678571428571429
2019-03-03T15:05:02.852700: step 62, loss 0.98736, accuracy 0.691406, precision 0.725609756097561, recall 0.7777777777777778
2019-03-03T15:05:03.084100: step 63, loss 0.918779, accuracy 0.71875, precision 0.7590361445783133, recall 0.7974683544303798
2019-03-03T15:05:03.315480: step 64, loss 0.784976, accuracy 0.710938, precision 0.7784810126582279, recall 0.7592592592592593
2019-03-03T15:05:03.539883: step 65, loss 0.900381, accuracy 0.710938, precision 0.7976190476190477, recall 0.7701149425287356
2019-03-03T15:05:03.767275: step 66, loss 0.983177, accuracy 0.6875, precision 0.7724550898203593, recall 0.7543859649122807
2019-03-03T15:05:03.992672: step 67, loss 0.990964, accuracy 0.691406, precision 0.7904191616766467, recall 0.75
2019-03-03T15:05:04.217582: step 68, loss 0.87672, accuracy 0.695312, precision 0.8, recall 0.7741935483870968
2019-03-03T15:05:04.444993: step 69, loss 0.698137, accuracy 0.730469, precision 0.8354430379746836, recall 0.7542857142857143
2019-03-03T15:05:04.668390: step 70, loss 0.897628, accuracy 0.691406, precision 0.7953216374269005, recall 0.7555555555555555
2019-03-03T15:05:04.905756: step 71, loss 0.981944, accuracy 0.714844, precision 0.7837837837837838, recall 0.8146067415730337
2019-03-03T15:05:05.139134: step 72, loss 0.771302, accuracy 0.734375, precision 0.8109756097560976, recall 0.7823529411764706
2019-03-03T15:05:05.368519: step 73, loss 0.974343, accuracy 0.699219, precision 0.7564102564102564, recall 0.7515923566878981
2019-03-03T15:05:05.592946: step 74, loss 1.04746, accuracy 0.664062, precision 0.7261146496815286, recall 0.7261146496815286
2019-03-03T15:05:05.817824: step 75, loss 0.870769, accuracy 0.695312, precision 0.7455621301775148, recall 0.782608695652174
2019-03-03T15:05:06.044727: step 76, loss 1.08191, accuracy 0.679688, precision 0.7457627118644068, recall 0.7810650887573964
2019-03-03T15:05:06.282093: step 77, loss 0.926413, accuracy 0.734375, precision 0.8208092485549133, recall 0.7932960893854749
2019-03-03T15:05:06.511479: step 78, loss 0.808446, accuracy 0.75, precision 0.8253012048192772, recall 0.7965116279069767
2019-03-03T15:05:06.750866: step 79, loss 0.767863, accuracy 0.738281, precision 0.8805031446540881, recall 0.7446808510638298
2019-03-03T15:05:06.990200: step 80, loss 0.975471, accuracy 0.652344, precision 0.7514792899408284, recall 0.7298850574712644
2019-03-03T15:05:07.216374: step 81, loss 0.849178, accuracy 0.75, precision 0.8265895953757225, recall 0.807909604519774
2019-03-03T15:05:07.406370: step 82, loss 0.879842, accuracy 0.717647, precision 0.7280701754385965, recall 0.83
2019-03-03T15:05:07.631792: step 83, loss 0.819017, accuracy 0.71875, precision 0.7950310559006211, recall 0.7664670658682635
2019-03-03T15:05:07.860176: step 84, loss 0.888195, accuracy 0.726562, precision 0.7740112994350282, recall 0.8203592814371258
2019-03-03T15:05:08.093556: step 85, loss 0.87773, accuracy 0.71875, precision 0.782608695652174, recall 0.7730061349693251
2019-03-03T15:05:08.323935: step 86, loss 0.743476, accuracy 0.746094, precision 0.8306010928961749, recall 0.8172043010752689
2019-03-03T15:05:08.555318: step 87, loss 0.92996, accuracy 0.707031, precision 0.80625, recall 0.7456647398843931
2019-03-03T15:05:08.788696: step 88, loss 0.8071, accuracy 0.722656, precision 0.8156424581005587, recall 0.7934782608695652
2019-03-03T15:05:09.020591: step 89, loss 0.744166, accuracy 0.746094, precision 0.8235294117647058, recall 0.8
2019-03-03T15:05:09.251490: step 90, loss 0.815946, accuracy 0.753906, precision 0.8670520231213873, recall 0.7894736842105263
2019-03-03T15:05:09.478881: step 91, loss 0.691657, accuracy 0.730469, precision 0.8579881656804734, recall 0.7631578947368421
2019-03-03T15:05:09.706271: step 92, loss 0.650598, accuracy 0.734375, precision 0.7627118644067796, recall 0.8385093167701864
2019-03-03T15:05:09.938683: step 93, loss 0.725965, accuracy 0.746094, precision 0.779874213836478, recall 0.8051948051948052
2019-03-03T15:05:10.164055: step 94, loss 0.770338, accuracy 0.734375, precision 0.8050314465408805, recall 0.7757575757575758
2019-03-03T15:05:10.394442: step 95, loss 0.614498, accuracy 0.769531, precision 0.8197674418604651, recall 0.834319526627219
2019-03-03T15:05:10.628847: step 96, loss 0.657945, accuracy 0.757812, precision 0.813953488372093, recall 0.8235294117647058
2019-03-03T15:05:10.861247: step 97, loss 0.695195, accuracy 0.75, precision 0.8353658536585366, recall 0.7873563218390804
2019-03-03T15:05:11.084641: step 98, loss 0.623154, accuracy 0.734375, precision 0.793939393939394, recall 0.793939393939394
2019-03-03T15:05:11.315533: step 99, loss 0.768665, accuracy 0.734375, precision 0.8072289156626506, recall 0.788235294117647
2019-03-03T15:05:11.542949: step 100, loss 0.699102, accuracy 0.730469, precision 0.8205128205128205, recall 0.757396449704142
2019-03-03T15:05:11.775303: step 101, loss 0.822823, accuracy 0.734375, precision 0.7878787878787878, recall 0.7975460122699386
2019-03-03T15:05:11.996715: step 102, loss 0.678196, accuracy 0.75, precision 0.8187134502923976, recall 0.8092485549132948
2019-03-03T15:05:12.226665: step 103, loss 0.724562, accuracy 0.734375, precision 0.8343558282208589, recall 0.768361581920904
2019-03-03T15:05:12.467022: step 104, loss 0.67142, accuracy 0.769531, precision 0.8461538461538461, recall 0.8125
2019-03-03T15:05:12.694414: step 105, loss 0.64481, accuracy 0.742188, precision 0.8023255813953488, recall 0.8117647058823529
2019-03-03T15:05:12.933802: step 106, loss 0.662494, accuracy 0.753906, precision 0.7898089171974523, recall 0.8051948051948052
2019-03-03T15:05:13.164160: step 107, loss 0.666129, accuracy 0.773438, precision 0.8295454545454546, recall 0.8390804597701149
2019-03-03T15:05:13.400554: step 108, loss 0.667597, accuracy 0.800781, precision 0.8502994011976048, recall 0.8452380952380952
2019-03-03T15:05:13.626930: step 109, loss 0.736889, accuracy 0.703125, precision 0.7771428571428571, recall 0.7861271676300579
2019-03-03T15:05:13.849421: step 110, loss 0.739141, accuracy 0.746094, precision 0.8466257668711656, recall 0.7752808988764045
2019-03-03T15:05:14.081843: step 111, loss 0.757969, accuracy 0.710938, precision 0.7724550898203593, recall 0.7818181818181819
2019-03-03T15:05:14.314193: step 112, loss 0.658312, accuracy 0.726562, precision 0.7810650887573964, recall 0.8
2019-03-03T15:05:14.545574: step 113, loss 0.860692, accuracy 0.734375, precision 0.8313253012048193, recall 0.7752808988764045
2019-03-03T15:05:14.774961: step 114, loss 0.62053, accuracy 0.816406, precision 0.8971428571428571, recall 0.8440860215053764
2019-03-03T15:05:15.009334: step 115, loss 0.677544, accuracy 0.714844, precision 0.7705882352941177, recall 0.793939393939394
2019-03-03T15:05:15.237726: step 116, loss 0.607727, accuracy 0.765625, precision 0.8418079096045198, recall 0.8232044198895028
2019-03-03T15:05:15.470104: step 117, loss 0.665341, accuracy 0.753906, precision 0.8488372093023255, recall 0.7978142076502732
2019-03-03T15:05:15.689518: step 118, loss 0.646912, accuracy 0.765625, precision 0.8490566037735849, recall 0.7894736842105263
2019-03-03T15:05:15.913918: step 119, loss 0.644684, accuracy 0.765625, precision 0.8520710059171598, recall 0.8044692737430168
2019-03-03T15:05:16.137320: step 120, loss 0.522545, accuracy 0.792969, precision 0.8235294117647058, recall 0.8588957055214724
2019-03-03T15:05:16.362721: step 121, loss 0.647105, accuracy 0.769531, precision 0.8514285714285714, recall 0.8186813186813187
2019-03-03T15:05:16.590621: step 122, loss 0.778587, accuracy 0.71875, precision 0.8125, recall 0.7558139534883721
2019-03-03T15:05:16.779630: step 123, loss 0.655755, accuracy 0.752941, precision 0.8035714285714286, recall 0.8181818181818182
2019-03-03T15:05:17.028964: step 124, loss 0.525055, accuracy 0.808594, precision 0.875, recall 0.84
2019-03-03T15:05:17.271342: step 125, loss 0.362864, accuracy 0.835938, precision 0.925, recall 0.8314606741573034
2019-03-03T15:05:17.501726: step 126, loss 0.475915, accuracy 0.808594, precision 0.8114285714285714, recall 0.8987341772151899
2019-03-03T15:05:17.734105: step 127, loss 0.619804, accuracy 0.765625, precision 0.823170731707317, recall 0.8132530120481928
2019-03-03T15:05:17.962492: step 128, loss 0.615722, accuracy 0.757812, precision 0.7961783439490446, recall 0.8064516129032258
2019-03-03T15:05:18.197395: step 129, loss 0.509827, accuracy 0.792969, precision 0.8562874251497006, recall 0.8313953488372093
2019-03-03T15:05:18.425784: step 130, loss 0.55807, accuracy 0.769531, precision 0.8392857142857143, recall 0.815028901734104
2019-03-03T15:05:18.656151: step 131, loss 0.551971, accuracy 0.8125, precision 0.8928571428571429, recall 0.8333333333333334
2019-03-03T15:05:18.897506: step 132, loss 0.645812, accuracy 0.734375, precision 0.8024691358024691, recall 0.7831325301204819
2019-03-03T15:05:19.128890: step 133, loss 0.569384, accuracy 0.761719, precision 0.85, recall 0.7861271676300579
2019-03-03T15:05:19.364259: step 134, loss 0.5384, accuracy 0.800781, precision 0.8636363636363636, recall 0.8491620111731844
2019-03-03T15:05:19.599142: step 135, loss 0.563109, accuracy 0.773438, precision 0.8238636363636364, recall 0.8430232558139535
2019-03-03T15:05:19.877400: step 136, loss 0.588518, accuracy 0.769531, precision 0.8588957055214724, recall 0.7954545454545454
2019-03-03T15:05:20.399571: step 137, loss 0.453635, accuracy 0.824219, precision 0.9011627906976745, recall 0.8469945355191257
2019-03-03T15:05:21.407451: step 138, loss 0.508884, accuracy 0.792969, precision 0.8426966292134831, recall 0.8571428571428571
2019-03-03T15:05:22.341857: step 139, loss 0.549969, accuracy 0.792969, precision 0.8514285714285714, recall 0.8465909090909091
2019-03-03T15:05:23.221835: step 140, loss 0.440313, accuracy 0.796875, precision 0.8604651162790697, recall 0.8409090909090909
2019-03-03T15:05:23.848113: step 141, loss 0.446877, accuracy 0.808594, precision 0.8555555555555555, recall 0.8700564971751412
2019-03-03T15:05:24.649752: step 142, loss 0.540709, accuracy 0.78125, precision 0.8587570621468926, recall 0.8306010928961749
2019-03-03T15:05:25.308035: step 143, loss 0.494998, accuracy 0.796875, precision 0.8715083798882681, recall 0.8432432432432433
2019-03-03T15:05:25.914804: step 144, loss 0.615889, accuracy 0.773438, precision 0.8869047619047619, recall 0.7925531914893617
2019-03-03T15:05:26.531030: step 145, loss 0.623749, accuracy 0.777344, precision 0.8597560975609756, recall 0.8057142857142857
2019-03-03T15:05:27.034199: step 146, loss 0.532804, accuracy 0.777344, precision 0.84, recall 0.8352272727272727
2019-03-03T15:05:27.498991: step 147, loss 0.378985, accuracy 0.851562, precision 0.9184782608695652, recall 0.8802083333333334
2019-03-03T15:05:27.950645: step 148, loss 0.63466, accuracy 0.722656, precision 0.8343558282208589, recall 0.7555555555555555
2019-03-03T15:05:28.428399: step 149, loss 0.513469, accuracy 0.789062, precision 0.8488372093023255, recall 0.8390804597701149
2019-03-03T15:05:28.945048: step 150, loss 0.476487, accuracy 0.8125, precision 0.8957055214723927, recall 0.8248587570621468
2019-03-03T15:05:29.412612: step 151, loss 0.58573, accuracy 0.789062, precision 0.8580645161290322, recall 0.806060606060606
2019-03-03T15:05:29.850980: step 152, loss 0.597699, accuracy 0.746094, precision 0.7531645569620253, recall 0.8206896551724138
2019-03-03T15:05:30.265016: step 153, loss 0.464894, accuracy 0.835938, precision 0.86, recall 0.86
2019-03-03T15:05:30.687404: step 154, loss 0.576634, accuracy 0.753906, precision 0.7678571428571429, recall 0.8431372549019608
2019-03-03T15:05:31.125032: step 155, loss 0.518923, accuracy 0.773438, precision 0.7734806629834254, recall 0.89171974522293
2019-03-03T15:05:31.515018: step 156, loss 0.570709, accuracy 0.78125, precision 0.8475609756097561, recall 0.8176470588235294
2019-03-03T15:05:31.907968: step 157, loss 0.543278, accuracy 0.789062, precision 0.8841463414634146, recall 0.8055555555555556
2019-03-03T15:05:32.272458: step 158, loss 0.593544, accuracy 0.773438, precision 0.8881987577639752, recall 0.7814207650273224
2019-03-03T15:05:32.650411: step 159, loss 0.39273, accuracy 0.84375, precision 0.9117647058823529, recall 0.8611111111111112
2019-03-03T15:05:33.046905: step 160, loss 0.51689, accuracy 0.800781, precision 0.9461077844311377, recall 0.79
2019-03-03T15:05:33.420868: step 161, loss 0.427795, accuracy 0.796875, precision 0.8690476190476191, recall 0.8295454545454546
2019-03-03T15:05:33.796773: step 162, loss 0.38472, accuracy 0.820312, precision 0.8531073446327684, recall 0.8830409356725146
2019-03-03T15:05:34.149906: step 163, loss 0.487333, accuracy 0.773438, precision 0.8353658536585366, recall 0.8154761904761905
2019-03-03T15:05:34.429159: step 164, loss 0.626877, accuracy 0.758824, precision 0.8165137614678899, recall 0.8090909090909091
2019-03-03T15:05:34.783208: step 165, loss 0.514539, accuracy 0.800781, precision 0.84375, recall 0.8385093167701864
2019-03-03T15:05:35.141256: step 166, loss 0.407488, accuracy 0.839844, precision 0.8682634730538922, recall 0.8841463414634146
2019-03-03T15:05:35.483351: step 167, loss 0.488195, accuracy 0.777344, precision 0.7888198757763976, recall 0.8466666666666667
2019-03-03T15:05:35.828957: step 168, loss 0.50573, accuracy 0.789062, precision 0.8198757763975155, recall 0.8407643312101911
2019-03-03T15:05:36.165096: step 169, loss 0.502478, accuracy 0.785156, precision 0.8313953488372093, recall 0.8461538461538461
2019-03-03T15:05:36.493737: step 170, loss 0.423051, accuracy 0.820312, precision 0.9055555555555556, recall 0.8489583333333334
2019-03-03T15:05:36.850771: step 171, loss 0.47216, accuracy 0.792969, precision 0.8786127167630058, recall 0.8260869565217391
2019-03-03T15:05:37.194850: step 172, loss 0.458708, accuracy 0.832031, precision 0.9075144508670521, recall 0.8532608695652174
2019-03-03T15:05:37.518022: step 173, loss 0.445602, accuracy 0.835938, precision 0.9239130434782609, recall 0.8585858585858586
2019-03-03T15:05:37.842125: step 174, loss 0.394646, accuracy 0.839844, precision 0.9135802469135802, recall 0.8457142857142858
2019-03-03T15:05:38.165785: step 175, loss 0.450145, accuracy 0.777344, precision 0.8695652173913043, recall 0.7954545454545454
2019-03-03T15:05:38.486926: step 176, loss 0.452533, accuracy 0.785156, precision 0.8488372093023255, recall 0.8342857142857143
2019-03-03T15:05:38.822030: step 177, loss 0.558305, accuracy 0.757812, precision 0.8333333333333334, recall 0.7831325301204819
2019-03-03T15:05:39.142173: step 178, loss 0.521364, accuracy 0.789062, precision 0.8344370860927153, recall 0.8129032258064516
2019-03-03T15:05:39.452188: step 179, loss 0.510284, accuracy 0.773438, precision 0.783625730994152, recall 0.864516129032258
2019-03-03T15:05:39.755393: step 180, loss 0.392654, accuracy 0.816406, precision 0.8166666666666667, recall 0.9130434782608695
2019-03-03T15:05:40.053585: step 181, loss 0.462696, accuracy 0.792969, precision 0.8248587570621468, recall 0.8690476190476191
2019-03-03T15:05:40.352808: step 182, loss 0.534245, accuracy 0.789062, precision 0.8392857142857143, recall 0.8392857142857143
2019-03-03T15:05:40.667475: step 183, loss 0.401655, accuracy 0.824219, precision 0.9090909090909091, recall 0.8333333333333334
2019-03-03T15:05:40.987653: step 184, loss 0.597968, accuracy 0.769531, precision 0.8682634730538922, recall 0.7967032967032966
2019-03-03T15:05:41.293805: step 185, loss 0.36531, accuracy 0.828125, precision 0.9064327485380117, recall 0.8469945355191257
2019-03-03T15:05:41.603985: step 186, loss 0.44144, accuracy 0.824219, precision 0.927710843373494, recall 0.8235294117647058
2019-03-03T15:05:41.901719: step 187, loss 0.553293, accuracy 0.777344, precision 0.8606060606060606, recall 0.8068181818181818
2019-03-03T15:05:42.204908: step 188, loss 0.449681, accuracy 0.816406, precision 0.8816568047337278, recall 0.8465909090909091
2019-03-03T15:05:42.492140: step 189, loss 0.453846, accuracy 0.824219, precision 0.8538011695906432, recall 0.8795180722891566
2019-03-03T15:05:42.797323: step 190, loss 0.419587, accuracy 0.804688, precision 0.8313953488372093, recall 0.8719512195121951
2019-03-03T15:05:43.085554: step 191, loss 0.490476, accuracy 0.792969, precision 0.8488372093023255, recall 0.8439306358381503
2019-03-03T15:05:43.388743: step 192, loss 0.427712, accuracy 0.792969, precision 0.7828947368421053, recall 0.8561151079136691
2019-03-03T15:05:43.685491: step 193, loss 0.542832, accuracy 0.8125, precision 0.8580645161290322, recall 0.8364779874213837
2019-03-03T15:05:43.973724: step 194, loss 0.475428, accuracy 0.777344, precision 0.8260869565217391, recall 0.8587570621468926
2019-03-03T15:05:44.263921: step 195, loss 0.439965, accuracy 0.816406, precision 0.8757763975155279, recall 0.8392857142857143
2019-03-03T15:05:44.551729: step 196, loss 0.421382, accuracy 0.859375, precision 0.9421965317919075, recall 0.8624338624338624
2019-03-03T15:05:44.844459: step 197, loss 0.402372, accuracy 0.835938, precision 0.8875, recall 0.8554216867469879
2019-03-03T15:05:45.127666: step 198, loss 0.428982, accuracy 0.78125, precision 0.8609625668449198, recall 0.8429319371727748
2019-03-03T15:05:45.415900: step 199, loss 0.407917, accuracy 0.851562, precision 0.9080459770114943, recall 0.8777777777777778
2019-03-03T15:05:45.686176: step 200, loss 0.41929, accuracy 0.816406, precision 0.8809523809523809, recall 0.8457142857142858
2019-03-03T15:05:45.955980: step 201, loss 0.463785, accuracy 0.816406, precision 0.9190751445086706, recall 0.828125
2019-03-03T15:05:46.227364: step 202, loss 0.452716, accuracy 0.808594, precision 0.8875739644970414, recall 0.8333333333333334
2019-03-03T15:05:46.504151: step 203, loss 0.484421, accuracy 0.796875, precision 0.8795180722891566, recall 0.8202247191011236
2019-03-03T15:05:46.798363: step 204, loss 0.453517, accuracy 0.820312, precision 0.9036144578313253, recall 0.8333333333333334
2019-03-03T15:05:47.019773: step 205, loss 0.480682, accuracy 0.8, precision 0.8130841121495327, recall 0.8613861386138614
2019-03-03T15:05:47.296033: step 206, loss 0.358662, accuracy 0.84375, precision 0.8833333333333333, recall 0.8932584269662921
2019-03-03T15:05:47.563356: step 207, loss 0.333457, accuracy 0.878906, precision 0.9192546583850931, recall 0.891566265060241
2019-03-03T15:05:47.832604: step 208, loss 0.445028, accuracy 0.777344, precision 0.7987012987012987, recall 0.825503355704698
2019-03-03T15:05:48.102393: step 209, loss 0.424901, accuracy 0.816406, precision 0.8520710059171598, recall 0.8674698795180723
2019-03-03T15:05:48.371675: step 210, loss 0.339988, accuracy 0.871094, precision 0.9050279329608939, recall 0.9101123595505618
2019-03-03T15:05:48.647466: step 211, loss 0.381345, accuracy 0.832031, precision 0.903954802259887, recall 0.8602150537634409
2019-03-03T15:05:48.948661: step 212, loss 0.424025, accuracy 0.816406, precision 0.9329268292682927, recall 0.8095238095238095
2019-03-03T15:05:49.213952: step 213, loss 0.44214, accuracy 0.851562, precision 0.9634146341463414, recall 0.8315789473684211
2019-03-03T15:05:49.479762: step 214, loss 0.329478, accuracy 0.875, precision 0.9405405405405406, recall 0.8923076923076924
2019-03-03T15:05:49.740573: step 215, loss 0.409458, accuracy 0.832031, precision 0.8959537572254336, recall 0.8611111111111112
2019-03-03T15:05:50.008889: step 216, loss 0.368571, accuracy 0.816406, precision 0.8554216867469879, recall 0.8606060606060606
2019-03-03T15:05:50.268161: step 217, loss 0.410268, accuracy 0.832031, precision 0.8881987577639752, recall 0.8511904761904762
2019-03-03T15:05:50.527502: step 218, loss 0.489855, accuracy 0.800781, precision 0.8523489932885906, recall 0.8141025641025641
2019-03-03T15:05:50.838160: step 219, loss 0.519847, accuracy 0.800781, precision 0.8057142857142857, recall 0.8924050632911392
2019-03-03T15:05:51.107954: step 220, loss 0.367456, accuracy 0.832031, precision 0.8383233532934131, recall 0.8974358974358975
2019-03-03T15:05:51.362274: step 221, loss 0.448226, accuracy 0.808594, precision 0.8627450980392157, recall 0.825
2019-03-03T15:05:51.617593: step 222, loss 0.385761, accuracy 0.820312, precision 0.844311377245509, recall 0.8757763975155279
2019-03-03T15:05:51.895848: step 223, loss 0.326508, accuracy 0.867188, precision 0.9310344827586207, recall 0.8804347826086957
2019-03-03T15:05:52.150169: step 224, loss 0.383093, accuracy 0.816406, precision 0.8820224719101124, recall 0.8579234972677595
2019-03-03T15:05:52.403512: step 225, loss 0.404187, accuracy 0.835938, precision 0.8908045977011494, recall 0.8707865168539326
2019-03-03T15:05:52.668808: step 226, loss 0.329315, accuracy 0.855469, precision 0.9415204678362573, recall 0.8563829787234043
2019-03-03T15:05:52.953084: step 227, loss 0.301567, accuracy 0.875, precision 0.943502824858757, recall 0.8835978835978836
2019-03-03T15:05:53.206373: step 228, loss 0.370833, accuracy 0.851562, precision 0.936046511627907, recall 0.8563829787234043
2019-03-03T15:05:53.461697: step 229, loss 0.320998, accuracy 0.855469, precision 0.8988095238095238, recall 0.8830409356725146
2019-03-03T15:05:53.727497: step 230, loss 0.419294, accuracy 0.8125, precision 0.8461538461538461, recall 0.8850574712643678
2019-03-03T15:05:53.985841: step 231, loss 0.433144, accuracy 0.800781, precision 0.845679012345679, recall 0.8404907975460123
2019-03-03T15:05:54.243119: step 232, loss 0.389788, accuracy 0.84375, precision 0.8982035928143712, recall 0.8670520231213873
2019-03-03T15:05:54.495444: step 233, loss 0.419121, accuracy 0.832031, precision 0.9085714285714286, recall 0.8548387096774194
2019-03-03T15:05:54.769710: step 234, loss 0.337268, accuracy 0.851562, precision 0.9171597633136095, recall 0.8659217877094972
2019-03-03T15:05:55.029016: step 235, loss 0.407343, accuracy 0.816406, precision 0.8571428571428571, recall 0.8622754491017964
2019-03-03T15:05:55.279788: step 236, loss 0.348636, accuracy 0.867188, precision 0.9090909090909091, recall 0.8875739644970414
2019-03-03T15:05:55.533626: step 237, loss 0.449965, accuracy 0.8125, precision 0.8452380952380952, recall 0.8658536585365854
2019-03-03T15:05:55.790940: step 238, loss 0.341531, accuracy 0.867188, precision 0.9190751445086706, recall 0.888268156424581
2019-03-03T15:05:56.042298: step 239, loss 0.383977, accuracy 0.804688, precision 0.8571428571428571, recall 0.825
2019-03-03T15:05:56.294627: step 240, loss 0.402545, accuracy 0.816406, precision 0.8670520231213873, recall 0.8620689655172413
2019-03-03T15:05:56.557924: step 241, loss 0.480277, accuracy 0.800781, precision 0.893491124260355, recall 0.8206521739130435
2019-03-03T15:05:56.824186: step 242, loss 0.347386, accuracy 0.851562, precision 0.9230769230769231, recall 0.861878453038674
2019-03-03T15:05:57.073518: step 243, loss 0.437515, accuracy 0.789062, precision 0.8552631578947368, recall 0.8024691358024691
2019-03-03T15:05:57.319866: step 244, loss 0.435792, accuracy 0.816406, precision 0.847682119205298, recall 0.8421052631578947
2019-03-03T15:05:57.566200: step 245, loss 0.343945, accuracy 0.84375, precision 0.8628571428571429, recall 0.9041916167664671
2019-03-03T15:05:57.772649: step 246, loss 0.360817, accuracy 0.841176, precision 0.8392857142857143, recall 0.912621359223301
2019-03-03T15:05:58.035195: step 247, loss 0.378281, accuracy 0.828125, precision 0.8695652173913043, recall 0.8588957055214724
2019-03-03T15:05:58.280056: step 248, loss 0.387295, accuracy 0.835938, precision 0.8901734104046243, recall 0.8700564971751412
2019-03-03T15:05:58.533414: step 249, loss 0.385629, accuracy 0.835938, precision 0.8941176470588236, recall 0.8636363636363636
2019-03-03T15:05:58.853524: step 250, loss 0.358336, accuracy 0.851562, precision 0.8914285714285715, recall 0.8914285714285715

Evaluation:
[[719  73]
 [139 225]]
2019-03-03T15:05:59.182562: step 250, loss 0.41852, accuracy 0.816609, precision 0.9078282828282829, recall 0.837995337995338

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551647087\checkpoints\model-250

2019-03-03T15:05:59.889320: step 251, loss 0.322513, accuracy 0.863281, precision 0.9235294117647059, recall 0.8770949720670391
2019-03-03T15:06:00.141644: step 252, loss 0.323314, accuracy 0.859375, precision 0.91875, recall 0.8647058823529412
2019-03-03T15:06:00.413916: step 253, loss 0.295562, accuracy 0.863281, precision 0.925, recall 0.8654970760233918
2019-03-03T15:06:00.719102: step 254, loss 0.446323, accuracy 0.820312, precision 0.8633540372670807, recall 0.852760736196319
2019-03-03T15:06:00.993421: step 255, loss 0.343254, accuracy 0.851562, precision 0.9155844155844156, recall 0.8493975903614458
2019-03-03T15:06:01.253741: step 256, loss 0.280985, accuracy 0.867188, precision 0.8728323699421965, recall 0.9263803680981595
2019-03-03T15:06:01.495096: step 257, loss 0.304542, accuracy 0.878906, precision 0.8876404494382022, recall 0.9349112426035503
2019-03-03T15:06:01.724280: step 258, loss 0.320668, accuracy 0.855469, precision 0.9030303030303031, recall 0.8764705882352941
2019-03-03T15:06:01.962658: step 259, loss 0.371605, accuracy 0.859375, precision 0.8870056497175142, recall 0.9075144508670521
2019-03-03T15:06:02.212503: step 260, loss 0.358205, accuracy 0.839844, precision 0.9358974358974359, recall 0.8248587570621468
2019-03-03T15:06:02.463830: step 261, loss 0.303776, accuracy 0.878906, precision 0.9421965317919075, recall 0.8858695652173914
2019-03-03T15:06:02.737099: step 262, loss 0.37976, accuracy 0.824219, precision 0.8816568047337278, recall 0.8563218390804598
2019-03-03T15:06:02.979525: step 263, loss 0.334301, accuracy 0.851562, precision 0.9090909090909091, recall 0.8791208791208791
2019-03-03T15:06:03.223872: step 264, loss 0.278137, accuracy 0.863281, precision 0.9532163742690059, recall 0.8578947368421053
2019-03-03T15:06:03.474318: step 265, loss 0.32282, accuracy 0.859375, precision 0.9226190476190477, recall 0.8707865168539326
2019-03-03T15:06:03.720171: step 266, loss 0.372876, accuracy 0.839844, precision 0.9156626506024096, recall 0.8491620111731844
2019-03-03T15:06:03.984463: step 267, loss 0.321005, accuracy 0.875, precision 0.9122807017543859, recall 0.9017341040462428
2019-03-03T15:06:04.230806: step 268, loss 0.353177, accuracy 0.871094, precision 0.8682634730538922, recall 0.9294871794871795
2019-03-03T15:06:04.481329: step 269, loss 0.35187, accuracy 0.847656, precision 0.8983050847457628, recall 0.8833333333333333
2019-03-03T15:06:04.739637: step 270, loss 0.317922, accuracy 0.886719, precision 0.9042553191489362, recall 0.9392265193370166
2019-03-03T15:06:04.999406: step 271, loss 0.37021, accuracy 0.851562, precision 0.8848484848484849, recall 0.8848484848484849
2019-03-03T15:06:05.251752: step 272, loss 0.339683, accuracy 0.851562, precision 0.9055555555555556, recall 0.8858695652173914
2019-03-03T15:06:05.500090: step 273, loss 0.39693, accuracy 0.816406, precision 0.8888888888888888, recall 0.8323699421965318
2019-03-03T15:06:05.735461: step 274, loss 0.327709, accuracy 0.867188, precision 0.9487179487179487, recall 0.8505747126436781
2019-03-03T15:06:05.981838: step 275, loss 0.339775, accuracy 0.847656, precision 0.91875, recall 0.8497109826589595
2019-03-03T15:06:06.226147: step 276, loss 0.319037, accuracy 0.875, precision 0.9451219512195121, recall 0.8707865168539326
2019-03-03T15:06:06.473514: step 277, loss 0.319056, accuracy 0.867188, precision 0.903954802259887, recall 0.903954802259887
2019-03-03T15:06:06.725856: step 278, loss 0.370915, accuracy 0.855469, precision 0.83125, recall 0.9300699300699301
2019-03-03T15:06:06.970202: step 279, loss 0.400787, accuracy 0.808594, precision 0.8518518518518519, recall 0.8466257668711656
2019-03-03T15:06:07.212554: step 280, loss 0.307585, accuracy 0.863281, precision 0.9096385542168675, recall 0.8830409356725146
2019-03-03T15:06:07.451915: step 281, loss 0.3295, accuracy 0.863281, precision 0.8870967741935484, recall 0.9217877094972067
2019-03-03T15:06:07.691288: step 282, loss 0.344611, accuracy 0.847656, precision 0.9202453987730062, recall 0.8522727272727273
2019-03-03T15:06:07.940626: step 283, loss 0.289852, accuracy 0.863281, precision 0.8902439024390244, recall 0.8957055214723927
2019-03-03T15:06:08.184010: step 284, loss 0.380638, accuracy 0.84375, precision 0.9349112426035503, recall 0.8449197860962567
2019-03-03T15:06:08.425366: step 285, loss 0.38508, accuracy 0.84375, precision 0.896969696969697, recall 0.8654970760233918
2019-03-03T15:06:08.677691: step 286, loss 0.332467, accuracy 0.859375, precision 0.9190751445086706, recall 0.8784530386740331
2019-03-03T15:06:08.884140: step 287, loss 0.305586, accuracy 0.888235, precision 0.9279279279279279, recall 0.9035087719298246
2019-03-03T15:06:09.140453: step 288, loss 0.367066, accuracy 0.824219, precision 0.8518518518518519, recall 0.8679245283018868
2019-03-03T15:06:09.378339: step 289, loss 0.259898, accuracy 0.878906, precision 0.9120879120879121, recall 0.9171270718232044
2019-03-03T15:06:09.613768: step 290, loss 0.334008, accuracy 0.867188, precision 0.9161676646706587, recall 0.884393063583815
2019-03-03T15:06:09.853127: step 291, loss 0.313635, accuracy 0.863281, precision 0.8876404494382022, recall 0.9132947976878613
2019-03-03T15:06:10.087500: step 292, loss 0.267709, accuracy 0.882812, precision 0.9006211180124224, recall 0.9119496855345912
2019-03-03T15:06:10.313896: step 293, loss 0.32416, accuracy 0.859375, precision 0.906832298136646, recall 0.874251497005988
2019-03-03T15:06:10.546274: step 294, loss 0.291285, accuracy 0.878906, precision 0.927710843373494, recall 0.8901734104046243
2019-03-03T15:06:10.789164: step 295, loss 0.215564, accuracy 0.90625, precision 0.9473684210526315, recall 0.9152542372881356
2019-03-03T15:06:11.026041: step 296, loss 0.302534, accuracy 0.890625, precision 0.9244186046511628, recall 0.9137931034482759
2019-03-03T15:06:11.256425: step 297, loss 0.365299, accuracy 0.808594, precision 0.8546511627906976, recall 0.8596491228070176
2019-03-03T15:06:11.483818: step 298, loss 0.317963, accuracy 0.859375, precision 0.9, recall 0.8895348837209303
2019-03-03T15:06:11.713205: step 299, loss 0.437373, accuracy 0.828125, precision 0.9367088607594937, recall 0.8131868131868132
2019-03-03T15:06:11.949580: step 300, loss 0.282235, accuracy 0.886719, precision 0.9418604651162791, recall 0.8950276243093923
2019-03-03T15:06:12.180959: step 301, loss 0.319729, accuracy 0.855469, precision 0.950920245398773, recall 0.842391304347826
2019-03-03T15:06:12.402367: step 302, loss 0.28759, accuracy 0.886719, precision 0.9337349397590361, recall 0.8959537572254336
2019-03-03T15:06:12.631806: step 303, loss 0.324181, accuracy 0.851562, precision 0.8633540372670807, recall 0.896774193548387
2019-03-03T15:06:12.876153: step 304, loss 0.335763, accuracy 0.871094, precision 0.8728323699421965, recall 0.9320987654320988
2019-03-03T15:06:13.104543: step 305, loss 0.289915, accuracy 0.875, precision 0.8603351955307262, recall 0.9565217391304348
2019-03-03T15:06:13.335924: step 306, loss 0.248059, accuracy 0.894531, precision 0.9491525423728814, recall 0.9032258064516129
2019-03-03T15:06:13.572292: step 307, loss 0.342078, accuracy 0.863281, precision 0.9457831325301205, recall 0.8579234972677595
2019-03-03T15:06:13.798687: step 308, loss 0.343484, accuracy 0.839844, precision 0.8971428571428571, recall 0.8722222222222222
2019-03-03T15:06:14.032684: step 309, loss 0.344135, accuracy 0.847656, precision 0.906832298136646, recall 0.8588235294117647
2019-03-03T15:06:14.257593: step 310, loss 0.338842, accuracy 0.863281, precision 0.9488636363636364, recall 0.8652849740932642
2019-03-03T15:06:14.482992: step 311, loss 0.290909, accuracy 0.878906, precision 0.9120879120879121, recall 0.9171270718232044
2019-03-03T15:06:14.730331: step 312, loss 0.380867, accuracy 0.835938, precision 0.9060402684563759, recall 0.8282208588957055
2019-03-03T15:06:14.959718: step 313, loss 0.321883, accuracy 0.867188, precision 0.9107142857142857, recall 0.8895348837209303
2019-03-03T15:06:15.187108: step 314, loss 0.274173, accuracy 0.898438, precision 0.96, recall 0.8780487804878049
2019-03-03T15:06:15.413502: step 315, loss 0.337748, accuracy 0.839844, precision 0.8562874251497006, recall 0.89375
2019-03-03T15:06:15.636905: step 316, loss 0.233887, accuracy 0.914062, precision 0.9289617486338798, recall 0.9497206703910615
2019-03-03T15:06:15.867290: step 317, loss 0.355507, accuracy 0.835938, precision 0.8522727272727273, recall 0.9036144578313253
2019-03-03T15:06:16.093685: step 318, loss 0.388245, accuracy 0.839844, precision 0.8742857142857143, recall 0.8895348837209303
2019-03-03T15:06:16.317088: step 319, loss 0.337528, accuracy 0.867188, precision 0.9069767441860465, recall 0.896551724137931
2019-03-03T15:06:16.548469: step 320, loss 0.37534, accuracy 0.832031, precision 0.8994082840236687, recall 0.8539325842696629
2019-03-03T15:06:16.794831: step 321, loss 0.312007, accuracy 0.859375, precision 0.95625, recall 0.8406593406593407
2019-03-03T15:06:17.026322: step 322, loss 0.325116, accuracy 0.867188, precision 0.9166666666666666, recall 0.8850574712643678
2019-03-03T15:06:17.255708: step 323, loss 0.290886, accuracy 0.855469, precision 0.9272727272727272, recall 0.8595505617977528
2019-03-03T15:06:17.480107: step 324, loss 0.324664, accuracy 0.863281, precision 0.9617834394904459, recall 0.8388888888888889
2019-03-03T15:06:17.705505: step 325, loss 0.276727, accuracy 0.898438, precision 0.9308176100628931, recall 0.9079754601226994
2019-03-03T15:06:17.935603: step 326, loss 0.269398, accuracy 0.890625, precision 0.9024390243902439, recall 0.925
2019-03-03T15:06:18.164816: step 327, loss 0.300846, accuracy 0.863281, precision 0.8604651162790697, recall 0.9308176100628931
2019-03-03T15:06:18.346331: step 328, loss 0.356125, accuracy 0.847059, precision 0.8632478632478633, recall 0.9099099099099099
2019-03-03T15:06:18.576764: step 329, loss 0.288083, accuracy 0.890625, precision 0.9085714285714286, recall 0.9298245614035088
2019-03-03T15:06:18.813130: step 330, loss 0.312268, accuracy 0.847656, precision 0.9012345679012346, recall 0.863905325443787
2019-03-03T15:06:19.042516: step 331, loss 0.326167, accuracy 0.859375, precision 0.896551724137931, recall 0.896551724137931
2019-03-03T15:06:19.271902: step 332, loss 0.33582, accuracy 0.871094, precision 0.9299363057324841, recall 0.8690476190476191
2019-03-03T15:06:19.512259: step 333, loss 0.296116, accuracy 0.863281, precision 0.9226190476190477, recall 0.8757062146892656
2019-03-03T15:06:19.735662: step 334, loss 0.244538, accuracy 0.894531, precision 0.9520958083832335, recall 0.8932584269662921
2019-03-03T15:06:19.955587: step 335, loss 0.248589, accuracy 0.902344, precision 0.9371069182389937, recall 0.9085365853658537
2019-03-03T15:06:20.181491: step 336, loss 0.264558, accuracy 0.878906, precision 0.9204545454545454, recall 0.9050279329608939
2019-03-03T15:06:20.412875: step 337, loss 0.295494, accuracy 0.878906, precision 0.922077922077922, recall 0.8819875776397516
2019-03-03T15:06:20.653229: step 338, loss 0.211231, accuracy 0.910156, precision 0.9141104294478528, recall 0.9430379746835443
2019-03-03T15:06:20.887016: step 339, loss 0.269571, accuracy 0.875, precision 0.9038461538461539, recall 0.8924050632911392
2019-03-03T15:06:21.113412: step 340, loss 0.285161, accuracy 0.878906, precision 0.8975903614457831, recall 0.9141104294478528
2019-03-03T15:06:21.340803: step 341, loss 0.285094, accuracy 0.875, precision 0.9171597633136095, recall 0.8959537572254336
2019-03-03T15:06:21.574691: step 342, loss 0.324243, accuracy 0.863281, precision 0.8888888888888888, recall 0.9047619047619048
2019-03-03T15:06:21.793635: step 343, loss 0.267593, accuracy 0.886719, precision 0.9325842696629213, recall 0.907103825136612
2019-03-03T15:06:22.020029: step 344, loss 0.235157, accuracy 0.910156, precision 0.9444444444444444, recall 0.9161676646706587
2019-03-03T15:06:22.245427: step 345, loss 0.223937, accuracy 0.917969, precision 0.9425287356321839, recall 0.9371428571428572
2019-03-03T15:06:22.481795: step 346, loss 0.236911, accuracy 0.902344, precision 0.9367816091954023, recall 0.9209039548022598
2019-03-03T15:06:22.712180: step 347, loss 0.256033, accuracy 0.894531, precision 0.9625668449197861, recall 0.9
2019-03-03T15:06:22.941565: step 348, loss 0.325492, accuracy 0.84375, precision 0.9101123595505618, recall 0.8709677419354839
2019-03-03T15:06:23.165966: step 349, loss 0.286251, accuracy 0.863281, precision 0.9411764705882353, recall 0.8648648648648649
2019-03-03T15:06:23.388591: step 350, loss 0.184069, accuracy 0.929688, precision 0.9675675675675676, recall 0.93717277486911
2019-03-03T15:06:23.621476: step 351, loss 0.345136, accuracy 0.828125, precision 0.8509316770186336, recall 0.8726114649681529
2019-03-03T15:06:23.846873: step 352, loss 0.351988, accuracy 0.859375, precision 0.9161676646706587, recall 0.8742857142857143
2019-03-03T15:06:24.075262: step 353, loss 0.294325, accuracy 0.886719, precision 0.9146341463414634, recall 0.9090909090909091
2019-03-03T15:06:24.299173: step 354, loss 0.276432, accuracy 0.875, precision 0.9096045197740112, recall 0.9096045197740112
2019-03-03T15:06:24.532550: step 355, loss 0.261084, accuracy 0.890625, precision 0.9217877094972067, recall 0.9217877094972067
2019-03-03T15:06:24.781890: step 356, loss 0.225543, accuracy 0.917969, precision 0.9371069182389937, recall 0.93125
2019-03-03T15:06:25.018256: step 357, loss 0.235708, accuracy 0.910156, precision 0.9523809523809523, recall 0.9142857142857143
2019-03-03T15:06:25.250634: step 358, loss 0.304254, accuracy 0.84375, precision 0.896774193548387, recall 0.852760736196319
2019-03-03T15:06:25.485004: step 359, loss 0.296256, accuracy 0.898438, precision 0.9401197604790419, recall 0.9075144508670521
2019-03-03T15:06:25.713393: step 360, loss 0.321811, accuracy 0.863281, precision 0.9221556886227545, recall 0.875
2019-03-03T15:06:25.948781: step 361, loss 0.267037, accuracy 0.894531, precision 0.9518072289156626, recall 0.8926553672316384
2019-03-03T15:06:26.183140: step 362, loss 0.310841, accuracy 0.855469, precision 0.8813559322033898, recall 0.9069767441860465
2019-03-03T15:06:26.419509: step 363, loss 0.324979, accuracy 0.855469, precision 0.8703703703703703, recall 0.8980891719745223
2019-03-03T15:06:26.667843: step 364, loss 0.314903, accuracy 0.855469, precision 0.8975903614457831, recall 0.8816568047337278
2019-03-03T15:06:26.912191: step 365, loss 0.278758, accuracy 0.871094, precision 0.9041916167664671, recall 0.8988095238095238
2019-03-03T15:06:27.148558: step 366, loss 0.285975, accuracy 0.878906, precision 0.9125, recall 0.8957055214723927
2019-03-03T15:06:27.381934: step 367, loss 0.249751, accuracy 0.902344, precision 0.9529411764705882, recall 0.9050279329608939
2019-03-03T15:06:27.615311: step 368, loss 0.263945, accuracy 0.882812, precision 0.9181286549707602, recall 0.9075144508670521
2019-03-03T15:06:27.812291: step 369, loss 0.259685, accuracy 0.882353, precision 0.9122807017543859, recall 0.9122807017543859
2019-03-03T15:06:28.044196: step 370, loss 0.260227, accuracy 0.867188, precision 0.896551724137931, recall 0.9069767441860465
2019-03-03T15:06:28.272585: step 371, loss 0.253012, accuracy 0.898438, precision 0.9529411764705882, recall 0.9
2019-03-03T15:06:28.505962: step 372, loss 0.28912, accuracy 0.871094, precision 0.9375, recall 0.8823529411764706
2019-03-03T15:06:28.756291: step 373, loss 0.273126, accuracy 0.882812, precision 0.9367816091954023, recall 0.8956043956043956
2019-03-03T15:06:28.990666: step 374, loss 0.211175, accuracy 0.886719, precision 0.9310344827586207, recall 0.9050279329608939
2019-03-03T15:06:29.218059: step 375, loss 0.262689, accuracy 0.871094, precision 0.9147727272727273, recall 0.8994413407821229
2019-03-03T15:06:29.454427: step 376, loss 0.292903, accuracy 0.882812, precision 0.9349112426035503, recall 0.8926553672316384
2019-03-03T15:06:29.686806: step 377, loss 0.212784, accuracy 0.917969, precision 0.9273743016759777, recall 0.9540229885057471
2019-03-03T15:06:29.917190: step 378, loss 0.230665, accuracy 0.910156, precision 0.9142857142857143, recall 0.9523809523809523
2019-03-03T15:06:30.145579: step 379, loss 0.296987, accuracy 0.871094, precision 0.9032258064516129, recall 0.8860759493670886
2019-03-03T15:06:30.378955: step 380, loss 0.210031, accuracy 0.925781, precision 0.9397590361445783, recall 0.9454545454545454
2019-03-03T15:06:30.618283: step 381, loss 0.227995, accuracy 0.902344, precision 0.9195402298850575, recall 0.935672514619883
2019-03-03T15:06:30.860763: step 382, loss 0.20404, accuracy 0.902344, precision 0.9431818181818182, recall 0.9171270718232044
2019-03-03T15:06:31.088658: step 383, loss 0.298955, accuracy 0.878906, precision 0.9520958083832335, recall 0.8736263736263736
2019-03-03T15:06:31.323032: step 384, loss 0.228911, accuracy 0.90625, precision 0.9612903225806452, recall 0.8922155688622755
2019-03-03T15:06:31.553443: step 385, loss 0.2874, accuracy 0.878906, precision 0.9176470588235294, recall 0.9017341040462428
2019-03-03T15:06:31.793775: step 386, loss 0.248341, accuracy 0.875, precision 0.9281437125748503, recall 0.8857142857142857
2019-03-03T15:06:32.039118: step 387, loss 0.225234, accuracy 0.902344, precision 0.9390243902439024, recall 0.9112426035502958
2019-03-03T15:06:32.268015: step 388, loss 0.268767, accuracy 0.878906, precision 0.8789808917197452, recall 0.92
2019-03-03T15:06:32.504906: step 389, loss 0.257437, accuracy 0.902344, precision 0.9213483146067416, recall 0.9371428571428572
2019-03-03T15:06:32.765718: step 390, loss 0.246507, accuracy 0.898438, precision 0.9451219512195121, recall 0.9011627906976745
2019-03-03T15:06:32.999095: step 391, loss 0.242823, accuracy 0.886719, precision 0.8953488372093024, recall 0.9333333333333333
2019-03-03T15:06:33.232471: step 392, loss 0.249579, accuracy 0.898438, precision 0.9120879120879121, recall 0.9431818181818182
2019-03-03T15:06:33.469370: step 393, loss 0.241647, accuracy 0.90625, precision 0.9693251533742331, recall 0.8926553672316384
2019-03-03T15:06:33.708730: step 394, loss 0.208566, accuracy 0.914062, precision 0.950920245398773, recall 0.9171597633136095
2019-03-03T15:06:33.940138: step 395, loss 0.217267, accuracy 0.914062, precision 0.9595375722543352, recall 0.9171270718232044
2019-03-03T15:06:34.167511: step 396, loss 0.263771, accuracy 0.890625, precision 0.9404761904761905, recall 0.8977272727272727
2019-03-03T15:06:34.408174: step 397, loss 0.242112, accuracy 0.90625, precision 0.936046511627907, recall 0.9252873563218391
2019-03-03T15:06:34.643543: step 398, loss 0.2458, accuracy 0.882812, precision 0.9263803680981595, recall 0.893491124260355
2019-03-03T15:06:34.878916: step 399, loss 0.254425, accuracy 0.894531, precision 0.9212121212121213, recall 0.9156626506024096
2019-03-03T15:06:35.107304: step 400, loss 0.207973, accuracy 0.898438, precision 0.9101796407185628, recall 0.9325153374233128
2019-03-03T15:06:35.340683: step 401, loss 0.26383, accuracy 0.894531, precision 0.9419354838709677, recall 0.8902439024390244
2019-03-03T15:06:35.590016: step 402, loss 0.242149, accuracy 0.902344, precision 0.927710843373494, recall 0.9221556886227545
2019-03-03T15:06:35.837495: step 403, loss 0.176055, accuracy 0.925781, precision 0.9285714285714286, recall 0.9570552147239264
2019-03-03T15:06:36.065393: step 404, loss 0.224483, accuracy 0.910156, precision 0.9112426035502958, recall 0.9506172839506173
2019-03-03T15:06:36.287800: step 405, loss 0.237155, accuracy 0.910156, precision 0.9418604651162791, recall 0.9257142857142857
2019-03-03T15:06:36.521176: step 406, loss 0.214903, accuracy 0.90625, precision 0.9371069182389937, recall 0.9141104294478528
2019-03-03T15:06:36.760535: step 407, loss 0.304805, accuracy 0.882812, precision 0.9112426035502958, recall 0.9112426035502958
2019-03-03T15:06:37.012161: step 408, loss 0.290098, accuracy 0.863281, precision 0.9426751592356688, recall 0.8505747126436781
2019-03-03T15:06:37.232103: step 409, loss 0.296196, accuracy 0.890625, precision 0.9322033898305084, recall 0.9116022099447514
2019-03-03T15:06:37.414129: step 410, loss 0.264933, accuracy 0.9, precision 0.9215686274509803, recall 0.912621359223301
2019-03-03T15:06:37.649518: step 411, loss 0.238146, accuracy 0.902344, precision 0.9451219512195121, recall 0.9064327485380117
2019-03-03T15:06:37.875913: step 412, loss 0.210514, accuracy 0.90625, precision 0.9207317073170732, recall 0.9320987654320988
2019-03-03T15:06:38.108291: step 413, loss 0.206899, accuracy 0.914062, precision 0.9252873563218391, recall 0.9470588235294117
2019-03-03T15:06:38.328702: step 414, loss 0.230859, accuracy 0.902344, precision 0.9294871794871795, recall 0.9119496855345912
2019-03-03T15:06:38.561081: step 415, loss 0.229287, accuracy 0.910156, precision 0.9433962264150944, recall 0.9146341463414634
2019-03-03T15:06:38.790468: step 416, loss 0.183223, accuracy 0.929688, precision 0.9602272727272727, recall 0.9388888888888889
2019-03-03T15:06:39.032829: step 417, loss 0.24546, accuracy 0.890625, precision 0.9112426035502958, recall 0.9221556886227545
2019-03-03T15:06:39.257219: step 418, loss 0.252636, accuracy 0.875, precision 0.9325153374233128, recall 0.8786127167630058
2019-03-03T15:06:39.491592: step 419, loss 0.207552, accuracy 0.929688, precision 0.9473684210526315, recall 0.9473684210526315
2019-03-03T15:06:39.719983: step 420, loss 0.23403, accuracy 0.894531, precision 0.96, recall 0.8936170212765957
2019-03-03T15:06:39.946376: step 421, loss 0.219376, accuracy 0.886719, precision 0.9005847953216374, recall 0.927710843373494
2019-03-03T15:06:40.178759: step 422, loss 0.23581, accuracy 0.898438, precision 0.9441340782122905, recall 0.9135135135135135
2019-03-03T15:06:40.407260: step 423, loss 0.19291, accuracy 0.917969, precision 0.9316770186335404, recall 0.9375
2019-03-03T15:06:40.643158: step 424, loss 0.230899, accuracy 0.898438, precision 0.930635838150289, recall 0.92
2019-03-03T15:06:40.877531: step 425, loss 0.233263, accuracy 0.914062, precision 0.9573170731707317, recall 0.9127906976744186
2019-03-03T15:06:41.131850: step 426, loss 0.236733, accuracy 0.890625, precision 0.925, recall 0.9024390243902439
2019-03-03T15:06:41.361238: step 427, loss 0.222686, accuracy 0.917969, precision 0.936046511627907, recall 0.9415204678362573
2019-03-03T15:06:41.597611: step 428, loss 0.214128, accuracy 0.90625, precision 0.9213483146067416, recall 0.9425287356321839
2019-03-03T15:06:41.833978: step 429, loss 0.203538, accuracy 0.917969, precision 0.9467455621301775, recall 0.9302325581395349
2019-03-03T15:06:42.060494: step 430, loss 0.220872, accuracy 0.898438, precision 0.9176470588235294, recall 0.9285714285714286
2019-03-03T15:06:42.286904: step 431, loss 0.223997, accuracy 0.90625, precision 0.9506172839506173, recall 0.9058823529411765
2019-03-03T15:06:42.521785: step 432, loss 0.20185, accuracy 0.917969, precision 0.9473684210526315, recall 0.9310344827586207
2019-03-03T15:06:42.767128: step 433, loss 0.2072, accuracy 0.917969, precision 0.9440993788819876, recall 0.926829268292683
2019-03-03T15:06:43.008257: step 434, loss 0.183869, accuracy 0.925781, precision 0.9485714285714286, recall 0.9431818181818182
2019-03-03T15:06:43.247635: step 435, loss 0.214904, accuracy 0.921875, precision 0.9428571428571428, recall 0.9428571428571428
2019-03-03T15:06:43.485996: step 436, loss 0.257109, accuracy 0.914062, precision 0.9411764705882353, recall 0.9302325581395349
2019-03-03T15:06:43.725358: step 437, loss 0.160605, accuracy 0.9375, precision 0.9573170731707317, recall 0.9457831325301205
2019-03-03T15:06:43.963719: step 438, loss 0.237624, accuracy 0.898438, precision 0.9254658385093167, recall 0.9141104294478528
2019-03-03T15:06:44.198093: step 439, loss 0.23728, accuracy 0.910156, precision 0.9415204678362573, recall 0.9252873563218391
2019-03-03T15:06:44.434462: step 440, loss 0.224019, accuracy 0.898438, precision 0.9382716049382716, recall 0.9047619047619048
2019-03-03T15:06:44.683309: step 441, loss 0.185938, accuracy 0.925781, precision 0.9263803680981595, recall 0.9556962025316456
2019-03-03T15:06:44.937540: step 442, loss 0.239193, accuracy 0.902344, precision 0.9230769230769231, recall 0.9285714285714286
2019-03-03T15:06:45.176408: step 443, loss 0.218693, accuracy 0.90625, precision 0.9375, recall 0.9269662921348315
2019-03-03T15:06:45.407806: step 444, loss 0.211118, accuracy 0.914062, precision 0.9281437125748503, recall 0.9393939393939394
2019-03-03T15:06:45.640188: step 445, loss 0.245885, accuracy 0.894531, precision 0.9221556886227545, recall 0.9166666666666666
2019-03-03T15:06:45.879544: step 446, loss 0.235677, accuracy 0.894531, precision 0.9349112426035503, recall 0.9080459770114943
2019-03-03T15:06:46.123895: step 447, loss 0.203949, accuracy 0.921875, precision 0.9540229885057471, recall 0.9325842696629213
2019-03-03T15:06:46.354274: step 448, loss 0.163886, accuracy 0.925781, precision 0.9659090909090909, recall 0.9289617486338798
2019-03-03T15:06:46.583262: step 449, loss 0.227304, accuracy 0.917969, precision 0.9393939393939394, recall 0.9337349397590361
2019-03-03T15:06:46.820416: step 450, loss 0.233888, accuracy 0.902344, precision 0.925, recall 0.9192546583850931
2019-03-03T15:06:47.025890: step 451, loss 0.23054, accuracy 0.905882, precision 0.9741379310344828, recall 0.8968253968253969
2019-03-03T15:06:47.266757: step 452, loss 0.159155, accuracy 0.933594, precision 0.9388888888888889, recall 0.9657142857142857
2019-03-03T15:06:47.497142: step 453, loss 0.223609, accuracy 0.886719, precision 0.9433962264150944, recall 0.8823529411764706
2019-03-03T15:06:47.739492: step 454, loss 0.217157, accuracy 0.902344, precision 0.9257142857142857, recall 0.9310344827586207
2019-03-03T15:06:47.970873: step 455, loss 0.218764, accuracy 0.914062, precision 0.9642857142857143, recall 0.9101123595505618
2019-03-03T15:06:48.212230: step 456, loss 0.170934, accuracy 0.925781, precision 0.9570552147239264, recall 0.9285714285714286
2019-03-03T15:06:48.445459: step 457, loss 0.223392, accuracy 0.902344, precision 0.9, recall 0.9503105590062112
2019-03-03T15:06:48.679344: step 458, loss 0.178104, accuracy 0.929688, precision 0.9333333333333333, recall 0.9655172413793104
2019-03-03T15:06:48.940645: step 459, loss 0.182368, accuracy 0.949219, precision 0.9642857142857143, recall 0.9585798816568047
2019-03-03T15:06:49.180005: step 460, loss 0.157464, accuracy 0.957031, precision 0.9649122807017544, recall 0.9705882352941176
2019-03-03T15:06:49.416889: step 461, loss 0.211592, accuracy 0.921875, precision 0.9707602339181286, recall 0.9171270718232044
2019-03-03T15:06:49.650266: step 462, loss 0.205988, accuracy 0.921875, precision 0.9673202614379085, recall 0.9079754601226994
2019-03-03T15:06:49.885417: step 463, loss 0.151814, accuracy 0.953125, precision 0.9751552795031055, recall 0.9515151515151515
2019-03-03T15:06:50.131270: step 464, loss 0.218268, accuracy 0.917969, precision 0.9152542372881356, recall 0.9642857142857143
2019-03-03T15:06:50.372624: step 465, loss 0.171618, accuracy 0.925781, precision 0.9583333333333334, recall 0.930635838150289
2019-03-03T15:06:50.613980: step 466, loss 0.248381, accuracy 0.914062, precision 0.9269662921348315, recall 0.9482758620689655
2019-03-03T15:06:50.860322: step 467, loss 0.189956, accuracy 0.921875, precision 0.9545454545454546, recall 0.9333333333333333
2019-03-03T15:06:51.118629: step 468, loss 0.239123, accuracy 0.894531, precision 0.9316770186335404, recall 0.9036144578313253
2019-03-03T15:06:51.356991: step 469, loss 0.206673, accuracy 0.914062, precision 0.9257142857142857, recall 0.9473684210526315
2019-03-03T15:06:51.592382: step 470, loss 0.149306, accuracy 0.9375, precision 0.9453551912568307, recall 0.9664804469273743
2019-03-03T15:06:51.830746: step 471, loss 0.147436, accuracy 0.945312, precision 0.9940119760479041, recall 0.9273743016759777
2019-03-03T15:06:52.087060: step 472, loss 0.234622, accuracy 0.917969, precision 0.968944099378882, recall 0.9069767441860465
2019-03-03T15:06:52.327418: step 473, loss 0.206141, accuracy 0.914062, precision 0.9567901234567902, recall 0.9117647058823529
2019-03-03T15:06:52.567775: step 474, loss 0.192926, accuracy 0.917969, precision 0.9447852760736196, recall 0.927710843373494
2019-03-03T15:06:52.803147: step 475, loss 0.216511, accuracy 0.902344, precision 0.9259259259259259, recall 0.9202453987730062
2019-03-03T15:06:53.047243: step 476, loss 0.174751, accuracy 0.929688, precision 0.9235294117647059, recall 0.9691358024691358
2019-03-03T15:06:53.288131: step 477, loss 0.236005, accuracy 0.898438, precision 0.9012345679012346, recall 0.9358974358974359
2019-03-03T15:06:53.546442: step 478, loss 0.196858, accuracy 0.921875, precision 0.9215686274509803, recall 0.9463087248322147
2019-03-03T15:06:53.792783: step 479, loss 0.237913, accuracy 0.902344, precision 0.925, recall 0.9192546583850931
2019-03-03T15:06:54.032147: step 480, loss 0.180808, accuracy 0.910156, precision 0.9386503067484663, recall 0.9216867469879518
2019-03-03T15:06:54.267193: step 481, loss 0.174212, accuracy 0.9375, precision 0.9423076923076923, recall 0.9545454545454546
2019-03-03T15:06:54.516947: step 482, loss 0.270846, accuracy 0.882812, precision 0.9254658385093167, recall 0.8922155688622755
2019-03-03T15:06:54.771286: step 483, loss 0.176797, accuracy 0.921875, precision 0.9473684210526315, recall 0.9364161849710982
2019-03-03T15:06:55.032585: step 484, loss 0.195562, accuracy 0.90625, precision 0.92, recall 0.9415204678362573
2019-03-03T15:06:55.281919: step 485, loss 0.193015, accuracy 0.914062, precision 0.945054945054945, recall 0.9347826086956522
2019-03-03T15:06:55.539233: step 486, loss 0.17309, accuracy 0.925781, precision 0.9608938547486033, recall 0.9347826086956522
2019-03-03T15:06:55.780586: step 487, loss 0.22934, accuracy 0.898438, precision 0.9884393063583815, recall 0.8769230769230769
2019-03-03T15:06:56.020262: step 488, loss 0.237253, accuracy 0.882812, precision 0.9302325581395349, recall 0.898876404494382
2019-03-03T15:06:56.260642: step 489, loss 0.245223, accuracy 0.898438, precision 0.9473684210526315, recall 0.9050279329608939
2019-03-03T15:06:56.503992: step 490, loss 0.193499, accuracy 0.902344, precision 0.9281045751633987, recall 0.9102564102564102
2019-03-03T15:06:56.748340: step 491, loss 0.191349, accuracy 0.917969, precision 0.92, recall 0.9583333333333334
2019-03-03T15:06:56.953791: step 492, loss 0.168084, accuracy 0.923529, precision 0.9210526315789473, recall 0.963302752293578
2019-03-03T15:06:57.200131: step 493, loss 0.145906, accuracy 0.9375, precision 0.9440993788819876, recall 0.9559748427672956
2019-03-03T15:06:57.441485: step 494, loss 0.196115, accuracy 0.914062, precision 0.9090909090909091, recall 0.9459459459459459
2019-03-03T15:06:57.704294: step 495, loss 0.1906, accuracy 0.921875, precision 0.9512195121951219, recall 0.9285714285714286
2019-03-03T15:06:57.966599: step 496, loss 0.203739, accuracy 0.917969, precision 0.9444444444444444, recall 0.9272727272727272
2019-03-03T15:06:58.204473: step 497, loss 0.168295, accuracy 0.941406, precision 0.9593023255813954, recall 0.953757225433526
2019-03-03T15:06:58.444860: step 498, loss 0.179444, accuracy 0.921875, precision 0.9565217391304348, recall 0.9221556886227545
2019-03-03T15:06:58.689208: step 499, loss 0.266032, accuracy 0.894531, precision 0.9588235294117647, recall 0.8907103825136612
2019-03-03T15:06:58.939539: step 500, loss 0.16801, accuracy 0.925781, precision 0.9753086419753086, recall 0.9132947976878613

Evaluation:
[[709  83]
 [ 99 265]]
2019-03-03T15:06:59.108087: step 500, loss 0.409525, accuracy 0.842561, precision 0.8952020202020202, recall 0.8774752475247525

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551647087\checkpoints\model-500


Process finished with exit code 0
