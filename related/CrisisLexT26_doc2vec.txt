"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
Pretrained Embedding: Doc2Vec
Italian: False
Loading data...
11566
Max Document length: 36
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 29102
Train/Dev split: 10410/1156
2019-03-03 12:48:54.314596: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
1WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:475: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551639073

2019-03-03T12:51:17.216764: step 1, loss 4.12742, accuracy 0.65625, precision 0.9060773480662984, recall 0.6978723404255319
2019-03-03T12:51:17.479063: step 2, loss 6.18802, accuracy 0.371094, precision 0.266304347826087, recall 0.6533333333333333
2019-03-03T12:51:17.705466: step 3, loss 3.7178, accuracy 0.546875, precision 0.5606936416184971, recall 0.708029197080292
2019-03-03T12:51:17.945837: step 4, loss 4.06785, accuracy 0.625, precision 0.845679012345679, recall 0.6586538461538461
2019-03-03T12:51:18.155788: step 5, loss 3.50025, accuracy 0.679688, precision 0.9090909090909091, recall 0.7079646017699115
2019-03-03T12:51:18.363233: step 6, loss 2.67679, accuracy 0.707031, precision 0.8888888888888888, recall 0.7441860465116279
2019-03-03T12:51:18.575697: step 7, loss 3.13696, accuracy 0.671875, precision 0.8418079096045198, recall 0.7268292682926829
2019-03-03T12:51:18.799099: step 8, loss 3.00535, accuracy 0.707031, precision 0.863905325443787, recall 0.7373737373737373
2019-03-03T12:51:19.017515: step 9, loss 3.09233, accuracy 0.691406, precision 0.7278481012658228, recall 0.7615894039735099
2019-03-03T12:51:19.238929: step 10, loss 3.85418, accuracy 0.617188, precision 0.5783132530120482, recall 0.7741935483870968
2019-03-03T12:51:19.460337: step 11, loss 2.85393, accuracy 0.6875, precision 0.6586826347305389, recall 0.8270676691729323
2019-03-03T12:51:19.683739: step 12, loss 3.99603, accuracy 0.585938, precision 0.5481927710843374, recall 0.7459016393442623
2019-03-03T12:51:19.942896: step 13, loss 2.40064, accuracy 0.640625, precision 0.6287425149700598, recall 0.7777777777777778
2019-03-03T12:51:20.179264: step 14, loss 2.45615, accuracy 0.667969, precision 0.6629213483146067, recall 0.8251748251748252
2019-03-03T12:51:20.445067: step 15, loss 2.9137, accuracy 0.730469, precision 0.8406593406593407, recall 0.7927461139896373
2019-03-03T12:51:20.709360: step 16, loss 2.59204, accuracy 0.667969, precision 0.8488372093023255, recall 0.7121951219512195
2019-03-03T12:51:21.055434: step 17, loss 3.33696, accuracy 0.691406, precision 0.8742138364779874, recall 0.702020202020202
2019-03-03T12:51:21.359622: step 18, loss 2.93598, accuracy 0.699219, precision 0.8988095238095238, recall 0.7156398104265402
2019-03-03T12:51:21.665803: step 19, loss 2.50184, accuracy 0.691406, precision 0.8780487804878049, recall 0.7093596059113301
2019-03-03T12:51:22.800263: step 20, loss 2.92472, accuracy 0.667969, precision 0.8662420382165605, recall 0.68
2019-03-03T12:51:23.792492: step 21, loss 2.52294, accuracy 0.636719, precision 0.676829268292683, recall 0.7350993377483444
2019-03-03T12:51:24.754787: step 22, loss 2.7138, accuracy 0.597656, precision 0.573170731707317, recall 0.7401574803149606
2019-03-03T12:51:25.844897: step 23, loss 2.40895, accuracy 0.628906, precision 0.5606936416184971, recall 0.8362068965517241
2019-03-03T12:51:26.922936: step 24, loss 2.92773, accuracy 0.609375, precision 0.5125, recall 0.7884615384615384
2019-03-03T12:51:27.996698: step 25, loss 2.00144, accuracy 0.652344, precision 0.6445783132530121, recall 0.781021897810219
2019-03-03T12:51:29.033634: step 26, loss 2.14311, accuracy 0.671875, precision 0.7272727272727273, recall 0.7547169811320755
2019-03-03T12:51:29.995584: step 27, loss 2.51338, accuracy 0.695312, precision 0.8545454545454545, recall 0.7230769230769231
2019-03-03T12:51:30.957740: step 28, loss 2.00256, accuracy 0.695312, precision 0.8816568047337278, recall 0.7198067632850241
2019-03-03T12:51:31.849404: step 29, loss 1.96405, accuracy 0.695312, precision 0.88125, recall 0.705
2019-03-03T12:51:32.850931: step 30, loss 2.09017, accuracy 0.675781, precision 0.813953488372093, recall 0.7329842931937173
2019-03-03T12:51:33.927577: step 31, loss 2.25739, accuracy 0.703125, precision 0.813953488372093, recall 0.7608695652173914
2019-03-03T12:51:34.956432: step 32, loss 2.02316, accuracy 0.679688, precision 0.7586206896551724, recall 0.7674418604651163
2019-03-03T12:51:36.013500: step 33, loss 2.56972, accuracy 0.625, precision 0.7232704402515723, recall 0.688622754491018
2019-03-03T12:51:37.048633: step 34, loss 1.89223, accuracy 0.660156, precision 0.6571428571428571, recall 0.8098591549295775
2019-03-03T12:51:38.038251: step 35, loss 1.89427, accuracy 0.609375, precision 0.5739644970414202, recall 0.776
2019-03-03T12:51:39.040502: step 36, loss 2.01002, accuracy 0.640625, precision 0.6514285714285715, recall 0.7862068965517242
2019-03-03T12:51:40.395545: step 37, loss 2.11414, accuracy 0.683594, precision 0.7727272727272727, recall 0.768361581920904
2019-03-03T12:51:41.367994: step 38, loss 1.51812, accuracy 0.730469, precision 0.8421052631578947, recall 0.7741935483870968
2019-03-03T12:51:42.321146: step 39, loss 1.83777, accuracy 0.730469, precision 0.8579881656804734, recall 0.7631578947368421
2019-03-03T12:51:43.260800: step 40, loss 2.0351, accuracy 0.667969, precision 0.8427672955974843, recall 0.6907216494845361
2019-03-03T12:51:43.726507: step 41, loss 1.92374, accuracy 0.682353, precision 0.853448275862069, recall 0.7279411764705882
2019-03-03T12:51:44.193713: step 42, loss 1.87658, accuracy 0.6875, precision 0.7784431137724551, recall 0.7514450867052023
2019-03-03T12:51:44.535263: step 43, loss 1.40657, accuracy 0.683594, precision 0.7202380952380952, recall 0.7806451612903226
2019-03-03T12:51:44.884351: step 44, loss 1.53503, accuracy 0.703125, precision 0.6871165644171779, recall 0.8175182481751825
2019-03-03T12:51:45.261347: step 45, loss 1.57745, accuracy 0.65625, precision 0.6521739130434783, recall 0.7664233576642335
2019-03-03T12:51:45.626415: step 46, loss 1.75585, accuracy 0.667969, precision 0.7142857142857143, recall 0.7467532467532467
2019-03-03T12:51:45.972741: step 47, loss 1.62941, accuracy 0.6875, precision 0.7371428571428571, recall 0.7914110429447853
2019-03-03T12:51:46.315822: step 48, loss 1.19761, accuracy 0.777344, precision 0.8579545454545454, recall 0.825136612021858
2019-03-03T12:51:46.690158: step 49, loss 1.75645, accuracy 0.714844, precision 0.8424242424242424, recall 0.7473118279569892
2019-03-03T12:51:47.034231: step 50, loss 1.73911, accuracy 0.71875, precision 0.8390804597701149, recall 0.7684210526315789
2019-03-03T12:51:47.368393: step 51, loss 1.60962, accuracy 0.707031, precision 0.847953216374269, recall 0.7474226804123711
2019-03-03T12:51:47.736406: step 52, loss 1.58493, accuracy 0.675781, precision 0.7602339181286549, recall 0.7558139534883721
2019-03-03T12:51:48.096966: step 53, loss 2.02375, accuracy 0.621094, precision 0.7228915662650602, recall 0.7017543859649122
2019-03-03T12:51:48.439041: step 54, loss 1.75726, accuracy 0.671875, precision 0.7514792899408284, recall 0.7514792899408284
2019-03-03T12:51:48.804066: step 55, loss 1.56055, accuracy 0.707031, precision 0.72, recall 0.8289473684210527
2019-03-03T12:51:49.168090: step 56, loss 1.36995, accuracy 0.722656, precision 0.7409638554216867, recall 0.8145695364238411
2019-03-03T12:51:49.523142: step 57, loss 1.62295, accuracy 0.699219, precision 0.75, recall 0.782608695652174
2019-03-03T12:51:49.857249: step 58, loss 1.2765, accuracy 0.679688, precision 0.75, recall 0.7764705882352941
2019-03-03T12:51:50.198337: step 59, loss 1.43456, accuracy 0.683594, precision 0.7976878612716763, recall 0.75
2019-03-03T12:51:50.542445: step 60, loss 1.38567, accuracy 0.738281, precision 0.8846153846153846, recall 0.7379679144385026
2019-03-03T12:51:50.851618: step 61, loss 1.46983, accuracy 0.71875, precision 0.819672131147541, recall 0.7936507936507936
2019-03-03T12:51:51.202681: step 62, loss 1.80108, accuracy 0.71875, precision 0.8546511627906976, recall 0.7577319587628866
2019-03-03T12:51:51.523739: step 63, loss 1.22699, accuracy 0.695312, precision 0.7624309392265194, recall 0.7976878612716763
2019-03-03T12:51:51.833065: step 64, loss 1.27317, accuracy 0.730469, precision 0.7965116279069767, recall 0.8011695906432749
2019-03-03T12:51:52.153210: step 65, loss 1.41787, accuracy 0.6875, precision 0.7531645569620253, recall 0.74375
2019-03-03T12:51:52.439445: step 66, loss 1.07834, accuracy 0.714844, precision 0.7566137566137566, recall 0.8411764705882353
2019-03-03T12:51:52.742634: step 67, loss 1.3557, accuracy 0.730469, precision 0.8057142857142857, recall 0.8011363636363636
2019-03-03T12:51:53.039839: step 68, loss 1.61188, accuracy 0.648438, precision 0.6987179487179487, recall 0.7171052631578947
2019-03-03T12:51:53.333566: step 69, loss 1.1843, accuracy 0.726562, precision 0.7928994082840237, recall 0.7928994082840237
2019-03-03T12:51:53.639718: step 70, loss 1.25162, accuracy 0.683594, precision 0.7588235294117647, recall 0.7633136094674556
2019-03-03T12:51:53.936928: step 71, loss 1.03312, accuracy 0.71875, precision 0.8, recall 0.7619047619047619
2019-03-03T12:51:54.232101: step 72, loss 1.44601, accuracy 0.707031, precision 0.8414634146341463, recall 0.7379679144385026
2019-03-03T12:51:54.515348: step 73, loss 1.28526, accuracy 0.699219, precision 0.7953216374269005, recall 0.7640449438202247
2019-03-03T12:51:54.811066: step 74, loss 1.23781, accuracy 0.667969, precision 0.7604790419161677, recall 0.7383720930232558
2019-03-03T12:51:55.102292: step 75, loss 1.28807, accuracy 0.726562, precision 0.8012820512820513, recall 0.7621951219512195
2019-03-03T12:51:55.400357: step 76, loss 1.16534, accuracy 0.675781, precision 0.7083333333333334, recall 0.7777777777777778
2019-03-03T12:51:55.701554: step 77, loss 0.868607, accuracy 0.796875, precision 0.848314606741573, recall 0.8579545454545454
2019-03-03T12:51:56.007734: step 78, loss 1.10276, accuracy 0.710938, precision 0.7247191011235955, recall 0.8376623376623377
2019-03-03T12:51:56.311092: step 79, loss 1.20156, accuracy 0.75, precision 0.7928994082840237, recall 0.8220858895705522
2019-03-03T12:51:56.620220: step 80, loss 1.26682, accuracy 0.679688, precision 0.8313953488372093, recall 0.7295918367346939
2019-03-03T12:51:56.908508: step 81, loss 1.19325, accuracy 0.734375, precision 0.8703703703703703, recall 0.75
2019-03-03T12:51:57.113927: step 82, loss 1.22616, accuracy 0.758824, precision 0.8425925925925926, recall 0.7913043478260869
2019-03-03T12:51:57.437063: step 83, loss 1.0501, accuracy 0.757812, precision 0.8418079096045198, recall 0.8142076502732241
2019-03-03T12:51:57.765186: step 84, loss 1.01059, accuracy 0.734375, precision 0.7810650887573964, recall 0.8098159509202454
2019-03-03T12:51:58.134200: step 85, loss 1.12307, accuracy 0.683594, precision 0.7012195121951219, recall 0.782312925170068
2019-03-03T12:51:58.472295: step 86, loss 0.859584, accuracy 0.761719, precision 0.821656050955414, recall 0.7962962962962963
2019-03-03T12:51:58.821363: step 87, loss 1.03904, accuracy 0.699219, precision 0.6971428571428572, recall 0.8356164383561644
2019-03-03T12:51:59.181402: step 88, loss 1.05193, accuracy 0.707031, precision 0.7657142857142857, recall 0.7976190476190477
2019-03-03T12:51:59.543432: step 89, loss 0.93438, accuracy 0.75, precision 0.8089887640449438, recall 0.8275862068965517
2019-03-03T12:51:59.969294: step 90, loss 1.17205, accuracy 0.742188, precision 0.8066298342541437, recall 0.8248587570621468
2019-03-03T12:52:00.338308: step 91, loss 1.05395, accuracy 0.738281, precision 0.8716216216216216, recall 0.7288135593220338
2019-03-03T12:52:00.692360: step 92, loss 1.06807, accuracy 0.714844, precision 0.8484848484848485, recall 0.7446808510638298
2019-03-03T12:52:01.111240: step 93, loss 0.750736, accuracy 0.773438, precision 0.8628571428571429, recall 0.8162162162162162
2019-03-03T12:52:01.469284: step 94, loss 0.965562, accuracy 0.714844, precision 0.8070175438596491, recall 0.7752808988764045
2019-03-03T12:52:01.817352: step 95, loss 1.01113, accuracy 0.707031, precision 0.7878787878787878, recall 0.7647058823529411
2019-03-03T12:52:02.148468: step 96, loss 1.11168, accuracy 0.699219, precision 0.7948717948717948, recall 0.7337278106508875
2019-03-03T12:52:02.567348: step 97, loss 0.981925, accuracy 0.71875, precision 0.7654320987654321, recall 0.7848101265822784
2019-03-03T12:52:03.045071: step 98, loss 1.04788, accuracy 0.691406, precision 0.686046511627907, recall 0.8251748251748252
2019-03-03T12:52:03.394136: step 99, loss 0.922684, accuracy 0.730469, precision 0.7383720930232558, recall 0.8410596026490066
2019-03-03T12:52:03.686355: step 100, loss 0.935914, accuracy 0.710938, precision 0.7964071856287425, recall 0.7687861271676301
2019-03-03T12:52:03.974586: step 101, loss 0.97117, accuracy 0.738281, precision 0.823170731707317, recall 0.7803468208092486
2019-03-03T12:52:04.280772: step 102, loss 0.836294, accuracy 0.753906, precision 0.8414634146341463, recall 0.7885714285714286
2019-03-03T12:52:04.585950: step 103, loss 0.822947, accuracy 0.78125, precision 0.8734939759036144, recall 0.8055555555555556
2019-03-03T12:52:04.982889: step 104, loss 0.911238, accuracy 0.707031, precision 0.7901234567901234, recall 0.757396449704142
2019-03-03T12:52:05.335944: step 105, loss 0.99662, accuracy 0.726562, precision 0.7848837209302325, recall 0.8035714285714286
2019-03-03T12:52:05.736873: step 106, loss 0.667211, accuracy 0.792969, precision 0.8469945355191257, recall 0.8611111111111112
2019-03-03T12:52:06.141790: step 107, loss 0.948027, accuracy 0.734375, precision 0.7962962962962963, recall 0.7865853658536586
2019-03-03T12:52:06.490858: step 108, loss 0.889925, accuracy 0.75, precision 0.8284023668639053, recall 0.8
2019-03-03T12:52:06.803023: step 109, loss 0.821453, accuracy 0.738281, precision 0.7734806629834254, recall 0.8433734939759037
2019-03-03T12:52:07.117183: step 110, loss 0.846468, accuracy 0.726562, precision 0.8022598870056498, recall 0.8022598870056498
2019-03-03T12:52:07.422367: step 111, loss 0.818416, accuracy 0.742188, precision 0.838150289017341, recall 0.7923497267759563
2019-03-03T12:52:07.728549: step 112, loss 0.934885, accuracy 0.71875, precision 0.8324022346368715, recall 0.7801047120418848
2019-03-03T12:52:08.016778: step 113, loss 0.697613, accuracy 0.773438, precision 0.8388888888888889, recall 0.8388888888888889
2019-03-03T12:52:08.312987: step 114, loss 0.853374, accuracy 0.734375, precision 0.8430232558139535, recall 0.7795698924731183
2019-03-03T12:52:08.637119: step 115, loss 0.767649, accuracy 0.730469, precision 0.834319526627219, recall 0.7747252747252747
2019-03-03T12:52:08.964244: step 116, loss 0.719903, accuracy 0.765625, precision 0.782608695652174, recall 0.8344370860927153
2019-03-03T12:52:09.284388: step 117, loss 0.756174, accuracy 0.753906, precision 0.8370786516853933, recall 0.8142076502732241
2019-03-03T12:52:09.595555: step 118, loss 0.70911, accuracy 0.71875, precision 0.8035714285714286, recall 0.7758620689655172
2019-03-03T12:52:09.913705: step 119, loss 0.909118, accuracy 0.730469, precision 0.8238993710691824, recall 0.7616279069767442
2019-03-03T12:52:10.259780: step 120, loss 0.874837, accuracy 0.71875, precision 0.8, recall 0.7909604519774012
2019-03-03T12:52:10.627797: step 121, loss 0.68369, accuracy 0.765625, precision 0.8095238095238095, recall 0.8292682926829268
2019-03-03T12:52:11.018752: step 122, loss 0.774438, accuracy 0.71875, precision 0.7806451612903226, recall 0.7610062893081762
2019-03-03T12:52:11.267088: step 123, loss 0.597008, accuracy 0.764706, precision 0.7699115044247787, recall 0.8613861386138614
2019-03-03T12:52:11.627124: step 124, loss 0.815769, accuracy 0.71875, precision 0.791907514450867, recall 0.791907514450867
2019-03-03T12:52:11.958239: step 125, loss 0.737091, accuracy 0.75, precision 0.8095238095238095, recall 0.8095238095238095
2019-03-03T12:52:12.280377: step 126, loss 0.63283, accuracy 0.835938, precision 0.9075144508670521, recall 0.8579234972677595
2019-03-03T12:52:12.579578: step 127, loss 0.879946, accuracy 0.722656, precision 0.8187134502923976, recall 0.7777777777777778
2019-03-03T12:52:12.868804: step 128, loss 0.735555, accuracy 0.773438, precision 0.8529411764705882, recall 0.8146067415730337
2019-03-03T12:52:13.219868: step 129, loss 0.679704, accuracy 0.769531, precision 0.8275862068965517, recall 0.8323699421965318
2019-03-03T12:52:13.558960: step 130, loss 0.831886, accuracy 0.765625, precision 0.8554913294797688, recall 0.8087431693989071
2019-03-03T12:52:13.834223: step 131, loss 0.730376, accuracy 0.777344, precision 0.8607594936708861, recall 0.7953216374269005
2019-03-03T12:52:14.104500: step 132, loss 0.5965, accuracy 0.789062, precision 0.8342857142857143, recall 0.8538011695906432
2019-03-03T12:52:14.393728: step 133, loss 0.656643, accuracy 0.796875, precision 0.815028901734104, recall 0.8757763975155279
2019-03-03T12:52:14.680960: step 134, loss 0.830362, accuracy 0.730469, precision 0.7976878612716763, recall 0.8023255813953488
2019-03-03T12:52:14.959222: step 135, loss 0.678295, accuracy 0.785156, precision 0.8734939759036144, recall 0.8100558659217877
2019-03-03T12:52:15.220517: step 136, loss 0.58308, accuracy 0.777344, precision 0.8284023668639053, recall 0.8333333333333334
2019-03-03T12:52:15.497776: step 137, loss 0.571425, accuracy 0.777344, precision 0.7770700636942676, recall 0.8472222222222222
2019-03-03T12:52:15.832879: step 138, loss 0.712795, accuracy 0.738281, precision 0.7471264367816092, recall 0.8496732026143791
2019-03-03T12:52:16.115379: step 139, loss 0.718046, accuracy 0.738281, precision 0.8012048192771084, recall 0.7964071856287425
2019-03-03T12:52:16.413582: step 140, loss 0.714516, accuracy 0.75, precision 0.7926829268292683, recall 0.8125
2019-03-03T12:52:16.728738: step 141, loss 0.629914, accuracy 0.765625, precision 0.8392857142857143, recall 0.8103448275862069
2019-03-03T12:52:17.054868: step 142, loss 0.708063, accuracy 0.746094, precision 0.8176470588235294, recall 0.8034682080924855
2019-03-03T12:52:17.305728: step 143, loss 0.602817, accuracy 0.757812, precision 0.8579545454545454, recall 0.8031914893617021
2019-03-03T12:52:17.567563: step 144, loss 0.665653, accuracy 0.75, precision 0.8523489932885906, recall 0.7514792899408284
2019-03-03T12:52:17.816413: step 145, loss 0.65975, accuracy 0.75, precision 0.8362573099415205, recall 0.7988826815642458
2019-03-03T12:52:18.082702: step 146, loss 0.717109, accuracy 0.726562, precision 0.7857142857142857, recall 0.7951807228915663
2019-03-03T12:52:18.341156: step 147, loss 0.652598, accuracy 0.765625, precision 0.8103448275862069, recall 0.8392857142857143
2019-03-03T12:52:18.577524: step 148, loss 0.594791, accuracy 0.800781, precision 0.834319526627219, recall 0.8597560975609756
2019-03-03T12:52:18.807913: step 149, loss 0.689069, accuracy 0.746094, precision 0.7818181818181819, recall 0.8164556962025317
2019-03-03T12:52:19.050264: step 150, loss 0.573402, accuracy 0.777344, precision 0.8098159509202454, recall 0.8354430379746836
2019-03-03T12:52:19.304585: step 151, loss 0.665795, accuracy 0.761719, precision 0.8146067415730337, recall 0.838150289017341
2019-03-03T12:52:19.581847: step 152, loss 0.539585, accuracy 0.785156, precision 0.8596491228070176, recall 0.8258426966292135
2019-03-03T12:52:19.842148: step 153, loss 0.584985, accuracy 0.792969, precision 0.903954802259887, recall 0.8163265306122449
2019-03-03T12:52:20.095470: step 154, loss 0.800801, accuracy 0.730469, precision 0.8417721518987342, recall 0.751412429378531
2019-03-03T12:52:20.412134: step 155, loss 0.662734, accuracy 0.78125, precision 0.8727272727272727, recall 0.8044692737430168
2019-03-03T12:52:20.667961: step 156, loss 0.604008, accuracy 0.765625, precision 0.844311377245509, recall 0.8057142857142857
2019-03-03T12:52:20.949751: step 157, loss 0.566911, accuracy 0.796875, precision 0.8023255813953488, recall 0.8846153846153846
2019-03-03T12:52:21.210424: step 158, loss 0.583245, accuracy 0.785156, precision 0.8647058823529412, recall 0.8212290502793296
2019-03-03T12:52:21.469731: step 159, loss 0.789972, accuracy 0.742188, precision 0.7942857142857143, recall 0.8224852071005917
2019-03-03T12:52:21.746994: step 160, loss 0.637222, accuracy 0.757812, precision 0.807909604519774, recall 0.8362573099415205
2019-03-03T12:52:22.030236: step 161, loss 0.623743, accuracy 0.78125, precision 0.8372093023255814, recall 0.8372093023255814
2019-03-03T12:52:22.330943: step 162, loss 0.570894, accuracy 0.785156, precision 0.8390804597701149, recall 0.8439306358381503
2019-03-03T12:52:22.556340: step 163, loss 0.714547, accuracy 0.765625, precision 0.8633540372670807, recall 0.7853107344632768
2019-03-03T12:52:22.729882: step 164, loss 0.762238, accuracy 0.782353, precision 0.875, recall 0.8099173553719008
2019-03-03T12:52:22.992056: step 165, loss 0.560698, accuracy 0.8125, precision 0.9107142857142857, recall 0.8225806451612904
2019-03-03T12:52:23.242666: step 166, loss 0.602978, accuracy 0.792969, precision 0.9047619047619048, recall 0.8042328042328042
2019-03-03T12:52:23.521432: step 167, loss 0.490143, accuracy 0.820312, precision 0.8823529411764706, recall 0.8522727272727273
2019-03-03T12:52:23.791709: step 168, loss 0.556176, accuracy 0.730469, precision 0.7734806629834254, recall 0.8333333333333334
2019-03-03T12:52:24.046853: step 169, loss 0.6447, accuracy 0.753906, precision 0.8011695906432749, recall 0.8253012048192772
2019-03-03T12:52:24.327097: step 170, loss 0.525975, accuracy 0.804688, precision 0.8295454545454546, recall 0.8795180722891566
2019-03-03T12:52:24.582415: step 171, loss 0.760459, accuracy 0.722656, precision 0.8280254777070064, recall 0.7471264367816092
2019-03-03T12:52:24.852692: step 172, loss 0.561232, accuracy 0.757812, precision 0.8430232558139535, recall 0.8055555555555556
2019-03-03T12:52:25.114991: step 173, loss 0.419419, accuracy 0.816406, precision 0.8674033149171271, recall 0.8722222222222222
2019-03-03T12:52:25.382277: step 174, loss 0.653304, accuracy 0.773438, precision 0.88, recall 0.7674418604651163
2019-03-03T12:52:25.642580: step 175, loss 0.471358, accuracy 0.820312, precision 0.8795180722891566, recall 0.8488372093023255
2019-03-03T12:52:25.883450: step 176, loss 0.675755, accuracy 0.761719, precision 0.7906976744186046, recall 0.84472049689441
2019-03-03T12:52:26.147743: step 177, loss 0.591956, accuracy 0.773438, precision 0.8352941176470589, recall 0.8255813953488372
2019-03-03T12:52:26.421013: step 178, loss 0.401665, accuracy 0.855469, precision 0.896551724137931, recall 0.8914285714285715
2019-03-03T12:52:26.657380: step 179, loss 0.520344, accuracy 0.792969, precision 0.8098159509202454, recall 0.8571428571428571
2019-03-03T12:52:26.910703: step 180, loss 0.551159, accuracy 0.785156, precision 0.8208092485549133, recall 0.8554216867469879
2019-03-03T12:52:27.167018: step 181, loss 0.546196, accuracy 0.804688, precision 0.8938547486033519, recall 0.837696335078534
2019-03-03T12:52:27.425329: step 182, loss 0.581073, accuracy 0.773438, precision 0.8895348837209303, recall 0.796875
2019-03-03T12:52:27.729515: step 183, loss 0.577558, accuracy 0.789062, precision 0.8313953488372093, recall 0.8511904761904762
2019-03-03T12:52:27.970776: step 184, loss 0.447843, accuracy 0.828125, precision 0.9053254437869822, recall 0.8453038674033149
2019-03-03T12:52:28.197682: step 185, loss 0.486813, accuracy 0.808594, precision 0.863905325443787, recall 0.8488372093023255
2019-03-03T12:52:28.434507: step 186, loss 0.570082, accuracy 0.753906, precision 0.7844311377245509, recall 0.8291139240506329
2019-03-03T12:52:28.668882: step 187, loss 0.512152, accuracy 0.8125, precision 0.9069767441860465, recall 0.8297872340425532
2019-03-03T12:52:28.899264: step 188, loss 0.650662, accuracy 0.75, precision 0.8427672955974843, recall 0.7745664739884393
2019-03-03T12:52:29.131642: step 189, loss 0.657601, accuracy 0.726562, precision 0.7748344370860927, recall 0.7647058823529411
2019-03-03T12:52:29.363532: step 190, loss 0.546817, accuracy 0.765625, precision 0.8044692737430168, recall 0.8520710059171598
2019-03-03T12:52:29.623838: step 191, loss 0.525964, accuracy 0.792969, precision 0.8452380952380952, recall 0.8402366863905325
2019-03-03T12:52:29.868183: step 192, loss 0.559221, accuracy 0.789062, precision 0.869281045751634, recall 0.7964071856287425
2019-03-03T12:52:30.132477: step 193, loss 0.508807, accuracy 0.785156, precision 0.8421052631578947, recall 0.8372093023255814
2019-03-03T12:52:30.379816: step 194, loss 0.615975, accuracy 0.761719, precision 0.8333333333333334, recall 0.8192090395480226
2019-03-03T12:52:30.602242: step 195, loss 0.589274, accuracy 0.75, precision 0.8218390804597702, recall 0.8125
2019-03-03T12:52:30.838121: step 196, loss 0.482167, accuracy 0.816406, precision 0.8682634730538922, recall 0.8529411764705882
2019-03-03T12:52:31.106402: step 197, loss 0.473979, accuracy 0.792969, precision 0.8505747126436781, recall 0.8457142857142858
2019-03-03T12:52:31.337783: step 198, loss 0.443151, accuracy 0.8125, precision 0.9, recall 0.8315217391304348
2019-03-03T12:52:31.567171: step 199, loss 0.526832, accuracy 0.75, precision 0.8323699421965318, recall 0.8044692737430168
2019-03-03T12:52:31.816524: step 200, loss 0.592409, accuracy 0.78125, precision 0.8427672955974843, recall 0.8121212121212121
2019-03-03T12:52:32.087798: step 201, loss 0.552139, accuracy 0.75, precision 0.7964071856287425, recall 0.8159509202453987
2019-03-03T12:52:32.332167: step 202, loss 0.663849, accuracy 0.769531, precision 0.8333333333333334, recall 0.8187134502923976
2019-03-03T12:52:32.631876: step 203, loss 0.639656, accuracy 0.769531, precision 0.7931034482758621, recall 0.8571428571428571
2019-03-03T12:52:32.882207: step 204, loss 0.46557, accuracy 0.816406, precision 0.875, recall 0.8603351955307262
2019-03-03T12:52:33.063721: step 205, loss 0.550529, accuracy 0.782353, precision 0.9, recall 0.792
2019-03-03T12:52:33.318042: step 206, loss 0.463319, accuracy 0.8125, precision 0.8837209302325582, recall 0.8444444444444444
2019-03-03T12:52:33.560394: step 207, loss 0.531532, accuracy 0.796875, precision 0.8333333333333334, recall 0.8536585365853658
2019-03-03T12:52:33.814715: step 208, loss 0.49443, accuracy 0.789062, precision 0.8, recall 0.8627450980392157
2019-03-03T12:52:34.056068: step 209, loss 0.491375, accuracy 0.777344, precision 0.8516129032258064, recall 0.7951807228915663
2019-03-03T12:52:34.315398: step 210, loss 0.593735, accuracy 0.777344, precision 0.8439306358381503, recall 0.8295454545454546
2019-03-03T12:52:34.608612: step 211, loss 0.436401, accuracy 0.847656, precision 0.8895348837209303, recall 0.884393063583815
2019-03-03T12:52:34.863932: step 212, loss 0.48346, accuracy 0.796875, precision 0.9235668789808917, recall 0.7837837837837838
2019-03-03T12:52:35.112289: step 213, loss 0.377736, accuracy 0.847656, precision 0.8988095238095238, recall 0.8728323699421965
2019-03-03T12:52:35.368114: step 214, loss 0.457821, accuracy 0.828125, precision 0.8823529411764706, recall 0.8823529411764706
2019-03-03T12:52:35.653352: step 215, loss 0.462075, accuracy 0.785156, precision 0.8609271523178808, recall 0.7926829268292683
2019-03-03T12:52:35.914562: step 216, loss 0.379563, accuracy 0.839844, precision 0.8674033149171271, recall 0.9022988505747126
2019-03-03T12:52:36.176850: step 217, loss 0.352229, accuracy 0.863281, precision 0.9139784946236559, recall 0.8994708994708994
2019-03-03T12:52:36.433191: step 218, loss 0.450356, accuracy 0.8125, precision 0.8902439024390244, recall 0.8295454545454546
2019-03-03T12:52:36.736399: step 219, loss 0.452233, accuracy 0.832031, precision 0.8947368421052632, recall 0.8595505617977528
2019-03-03T12:52:37.036597: step 220, loss 0.468983, accuracy 0.832031, precision 0.8651685393258427, recall 0.8901734104046243
2019-03-03T12:52:37.331810: step 221, loss 0.419236, accuracy 0.847656, precision 0.9050279329608939, recall 0.8804347826086957
2019-03-03T12:52:37.642975: step 222, loss 0.402066, accuracy 0.835938, precision 0.898876404494382, recall 0.8695652173913043
2019-03-03T12:52:37.928212: step 223, loss 0.565907, accuracy 0.757812, precision 0.8834355828220859, recall 0.7700534759358288
2019-03-03T12:52:38.184527: step 224, loss 0.562131, accuracy 0.738281, precision 0.8470588235294118, recall 0.7783783783783784
2019-03-03T12:52:38.495695: step 225, loss 0.363539, accuracy 0.847656, precision 0.9080459770114943, recall 0.8729281767955801
2019-03-03T12:52:38.762980: step 226, loss 0.409653, accuracy 0.832031, precision 0.8491620111731844, recall 0.9047619047619048
2019-03-03T12:52:39.057195: step 227, loss 0.470567, accuracy 0.800781, precision 0.8782051282051282, recall 0.8106508875739645
2019-03-03T12:52:39.346421: step 228, loss 0.430511, accuracy 0.808594, precision 0.8607594936708861, recall 0.8343558282208589
2019-03-03T12:52:39.633519: step 229, loss 0.462258, accuracy 0.796875, precision 0.8579545454545454, recall 0.848314606741573
2019-03-03T12:52:39.913640: step 230, loss 0.444117, accuracy 0.84375, precision 0.8941176470588236, recall 0.8735632183908046
2019-03-03T12:52:40.219820: step 231, loss 0.502666, accuracy 0.777344, precision 0.8470588235294118, recall 0.8228571428571428
2019-03-03T12:52:40.491677: step 232, loss 0.462338, accuracy 0.824219, precision 0.8333333333333334, recall 0.9006211180124224
2019-03-03T12:52:40.787891: step 233, loss 0.574682, accuracy 0.746094, precision 0.815028901734104, recall 0.8103448275862069
2019-03-03T12:52:41.108027: step 234, loss 0.633954, accuracy 0.753906, precision 0.8404907975460123, recall 0.7873563218390804
2019-03-03T12:52:41.491003: step 235, loss 0.480658, accuracy 0.808594, precision 0.896774193548387, recall 0.8081395348837209
2019-03-03T12:52:41.837603: step 236, loss 0.527051, accuracy 0.796875, precision 0.9064327485380117, recall 0.8115183246073299
2019-03-03T12:52:42.161235: step 237, loss 0.484034, accuracy 0.792969, precision 0.874251497005988, recall 0.8202247191011236
2019-03-03T12:52:42.516285: step 238, loss 0.563044, accuracy 0.773438, precision 0.8545454545454545, recall 0.8057142857142857
2019-03-03T12:52:42.839612: step 239, loss 0.529444, accuracy 0.777344, precision 0.8135593220338984, recall 0.8571428571428571
2019-03-03T12:52:43.124357: step 240, loss 0.47521, accuracy 0.765625, precision 0.806060606060606, recall 0.8260869565217391
2019-03-03T12:52:43.454474: step 241, loss 0.394352, accuracy 0.835938, precision 0.8895027624309392, recall 0.8797814207650273
2019-03-03T12:52:43.744969: step 242, loss 0.390448, accuracy 0.820312, precision 0.8493975903614458, recall 0.8703703703703703
2019-03-03T12:52:44.024222: step 243, loss 0.382813, accuracy 0.820312, precision 0.8895348837209303, recall 0.85
2019-03-03T12:52:44.321402: step 244, loss 0.546651, accuracy 0.789062, precision 0.8827160493827161, recall 0.8033707865168539
2019-03-03T12:52:44.631571: step 245, loss 0.471624, accuracy 0.796875, precision 0.8853503184713376, recall 0.8034682080924855
2019-03-03T12:52:44.829044: step 246, loss 0.489555, accuracy 0.794118, precision 0.8545454545454545, recall 0.831858407079646
2019-03-03T12:52:45.134355: step 247, loss 0.436676, accuracy 0.800781, precision 0.9230769230769231, recall 0.7868852459016393
2019-03-03T12:52:45.443529: step 248, loss 0.352137, accuracy 0.847656, precision 0.8941176470588236, recall 0.8786127167630058
2019-03-03T12:52:45.760192: step 249, loss 0.324134, accuracy 0.851562, precision 0.877906976744186, recall 0.8988095238095238
2019-03-03T12:52:46.070361: step 250, loss 0.408891, accuracy 0.796875, precision 0.8352272727272727, recall 0.8647058823529412

Evaluation:
[[647 108]
 [120 281]]
2019-03-03T12:52:49.387446: step 250, loss 0.450817, accuracy 0.802768, precision 0.856953642384106, recall 0.8435462842242504

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551639073\checkpoints\model-250

2019-03-03T12:52:50.402247: step 251, loss 0.396864, accuracy 0.820312, precision 0.8488372093023255, recall 0.8795180722891566
2019-03-03T12:52:50.836083: step 252, loss 0.443416, accuracy 0.792969, precision 0.8614457831325302, recall 0.8265895953757225
2019-03-03T12:52:51.272674: step 253, loss 0.390195, accuracy 0.820312, precision 0.8947368421052632, recall 0.8453038674033149
2019-03-03T12:52:51.703522: step 254, loss 0.44052, accuracy 0.800781, precision 0.8587570621468926, recall 0.8539325842696629
2019-03-03T12:52:52.079517: step 255, loss 0.448804, accuracy 0.785156, precision 0.8846153846153846, recall 0.7885714285714286
2019-03-03T12:52:52.526322: step 256, loss 0.45789, accuracy 0.785156, precision 0.8875739644970414, recall 0.8064516129032258
2019-03-03T12:52:52.976119: step 257, loss 0.451412, accuracy 0.808594, precision 0.8896103896103896, recall 0.8106508875739645
2019-03-03T12:52:53.343138: step 258, loss 0.374593, accuracy 0.835938, precision 0.8932584269662921, recall 0.8736263736263736
2019-03-03T12:52:53.731108: step 259, loss 0.425299, accuracy 0.824219, precision 0.8587570621468926, recall 0.8837209302325582
2019-03-03T12:52:54.126051: step 260, loss 0.474905, accuracy 0.765625, precision 0.8409090909090909, recall 0.8222222222222222
2019-03-03T12:52:54.524498: step 261, loss 0.390136, accuracy 0.839844, precision 0.8597560975609756, recall 0.8867924528301887
2019-03-03T12:52:54.908470: step 262, loss 0.356889, accuracy 0.84375, precision 0.8830409356725146, recall 0.8830409356725146
2019-03-03T12:52:55.300422: step 263, loss 0.479462, accuracy 0.765625, precision 0.8502994011976048, recall 0.8022598870056498
2019-03-03T12:52:55.676417: step 264, loss 0.486455, accuracy 0.800781, precision 0.8764705882352941, recall 0.8324022346368715
2019-03-03T12:52:56.070257: step 265, loss 0.444664, accuracy 0.789062, precision 0.8648648648648649, recall 0.8465608465608465
2019-03-03T12:52:56.475174: step 266, loss 0.349517, accuracy 0.847656, precision 0.9263803680981595, recall 0.848314606741573
2019-03-03T12:52:56.874983: step 267, loss 0.361323, accuracy 0.851562, precision 0.9314285714285714, recall 0.8624338624338624
2019-03-03T12:52:57.258934: step 268, loss 0.372195, accuracy 0.816406, precision 0.888235294117647, recall 0.8435754189944135
2019-03-03T12:52:57.639914: step 269, loss 0.360389, accuracy 0.855469, precision 0.9375, recall 0.847457627118644
2019-03-03T12:52:58.009925: step 270, loss 0.513101, accuracy 0.78125, precision 0.8132530120481928, recall 0.84375
2019-03-03T12:52:58.399882: step 271, loss 0.474701, accuracy 0.78125, precision 0.8587570621468926, recall 0.8306010928961749
2019-03-03T12:52:58.780864: step 272, loss 0.410745, accuracy 0.820312, precision 0.8511904761904762, recall 0.8719512195121951
2019-03-03T12:52:59.150873: step 273, loss 0.439448, accuracy 0.835938, precision 0.8975903614457831, recall 0.8563218390804598
2019-03-03T12:52:59.468536: step 274, loss 0.360815, accuracy 0.832031, precision 0.877906976744186, recall 0.8728323699421965
2019-03-03T12:52:59.817604: step 275, loss 0.407192, accuracy 0.84375, precision 0.9066666666666666, recall 0.8395061728395061
2019-03-03T12:53:00.172653: step 276, loss 0.424973, accuracy 0.8125, precision 0.8901734104046243, recall 0.8415300546448088
2019-03-03T12:53:00.519695: step 277, loss 0.429589, accuracy 0.8125, precision 0.8988095238095238, recall 0.8296703296703297
2019-03-03T12:53:00.983456: step 278, loss 0.357048, accuracy 0.839844, precision 0.9122807017543859, recall 0.8571428571428571
2019-03-03T12:53:01.385381: step 279, loss 0.452063, accuracy 0.792969, precision 0.8488372093023255, recall 0.8439306358381503
2019-03-03T12:53:01.779328: step 280, loss 0.374488, accuracy 0.84375, precision 0.8888888888888888, recall 0.8786127167630058
2019-03-03T12:53:02.110443: step 281, loss 0.47063, accuracy 0.792969, precision 0.9012345679012346, recall 0.7978142076502732
2019-03-03T12:53:02.434577: step 282, loss 0.48294, accuracy 0.796875, precision 0.8484848484848485, recall 0.8383233532934131
2019-03-03T12:53:02.801597: step 283, loss 0.435577, accuracy 0.816406, precision 0.8728323699421965, recall 0.8579545454545454
2019-03-03T12:53:03.143684: step 284, loss 0.376923, accuracy 0.828125, precision 0.8313253012048193, recall 0.8961038961038961
2019-03-03T12:53:03.533637: step 285, loss 0.473953, accuracy 0.789062, precision 0.8571428571428571, recall 0.8379888268156425
2019-03-03T12:53:03.953515: step 286, loss 0.369688, accuracy 0.835938, precision 0.8736263736263736, recall 0.8932584269662921
2019-03-03T12:53:04.231775: step 287, loss 0.463941, accuracy 0.782353, precision 0.8037383177570093, recall 0.8431372549019608
2019-03-03T12:53:04.593321: step 288, loss 0.409759, accuracy 0.839844, precision 0.8888888888888888, recall 0.8936170212765957
2019-03-03T12:53:04.921445: step 289, loss 0.476788, accuracy 0.796875, precision 0.9294871794871795, recall 0.7795698924731183
2019-03-03T12:53:05.259537: step 290, loss 0.319737, accuracy 0.855469, precision 0.9239766081871345, recall 0.8681318681318682
2019-03-03T12:53:05.619575: step 291, loss 0.418178, accuracy 0.808594, precision 0.9101123595505618, recall 0.8307692307692308
2019-03-03T12:53:05.986594: step 292, loss 0.308836, accuracy 0.875, precision 0.9156626506024096, recall 0.8941176470588236
2019-03-03T12:53:06.290132: step 293, loss 0.327474, accuracy 0.839844, precision 0.9127906976744186, recall 0.8579234972677595
2019-03-03T12:53:06.639272: step 294, loss 0.346355, accuracy 0.839844, precision 0.8820224719101124, recall 0.8870056497175142
2019-03-03T12:53:07.003261: step 295, loss 0.328242, accuracy 0.84375, precision 0.8688524590163934, recall 0.9085714285714286
2019-03-03T12:53:07.351331: step 296, loss 0.457807, accuracy 0.808594, precision 0.8539325842696629, recall 0.8685714285714285
2019-03-03T12:53:07.678480: step 297, loss 0.451515, accuracy 0.828125, precision 0.8705882352941177, recall 0.8705882352941177
2019-03-03T12:53:07.994611: step 298, loss 0.382827, accuracy 0.839844, precision 0.8876404494382022, recall 0.88268156424581
2019-03-03T12:53:08.287827: step 299, loss 0.428445, accuracy 0.828125, precision 0.9320987654320988, recall 0.8206521739130435
2019-03-03T12:53:08.632907: step 300, loss 0.455726, accuracy 0.8125, precision 0.9349112426035503, recall 0.8102564102564103
2019-03-03T12:53:09.001919: step 301, loss 0.343166, accuracy 0.871094, precision 0.9467455621301775, recall 0.8695652173913043
2019-03-03T12:53:09.307103: step 302, loss 0.469134, accuracy 0.8125, precision 0.8895348837209303, recall 0.8406593406593407
2019-03-03T12:53:09.598834: step 303, loss 0.540909, accuracy 0.78125, precision 0.8383233532934131, recall 0.8284023668639053
2019-03-03T12:53:09.891052: step 304, loss 0.384707, accuracy 0.832031, precision 0.8983050847457628, recall 0.8641304347826086
2019-03-03T12:53:10.175291: step 305, loss 0.425284, accuracy 0.792969, precision 0.8648648648648649, recall 0.7950310559006211
2019-03-03T12:53:10.458533: step 306, loss 0.4646, accuracy 0.761719, precision 0.7823529411764706, recall 0.8471337579617835
2019-03-03T12:53:10.798625: step 307, loss 0.353604, accuracy 0.832031, precision 0.8922155688622755, recall 0.8563218390804598
2019-03-03T12:53:11.088850: step 308, loss 0.491465, accuracy 0.792969, precision 0.8047337278106509, recall 0.8717948717948718
2019-03-03T12:53:11.341173: step 309, loss 0.414644, accuracy 0.8125, precision 0.8647058823529412, recall 0.8546511627906976
2019-03-03T12:53:11.576544: step 310, loss 0.435434, accuracy 0.796875, precision 0.8782051282051282, recall 0.8058823529411765
2019-03-03T12:53:11.810917: step 311, loss 0.406959, accuracy 0.785156, precision 0.8392857142857143, recall 0.834319526627219
2019-03-03T12:53:12.053273: step 312, loss 0.360521, accuracy 0.859375, precision 0.9, recall 0.8895348837209303
2019-03-03T12:53:12.299748: step 313, loss 0.399634, accuracy 0.796875, precision 0.9064327485380117, recall 0.8115183246073299
2019-03-03T12:53:12.563341: step 314, loss 0.425424, accuracy 0.847656, precision 0.9298245614035088, recall 0.8548387096774194
2019-03-03T12:53:12.856557: step 315, loss 0.393855, accuracy 0.824219, precision 0.9132947976878613, recall 0.8404255319148937
2019-03-03T12:53:13.114867: step 316, loss 0.390416, accuracy 0.84375, precision 0.8819875776397516, recall 0.8711656441717791
2019-03-03T12:53:13.367192: step 317, loss 0.415808, accuracy 0.816406, precision 0.8805031446540881, recall 0.8333333333333334
2019-03-03T12:53:13.620030: step 318, loss 0.354778, accuracy 0.820312, precision 0.844311377245509, recall 0.8757763975155279
2019-03-03T12:53:13.903273: step 319, loss 0.549836, accuracy 0.789062, precision 0.8024691358024691, recall 0.8552631578947368
2019-03-03T12:53:14.157109: step 320, loss 0.326438, accuracy 0.84375, precision 0.8953488372093024, recall 0.875
2019-03-03T12:53:14.424396: step 321, loss 0.42653, accuracy 0.824219, precision 0.8975903614457831, recall 0.8418079096045198
2019-03-03T12:53:14.689685: step 322, loss 0.397621, accuracy 0.804688, precision 0.8957055214723927, recall 0.8156424581005587
2019-03-03T12:53:14.929044: step 323, loss 0.364388, accuracy 0.820312, precision 0.9181286549707602, recall 0.8306878306878307
2019-03-03T12:53:15.157801: step 324, loss 0.392285, accuracy 0.832031, precision 0.8670886075949367, recall 0.8616352201257862
2019-03-03T12:53:15.409010: step 325, loss 0.305845, accuracy 0.859375, precision 0.9213483146067416, recall 0.8817204301075269
2019-03-03T12:53:15.674309: step 326, loss 0.3922, accuracy 0.847656, precision 0.9041916167664671, recall 0.867816091954023
2019-03-03T12:53:15.941582: step 327, loss 0.364429, accuracy 0.855469, precision 0.907103825136612, recall 0.8924731182795699
2019-03-03T12:53:16.116114: step 328, loss 0.348861, accuracy 0.864706, precision 0.8942307692307693, recall 0.8857142857142857
2019-03-03T12:53:16.386391: step 329, loss 0.329094, accuracy 0.867188, precision 0.93125, recall 0.8662790697674418
2019-03-03T12:53:16.664160: step 330, loss 0.329928, accuracy 0.867188, precision 0.9375, recall 0.8776595744680851
2019-03-03T12:53:16.902522: step 331, loss 0.427755, accuracy 0.8125, precision 0.863905325443787, recall 0.8538011695906432
2019-03-03T12:53:17.160833: step 332, loss 0.366204, accuracy 0.816406, precision 0.893491124260355, recall 0.8388888888888889
2019-03-03T12:53:17.447067: step 333, loss 0.345561, accuracy 0.851562, precision 0.9190751445086706, recall 0.8688524590163934
2019-03-03T12:53:17.754243: step 334, loss 0.321801, accuracy 0.855469, precision 0.8765432098765432, recall 0.8930817610062893
2019-03-03T12:53:18.051958: step 335, loss 0.363479, accuracy 0.832031, precision 0.874251497005988, recall 0.8690476190476191
2019-03-03T12:53:18.337197: step 336, loss 0.323811, accuracy 0.851562, precision 0.8715083798882681, recall 0.9122807017543859
2019-03-03T12:53:18.651868: step 337, loss 0.363894, accuracy 0.863281, precision 0.903954802259887, recall 0.898876404494382
2019-03-03T12:53:18.969020: step 338, loss 0.320373, accuracy 0.851562, precision 0.903954802259887, recall 0.8839779005524862
2019-03-03T12:53:19.249207: step 339, loss 0.36798, accuracy 0.851562, precision 0.9017341040462428, recall 0.8813559322033898
2019-03-03T12:53:19.536351: step 340, loss 0.300461, accuracy 0.898438, precision 0.9451219512195121, recall 0.9011627906976745
2019-03-03T12:53:19.789673: step 341, loss 0.388746, accuracy 0.832031, precision 0.9137931034482759, recall 0.8502673796791443
2019-03-03T12:53:20.015579: step 342, loss 0.319803, accuracy 0.855469, precision 0.9470588235294117, recall 0.8518518518518519
2019-03-03T12:53:20.235500: step 343, loss 0.297391, accuracy 0.886719, precision 0.9151515151515152, recall 0.9096385542168675
2019-03-03T12:53:20.478850: step 344, loss 0.375734, accuracy 0.832031, precision 0.9036144578313253, recall 0.847457627118644
2019-03-03T12:53:20.766989: step 345, loss 0.37107, accuracy 0.863281, precision 0.9181286549707602, recall 0.8820224719101124
2019-03-03T12:53:21.038263: step 346, loss 0.352712, accuracy 0.847656, precision 0.8833333333333333, recall 0.8983050847457628
2019-03-03T12:53:21.298579: step 347, loss 0.319705, accuracy 0.863281, precision 0.8920454545454546, recall 0.9075144508670521
2019-03-03T12:53:21.544423: step 348, loss 0.372712, accuracy 0.808594, precision 0.8862275449101796, recall 0.8314606741573034
2019-03-03T12:53:21.766833: step 349, loss 0.3289, accuracy 0.84375, precision 0.9318181818181818, recall 0.8541666666666666
2019-03-03T12:53:22.017672: step 350, loss 0.414594, accuracy 0.832031, precision 0.9011627906976745, recall 0.856353591160221
2019-03-03T12:53:22.304903: step 351, loss 0.382203, accuracy 0.824219, precision 0.9142857142857143, recall 0.8421052631578947
2019-03-03T12:53:22.622055: step 352, loss 0.452552, accuracy 0.824219, precision 0.9190751445086706, recall 0.8368421052631579
2019-03-03T12:53:22.952173: step 353, loss 0.351326, accuracy 0.851562, precision 0.9244186046511628, recall 0.8641304347826086
2019-03-03T12:53:23.206492: step 354, loss 0.343209, accuracy 0.851562, precision 0.9080459770114943, recall 0.8777777777777778
2019-03-03T12:53:23.453818: step 355, loss 0.409927, accuracy 0.796875, precision 0.8166666666666667, recall 0.8855421686746988
2019-03-03T12:53:23.708137: step 356, loss 0.401008, accuracy 0.8125, precision 0.8855421686746988, recall 0.8352272727272727
2019-03-03T12:53:24.014827: step 357, loss 0.405851, accuracy 0.8125, precision 0.8711656441717791, recall 0.8402366863905325
2019-03-03T12:53:24.310037: step 358, loss 0.452879, accuracy 0.78125, precision 0.8169934640522876, recall 0.8169934640522876
2019-03-03T12:53:24.605184: step 359, loss 0.353084, accuracy 0.832031, precision 0.8562874251497006, recall 0.8827160493827161
2019-03-03T12:53:24.888581: step 360, loss 0.309862, accuracy 0.855469, precision 0.8674698795180723, recall 0.9056603773584906
2019-03-03T12:53:25.178763: step 361, loss 0.432528, accuracy 0.816406, precision 0.8580246913580247, recall 0.852760736196319
2019-03-03T12:53:25.434054: step 362, loss 0.338099, accuracy 0.851562, precision 0.9041916167664671, recall 0.8728323699421965
2019-03-03T12:53:25.737243: step 363, loss 0.332062, accuracy 0.863281, precision 0.9117647058823529, recall 0.8857142857142857
2019-03-03T12:53:26.021485: step 364, loss 0.379835, accuracy 0.839844, precision 0.9171974522292994, recall 0.8372093023255814
2019-03-03T12:53:26.331517: step 365, loss 0.332344, accuracy 0.863281, precision 0.9190751445086706, recall 0.8833333333333333
2019-03-03T12:53:26.625723: step 366, loss 0.454831, accuracy 0.78125, precision 0.8733333333333333, recall 0.7797619047619048
2019-03-03T12:53:26.935988: step 367, loss 0.411607, accuracy 0.820312, precision 0.9096774193548387, recall 0.815028901734104
2019-03-03T12:53:27.260061: step 368, loss 0.368146, accuracy 0.839844, precision 0.8611111111111112, recall 0.9064327485380117
2019-03-03T12:53:27.448555: step 369, loss 0.407302, accuracy 0.805882, precision 0.8230088495575221, recall 0.8773584905660378
2019-03-03T12:53:27.711852: step 370, loss 0.375633, accuracy 0.839844, precision 0.8786127167630058, recall 0.8837209302325582
2019-03-03T12:53:28.009057: step 371, loss 0.308746, accuracy 0.871094, precision 0.8967391304347826, recall 0.9217877094972067
2019-03-03T12:53:28.281366: step 372, loss 0.258028, accuracy 0.886719, precision 0.9318181818181818, recall 0.9060773480662984
2019-03-03T12:53:28.581563: step 373, loss 0.386539, accuracy 0.84375, precision 0.9349112426035503, recall 0.8449197860962567
2019-03-03T12:53:28.879979: step 374, loss 0.370525, accuracy 0.84375, precision 0.9567901234567902, recall 0.824468085106383
2019-03-03T12:53:29.154246: step 375, loss 0.405013, accuracy 0.808594, precision 0.9367088607594937, recall 0.7914438502673797
2019-03-03T12:53:29.434497: step 376, loss 0.345853, accuracy 0.824219, precision 0.9101123595505618, recall 0.8481675392670157
2019-03-03T12:53:29.713749: step 377, loss 0.35435, accuracy 0.847656, precision 0.8834355828220859, recall 0.8780487804878049
2019-03-03T12:53:30.003975: step 378, loss 0.340257, accuracy 0.847656, precision 0.8770949720670391, recall 0.9022988505747126
2019-03-03T12:53:30.257298: step 379, loss 0.407176, accuracy 0.816406, precision 0.8176100628930818, recall 0.8783783783783784
2019-03-03T12:53:30.551583: step 380, loss 0.379895, accuracy 0.808594, precision 0.8035714285714286, recall 0.8940397350993378
2019-03-03T12:53:30.823855: step 381, loss 0.346365, accuracy 0.835938, precision 0.8563218390804598, recall 0.8975903614457831
2019-03-03T12:53:31.092152: step 382, loss 0.385357, accuracy 0.851562, precision 0.88268156424581, recall 0.9028571428571428
2019-03-03T12:53:31.383359: step 383, loss 0.338574, accuracy 0.839844, precision 0.9349112426035503, recall 0.8404255319148937
2019-03-03T12:53:31.644053: step 384, loss 0.382806, accuracy 0.835938, precision 0.9518072289156626, recall 0.8229166666666666
2019-03-03T12:53:31.909609: step 385, loss 0.371378, accuracy 0.84375, precision 0.926829268292683, recall 0.8444444444444444
2019-03-03T12:53:32.178884: step 386, loss 0.394392, accuracy 0.820312, precision 0.9364161849710982, recall 0.8223350253807107
2019-03-03T12:53:32.441183: step 387, loss 0.339611, accuracy 0.855469, precision 0.9272727272727272, recall 0.8595505617977528
2019-03-03T12:53:32.704479: step 388, loss 0.392013, accuracy 0.824219, precision 0.8909090909090909, recall 0.8448275862068966
2019-03-03T12:53:32.996659: step 389, loss 0.33259, accuracy 0.859375, precision 0.8924050632911392, recall 0.88125
2019-03-03T12:53:33.275426: step 390, loss 0.327591, accuracy 0.855469, precision 0.86875, recall 0.896774193548387
2019-03-03T12:53:33.565650: step 391, loss 0.363471, accuracy 0.839844, precision 0.8538011695906432, recall 0.9012345679012346
2019-03-03T12:53:33.849167: step 392, loss 0.362725, accuracy 0.84375, precision 0.8410596026490066, recall 0.8881118881118881
2019-03-03T12:53:34.136389: step 393, loss 0.307277, accuracy 0.882812, precision 0.9044943820224719, recall 0.9252873563218391
2019-03-03T12:53:34.434592: step 394, loss 0.333835, accuracy 0.859375, precision 0.9166666666666666, recall 0.875
2019-03-03T12:53:34.750747: step 395, loss 0.331368, accuracy 0.863281, precision 0.9532163742690059, recall 0.8578947368421053
2019-03-03T12:53:35.051941: step 396, loss 0.375865, accuracy 0.820312, precision 0.9337349397590361, recall 0.8157894736842105
2019-03-03T12:53:35.368096: step 397, loss 0.353382, accuracy 0.867188, precision 0.91875, recall 0.875
2019-03-03T12:53:35.680264: step 398, loss 0.323434, accuracy 0.878906, precision 0.9408284023668639, recall 0.8833333333333333
2019-03-03T12:53:35.975474: step 399, loss 0.326408, accuracy 0.855469, precision 0.8994082840236687, recall 0.8837209302325582
2019-03-03T12:53:36.293622: step 400, loss 0.276518, accuracy 0.890625, precision 0.930635838150289, recall 0.9096045197740112
2019-03-03T12:53:36.609781: step 401, loss 0.328434, accuracy 0.855469, precision 0.8888888888888888, recall 0.8834355828220859
2019-03-03T12:53:36.939585: step 402, loss 0.310132, accuracy 0.863281, precision 0.9197860962566845, recall 0.8958333333333334
2019-03-03T12:53:37.204874: step 403, loss 0.305592, accuracy 0.882812, precision 0.9259259259259259, recall 0.8928571428571429
2019-03-03T12:53:37.475151: step 404, loss 0.33432, accuracy 0.847656, precision 0.9074074074074074, recall 0.8596491228070176
2019-03-03T12:53:37.761393: step 405, loss 0.447537, accuracy 0.839844, precision 0.8850574712643678, recall 0.88
2019-03-03T12:53:38.023691: step 406, loss 0.399363, accuracy 0.808594, precision 0.8614457831325302, recall 0.8461538461538461
2019-03-03T12:53:38.274224: step 407, loss 0.335056, accuracy 0.859375, precision 0.9190751445086706, recall 0.8784530386740331
2019-03-03T12:53:38.552541: step 408, loss 0.288067, accuracy 0.867188, precision 0.9431818181818182, recall 0.8736842105263158
2019-03-03T12:53:38.845757: step 409, loss 0.322695, accuracy 0.851562, precision 0.9044943820224719, recall 0.8846153846153846
2019-03-03T12:53:39.031260: step 410, loss 0.298008, accuracy 0.882353, precision 0.8925619834710744, recall 0.9391304347826087
2019-03-03T12:53:39.300050: step 411, loss 0.340743, accuracy 0.847656, precision 0.9176470588235294, recall 0.861878453038674
2019-03-03T12:53:39.574317: step 412, loss 0.324887, accuracy 0.863281, precision 0.9378531073446328, recall 0.8736842105263158
2019-03-03T12:53:39.824647: step 413, loss 0.357994, accuracy 0.820312, precision 0.9310344827586207, recall 0.826530612244898
2019-03-03T12:53:40.069991: step 414, loss 0.29007, accuracy 0.871094, precision 0.9180327868852459, recall 0.9032258064516129
2019-03-03T12:53:40.311979: step 415, loss 0.260053, accuracy 0.882812, precision 0.9440993788819876, recall 0.8786127167630058
2019-03-03T12:53:40.586495: step 416, loss 0.360479, accuracy 0.832031, precision 0.88125, recall 0.8545454545454545
2019-03-03T12:53:40.907151: step 417, loss 0.333729, accuracy 0.863281, precision 0.9069767441860465, recall 0.8914285714285715
2019-03-03T12:53:41.179313: step 418, loss 0.304778, accuracy 0.882812, precision 0.9252873563218391, recall 0.9044943820224719
2019-03-03T12:53:41.425813: step 419, loss 0.355016, accuracy 0.851562, precision 0.9006211180124224, recall 0.8682634730538922
2019-03-03T12:53:41.689108: step 420, loss 0.350688, accuracy 0.832031, precision 0.8805031446540881, recall 0.8536585365853658
2019-03-03T12:53:41.967497: step 421, loss 0.315564, accuracy 0.863281, precision 0.8820224719101124, recall 0.9181286549707602
2019-03-03T12:53:42.230183: step 422, loss 0.268618, accuracy 0.871094, precision 0.9034090909090909, recall 0.9085714285714286
2019-03-03T12:53:42.496471: step 423, loss 0.329768, accuracy 0.867188, precision 0.9337016574585635, recall 0.8848167539267016
2019-03-03T12:53:42.770224: step 424, loss 0.296263, accuracy 0.867188, precision 0.9367816091954023, recall 0.8763440860215054
2019-03-03T12:53:43.040503: step 425, loss 0.337635, accuracy 0.863281, precision 0.9553072625698324, recall 0.8636363636363636
2019-03-03T12:53:43.305797: step 426, loss 0.367245, accuracy 0.847656, precision 0.8971428571428571, recall 0.8820224719101124
2019-03-03T12:53:43.600007: step 427, loss 0.316645, accuracy 0.886719, precision 0.9529411764705882, recall 0.8852459016393442
2019-03-03T12:53:43.899207: step 428, loss 0.250007, accuracy 0.878906, precision 0.9137931034482759, recall 0.9085714285714286
2019-03-03T12:53:44.176975: step 429, loss 0.341295, accuracy 0.851562, precision 0.8994413407821229, recall 0.8895027624309392
2019-03-03T12:53:44.451241: step 430, loss 0.343729, accuracy 0.859375, precision 0.8875739644970414, recall 0.8982035928143712
2019-03-03T12:53:44.733487: step 431, loss 0.3347, accuracy 0.84375, precision 0.8947368421052632, recall 0.85
2019-03-03T12:53:45.009747: step 432, loss 0.31249, accuracy 0.894531, precision 0.9342105263157895, recall 0.8930817610062893
2019-03-03T12:53:45.261075: step 433, loss 0.330637, accuracy 0.835938, precision 0.8763440860215054, recall 0.8956043956043956
2019-03-03T12:53:45.518386: step 434, loss 0.351729, accuracy 0.863281, precision 0.896969696969697, recall 0.891566265060241
2019-03-03T12:53:45.763462: step 435, loss 0.297069, accuracy 0.878906, precision 0.9244186046511628, recall 0.8983050847457628
2019-03-03T12:53:46.009399: step 436, loss 0.297346, accuracy 0.878906, precision 0.9028571428571428, recall 0.9186046511627907
2019-03-03T12:53:46.241754: step 437, loss 0.294981, accuracy 0.871094, precision 0.8908045977011494, recall 0.9171597633136095
2019-03-03T12:53:46.485619: step 438, loss 0.286555, accuracy 0.886719, precision 0.9302325581395349, recall 0.903954802259887
2019-03-03T12:53:46.741741: step 439, loss 0.354362, accuracy 0.839844, precision 0.8974358974358975, recall 0.8484848484848485
2019-03-03T12:53:47.000218: step 440, loss 0.299139, accuracy 0.875, precision 0.9444444444444444, recall 0.8854166666666666
2019-03-03T12:53:47.240088: step 441, loss 0.41018, accuracy 0.808594, precision 0.9127906976744186, recall 0.8219895287958116
2019-03-03T12:53:47.469475: step 442, loss 0.321581, accuracy 0.84375, precision 0.8902439024390244, recall 0.8690476190476191
2019-03-03T12:53:47.694874: step 443, loss 0.324452, accuracy 0.847656, precision 0.89937106918239, recall 0.8614457831325302
2019-03-03T12:53:47.927252: step 444, loss 0.34067, accuracy 0.855469, precision 0.9101796407185628, recall 0.8735632183908046
2019-03-03T12:53:48.162622: step 445, loss 0.367055, accuracy 0.839844, precision 0.8957055214723927, recall 0.8588235294117647
2019-03-03T12:53:48.393005: step 446, loss 0.375829, accuracy 0.847656, precision 0.8787878787878788, recall 0.8841463414634146
2019-03-03T12:53:48.643973: step 447, loss 0.387053, accuracy 0.832031, precision 0.86875, recall 0.8633540372670807
2019-03-03T12:53:48.909282: step 448, loss 0.320323, accuracy 0.84375, precision 0.9263803680981595, recall 0.8435754189944135
2019-03-03T12:53:49.154624: step 449, loss 0.381819, accuracy 0.828125, precision 0.8895705521472392, recall 0.847953216374269
2019-03-03T12:53:49.388001: step 450, loss 0.319672, accuracy 0.847656, precision 0.9221556886227545, recall 0.8555555555555555
2019-03-03T12:53:49.554529: step 451, loss 0.336257, accuracy 0.835294, precision 0.8962264150943396, recall 0.8482142857142857
2019-03-03T12:53:49.793888: step 452, loss 0.316657, accuracy 0.867188, precision 0.9064327485380117, recall 0.8959537572254336
2019-03-03T12:53:50.059182: step 453, loss 0.313315, accuracy 0.851562, precision 0.9064327485380117, recall 0.8757062146892656
2019-03-03T12:53:50.289827: step 454, loss 0.301239, accuracy 0.859375, precision 0.9065934065934066, recall 0.8967391304347826
2019-03-03T12:53:50.519489: step 455, loss 0.270654, accuracy 0.867188, precision 0.9047619047619048, recall 0.9144385026737968
2019-03-03T12:53:50.784781: step 456, loss 0.292342, accuracy 0.859375, precision 0.9011627906976745, recall 0.8908045977011494
2019-03-03T12:53:51.076997: step 457, loss 0.258588, accuracy 0.875, precision 0.9248554913294798, recall 0.8938547486033519
2019-03-03T12:53:51.327328: step 458, loss 0.34229, accuracy 0.847656, precision 0.9265536723163842, recall 0.8631578947368421
2019-03-03T12:53:51.561701: step 459, loss 0.369766, accuracy 0.828125, precision 0.9107142857142857, recall 0.8406593406593407
2019-03-03T12:53:51.805560: step 460, loss 0.328155, accuracy 0.855469, precision 0.9620253164556962, recall 0.8306010928961749
2019-03-03T12:53:52.076343: step 461, loss 0.279064, accuracy 0.875, precision 0.9261363636363636, recall 0.8956043956043956
2019-03-03T12:53:52.360584: step 462, loss 0.361712, accuracy 0.851562, precision 0.9006622516556292, recall 0.8553459119496856
2019-03-03T12:53:52.675215: step 463, loss 0.268687, accuracy 0.890625, precision 0.8888888888888888, recall 0.9440993788819876
2019-03-03T12:53:52.958511: step 464, loss 0.288345, accuracy 0.875, precision 0.9005847953216374, recall 0.9112426035502958
2019-03-03T12:53:53.207844: step 465, loss 0.307071, accuracy 0.867188, precision 0.9012345679012346, recall 0.8902439024390244
2019-03-03T12:53:53.469147: step 466, loss 0.310084, accuracy 0.875, precision 0.9239766081871345, recall 0.8926553672316384
2019-03-03T12:53:53.751393: step 467, loss 0.314851, accuracy 0.847656, precision 0.8867924528301887, recall 0.8703703703703703
2019-03-03T12:53:54.009655: step 468, loss 0.24705, accuracy 0.894531, precision 0.9352941176470588, recall 0.9085714285714286
2019-03-03T12:53:54.239043: step 469, loss 0.332056, accuracy 0.847656, precision 0.9079754601226994, recall 0.8604651162790697
2019-03-03T12:53:54.475432: step 470, loss 0.281124, accuracy 0.886719, precision 0.9130434782608695, recall 0.9074074074074074
2019-03-03T12:53:54.729752: step 471, loss 0.34928, accuracy 0.859375, precision 0.9135802469135802, recall 0.8705882352941177
2019-03-03T12:53:54.972141: step 472, loss 0.354028, accuracy 0.871094, precision 0.9386503067484663, recall 0.8693181818181818
2019-03-03T12:53:55.203545: step 473, loss 0.396509, accuracy 0.84375, precision 0.8719512195121951, recall 0.8827160493827161
2019-03-03T12:53:55.420961: step 474, loss 0.315722, accuracy 0.871094, precision 0.9011627906976745, recall 0.9064327485380117
2019-03-03T12:53:55.641371: step 475, loss 0.360445, accuracy 0.832031, precision 0.8255813953488372, recall 0.9161290322580645
2019-03-03T12:53:55.876741: step 476, loss 0.348019, accuracy 0.835938, precision 0.884393063583815, recall 0.8742857142857143
2019-03-03T12:53:56.102583: step 477, loss 0.353581, accuracy 0.859375, precision 0.9310344827586207, recall 0.8709677419354839
2019-03-03T12:53:56.323491: step 478, loss 0.336772, accuracy 0.855469, precision 0.9515151515151515, recall 0.8440860215053764
2019-03-03T12:53:56.549886: step 479, loss 0.342235, accuracy 0.859375, precision 0.9390243902439024, recall 0.8555555555555555
2019-03-03T12:53:56.798373: step 480, loss 0.379976, accuracy 0.800781, precision 0.8385093167701864, recall 0.84375
2019-03-03T12:53:57.045712: step 481, loss 0.314196, accuracy 0.84375, precision 0.8963414634146342, recall 0.8647058823529412
2019-03-03T12:53:57.272427: step 482, loss 0.28719, accuracy 0.875, precision 0.9119496855345912, recall 0.8895705521472392
2019-03-03T12:53:57.497823: step 483, loss 0.329048, accuracy 0.835938, precision 0.8554913294797688, recall 0.896969696969697
2019-03-03T12:53:57.730576: step 484, loss 0.329978, accuracy 0.859375, precision 0.8816568047337278, recall 0.9030303030303031
2019-03-03T12:53:57.991879: step 485, loss 0.262993, accuracy 0.890625, precision 0.9310344827586207, recall 0.9101123595505618
2019-03-03T12:53:58.217274: step 486, loss 0.373691, accuracy 0.84375, precision 0.9064327485380117, recall 0.8659217877094972
2019-03-03T12:53:58.447658: step 487, loss 0.305782, accuracy 0.863281, precision 0.9135135135135135, recall 0.898936170212766
2019-03-03T12:53:58.690011: step 488, loss 0.342198, accuracy 0.855469, precision 0.9411764705882353, recall 0.8556149732620321
2019-03-03T12:53:58.957297: step 489, loss 0.292458, accuracy 0.851562, precision 0.9273743016759777, recall 0.8691099476439791
2019-03-03T12:53:59.192824: step 490, loss 0.274229, accuracy 0.867188, precision 0.9375, recall 0.8776595744680851
2019-03-03T12:53:59.429191: step 491, loss 0.263297, accuracy 0.886719, precision 0.9651162790697675, recall 0.8783068783068783
2019-03-03T12:53:59.590759: step 492, loss 0.326598, accuracy 0.852941, precision 0.9405940594059405, recall 0.8333333333333334
2019-03-03T12:53:59.820170: step 493, loss 0.276107, accuracy 0.886719, precision 0.9181286549707602, recall 0.9127906976744186
2019-03-03T12:54:00.049558: step 494, loss 0.339023, accuracy 0.835938, precision 0.8659217877094972, recall 0.8959537572254336
2019-03-03T12:54:00.285925: step 495, loss 0.336581, accuracy 0.839844, precision 0.8757396449704142, recall 0.8809523809523809
2019-03-03T12:54:00.529275: step 496, loss 0.324369, accuracy 0.859375, precision 0.9107142857142857, recall 0.8793103448275862
2019-03-03T12:54:00.771627: step 497, loss 0.285274, accuracy 0.875, precision 0.9345238095238095, recall 0.8820224719101124
2019-03-03T12:54:01.011984: step 498, loss 0.364121, accuracy 0.820312, precision 0.8947368421052632, recall 0.8453038674033149
2019-03-03T12:54:01.237406: step 499, loss 0.333132, accuracy 0.84375, precision 0.9144736842105263, recall 0.8373493975903614
2019-03-03T12:54:01.459812: step 500, loss 0.327915, accuracy 0.859375, precision 0.9146341463414634, recall 0.872093023255814

Evaluation:
[[680  75]
 [148 253]]
2019-03-03T12:54:01.853758: step 500, loss 0.44375, accuracy 0.807093, precision 0.9006622516556292, recall 0.821256038647343

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551639073\checkpoints\model-500


Process finished with exit code 0
