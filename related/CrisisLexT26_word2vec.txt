"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
Pretrained Embedding: word2vec
Italian: False
Loading data...
11566
Max Document length: 36
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 29102
Train/Dev split: 10410/1156
2019-03-03 12:42:55.800068: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:475: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551638619

2019-03-03T12:43:47.132998: step 1, loss 4.4391, accuracy 0.539062, precision 0.6428571428571429, recall 0.6506024096385542
2019-03-03T12:43:47.679533: step 2, loss 3.88979, accuracy 0.691406, precision 0.8802395209580839, recall 0.7135922330097088
2019-03-03T12:43:48.167229: step 3, loss 4.1113, accuracy 0.667969, precision 0.81875, recall 0.7005347593582888
2019-03-03T12:43:48.637978: step 4, loss 4.07405, accuracy 0.617188, precision 0.5697674418604651, recall 0.8032786885245902
2019-03-03T12:43:49.108968: step 5, loss 3.97993, accuracy 0.652344, precision 0.6265822784810127, recall 0.7674418604651163
2019-03-03T12:43:49.528363: step 6, loss 4.41434, accuracy 0.640625, precision 0.6966292134831461, recall 0.7654320987654321
2019-03-03T12:43:49.934275: step 7, loss 3.80394, accuracy 0.691406, precision 0.7440476190476191, recall 0.7763975155279503
2019-03-03T12:43:50.291319: step 8, loss 3.55694, accuracy 0.722656, precision 0.7965116279069767, recall 0.791907514450867
2019-03-03T12:43:50.646372: step 9, loss 3.34621, accuracy 0.664062, precision 0.7597765363128491, recall 0.7597765363128491
2019-03-03T12:43:50.966515: step 10, loss 3.04487, accuracy 0.707031, precision 0.8232044198895028, recall 0.7760416666666666
2019-03-03T12:43:51.270225: step 11, loss 2.88287, accuracy 0.699219, precision 0.7659574468085106, recall 0.8135593220338984
2019-03-03T12:43:51.568497: step 12, loss 4.22378, accuracy 0.582031, precision 0.7100591715976331, recall 0.6741573033707865
2019-03-03T12:43:51.839283: step 13, loss 3.51079, accuracy 0.621094, precision 0.7247191011235955, recall 0.7288135593220338
2019-03-03T12:43:52.103600: step 14, loss 3.13826, accuracy 0.617188, precision 0.7516339869281046, recall 0.6571428571428571
2019-03-03T12:43:52.370884: step 15, loss 3.18948, accuracy 0.644531, precision 0.8033707865168539, recall 0.7185929648241206
2019-03-03T12:43:52.624219: step 16, loss 3.53845, accuracy 0.625, precision 0.7467532467532467, recall 0.6686046511627907
2019-03-03T12:43:52.874549: step 17, loss 3.40339, accuracy 0.617188, precision 0.711764705882353, recall 0.711764705882353
2019-03-03T12:43:53.125904: step 18, loss 2.83061, accuracy 0.660156, precision 0.7166666666666667, recall 0.7818181818181819
2019-03-03T12:43:53.371247: step 19, loss 3.67185, accuracy 0.640625, precision 0.7869822485207101, recall 0.7037037037037037
2019-03-03T12:43:53.610607: step 20, loss 2.59248, accuracy 0.695312, precision 0.71875, recall 0.777027027027027
2019-03-03T12:43:53.854955: step 21, loss 2.45784, accuracy 0.6875, precision 0.7305389221556886, recall 0.7770700636942676
2019-03-03T12:43:54.111269: step 22, loss 2.44317, accuracy 0.679688, precision 0.7160493827160493, recall 0.7631578947368421
2019-03-03T12:43:54.385536: step 23, loss 3.57724, accuracy 0.664062, precision 0.6942675159235668, recall 0.7414965986394558
2019-03-03T12:43:54.643843: step 24, loss 2.76547, accuracy 0.667969, precision 0.7515527950310559, recall 0.7289156626506024
2019-03-03T12:43:54.894684: step 25, loss 2.49922, accuracy 0.707031, precision 0.7251461988304093, recall 0.8157894736842105
2019-03-03T12:43:55.143119: step 26, loss 2.09925, accuracy 0.6875, precision 0.7948717948717948, recall 0.7209302325581395
2019-03-03T12:43:55.381990: step 27, loss 2.58158, accuracy 0.6875, precision 0.7232704402515723, recall 0.7615894039735099
2019-03-03T12:43:55.623346: step 28, loss 2.80818, accuracy 0.640625, precision 0.7045454545454546, recall 0.7560975609756098
2019-03-03T12:43:55.865080: step 29, loss 2.85312, accuracy 0.6875, precision 0.7602339181286549, recall 0.7692307692307693
2019-03-03T12:43:56.121903: step 30, loss 2.65261, accuracy 0.664062, precision 0.7572254335260116, recall 0.7485714285714286
2019-03-03T12:43:56.366250: step 31, loss 2.28788, accuracy 0.734375, precision 0.8181818181818182, recall 0.8
2019-03-03T12:43:56.620576: step 32, loss 2.59432, accuracy 0.671875, precision 0.7861271676300579, recall 0.7431693989071039
2019-03-03T12:43:56.873902: step 33, loss 2.23241, accuracy 0.71875, precision 0.7988505747126436, recall 0.7897727272727273
2019-03-03T12:43:57.110067: step 34, loss 2.42895, accuracy 0.644531, precision 0.7639751552795031, recall 0.6988636363636364
2019-03-03T12:43:57.361395: step 35, loss 2.51873, accuracy 0.660156, precision 0.7885714285714286, recall 0.7340425531914894
2019-03-03T12:43:57.610755: step 36, loss 2.07657, accuracy 0.707031, precision 0.7816091954022989, recall 0.7861271676300579
2019-03-03T12:43:57.853127: step 37, loss 2.34866, accuracy 0.703125, precision 0.845679012345679, recall 0.7287234042553191
2019-03-03T12:43:58.114428: step 38, loss 2.37204, accuracy 0.675781, precision 0.8048780487804879, recall 0.7213114754098361
2019-03-03T12:43:58.361766: step 39, loss 2.32581, accuracy 0.640625, precision 0.6526946107784432, recall 0.7622377622377622
2019-03-03T12:43:58.605116: step 40, loss 2.46405, accuracy 0.621094, precision 0.6206896551724138, recall 0.7769784172661871
2019-03-03T12:43:58.775661: step 41, loss 2.3515, accuracy 0.652941, precision 0.628099173553719, recall 0.8444444444444444
2019-03-03T12:43:59.027986: step 42, loss 1.97938, accuracy 0.699219, precision 0.8035714285714286, recall 0.7541899441340782
2019-03-03T12:43:59.300276: step 43, loss 2.282, accuracy 0.660156, precision 0.7391304347826086, recall 0.725609756097561
2019-03-03T12:43:59.548612: step 44, loss 2.01525, accuracy 0.726562, precision 0.8654970760233918, recall 0.7589743589743589
2019-03-03T12:43:59.792960: step 45, loss 2.01963, accuracy 0.746094, precision 0.8858695652173914, recall 0.7874396135265701
2019-03-03T12:44:00.054284: step 46, loss 2.20069, accuracy 0.652344, precision 0.7514792899408284, recall 0.7298850574712644
2019-03-03T12:44:00.302621: step 47, loss 2.36696, accuracy 0.683594, precision 0.8269230769230769, recall 0.7049180327868853
2019-03-03T12:44:00.544972: step 48, loss 2.10776, accuracy 0.648438, precision 0.7305389221556886, recall 0.7305389221556886
2019-03-03T12:44:00.792337: step 49, loss 1.79739, accuracy 0.722656, precision 0.7283950617283951, recall 0.8137931034482758
2019-03-03T12:44:01.036681: step 50, loss 1.84916, accuracy 0.664062, precision 0.6629834254143646, recall 0.8275862068965517
2019-03-03T12:44:01.287014: step 51, loss 1.83741, accuracy 0.738281, precision 0.7396449704142012, recall 0.8445945945945946
2019-03-03T12:44:01.529365: step 52, loss 1.70613, accuracy 0.6875, precision 0.7705882352941177, recall 0.7616279069767442
2019-03-03T12:44:01.773719: step 53, loss 1.45684, accuracy 0.734375, precision 0.7628205128205128, recall 0.7933333333333333
2019-03-03T12:44:02.016065: step 54, loss 1.74, accuracy 0.734375, precision 0.8390804597701149, recall 0.7849462365591398
2019-03-03T12:44:02.270388: step 55, loss 2.1122, accuracy 0.691406, precision 0.8291139240506329, recall 0.7158469945355191
2019-03-03T12:44:02.518752: step 56, loss 1.82311, accuracy 0.730469, precision 0.8475609756097561, recall 0.7595628415300546
2019-03-03T12:44:02.768090: step 57, loss 1.89914, accuracy 0.734375, precision 0.834319526627219, recall 0.7790055248618785
2019-03-03T12:44:03.011948: step 58, loss 1.69831, accuracy 0.714844, precision 0.7732558139534884, recall 0.7964071856287425
2019-03-03T12:44:03.266792: step 59, loss 1.73102, accuracy 0.675781, precision 0.703030303030303, recall 0.7733333333333333
2019-03-03T12:44:03.510141: step 60, loss 1.88811, accuracy 0.699219, precision 0.7177914110429447, recall 0.7905405405405406
2019-03-03T12:44:03.751495: step 61, loss 1.80098, accuracy 0.726562, precision 0.7206703910614525, recall 0.8657718120805369
2019-03-03T12:44:03.995842: step 62, loss 1.83347, accuracy 0.695312, precision 0.7349397590361446, recall 0.782051282051282
2019-03-03T12:44:04.241696: step 63, loss 1.82422, accuracy 0.726562, precision 0.8402366863905325, recall 0.7675675675675676
2019-03-03T12:44:04.488080: step 64, loss 1.54225, accuracy 0.75, precision 0.8674698795180723, recall 0.7741935483870968
2019-03-03T12:44:04.747897: step 65, loss 1.52226, accuracy 0.761719, precision 0.8596491228070176, recall 0.7989130434782609
2019-03-03T12:44:05.013193: step 66, loss 1.7225, accuracy 0.703125, precision 0.8823529411764706, recall 0.6994818652849741
2019-03-03T12:44:05.253059: step 67, loss 1.64394, accuracy 0.714844, precision 0.8192771084337349, recall 0.7597765363128491
2019-03-03T12:44:05.504899: step 68, loss 1.42906, accuracy 0.679688, precision 0.7398843930635838, recall 0.7757575757575758
2019-03-03T12:44:05.758222: step 69, loss 1.83154, accuracy 0.714844, precision 0.7547169811320755, recall 0.7792207792207793
2019-03-03T12:44:06.001571: step 70, loss 1.52103, accuracy 0.730469, precision 0.7592592592592593, recall 0.803921568627451
2019-03-03T12:44:06.249908: step 71, loss 1.43054, accuracy 0.722656, precision 0.7325581395348837, recall 0.8344370860927153
2019-03-03T12:44:06.485277: step 72, loss 1.25494, accuracy 0.773438, precision 0.7967032967032966, recall 0.8734939759036144
2019-03-03T12:44:06.730622: step 73, loss 1.87431, accuracy 0.699219, precision 0.7650602409638554, recall 0.7696969696969697
2019-03-03T12:44:06.978979: step 74, loss 2.00388, accuracy 0.699219, precision 0.8032786885245902, recall 0.7819148936170213
2019-03-03T12:44:07.229824: step 75, loss 1.75171, accuracy 0.707031, precision 0.8417721518987342, recall 0.726775956284153
2019-03-03T12:44:07.479149: step 76, loss 1.2996, accuracy 0.734375, precision 0.8135593220338984, recall 0.8044692737430168
2019-03-03T12:44:07.719508: step 77, loss 1.77163, accuracy 0.691406, precision 0.7962962962962963, recall 0.7371428571428571
2019-03-03T12:44:07.969839: step 78, loss 1.77353, accuracy 0.6875, precision 0.7415730337078652, recall 0.7951807228915663
2019-03-03T12:44:08.222673: step 79, loss 1.73763, accuracy 0.71875, precision 0.7790697674418605, recall 0.7976190476190477
2019-03-03T12:44:08.465045: step 80, loss 1.56956, accuracy 0.722656, precision 0.7759562841530054, recall 0.8255813953488372
2019-03-03T12:44:08.730336: step 81, loss 1.4313, accuracy 0.726562, precision 0.8, recall 0.8089887640449438
2019-03-03T12:44:08.906863: step 82, loss 1.09598, accuracy 0.747059, precision 0.8166666666666667, recall 0.8235294117647058
2019-03-03T12:44:09.160186: step 83, loss 1.61985, accuracy 0.726562, precision 0.8022598870056498, recall 0.8022598870056498
2019-03-03T12:44:09.412511: step 84, loss 1.11766, accuracy 0.769531, precision 0.8695652173913043, recall 0.8205128205128205
2019-03-03T12:44:09.661844: step 85, loss 1.59761, accuracy 0.726562, precision 0.8064516129032258, recall 0.7575757575757576
2019-03-03T12:44:09.919157: step 86, loss 1.05644, accuracy 0.753906, precision 0.8607594936708861, recall 0.768361581920904
2019-03-03T12:44:10.168490: step 87, loss 1.46747, accuracy 0.695312, precision 0.7950310559006211, recall 0.7398843930635838
2019-03-03T12:44:10.411842: step 88, loss 1.23909, accuracy 0.742188, precision 0.8051948051948052, recall 0.775
2019-03-03T12:44:10.653199: step 89, loss 1.06558, accuracy 0.746094, precision 0.759493670886076, recall 0.8163265306122449
2019-03-03T12:44:10.904427: step 90, loss 1.54043, accuracy 0.6875, precision 0.6779661016949152, recall 0.8391608391608392
2019-03-03T12:44:11.143785: step 91, loss 1.47538, accuracy 0.707031, precision 0.7365269461077845, recall 0.7987012987012987
2019-03-03T12:44:11.395114: step 92, loss 1.11006, accuracy 0.734375, precision 0.7965116279069767, recall 0.8058823529411765
2019-03-03T12:44:11.641458: step 93, loss 1.47092, accuracy 0.71875, precision 0.8248587570621468, recall 0.7807486631016043
2019-03-03T12:44:11.889791: step 94, loss 1.43573, accuracy 0.710938, precision 0.791907514450867, recall 0.7828571428571428
2019-03-03T12:44:12.132653: step 95, loss 1.3086, accuracy 0.78125, precision 0.9190751445086706, recall 0.7910447761194029
2019-03-03T12:44:12.387977: step 96, loss 1.2976, accuracy 0.734375, precision 0.875, recall 0.7446808510638298
2019-03-03T12:44:12.647283: step 97, loss 1.02304, accuracy 0.777344, precision 0.8448275862068966, recall 0.8305084745762712
2019-03-03T12:44:12.900605: step 98, loss 1.25145, accuracy 0.777344, precision 0.874251497005988, recall 0.8021978021978022
2019-03-03T12:44:13.149939: step 99, loss 1.4575, accuracy 0.722656, precision 0.7821229050279329, recall 0.813953488372093
2019-03-03T12:44:13.404790: step 100, loss 1.63607, accuracy 0.664062, precision 0.6783625730994152, recall 0.7891156462585034
2019-03-03T12:44:13.655119: step 101, loss 1.42923, accuracy 0.703125, precision 0.7209302325581395, recall 0.8157894736842105
2019-03-03T12:44:13.906449: step 102, loss 1.16875, accuracy 0.738281, precision 0.7724550898203593, recall 0.8164556962025317
2019-03-03T12:44:14.156777: step 103, loss 1.36558, accuracy 0.714844, precision 0.7951807228915663, recall 0.7719298245614035
2019-03-03T12:44:14.403120: step 104, loss 1.26383, accuracy 0.710938, precision 0.8562874251497006, recall 0.7409326424870466
2019-03-03T12:44:14.644858: step 105, loss 1.32858, accuracy 0.742188, precision 0.8146067415730337, recall 0.8146067415730337
2019-03-03T12:44:14.895187: step 106, loss 1.17432, accuracy 0.757812, precision 0.848314606741573, recall 0.8118279569892473
2019-03-03T12:44:15.145519: step 107, loss 1.24843, accuracy 0.761719, precision 0.8700564971751412, recall 0.8020833333333334
2019-03-03T12:44:15.395849: step 108, loss 1.15129, accuracy 0.765625, precision 0.8235294117647058, recall 0.8235294117647058
2019-03-03T12:44:15.637205: step 109, loss 1.13438, accuracy 0.703125, precision 0.7579617834394905, recall 0.7579617834394905
2019-03-03T12:44:15.886537: step 110, loss 1.17757, accuracy 0.726562, precision 0.7555555555555555, recall 0.8395061728395061
2019-03-03T12:44:16.125897: step 111, loss 0.979221, accuracy 0.75, precision 0.7716049382716049, recall 0.8223684210526315
2019-03-03T12:44:16.375231: step 112, loss 1.00033, accuracy 0.757812, precision 0.8128654970760234, recall 0.8224852071005917
2019-03-03T12:44:16.618580: step 113, loss 0.961406, accuracy 0.765625, precision 0.8103448275862069, recall 0.8392857142857143
2019-03-03T12:44:16.876438: step 114, loss 1.11389, accuracy 0.742188, precision 0.8, recall 0.7901234567901234
2019-03-03T12:44:17.122779: step 115, loss 0.915937, accuracy 0.777344, precision 0.8457142857142858, recall 0.8314606741573034
2019-03-03T12:44:17.368124: step 116, loss 1.18735, accuracy 0.726562, precision 0.8023952095808383, recall 0.783625730994152
2019-03-03T12:44:17.625467: step 117, loss 1.2288, accuracy 0.75, precision 0.8074534161490683, recall 0.7975460122699386
2019-03-03T12:44:17.868816: step 118, loss 1.01923, accuracy 0.722656, precision 0.7692307692307693, recall 0.8024691358024691
2019-03-03T12:44:18.107179: step 119, loss 1.23385, accuracy 0.761719, precision 0.8235294117647058, recall 0.8187134502923976
2019-03-03T12:44:18.363493: step 120, loss 0.813012, accuracy 0.800781, precision 0.8809523809523809, recall 0.8268156424581006
2019-03-03T12:44:18.630779: step 121, loss 0.952834, accuracy 0.761719, precision 0.806060606060606, recall 0.8209876543209876
2019-03-03T12:44:18.888601: step 122, loss 1.14215, accuracy 0.730469, precision 0.8242424242424242, recall 0.7727272727272727
2019-03-03T12:44:19.057170: step 123, loss 0.882761, accuracy 0.770588, precision 0.8, recall 0.8648648648648649
2019-03-03T12:44:19.306014: step 124, loss 0.903831, accuracy 0.785156, precision 0.8520710059171598, recall 0.8275862068965517
2019-03-03T12:44:19.551357: step 125, loss 0.911061, accuracy 0.738281, precision 0.7888888888888889, recall 0.8304093567251462
2019-03-03T12:44:19.799694: step 126, loss 0.955604, accuracy 0.792969, precision 0.8546511627906976, recall 0.84
2019-03-03T12:44:20.055011: step 127, loss 0.988097, accuracy 0.742188, precision 0.834319526627219, recall 0.7877094972067039
2019-03-03T12:44:20.301863: step 128, loss 0.96073, accuracy 0.773438, precision 0.8488372093023255, recall 0.8202247191011236
2019-03-03T12:44:20.546206: step 129, loss 0.944348, accuracy 0.769531, precision 0.8457142857142858, recall 0.8222222222222222
2019-03-03T12:44:20.788072: step 130, loss 1.13263, accuracy 0.765625, precision 0.8711656441717791, recall 0.7845303867403315
2019-03-03T12:44:21.037918: step 131, loss 1.08232, accuracy 0.734375, precision 0.8070175438596491, recall 0.7976878612716763
2019-03-03T12:44:21.296250: step 132, loss 0.763353, accuracy 0.8125, precision 0.8848484848484849, recall 0.8342857142857143
2019-03-03T12:44:21.546073: step 133, loss 0.981866, accuracy 0.773438, precision 0.8407643312101911, recall 0.8
2019-03-03T12:44:21.796914: step 134, loss 0.696433, accuracy 0.761719, precision 0.7671957671957672, recall 0.8950617283950617
2019-03-03T12:44:22.042293: step 135, loss 0.776656, accuracy 0.753906, precision 0.7844311377245509, recall 0.8291139240506329
2019-03-03T12:44:22.281654: step 136, loss 0.858673, accuracy 0.761719, precision 0.8011695906432749, recall 0.8353658536585366
2019-03-03T12:44:22.537154: step 137, loss 0.974164, accuracy 0.769531, precision 0.8258426966292135, recall 0.84
2019-03-03T12:44:22.792983: step 138, loss 0.939803, accuracy 0.789062, precision 0.8930817610062893, recall 0.7932960893854749
2019-03-03T12:44:23.042442: step 139, loss 0.959378, accuracy 0.753906, precision 0.8793103448275862, recall 0.7846153846153846
2019-03-03T12:44:23.298757: step 140, loss 0.874834, accuracy 0.753906, precision 0.8218390804597702, recall 0.8171428571428572
2019-03-03T12:44:23.554075: step 141, loss 0.92248, accuracy 0.761719, precision 0.8520710059171598, recall 0.8
2019-03-03T12:44:23.809900: step 142, loss 1.01141, accuracy 0.75, precision 0.8106508875739645, recall 0.8106508875739645
2019-03-03T12:44:24.054274: step 143, loss 0.996482, accuracy 0.734375, precision 0.7777777777777778, recall 0.7974683544303798
2019-03-03T12:44:24.329538: step 144, loss 0.914755, accuracy 0.742188, precision 0.7906976744186046, recall 0.8192771084337349
2019-03-03T12:44:24.613778: step 145, loss 0.856128, accuracy 0.730469, precision 0.735632183908046, recall 0.847682119205298
2019-03-03T12:44:24.865109: step 146, loss 1.05811, accuracy 0.703125, precision 0.7556818181818182, recall 0.8012048192771084
2019-03-03T12:44:25.119440: step 147, loss 1.05822, accuracy 0.734375, precision 0.7797619047619048, recall 0.808641975308642
2019-03-03T12:44:25.367858: step 148, loss 0.843908, accuracy 0.742188, precision 0.8461538461538461, recall 0.7814207650273224
2019-03-03T12:44:25.631100: step 149, loss 0.886397, accuracy 0.808594, precision 0.9447852760736196, recall 0.7938144329896907
2019-03-03T12:44:25.880228: step 150, loss 1.1492, accuracy 0.726562, precision 0.8741721854304636, recall 0.7213114754098361
2019-03-03T12:44:26.137540: step 151, loss 0.784452, accuracy 0.777344, precision 0.8529411764705882, recall 0.8192090395480226
2019-03-03T12:44:26.392876: step 152, loss 0.968677, accuracy 0.742188, precision 0.8121212121212121, recall 0.7928994082840237
2019-03-03T12:44:26.638246: step 153, loss 1.10016, accuracy 0.707031, precision 0.6790123456790124, recall 0.8270676691729323
2019-03-03T12:44:26.898549: step 154, loss 0.941526, accuracy 0.738281, precision 0.7678571428571429, recall 0.821656050955414
2019-03-03T12:44:27.147883: step 155, loss 0.840012, accuracy 0.75, precision 0.7470588235294118, recall 0.8581081081081081
2019-03-03T12:44:27.388241: step 156, loss 0.6437, accuracy 0.773438, precision 0.8181818181818182, recall 0.8470588235294118
2019-03-03T12:44:27.628110: step 157, loss 0.800449, accuracy 0.804688, precision 0.8786127167630058, recall 0.8397790055248618
2019-03-03T12:44:27.873962: step 158, loss 0.953372, accuracy 0.765625, precision 0.9024390243902439, recall 0.7708333333333334
2019-03-03T12:44:28.124293: step 159, loss 1.1432, accuracy 0.734375, precision 0.844311377245509, recall 0.7704918032786885
2019-03-03T12:44:28.365647: step 160, loss 0.993597, accuracy 0.730469, precision 0.8214285714285714, recall 0.7796610169491526
2019-03-03T12:44:28.609019: step 161, loss 0.707951, accuracy 0.769531, precision 0.8373493975903614, recall 0.8128654970760234
2019-03-03T12:44:28.863339: step 162, loss 0.902111, accuracy 0.726562, precision 0.7978142076502732, recall 0.8156424581005587
2019-03-03T12:44:29.122670: step 163, loss 0.818267, accuracy 0.765625, precision 0.8024691358024691, recall 0.8227848101265823
2019-03-03T12:44:29.290241: step 164, loss 1.12531, accuracy 0.717647, precision 0.7403846153846154, recall 0.7857142857142857
2019-03-03T12:44:29.536094: step 165, loss 0.655383, accuracy 0.789062, precision 0.8343949044585988, recall 0.8238993710691824
2019-03-03T12:44:29.778447: step 166, loss 0.740045, accuracy 0.769531, precision 0.7428571428571429, recall 0.9027777777777778
2019-03-03T12:44:30.039755: step 167, loss 0.697239, accuracy 0.789062, precision 0.8032786885245902, recall 0.8909090909090909
2019-03-03T12:44:30.286095: step 168, loss 0.889421, accuracy 0.742188, precision 0.8, recall 0.8095238095238095
2019-03-03T12:44:30.530966: step 169, loss 0.707221, accuracy 0.78125, precision 0.8837209302325582, recall 0.8085106382978723
2019-03-03T12:44:30.780299: step 170, loss 0.950163, accuracy 0.769531, precision 0.9130434782608695, recall 0.765625
2019-03-03T12:44:31.025643: step 171, loss 0.760389, accuracy 0.753906, precision 0.8888888888888888, recall 0.7755102040816326
2019-03-03T12:44:31.275974: step 172, loss 0.811332, accuracy 0.789062, precision 0.9314285714285714, recall 0.7951219512195122
2019-03-03T12:44:31.518835: step 173, loss 0.684623, accuracy 0.800781, precision 0.9151515151515152, recall 0.8031914893617021
2019-03-03T12:44:31.766194: step 174, loss 0.689999, accuracy 0.78125, precision 0.7966101694915254, recall 0.8757763975155279
2019-03-03T12:44:32.014043: step 175, loss 0.780947, accuracy 0.757812, precision 0.7441860465116279, recall 0.8767123287671232
2019-03-03T12:44:32.258389: step 176, loss 0.641124, accuracy 0.757812, precision 0.7701863354037267, recall 0.8322147651006712
2019-03-03T12:44:32.499744: step 177, loss 0.483896, accuracy 0.824219, precision 0.8305084745762712, recall 0.9074074074074074
2019-03-03T12:44:32.750075: step 178, loss 0.671975, accuracy 0.769531, precision 0.7894736842105263, recall 0.8544303797468354
2019-03-03T12:44:33.001923: step 179, loss 0.694713, accuracy 0.777344, precision 0.8536585365853658, recall 0.8092485549132948
2019-03-03T12:44:33.252777: step 180, loss 0.699037, accuracy 0.816406, precision 0.9107142857142857, recall 0.827027027027027
2019-03-03T12:44:33.503111: step 181, loss 0.544339, accuracy 0.832031, precision 0.9322033898305084, recall 0.8418367346938775
2019-03-03T12:44:33.737483: step 182, loss 0.83676, accuracy 0.734375, precision 0.8766233766233766, recall 0.7336956521739131
2019-03-03T12:44:33.993808: step 183, loss 0.761193, accuracy 0.777344, precision 0.8860759493670886, recall 0.7821229050279329
2019-03-03T12:44:34.244814: step 184, loss 0.792896, accuracy 0.777344, precision 0.8484848484848485, recall 0.813953488372093
2019-03-03T12:44:34.487169: step 185, loss 0.805222, accuracy 0.753906, precision 0.791907514450867, recall 0.8353658536585366
2019-03-03T12:44:34.723536: step 186, loss 0.692105, accuracy 0.742188, precision 0.7528735632183908, recall 0.8506493506493507
2019-03-03T12:44:34.977223: step 187, loss 0.612657, accuracy 0.753906, precision 0.7625, recall 0.8299319727891157
2019-03-03T12:44:35.220729: step 188, loss 0.752719, accuracy 0.71875, precision 0.7628205128205128, recall 0.7727272727272727
2019-03-03T12:44:35.465100: step 189, loss 0.713752, accuracy 0.785156, precision 0.8727272727272727, recall 0.8089887640449438
2019-03-03T12:44:35.712949: step 190, loss 0.754142, accuracy 0.75, precision 0.8053691275167785, recall 0.7741935483870968
2019-03-03T12:44:35.962288: step 191, loss 0.727361, accuracy 0.769531, precision 0.8212290502793296, recall 0.8448275862068966
2019-03-03T12:44:36.223584: step 192, loss 0.789319, accuracy 0.734375, precision 0.7932960893854749, recall 0.8208092485549133
2019-03-03T12:44:36.483887: step 193, loss 0.633157, accuracy 0.789062, precision 0.8571428571428571, recall 0.8379888268156425
2019-03-03T12:44:36.738208: step 194, loss 0.781574, accuracy 0.761719, precision 0.8263473053892215, recall 0.8117647058823529
2019-03-03T12:44:36.998531: step 195, loss 0.689287, accuracy 0.792969, precision 0.896969696969697, recall 0.8043478260869565
2019-03-03T12:44:37.256840: step 196, loss 0.698705, accuracy 0.800781, precision 0.8806818181818182, recall 0.8378378378378378
2019-03-03T12:44:37.515151: step 197, loss 0.660875, accuracy 0.804688, precision 0.8595505617977528, recall 0.8595505617977528
2019-03-03T12:44:37.773460: step 198, loss 0.580355, accuracy 0.792969, precision 0.8938547486033519, recall 0.8247422680412371
2019-03-03T12:44:38.025785: step 199, loss 0.633253, accuracy 0.769531, precision 0.8502994011976048, recall 0.8068181818181818
2019-03-03T12:44:38.281103: step 200, loss 0.687571, accuracy 0.753906, precision 0.78125, recall 0.8169934640522876
2019-03-03T12:44:38.537419: step 201, loss 0.81715, accuracy 0.726562, precision 0.7430167597765364, recall 0.8471337579617835
2019-03-03T12:44:38.782780: step 202, loss 0.574123, accuracy 0.792969, precision 0.808641975308642, recall 0.8562091503267973
2019-03-03T12:44:39.037095: step 203, loss 0.734639, accuracy 0.738281, precision 0.8287292817679558, recall 0.8064516129032258
2019-03-03T12:44:39.297762: step 204, loss 0.56653, accuracy 0.808594, precision 0.9006211180124224, recall 0.8146067415730337
2019-03-03T12:44:39.472556: step 205, loss 0.672089, accuracy 0.794118, precision 0.8389830508474576, recall 0.8608695652173913
2019-03-03T12:44:39.721912: step 206, loss 0.615004, accuracy 0.796875, precision 0.8869047619047619, recall 0.8186813186813187
2019-03-03T12:44:39.978246: step 207, loss 0.609261, accuracy 0.761719, precision 0.8148148148148148, recall 0.8098159509202454
2019-03-03T12:44:40.229639: step 208, loss 0.68362, accuracy 0.730469, precision 0.8192771084337349, recall 0.7771428571428571
2019-03-03T12:44:40.477975: step 209, loss 0.542393, accuracy 0.796875, precision 0.8461538461538461, recall 0.8461538461538461
2019-03-03T12:44:40.717684: step 210, loss 0.59595, accuracy 0.777344, precision 0.8048780487804879, recall 0.8407643312101911
2019-03-03T12:44:40.967029: step 211, loss 0.670234, accuracy 0.761719, precision 0.7735849056603774, recall 0.831081081081081
2019-03-03T12:44:41.217354: step 212, loss 0.609536, accuracy 0.8125, precision 0.845679012345679, recall 0.85625
2019-03-03T12:44:41.467618: step 213, loss 0.609552, accuracy 0.78125, precision 0.8418079096045198, recall 0.8418079096045198
2019-03-03T12:44:41.715949: step 214, loss 0.693784, accuracy 0.75, precision 0.8022598870056498, recall 0.8304093567251462
2019-03-03T12:44:41.965420: step 215, loss 0.648635, accuracy 0.792969, precision 0.8430232558139535, recall 0.847953216374269
2019-03-03T12:44:42.203308: step 216, loss 0.578819, accuracy 0.796875, precision 0.9036144578313253, recall 0.8064516129032258
2019-03-03T12:44:42.449639: step 217, loss 0.672889, accuracy 0.78125, precision 0.8908045977011494, recall 0.8072916666666666
2019-03-03T12:44:42.703961: step 218, loss 0.546402, accuracy 0.800781, precision 0.8848484848484849, recall 0.8202247191011236
2019-03-03T12:44:42.956286: step 219, loss 0.709629, accuracy 0.75, precision 0.815028901734104, recall 0.815028901734104
2019-03-03T12:44:43.205873: step 220, loss 0.748804, accuracy 0.726562, precision 0.7756410256410257, recall 0.7756410256410257
2019-03-03T12:44:43.440757: step 221, loss 0.543236, accuracy 0.78125, precision 0.8238993710691824, recall 0.8238993710691824
2019-03-03T12:44:43.685104: step 222, loss 0.525593, accuracy 0.769531, precision 0.7900552486187845, recall 0.8719512195121951
2019-03-03T12:44:43.935941: step 223, loss 0.537845, accuracy 0.808594, precision 0.8154761904761905, recall 0.8838709677419355
2019-03-03T12:44:44.181285: step 224, loss 0.462437, accuracy 0.808594, precision 0.8011363636363636, recall 0.9096774193548387
2019-03-03T12:44:44.434601: step 225, loss 0.694041, accuracy 0.765625, precision 0.8490566037735849, recall 0.7894736842105263
2019-03-03T12:44:44.685459: step 226, loss 0.585487, accuracy 0.800781, precision 0.9147727272727273, recall 0.817258883248731
2019-03-03T12:44:44.931344: step 227, loss 0.566736, accuracy 0.824219, precision 0.9325842696629213, recall 0.8341708542713567
2019-03-03T12:44:45.172701: step 228, loss 0.724862, accuracy 0.757812, precision 0.8670886075949367, recall 0.7696629213483146
2019-03-03T12:44:45.427019: step 229, loss 0.487652, accuracy 0.828125, precision 0.9213483146067416, recall 0.845360824742268
2019-03-03T12:44:45.673360: step 230, loss 0.521669, accuracy 0.773438, precision 0.8435754189944135, recall 0.8342541436464088
2019-03-03T12:44:45.925686: step 231, loss 0.455113, accuracy 0.804688, precision 0.8606060606060606, recall 0.8402366863905325
2019-03-03T12:44:46.172053: step 232, loss 0.596923, accuracy 0.765625, precision 0.7702702702702703, recall 0.8142857142857143
2019-03-03T12:44:46.425885: step 233, loss 0.543431, accuracy 0.773438, precision 0.7784090909090909, recall 0.8782051282051282
2019-03-03T12:44:46.673223: step 234, loss 0.587754, accuracy 0.792969, precision 0.8203592814371258, recall 0.85625
2019-03-03T12:44:46.926067: step 235, loss 0.587856, accuracy 0.78125, precision 0.8433734939759037, recall 0.8235294117647058
2019-03-03T12:44:47.169416: step 236, loss 0.639239, accuracy 0.761719, precision 0.7962962962962963, recall 0.821656050955414
2019-03-03T12:44:47.421252: step 237, loss 0.628189, accuracy 0.765625, precision 0.875, recall 0.7903225806451613
2019-03-03T12:44:47.673578: step 238, loss 0.438464, accuracy 0.839844, precision 0.9289617486338798, recall 0.8585858585858586
2019-03-03T12:44:47.916437: step 239, loss 0.639841, accuracy 0.78125, precision 0.8265895953757225, recall 0.8461538461538461
2019-03-03T12:44:48.164774: step 240, loss 0.393096, accuracy 0.84375, precision 0.8988095238095238, recall 0.867816091954023
2019-03-03T12:44:48.414109: step 241, loss 0.549841, accuracy 0.832031, precision 0.8588957055214724, recall 0.875
2019-03-03T12:44:48.656465: step 242, loss 0.627311, accuracy 0.753906, precision 0.8360655737704918, recall 0.8225806451612904
2019-03-03T12:44:48.912780: step 243, loss 0.530122, accuracy 0.800781, precision 0.8597560975609756, recall 0.834319526627219
2019-03-03T12:44:49.163621: step 244, loss 0.719801, accuracy 0.75, precision 0.8011049723756906, recall 0.838150289017341
2019-03-03T12:44:49.413971: step 245, loss 0.501307, accuracy 0.796875, precision 0.867816091954023, recall 0.8388888888888889
2019-03-03T12:44:49.584027: step 246, loss 0.466403, accuracy 0.8, precision 0.8672566371681416, recall 0.8376068376068376
2019-03-03T12:44:49.843333: step 247, loss 0.494946, accuracy 0.824219, precision 0.8988095238095238, recall 0.8435754189944135
2019-03-03T12:44:50.101643: step 248, loss 0.597235, accuracy 0.777344, precision 0.8774193548387097, recall 0.7816091954022989
2019-03-03T12:44:50.344997: step 249, loss 0.422467, accuracy 0.820312, precision 0.8862275449101796, recall 0.8457142857142858
2019-03-03T12:44:50.597152: step 250, loss 0.392797, accuracy 0.847656, precision 0.8795180722891566, recall 0.8848484848484849

Evaluation:
[[646 112]
 [130 268]]
2019-03-03T12:44:54.396376: step 250, loss 0.499355, accuracy 0.790657, precision 0.8522427440633246, recall 0.8324742268041238

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551638619\checkpoints\model-250

2019-03-03T12:44:55.764007: step 251, loss 0.586507, accuracy 0.796875, precision 0.8765432098765432, recall 0.8160919540229885
2019-03-03T12:44:56.206824: step 252, loss 0.526159, accuracy 0.796875, precision 0.8271604938271605, recall 0.8481012658227848
2019-03-03T12:44:56.684556: step 253, loss 0.555522, accuracy 0.769531, precision 0.7877094972067039, recall 0.8703703703703703
2019-03-03T12:44:57.154544: step 254, loss 0.510764, accuracy 0.804688, precision 0.819672131147541, recall 0.8982035928143712
2019-03-03T12:44:57.620483: step 255, loss 0.418044, accuracy 0.832031, precision 0.8765432098765432, recall 0.8606060606060606
2019-03-03T12:44:58.066316: step 256, loss 0.411063, accuracy 0.832031, precision 0.9337349397590361, recall 0.8288770053475936
2019-03-03T12:44:58.470751: step 257, loss 0.552798, accuracy 0.796875, precision 0.9032258064516129, recall 0.7909604519774012
2019-03-03T12:44:58.837769: step 258, loss 0.408402, accuracy 0.839844, precision 0.9156626506024096, recall 0.8491620111731844
2019-03-03T12:44:59.195813: step 259, loss 0.536146, accuracy 0.792969, precision 0.8580246913580247, recall 0.8224852071005917
2019-03-03T12:44:59.524470: step 260, loss 0.546705, accuracy 0.777344, precision 0.8171428571428572, recall 0.8511904761904762
2019-03-03T12:44:59.888454: step 261, loss 0.467439, accuracy 0.824219, precision 0.848314606741573, recall 0.893491124260355
2019-03-03T12:45:00.221562: step 262, loss 0.509421, accuracy 0.824219, precision 0.893491124260355, recall 0.848314606741573
2019-03-03T12:45:00.534688: step 263, loss 0.431042, accuracy 0.824219, precision 0.8950617283950617, recall 0.838150289017341
2019-03-03T12:45:00.825928: step 264, loss 0.453722, accuracy 0.800781, precision 0.8444444444444444, recall 0.8685714285714285
2019-03-03T12:45:01.091728: step 265, loss 0.486129, accuracy 0.785156, precision 0.8097826086956522, recall 0.8816568047337278
2019-03-03T12:45:01.350057: step 266, loss 0.554731, accuracy 0.769531, precision 0.7941176470588235, recall 0.8490566037735849
2019-03-03T12:45:01.596398: step 267, loss 0.501675, accuracy 0.8125, precision 0.8628571428571429, recall 0.8628571428571429
2019-03-03T12:45:01.846729: step 268, loss 0.489093, accuracy 0.796875, precision 0.8713450292397661, recall 0.8324022346368715
2019-03-03T12:45:02.098078: step 269, loss 0.474685, accuracy 0.796875, precision 0.8848484848484849, recall 0.8156424581005587
2019-03-03T12:45:02.346414: step 270, loss 0.537458, accuracy 0.765625, precision 0.8862275449101796, recall 0.783068783068783
2019-03-03T12:45:02.608230: step 271, loss 0.48914, accuracy 0.816406, precision 0.8816568047337278, recall 0.8465909090909091
2019-03-03T12:45:02.850575: step 272, loss 0.525018, accuracy 0.773438, precision 0.861271676300578, recall 0.8142076502732241
2019-03-03T12:45:03.105894: step 273, loss 0.573686, accuracy 0.777344, precision 0.7719298245614035, recall 0.88
2019-03-03T12:45:03.350240: step 274, loss 0.595764, accuracy 0.785156, precision 0.8074534161490683, recall 0.8441558441558441
2019-03-03T12:45:03.597100: step 275, loss 0.459534, accuracy 0.800781, precision 0.84, recall 0.8647058823529412
2019-03-03T12:45:03.836486: step 276, loss 0.476516, accuracy 0.816406, precision 0.8903225806451613, recall 0.8214285714285714
2019-03-03T12:45:04.088811: step 277, loss 0.482962, accuracy 0.820312, precision 0.9047619047619048, recall 0.8351648351648352
2019-03-03T12:45:04.340140: step 278, loss 0.515823, accuracy 0.804688, precision 0.8552631578947368, recall 0.8227848101265823
2019-03-03T12:45:04.587479: step 279, loss 0.463196, accuracy 0.820312, precision 0.8647058823529412, recall 0.8647058823529412
2019-03-03T12:45:04.843802: step 280, loss 0.537879, accuracy 0.777344, precision 0.8203592814371258, recall 0.8353658536585366
2019-03-03T12:45:05.089128: step 281, loss 0.505083, accuracy 0.808594, precision 0.8606060606060606, recall 0.8452380952380952
2019-03-03T12:45:05.337465: step 282, loss 0.428255, accuracy 0.8125, precision 0.8305084745762712, recall 0.8909090909090909
2019-03-03T12:45:05.584803: step 283, loss 0.472003, accuracy 0.84375, precision 0.864406779661017, recall 0.9053254437869822
2019-03-03T12:45:05.832144: step 284, loss 0.353335, accuracy 0.832031, precision 0.9117647058823529, recall 0.8469945355191257
2019-03-03T12:45:06.101423: step 285, loss 0.381564, accuracy 0.847656, precision 0.9047619047619048, recall 0.890625
2019-03-03T12:45:06.386659: step 286, loss 0.488107, accuracy 0.800781, precision 0.8786127167630058, recall 0.8351648351648352
2019-03-03T12:45:06.563188: step 287, loss 0.599413, accuracy 0.788235, precision 0.8869565217391304, recall 0.816
2019-03-03T12:45:06.821500: step 288, loss 0.52959, accuracy 0.785156, precision 0.896969696969697, recall 0.7956989247311828
2019-03-03T12:45:07.080332: step 289, loss 0.4037, accuracy 0.808594, precision 0.8588957055214724, recall 0.8433734939759037
2019-03-03T12:45:07.327711: step 290, loss 0.435294, accuracy 0.835938, precision 0.8728323699421965, recall 0.8830409356725146
2019-03-03T12:45:07.581033: step 291, loss 0.490941, accuracy 0.800781, precision 0.8275862068965517, recall 0.8727272727272727
2019-03-03T12:45:07.830379: step 292, loss 0.491402, accuracy 0.785156, precision 0.8106508875739645, recall 0.85625
2019-03-03T12:45:08.084687: step 293, loss 0.465561, accuracy 0.820312, precision 0.875, recall 0.8433734939759037
2019-03-03T12:45:08.334532: step 294, loss 0.393734, accuracy 0.84375, precision 0.8461538461538461, recall 0.927710843373494
2019-03-03T12:45:08.577880: step 295, loss 0.439016, accuracy 0.816406, precision 0.8688524590163934, recall 0.8736263736263736
2019-03-03T12:45:08.818239: step 296, loss 0.440074, accuracy 0.816406, precision 0.8793103448275862, recall 0.8547486033519553
2019-03-03T12:45:09.074163: step 297, loss 0.415711, accuracy 0.835938, precision 0.9195402298850575, recall 0.851063829787234
2019-03-03T12:45:09.314520: step 298, loss 0.58137, accuracy 0.753906, precision 0.8764705882352941, recall 0.7801047120418848
2019-03-03T12:45:09.561858: step 299, loss 0.320783, accuracy 0.84375, precision 0.936046511627907, recall 0.8473684210526315
2019-03-03T12:45:09.812194: step 300, loss 0.492136, accuracy 0.808594, precision 0.9181286549707602, recall 0.8177083333333334
2019-03-03T12:45:10.073496: step 301, loss 0.430082, accuracy 0.816406, precision 0.861271676300578, recall 0.8662790697674418
2019-03-03T12:45:10.320075: step 302, loss 0.411657, accuracy 0.808594, precision 0.8563218390804598, recall 0.861271676300578
2019-03-03T12:45:10.564421: step 303, loss 0.358894, accuracy 0.859375, precision 0.8662790697674418, recall 0.9197530864197531
2019-03-03T12:45:10.811904: step 304, loss 0.447322, accuracy 0.816406, precision 0.8435754189944135, recall 0.888235294117647
2019-03-03T12:45:11.068729: step 305, loss 0.407193, accuracy 0.839844, precision 0.9177215189873418, recall 0.838150289017341
2019-03-03T12:45:11.315072: step 306, loss 0.609439, accuracy 0.753906, precision 0.8263473053892215, recall 0.8023255813953488
2019-03-03T12:45:11.555872: step 307, loss 0.421846, accuracy 0.828125, precision 0.8698224852071006, recall 0.8698224852071006
2019-03-03T12:45:11.803680: step 308, loss 0.509899, accuracy 0.804688, precision 0.8982035928143712, recall 0.819672131147541
2019-03-03T12:45:12.044070: step 309, loss 0.387073, accuracy 0.84375, precision 0.875, recall 0.8953488372093024
2019-03-03T12:45:12.285943: step 310, loss 0.378582, accuracy 0.84375, precision 0.9027027027027027, recall 0.8835978835978836
2019-03-03T12:45:12.539260: step 311, loss 0.457584, accuracy 0.8125, precision 0.8757763975155279, recall 0.834319526627219
2019-03-03T12:45:12.781610: step 312, loss 0.39644, accuracy 0.839844, precision 0.8982035928143712, recall 0.8620689655172413
2019-03-03T12:45:13.024475: step 313, loss 0.441188, accuracy 0.828125, precision 0.8837209302325582, recall 0.8636363636363636
2019-03-03T12:45:13.266826: step 314, loss 0.448129, accuracy 0.792969, precision 0.8553459119496856, recall 0.8192771084337349
2019-03-03T12:45:13.510176: step 315, loss 0.503273, accuracy 0.785156, precision 0.8258064516129032, recall 0.8205128205128205
2019-03-03T12:45:13.773824: step 316, loss 0.619891, accuracy 0.757812, precision 0.8074534161490683, recall 0.8074534161490683
2019-03-03T12:45:14.021188: step 317, loss 0.491543, accuracy 0.789062, precision 0.8143712574850299, recall 0.8553459119496856
2019-03-03T12:45:14.265534: step 318, loss 0.50508, accuracy 0.8125, precision 0.877906976744186, recall 0.848314606741573
2019-03-03T12:45:14.514869: step 319, loss 0.50137, accuracy 0.820312, precision 0.9192546583850931, recall 0.8176795580110497
2019-03-03T12:45:14.767194: step 320, loss 0.440881, accuracy 0.832031, precision 0.9161676646706587, recall 0.8406593406593407
2019-03-03T12:45:15.019030: step 321, loss 0.498464, accuracy 0.789062, precision 0.84472049689441, recall 0.8242424242424242
2019-03-03T12:45:15.270866: step 322, loss 0.483314, accuracy 0.800781, precision 0.8571428571428571, recall 0.8421052631578947
2019-03-03T12:45:15.512219: step 323, loss 0.477216, accuracy 0.796875, precision 0.8352272727272727, recall 0.8647058823529412
2019-03-03T12:45:15.764545: step 324, loss 0.381138, accuracy 0.84375, precision 0.9025974025974026, recall 0.8475609756097561
2019-03-03T12:45:16.019863: step 325, loss 0.49535, accuracy 0.804688, precision 0.8282208588957055, recall 0.8598726114649682
2019-03-03T12:45:16.275180: step 326, loss 0.439689, accuracy 0.78125, precision 0.8383233532934131, recall 0.8284023668639053
2019-03-03T12:45:16.527680: step 327, loss 0.445998, accuracy 0.824219, precision 0.8813559322033898, recall 0.8666666666666667
2019-03-03T12:45:16.696230: step 328, loss 0.384669, accuracy 0.847059, precision 0.8983050847457628, recall 0.8833333333333333
2019-03-03T12:45:16.976990: step 329, loss 0.418292, accuracy 0.816406, precision 0.9006622516556292, recall 0.8095238095238095
2019-03-03T12:45:17.235328: step 330, loss 0.352119, accuracy 0.867188, precision 0.9176470588235294, recall 0.8863636363636364
2019-03-03T12:45:17.489648: step 331, loss 0.420067, accuracy 0.820312, precision 0.8579881656804734, recall 0.8682634730538922
2019-03-03T12:45:17.729028: step 332, loss 0.353601, accuracy 0.847656, precision 0.8928571428571429, recall 0.8771929824561403
2019-03-03T12:45:17.995917: step 333, loss 0.379302, accuracy 0.832031, precision 0.896551724137931, recall 0.861878453038674
2019-03-03T12:45:18.248750: step 334, loss 0.40542, accuracy 0.820312, precision 0.8670886075949367, recall 0.845679012345679
2019-03-03T12:45:18.498084: step 335, loss 0.375579, accuracy 0.859375, precision 0.8928571428571429, recall 0.8928571428571429
2019-03-03T12:45:18.740436: step 336, loss 0.394092, accuracy 0.839844, precision 0.8633540372670807, recall 0.879746835443038
2019-03-03T12:45:18.985781: step 337, loss 0.361258, accuracy 0.824219, precision 0.8742138364779874, recall 0.8475609756097561
2019-03-03T12:45:19.228640: step 338, loss 0.419957, accuracy 0.832031, precision 0.8529411764705882, recall 0.8895705521472392
2019-03-03T12:45:19.472010: step 339, loss 0.426443, accuracy 0.808594, precision 0.8611111111111112, recall 0.8659217877094972
2019-03-03T12:45:19.717864: step 340, loss 0.502346, accuracy 0.785156, precision 0.8682634730538922, recall 0.8146067415730337
2019-03-03T12:45:19.978168: step 341, loss 0.411085, accuracy 0.851562, precision 0.9030303030303031, recall 0.8713450292397661
2019-03-03T12:45:20.234491: step 342, loss 0.458958, accuracy 0.804688, precision 0.8802395209580839, recall 0.8305084745762712
2019-03-03T12:45:20.479191: step 343, loss 0.5059, accuracy 0.789062, precision 0.8926553672316384, recall 0.8186528497409327
2019-03-03T12:45:20.724382: step 344, loss 0.355131, accuracy 0.855469, precision 0.9261363636363636, recall 0.8716577540106952
2019-03-03T12:45:20.968698: step 345, loss 0.388696, accuracy 0.824219, precision 0.8554913294797688, recall 0.8809523809523809
2019-03-03T12:45:21.225532: step 346, loss 0.489834, accuracy 0.804688, precision 0.8502994011976048, recall 0.8502994011976048
2019-03-03T12:45:21.470876: step 347, loss 0.395284, accuracy 0.832031, precision 0.9017341040462428, recall 0.8571428571428571
2019-03-03T12:45:21.719212: step 348, loss 0.49689, accuracy 0.789062, precision 0.8323699421965318, recall 0.8520710059171598
2019-03-03T12:45:21.968546: step 349, loss 0.390576, accuracy 0.847656, precision 0.9130434782608695, recall 0.8546511627906976
2019-03-03T12:45:22.219873: step 350, loss 0.4097, accuracy 0.828125, precision 0.888235294117647, recall 0.8579545454545454
2019-03-03T12:45:22.454756: step 351, loss 0.386169, accuracy 0.851562, precision 0.9064327485380117, recall 0.8757062146892656
2019-03-03T12:45:22.704091: step 352, loss 0.454101, accuracy 0.792969, precision 0.8411764705882353, recall 0.8461538461538461
2019-03-03T12:45:22.959408: step 353, loss 0.478742, accuracy 0.816406, precision 0.8713450292397661, recall 0.8563218390804598
2019-03-03T12:45:23.201759: step 354, loss 0.457218, accuracy 0.78125, precision 0.8855421686746988, recall 0.7989130434782609
2019-03-03T12:45:23.441390: step 355, loss 0.424804, accuracy 0.792969, precision 0.8713450292397661, recall 0.8277777777777777
2019-03-03T12:45:23.698738: step 356, loss 0.345184, accuracy 0.824219, precision 0.8855421686746988, recall 0.8497109826589595
2019-03-03T12:45:23.940092: step 357, loss 0.345567, accuracy 0.820312, precision 0.839572192513369, recall 0.9075144508670521
2019-03-03T12:45:24.199938: step 358, loss 0.433467, accuracy 0.796875, precision 0.8539325842696629, recall 0.8539325842696629
2019-03-03T12:45:24.440297: step 359, loss 0.35074, accuracy 0.839844, precision 0.9112426035502958, recall 0.8555555555555555
2019-03-03T12:45:24.681652: step 360, loss 0.410555, accuracy 0.828125, precision 0.9053254437869822, recall 0.8453038674033149
2019-03-03T12:45:24.924003: step 361, loss 0.38596, accuracy 0.84375, precision 0.9226190476190477, recall 0.8516483516483516
2019-03-03T12:45:25.182319: step 362, loss 0.300438, accuracy 0.855469, precision 0.9302325581395349, recall 0.8648648648648649
2019-03-03T12:45:25.441111: step 363, loss 0.410617, accuracy 0.820312, precision 0.86875, recall 0.8475609756097561
2019-03-03T12:45:25.682467: step 364, loss 0.38531, accuracy 0.828125, precision 0.8690476190476191, recall 0.8690476190476191
2019-03-03T12:45:25.931953: step 365, loss 0.456739, accuracy 0.796875, precision 0.8421052631578947, recall 0.8520710059171598
2019-03-03T12:45:26.185295: step 366, loss 0.39885, accuracy 0.828125, precision 0.8820224719101124, recall 0.8722222222222222
2019-03-03T12:45:26.425653: step 367, loss 0.485677, accuracy 0.769531, precision 0.8187134502923976, recall 0.8333333333333334
2019-03-03T12:45:26.673994: step 368, loss 0.485895, accuracy 0.773438, precision 0.808641975308642, recall 0.8291139240506329
2019-03-03T12:45:26.845534: step 369, loss 0.409207, accuracy 0.817647, precision 0.8648648648648649, recall 0.8571428571428571
2019-03-03T12:45:27.093894: step 370, loss 0.350304, accuracy 0.832031, precision 0.8895705521472392, recall 0.8529411764705882
2019-03-03T12:45:27.335267: step 371, loss 0.421717, accuracy 0.785156, precision 0.8085106382978723, recall 0.8888888888888888
2019-03-03T12:45:27.586596: step 372, loss 0.320302, accuracy 0.875, precision 0.9322033898305084, recall 0.8918918918918919
2019-03-03T12:45:27.827950: step 373, loss 0.345344, accuracy 0.855469, precision 0.94, recall 0.834319526627219
2019-03-03T12:45:28.073294: step 374, loss 0.371694, accuracy 0.835938, precision 0.9447852760736196, recall 0.8235294117647058
2019-03-03T12:45:28.320644: step 375, loss 0.403975, accuracy 0.816406, precision 0.8588235294117647, recall 0.863905325443787
2019-03-03T12:45:28.568979: step 376, loss 0.415862, accuracy 0.8125, precision 0.8902439024390244, recall 0.8295454545454546
2019-03-03T12:45:28.814835: step 377, loss 0.331009, accuracy 0.832031, precision 0.8411764705882353, recall 0.89937106918239
2019-03-03T12:45:29.063189: step 378, loss 0.373609, accuracy 0.832031, precision 0.8666666666666667, recall 0.8719512195121951
2019-03-03T12:45:29.313951: step 379, loss 0.457382, accuracy 0.8125, precision 0.8860759493670886, recall 0.8235294117647058
2019-03-03T12:45:29.569287: step 380, loss 0.365, accuracy 0.832031, precision 0.8771929824561403, recall 0.872093023255814
2019-03-03T12:45:29.805656: step 381, loss 0.380882, accuracy 0.839844, precision 0.8870056497175142, recall 0.8820224719101124
2019-03-03T12:45:30.051996: step 382, loss 0.35728, accuracy 0.851562, precision 0.8795811518324608, recall 0.9180327868852459
2019-03-03T12:45:30.291874: step 383, loss 0.412367, accuracy 0.808594, precision 0.9117647058823529, recall 0.8201058201058201
2019-03-03T12:45:30.555181: step 384, loss 0.413293, accuracy 0.816406, precision 0.9107142857142857, recall 0.827027027027027
2019-03-03T12:45:30.799516: step 385, loss 0.407664, accuracy 0.832031, precision 0.9235294117647059, recall 0.839572192513369
2019-03-03T12:45:31.050361: step 386, loss 0.34699, accuracy 0.859375, precision 0.9476744186046512, recall 0.8578947368421053
2019-03-03T12:45:31.297699: step 387, loss 0.407431, accuracy 0.851562, precision 0.9221556886227545, recall 0.8603351955307262
2019-03-03T12:45:31.548031: step 388, loss 0.393529, accuracy 0.835938, precision 0.8758169934640523, recall 0.8535031847133758
2019-03-03T12:45:31.787415: step 389, loss 0.397953, accuracy 0.832031, precision 0.8224852071005917, recall 0.9144736842105263
2019-03-03T12:45:32.033756: step 390, loss 0.381505, accuracy 0.851562, precision 0.8406593406593407, recall 0.9444444444444444
2019-03-03T12:45:32.278175: step 391, loss 0.464636, accuracy 0.789062, precision 0.8125, recall 0.8441558441558441
2019-03-03T12:45:32.553610: step 392, loss 0.407773, accuracy 0.8125, precision 0.8653846153846154, recall 0.8333333333333334
2019-03-03T12:45:32.804456: step 393, loss 0.367715, accuracy 0.828125, precision 0.8971428571428571, recall 0.8579234972677595
2019-03-03T12:45:33.051801: step 394, loss 0.389828, accuracy 0.847656, precision 0.9518072289156626, recall 0.8359788359788359
2019-03-03T12:45:33.296831: step 395, loss 0.405302, accuracy 0.832031, precision 0.9281767955801105, recall 0.8484848484848485
2019-03-03T12:45:33.547183: step 396, loss 0.411873, accuracy 0.800781, precision 0.9181286549707602, recall 0.8092783505154639
2019-03-03T12:45:33.785060: step 397, loss 0.461933, accuracy 0.824219, precision 0.9030303030303031, recall 0.8370786516853933
2019-03-03T12:45:34.036387: step 398, loss 0.377273, accuracy 0.847656, precision 0.8571428571428571, recall 0.9056603773584906
2019-03-03T12:45:34.282729: step 399, loss 0.438002, accuracy 0.800781, precision 0.806060606060606, recall 0.875
2019-03-03T12:45:34.527080: step 400, loss 0.392609, accuracy 0.816406, precision 0.8170731707317073, recall 0.8874172185430463
2019-03-03T12:45:34.770198: step 401, loss 0.409099, accuracy 0.828125, precision 0.8538011695906432, recall 0.8848484848484849
2019-03-03T12:45:35.024518: step 402, loss 0.33881, accuracy 0.84375, precision 0.8734939759036144, recall 0.8841463414634146
2019-03-03T12:45:35.272853: step 403, loss 0.414528, accuracy 0.816406, precision 0.8827160493827161, recall 0.8362573099415205
2019-03-03T12:45:35.520212: step 404, loss 0.442475, accuracy 0.804688, precision 0.8870056497175142, recall 0.839572192513369
2019-03-03T12:45:35.767551: step 405, loss 0.402773, accuracy 0.839844, precision 0.9408284023668639, recall 0.8368421052631579
2019-03-03T12:45:36.029850: step 406, loss 0.415175, accuracy 0.808594, precision 0.8809523809523809, recall 0.8361581920903954
2019-03-03T12:45:36.282177: step 407, loss 0.389805, accuracy 0.820312, precision 0.9005847953216374, recall 0.8415300546448088
2019-03-03T12:45:36.526523: step 408, loss 0.376294, accuracy 0.84375, precision 0.903954802259887, recall 0.8743169398907104
2019-03-03T12:45:36.783835: step 409, loss 0.333213, accuracy 0.875, precision 0.8862275449101796, recall 0.9192546583850931
2019-03-03T12:45:36.946912: step 410, loss 0.302673, accuracy 0.852941, precision 0.8487394957983193, recall 0.9351851851851852
2019-03-03T12:45:37.195777: step 411, loss 0.341401, accuracy 0.839844, precision 0.8910256410256411, recall 0.852760736196319
2019-03-03T12:45:37.441107: step 412, loss 0.390465, accuracy 0.824219, precision 0.881578947368421, recall 0.8322981366459627
2019-03-03T12:45:37.688446: step 413, loss 0.348742, accuracy 0.863281, precision 0.8922155688622755, recall 0.8975903614457831
2019-03-03T12:45:37.943764: step 414, loss 0.454077, accuracy 0.804688, precision 0.8579881656804734, recall 0.847953216374269
2019-03-03T12:45:38.185119: step 415, loss 0.301599, accuracy 0.871094, precision 0.9289940828402367, recall 0.8820224719101124
2019-03-03T12:45:38.427470: step 416, loss 0.442946, accuracy 0.816406, precision 0.9017341040462428, recall 0.8387096774193549
2019-03-03T12:45:38.678309: step 417, loss 0.455336, accuracy 0.808594, precision 0.8924050632911392, recall 0.815028901734104
2019-03-03T12:45:38.920661: step 418, loss 0.373377, accuracy 0.835938, precision 0.9333333333333333, recall 0.8324324324324325
2019-03-03T12:45:39.163014: step 419, loss 0.293076, accuracy 0.863281, precision 0.9069767441860465, recall 0.8914285714285715
2019-03-03T12:45:39.413458: step 420, loss 0.383234, accuracy 0.8125, precision 0.8786127167630058, recall 0.8491620111731844
2019-03-03T12:45:39.653817: step 421, loss 0.411467, accuracy 0.84375, precision 0.9069767441860465, recall 0.8666666666666667
2019-03-03T12:45:39.901165: step 422, loss 0.39279, accuracy 0.824219, precision 0.8294117647058824, recall 0.8980891719745223
2019-03-03T12:45:40.145511: step 423, loss 0.374047, accuracy 0.820312, precision 0.8424242424242424, recall 0.8742138364779874
2019-03-03T12:45:40.387871: step 424, loss 0.398912, accuracy 0.808594, precision 0.8741721854304636, recall 0.8148148148148148
2019-03-03T12:45:40.636205: step 425, loss 0.295919, accuracy 0.890625, precision 0.907608695652174, recall 0.9382022471910112
2019-03-03T12:45:40.901498: step 426, loss 0.327962, accuracy 0.875, precision 0.9401197604790419, recall 0.8770949720670391
2019-03-03T12:45:41.146353: step 427, loss 0.378876, accuracy 0.839844, precision 0.9657142857142857, recall 0.8284313725490197
2019-03-03T12:45:41.405169: step 428, loss 0.346673, accuracy 0.84375, precision 0.9222222222222223, recall 0.8645833333333334
2019-03-03T12:45:41.653505: step 429, loss 0.370804, accuracy 0.863281, precision 0.9289940828402367, recall 0.8722222222222222
2019-03-03T12:45:41.889901: step 430, loss 0.414072, accuracy 0.8125, precision 0.927710843373494, recall 0.8105263157894737
2019-03-03T12:45:42.129261: step 431, loss 0.352665, accuracy 0.863281, precision 0.8876404494382022, recall 0.9132947976878613
2019-03-03T12:45:42.371615: step 432, loss 0.437365, accuracy 0.792969, precision 0.7962962962962963, recall 0.8657718120805369
2019-03-03T12:45:42.616958: step 433, loss 0.407702, accuracy 0.800781, precision 0.8363636363636363, recall 0.8518518518518519
2019-03-03T12:45:42.868287: step 434, loss 0.348196, accuracy 0.84375, precision 0.8690476190476191, recall 0.8902439024390244
2019-03-03T12:45:43.107160: step 435, loss 0.391803, accuracy 0.8125, precision 0.8793103448275862, recall 0.85
2019-03-03T12:45:43.676906: step 436, loss 0.355135, accuracy 0.847656, precision 0.9161290322580645, recall 0.8452380952380952
2019-03-03T12:45:44.896179: step 437, loss 0.320512, accuracy 0.851562, precision 0.9269662921348315, recall 0.868421052631579
2019-03-03T12:45:45.658189: step 438, loss 0.283275, accuracy 0.894531, precision 0.9319371727748691, recall 0.9270833333333334
2019-03-03T12:45:46.180792: step 439, loss 0.366034, accuracy 0.871094, precision 0.9467455621301775, recall 0.8695652173913043
2019-03-03T12:45:46.730838: step 440, loss 0.357209, accuracy 0.847656, precision 0.9085365853658537, recall 0.861271676300578
2019-03-03T12:45:47.235034: step 441, loss 0.37809, accuracy 0.816406, precision 0.896969696969697, recall 0.8314606741573034
2019-03-03T12:45:47.750170: step 442, loss 0.383022, accuracy 0.8125, precision 0.8862275449101796, recall 0.8361581920903954
2019-03-03T12:45:48.238721: step 443, loss 0.360057, accuracy 0.839844, precision 0.8679245283018868, recall 0.8734177215189873
2019-03-03T12:45:48.730408: step 444, loss 0.372234, accuracy 0.867188, precision 0.9058823529411765, recall 0.8953488372093024
2019-03-03T12:45:49.271960: step 445, loss 0.373136, accuracy 0.828125, precision 0.863905325443787, recall 0.874251497005988
2019-03-03T12:45:49.761716: step 446, loss 0.443, accuracy 0.84375, precision 0.85, recall 0.8947368421052632
2019-03-03T12:45:50.262383: step 447, loss 0.313535, accuracy 0.859375, precision 0.9298245614035088, recall 0.8688524590163934
2019-03-03T12:45:50.747087: step 448, loss 0.405201, accuracy 0.8125, precision 0.9085714285714286, recall 0.8324607329842932
2019-03-03T12:45:51.258717: step 449, loss 0.341218, accuracy 0.867188, precision 0.9027027027027027, recall 0.912568306010929
2019-03-03T12:45:51.733450: step 450, loss 0.340311, accuracy 0.820312, precision 0.8777777777777778, recall 0.8681318681318682
2019-03-03T12:45:52.046614: step 451, loss 0.305606, accuracy 0.841176, precision 0.923728813559322, recall 0.8582677165354331
2019-03-03T12:45:52.492687: step 452, loss 0.308563, accuracy 0.84375, precision 0.9382022471910112, recall 0.8520408163265306
2019-03-03T12:45:52.983903: step 453, loss 0.39288, accuracy 0.835938, precision 0.9012345679012346, recall 0.8488372093023255
2019-03-03T12:45:53.472637: step 454, loss 0.304724, accuracy 0.855469, precision 0.9156626506024096, recall 0.8685714285714285
2019-03-03T12:45:53.929497: step 455, loss 0.387061, accuracy 0.824219, precision 0.9025974025974026, recall 0.8224852071005917
2019-03-03T12:45:54.373310: step 456, loss 0.430465, accuracy 0.820312, precision 0.8827160493827161, recall 0.8411764705882353
2019-03-03T12:45:54.895911: step 457, loss 0.351587, accuracy 0.839844, precision 0.8830409356725146, recall 0.877906976744186
2019-03-03T12:45:55.372677: step 458, loss 0.339433, accuracy 0.839844, precision 0.8418079096045198, recall 0.9197530864197531
2019-03-03T12:45:55.840950: step 459, loss 0.316014, accuracy 0.851562, precision 0.9119496855345912, recall 0.8579881656804734
2019-03-03T12:45:56.327750: step 460, loss 0.384269, accuracy 0.824219, precision 0.8554216867469879, recall 0.8711656441717791
2019-03-03T12:45:56.789485: step 461, loss 0.293466, accuracy 0.882812, precision 0.8810810810810811, recall 0.9532163742690059
2019-03-03T12:45:57.265279: step 462, loss 0.319707, accuracy 0.863281, precision 0.9212121212121213, recall 0.8735632183908046
2019-03-03T12:45:57.742976: step 463, loss 0.349882, accuracy 0.84375, precision 0.9707602339181286, recall 0.8258706467661692
2019-03-03T12:45:58.200899: step 464, loss 0.334706, accuracy 0.847656, precision 0.9259259259259259, recall 0.847457627118644
2019-03-03T12:45:58.676630: step 465, loss 0.378898, accuracy 0.859375, precision 0.9386503067484663, recall 0.8547486033519553
2019-03-03T12:45:59.155349: step 466, loss 0.344199, accuracy 0.855469, precision 0.8896103896103896, recall 0.8726114649681529
2019-03-03T12:45:59.616119: step 467, loss 0.360611, accuracy 0.847656, precision 0.9, recall 0.8852459016393442
2019-03-03T12:46:00.095835: step 468, loss 0.385234, accuracy 0.824219, precision 0.8670520231213873, recall 0.872093023255814
2019-03-03T12:46:00.561588: step 469, loss 0.361747, accuracy 0.820312, precision 0.845679012345679, recall 0.8670886075949367
2019-03-03T12:46:01.008939: step 470, loss 0.371299, accuracy 0.808594, precision 0.810126582278481, recall 0.8707482993197279
2019-03-03T12:46:01.486175: step 471, loss 0.264101, accuracy 0.894531, precision 0.9053254437869822, recall 0.9329268292682927
2019-03-03T12:46:01.957951: step 472, loss 0.375925, accuracy 0.828125, precision 0.9047619047619048, recall 0.8444444444444444
2019-03-03T12:46:02.431682: step 473, loss 0.334424, accuracy 0.851562, precision 0.9252873563218391, recall 0.8655913978494624
2019-03-03T12:46:02.887465: step 474, loss 0.375697, accuracy 0.820312, precision 0.9240506329113924, recall 0.8111111111111111
2019-03-03T12:46:03.352221: step 475, loss 0.406051, accuracy 0.824219, precision 0.9230769230769231, recall 0.8297872340425532
2019-03-03T12:46:03.808039: step 476, loss 0.279805, accuracy 0.898438, precision 0.9476744186046512, recall 0.9055555555555556
2019-03-03T12:46:04.300761: step 477, loss 0.355991, accuracy 0.828125, precision 0.8895348837209303, recall 0.8595505617977528
2019-03-03T12:46:04.773493: step 478, loss 0.393854, accuracy 0.824219, precision 0.8387096774193549, recall 0.8666666666666667
2019-03-03T12:46:05.282526: step 479, loss 0.348214, accuracy 0.800781, precision 0.8426966292134831, recall 0.8670520231213873
2019-03-03T12:46:05.751818: step 480, loss 0.336116, accuracy 0.863281, precision 0.9130434782608695, recall 0.8983957219251337
2019-03-03T12:46:06.226605: step 481, loss 0.369869, accuracy 0.835938, precision 0.8698224852071006, recall 0.8802395209580839
2019-03-03T12:46:06.689189: step 482, loss 0.357149, accuracy 0.84375, precision 0.9221556886227545, recall 0.850828729281768
2019-03-03T12:46:07.191048: step 483, loss 0.356297, accuracy 0.832031, precision 0.8702702702702703, recall 0.8944444444444445
2019-03-03T12:46:07.654329: step 484, loss 0.313622, accuracy 0.863281, precision 0.9397590361445783, recall 0.861878453038674
2019-03-03T12:46:08.107119: step 485, loss 0.341561, accuracy 0.855469, precision 0.9457831325301205, recall 0.8486486486486486
2019-03-03T12:46:08.578924: step 486, loss 0.444673, accuracy 0.792969, precision 0.8787878787878788, recall 0.8146067415730337
2019-03-03T12:46:09.043651: step 487, loss 0.307056, accuracy 0.878906, precision 0.9217877094972067, recall 0.9065934065934066
2019-03-03T12:46:09.507924: step 488, loss 0.320913, accuracy 0.882812, precision 0.9257142857142857, recall 0.9050279329608939
2019-03-03T12:46:09.971200: step 489, loss 0.277569, accuracy 0.890625, precision 0.9096045197740112, recall 0.930635838150289
2019-03-03T12:46:10.437471: step 490, loss 0.330513, accuracy 0.855469, precision 0.8876404494382022, recall 0.9028571428571428
2019-03-03T12:46:10.919385: step 491, loss 0.357594, accuracy 0.820312, precision 0.8855421686746988, recall 0.8448275862068966
2019-03-03T12:46:11.227589: step 492, loss 0.311703, accuracy 0.852941, precision 0.896551724137931, recall 0.8888888888888888
2019-03-03T12:46:11.686874: step 493, loss 0.377131, accuracy 0.824219, precision 0.9096774193548387, recall 0.8197674418604651
2019-03-03T12:46:12.170578: step 494, loss 0.333671, accuracy 0.835938, precision 0.9202453987730062, recall 0.8379888268156425
2019-03-03T12:46:12.648339: step 495, loss 0.294376, accuracy 0.882812, precision 0.9473684210526315, recall 0.8852459016393442
2019-03-03T12:46:13.134001: step 496, loss 0.312568, accuracy 0.847656, precision 0.877906976744186, recall 0.893491124260355
2019-03-03T12:46:13.622756: step 497, loss 0.308676, accuracy 0.875, precision 0.906832298136646, recall 0.8957055214723927
2019-03-03T12:46:14.126447: step 498, loss 0.349085, accuracy 0.839844, precision 0.8397790055248618, recall 0.926829268292683
2019-03-03T12:46:14.606291: step 499, loss 0.339613, accuracy 0.847656, precision 0.8888888888888888, recall 0.8837209302325582
2019-03-03T12:46:15.101225: step 500, loss 0.374208, accuracy 0.835938, precision 0.8795180722891566, recall 0.8690476190476191

Evaluation:
[[666  92]
 [147 251]]
2019-03-03T12:46:15.971421: step 500, loss 0.450053, accuracy 0.793253, precision 0.8786279683377308, recall 0.8191881918819188

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551638619\checkpoints\model-500


Process finished with exit code 0
