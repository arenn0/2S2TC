"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
fastText model loaded
Pretrained Embedding: fastText
Italian: False
Loading data...
11566
Max Document length: 36
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 1
Train/Dev split: 10410/1156
2019-03-03 13:25:35.994564: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:475: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551641136

2019-03-03T13:25:38.596824: step 1, loss 2.21814, accuracy 0.472656, precision 0.5269461077844312, recall 0.6111111111111112
2019-03-03T13:25:39.032193: step 2, loss 4.57775, accuracy 0.640625, precision 0.9879518072289156, recall 0.6456692913385826
2019-03-03T13:25:39.459054: step 3, loss 2.52136, accuracy 0.621094, precision 0.9005847953216374, recall 0.6581196581196581
2019-03-03T13:25:39.898411: step 4, loss 2.77208, accuracy 0.425781, precision 0.23125, recall 0.6065573770491803
2019-03-03T13:25:40.365673: step 5, loss 3.22707, accuracy 0.382812, precision 0.1807909604519774, recall 0.7111111111111111
2019-03-03T13:25:40.793801: step 6, loss 2.14045, accuracy 0.511719, precision 0.5287356321839081, recall 0.6814814814814815
2019-03-03T13:25:41.233088: step 7, loss 2.30649, accuracy 0.625, precision 0.8787878787878788, recall 0.6561085972850679
2019-03-03T13:25:41.665916: step 8, loss 2.49319, accuracy 0.632812, precision 0.9457831325301205, recall 0.6487603305785123
2019-03-03T13:25:42.120237: step 9, loss 2.29699, accuracy 0.671875, precision 0.95, recall 0.6951219512195121
2019-03-03T13:25:42.563085: step 10, loss 2.75882, accuracy 0.609375, precision 0.9539473684210527, recall 0.6092436974789915
2019-03-03T13:25:42.996379: step 11, loss 1.82311, accuracy 0.578125, precision 0.7710843373493976, recall 0.6464646464646465
2019-03-03T13:25:43.433235: step 12, loss 1.67261, accuracy 0.515625, precision 0.5056179775280899, recall 0.7142857142857143
2019-03-03T13:25:43.866465: step 13, loss 2.45966, accuracy 0.402344, precision 0.3021978021978022, recall 0.6790123456790124
2019-03-03T13:25:44.351194: step 14, loss 1.94577, accuracy 0.476562, precision 0.3815028901734104, recall 0.7096774193548387
2019-03-03T13:25:45.571573: step 15, loss 1.75449, accuracy 0.507812, precision 0.546583850931677, recall 0.624113475177305
2019-03-03T13:25:47.562838: step 16, loss 1.39877, accuracy 0.613281, precision 0.7283236994219653, recall 0.7078651685393258
2019-03-03T13:25:49.593755: step 17, loss 1.39248, accuracy 0.664062, precision 0.8688524590163934, recall 0.7194570135746606
2019-03-03T13:25:51.588121: step 18, loss 1.72947, accuracy 0.679688, precision 0.9252873563218391, recall 0.7
2019-03-03T13:25:53.661446: step 19, loss 2.04673, accuracy 0.621094, precision 0.9197530864197531, recall 0.6394849785407726
2019-03-03T13:25:55.670950: step 20, loss 1.58289, accuracy 0.652344, precision 0.8977272727272727, recall 0.6899563318777293
2019-03-03T13:25:57.765880: step 21, loss 1.18011, accuracy 0.613281, precision 0.7558139534883721, recall 0.6951871657754011
2019-03-03T13:25:59.818396: step 22, loss 1.45936, accuracy 0.609375, precision 0.650887573964497, recall 0.7284768211920529
2019-03-03T13:26:02.000491: step 23, loss 1.49755, accuracy 0.558594, precision 0.5416666666666666, recall 0.7165354330708661
2019-03-03T13:26:03.335477: step 24, loss 1.33522, accuracy 0.539062, precision 0.5157232704402516, recall 0.6666666666666666
2019-03-03T13:26:04.095795: step 25, loss 1.29615, accuracy 0.558594, precision 0.6024844720496895, recall 0.6643835616438356
2019-03-03T13:26:04.851858: step 26, loss 1.26811, accuracy 0.535156, precision 0.630057803468208, recall 0.6646341463414634
2019-03-03T13:26:05.603630: step 27, loss 1.25685, accuracy 0.578125, precision 0.73125, recall 0.6428571428571429
2019-03-03T13:26:06.382802: step 28, loss 1.29493, accuracy 0.542969, precision 0.7133757961783439, recall 0.6086956521739131
2019-03-03T13:26:07.141792: step 29, loss 0.833306, accuracy 0.691406, precision 0.8162162162162162, recall 0.7704081632653061
2019-03-03T13:26:07.848436: step 30, loss 1.04621, accuracy 0.628906, precision 0.7687861271676301, recall 0.7074468085106383
2019-03-03T13:26:08.579518: step 31, loss 1.07377, accuracy 0.652344, precision 0.8395061728395061, recall 0.6834170854271356
2019-03-03T13:26:09.298582: step 32, loss 1.22947, accuracy 0.554688, precision 0.6325301204819277, recall 0.6645569620253164
2019-03-03T13:26:10.135370: step 33, loss 0.948468, accuracy 0.628906, precision 0.7192982456140351, recall 0.7235294117647059
2019-03-03T13:26:10.850586: step 34, loss 1.01467, accuracy 0.609375, precision 0.6904761904761905, recall 0.7073170731707317
2019-03-03T13:26:11.486918: step 35, loss 0.938021, accuracy 0.632812, precision 0.6720430107526881, recall 0.7911392405063291
2019-03-03T13:26:12.136795: step 36, loss 0.79614, accuracy 0.679688, precision 0.7732558139534884, recall 0.7556818181818182
2019-03-03T13:26:12.761634: step 37, loss 0.853712, accuracy 0.652344, precision 0.7722222222222223, recall 0.7433155080213903
2019-03-03T13:26:13.379526: step 38, loss 1.05471, accuracy 0.648438, precision 0.8695652173913043, recall 0.6698564593301436
2019-03-03T13:26:14.000149: step 39, loss 0.827917, accuracy 0.679688, precision 0.8690476190476191, recall 0.7087378640776699
2019-03-03T13:26:14.626308: step 40, loss 0.818377, accuracy 0.671875, precision 0.8011695906432749, recall 0.732620320855615
2019-03-03T13:26:15.072359: step 41, loss 0.708813, accuracy 0.682353, precision 0.7339449541284404, recall 0.7619047619047619
2019-03-03T13:26:15.649327: step 42, loss 0.896117, accuracy 0.632812, precision 0.6892655367231638, recall 0.7577639751552795
2019-03-03T13:26:16.286622: step 43, loss 0.703733, accuracy 0.683594, precision 0.73125, recall 0.7548387096774194
2019-03-03T13:26:16.866608: step 44, loss 0.622485, accuracy 0.734375, precision 0.7607361963190185, recall 0.8104575163398693
2019-03-03T13:26:17.442095: step 45, loss 0.589682, accuracy 0.730469, precision 0.7840909090909091, recall 0.8165680473372781
2019-03-03T13:26:18.021545: step 46, loss 0.825875, accuracy 0.675781, precision 0.8176470588235294, recall 0.7277486910994765
2019-03-03T13:26:18.604012: step 47, loss 0.665648, accuracy 0.695312, precision 0.8630952380952381, recall 0.725
2019-03-03T13:26:19.161071: step 48, loss 0.592127, accuracy 0.765625, precision 0.8830409356725146, recall 0.7905759162303665
2019-03-03T13:26:19.717707: step 49, loss 0.792841, accuracy 0.6875, precision 0.8269230769230769, recall 0.7087912087912088
2019-03-03T13:26:20.315634: step 50, loss 0.512077, accuracy 0.765625, precision 0.8587570621468926, recall 0.8128342245989305
2019-03-03T13:26:20.853707: step 51, loss 0.549324, accuracy 0.742188, precision 0.7878787878787878, recall 0.8074534161490683
2019-03-03T13:26:21.388786: step 52, loss 0.557798, accuracy 0.789062, precision 0.8081395348837209, recall 0.86875
2019-03-03T13:26:21.927884: step 53, loss 0.617386, accuracy 0.71875, precision 0.7844311377245509, recall 0.7844311377245509
2019-03-03T13:26:22.490697: step 54, loss 0.578447, accuracy 0.746094, precision 0.823170731707317, recall 0.7894736842105263
2019-03-03T13:26:23.018288: step 55, loss 0.625966, accuracy 0.714844, precision 0.79375, recall 0.7604790419161677
2019-03-03T13:26:23.541435: step 56, loss 0.580259, accuracy 0.722656, precision 0.7759562841530054, recall 0.8255813953488372
2019-03-03T13:26:24.091493: step 57, loss 0.678124, accuracy 0.722656, precision 0.8658536585365854, recall 0.743455497382199
2019-03-03T13:26:24.623745: step 58, loss 0.501087, accuracy 0.78125, precision 0.8705882352941177, recall 0.8131868131868132
2019-03-03T13:26:25.141362: step 59, loss 0.505663, accuracy 0.757812, precision 0.8780487804878049, recall 0.7741935483870968
2019-03-03T13:26:25.643045: step 60, loss 0.492645, accuracy 0.78125, precision 0.8705882352941177, recall 0.8131868131868132
2019-03-03T13:26:26.192161: step 61, loss 0.548205, accuracy 0.75, precision 0.815028901734104, recall 0.815028901734104
2019-03-03T13:26:26.696832: step 62, loss 0.556102, accuracy 0.707031, precision 0.7468354430379747, recall 0.7712418300653595
2019-03-03T13:26:27.207716: step 63, loss 0.486537, accuracy 0.753906, precision 0.7978142076502732, recall 0.8488372093023255
2019-03-03T13:26:27.728879: step 64, loss 0.534345, accuracy 0.761719, precision 0.8682634730538922, recall 0.7880434782608695
2019-03-03T13:26:28.278937: step 65, loss 0.412957, accuracy 0.816406, precision 0.8773006134969326, recall 0.8411764705882353
2019-03-03T13:26:28.773613: step 66, loss 0.40894, accuracy 0.835938, precision 0.9314285714285714, recall 0.844559585492228
2019-03-03T13:26:29.261919: step 67, loss 0.555883, accuracy 0.746094, precision 0.815028901734104, recall 0.8103448275862069
2019-03-03T13:26:29.752137: step 68, loss 0.474927, accuracy 0.75, precision 0.8135593220338984, recall 0.8228571428571428
2019-03-03T13:26:30.280724: step 69, loss 0.45606, accuracy 0.796875, precision 0.8709677419354839, recall 0.8526315789473684
2019-03-03T13:26:30.770415: step 70, loss 0.523559, accuracy 0.761719, precision 0.8963414634146342, recall 0.7696335078534031
2019-03-03T13:26:31.268084: step 71, loss 0.497046, accuracy 0.742188, precision 0.8654970760233918, recall 0.774869109947644
2019-03-03T13:26:31.755779: step 72, loss 0.460582, accuracy 0.773438, precision 0.89375, recall 0.7771739130434783
2019-03-03T13:26:32.281380: step 73, loss 0.474744, accuracy 0.785156, precision 0.8654970760233918, recall 0.8222222222222222
2019-03-03T13:26:32.781064: step 74, loss 0.387905, accuracy 0.832031, precision 0.9005524861878453, recall 0.8670212765957447
2019-03-03T13:26:33.267638: step 75, loss 0.519383, accuracy 0.777344, precision 0.8068181818181818, recall 0.8606060606060606
2019-03-03T13:26:33.752343: step 76, loss 0.394553, accuracy 0.824219, precision 0.8465909090909091, recall 0.8922155688622755
2019-03-03T13:26:34.272966: step 77, loss 0.436843, accuracy 0.777344, precision 0.8802395209580839, recall 0.7989130434782609
2019-03-03T13:26:34.759202: step 78, loss 0.436184, accuracy 0.78125, precision 0.8265895953757225, recall 0.8461538461538461
2019-03-03T13:26:35.243939: step 79, loss 0.530851, accuracy 0.769531, precision 0.9194630872483222, recall 0.7445652173913043
2019-03-03T13:26:35.723600: step 80, loss 0.379284, accuracy 0.824219, precision 0.8654970760233918, recall 0.8705882352941177
2019-03-03T13:26:36.226764: step 81, loss 0.470681, accuracy 0.789062, precision 0.8614457831325302, recall 0.8218390804597702
2019-03-03T13:26:36.581845: step 82, loss 0.416225, accuracy 0.829412, precision 0.9166666666666666, recall 0.8527131782945736
2019-03-03T13:26:37.062097: step 83, loss 0.379397, accuracy 0.832031, precision 0.9221556886227545, recall 0.8369565217391305
2019-03-03T13:26:37.534310: step 84, loss 0.338192, accuracy 0.847656, precision 0.9337016574585635, recall 0.8622448979591837
2019-03-03T13:26:38.057024: step 85, loss 0.335255, accuracy 0.863281, precision 0.8888888888888888, recall 0.8944099378881988
2019-03-03T13:26:38.555715: step 86, loss 0.328136, accuracy 0.847656, precision 0.8994082840236687, recall 0.8735632183908046
2019-03-03T13:26:39.049925: step 87, loss 0.366518, accuracy 0.84375, precision 0.9056603773584906, recall 0.8520710059171598
2019-03-03T13:26:39.542608: step 88, loss 0.355027, accuracy 0.863281, precision 0.9036144578313253, recall 0.8875739644970414
2019-03-03T13:26:40.060752: step 89, loss 0.330479, accuracy 0.867188, precision 0.9272727272727272, recall 0.8742857142857143
2019-03-03T13:26:40.579385: step 90, loss 0.401816, accuracy 0.816406, precision 0.8647058823529412, recall 0.8596491228070176
2019-03-03T13:26:41.087367: step 91, loss 0.282177, accuracy 0.90625, precision 0.9378531073446328, recall 0.9273743016759777
2019-03-03T13:26:41.590052: step 92, loss 0.366318, accuracy 0.851562, precision 0.9216867469879518, recall 0.8595505617977528
2019-03-03T13:26:42.143511: step 93, loss 0.279894, accuracy 0.871094, precision 0.8837209302325582, recall 0.9212121212121213
2019-03-03T13:26:42.676686: step 94, loss 0.391196, accuracy 0.832031, precision 0.9, recall 0.8547486033519553
2019-03-03T13:26:43.198826: step 95, loss 0.357698, accuracy 0.847656, precision 0.922077922077922, recall 0.8402366863905325
2019-03-03T13:26:43.746589: step 96, loss 0.420384, accuracy 0.8125, precision 0.9084967320261438, recall 0.8034682080924855
2019-03-03T13:26:44.308105: step 97, loss 0.344813, accuracy 0.851562, precision 0.8928571428571429, recall 0.8823529411764706
2019-03-03T13:26:44.808300: step 98, loss 0.358677, accuracy 0.832031, precision 0.8647058823529412, recall 0.8802395209580839
2019-03-03T13:26:45.341236: step 99, loss 0.390514, accuracy 0.8125, precision 0.8837209302325582, recall 0.8444444444444444
2019-03-03T13:26:45.889106: step 100, loss 0.384618, accuracy 0.835938, precision 0.8383233532934131, recall 0.9032258064516129
2019-03-03T13:26:46.448166: step 101, loss 0.321209, accuracy 0.875, precision 0.9248554913294798, recall 0.8938547486033519
2019-03-03T13:26:46.956706: step 102, loss 0.312078, accuracy 0.871094, precision 0.9273743016759777, recall 0.8924731182795699
2019-03-03T13:26:47.474832: step 103, loss 0.296333, accuracy 0.859375, precision 0.9021739130434783, recall 0.9021739130434783
2019-03-03T13:26:48.046813: step 104, loss 0.274608, accuracy 0.875, precision 0.9315789473684211, recall 0.9030612244897959
2019-03-03T13:26:48.573404: step 105, loss 0.290929, accuracy 0.890625, precision 0.9695121951219512, recall 0.8736263736263736
2019-03-03T13:26:49.089049: step 106, loss 0.330424, accuracy 0.859375, precision 0.9397590361445783, recall 0.8571428571428571
2019-03-03T13:26:49.586734: step 107, loss 0.263589, accuracy 0.925781, precision 0.9826589595375722, recall 0.9139784946236559
2019-03-03T13:26:50.119068: step 108, loss 0.405546, accuracy 0.828125, precision 0.8928571428571429, recall 0.8522727272727273
2019-03-03T13:26:50.621730: step 109, loss 0.357077, accuracy 0.859375, precision 0.9122807017543859, recall 0.8813559322033898
2019-03-03T13:26:51.127885: step 110, loss 0.284287, accuracy 0.917969, precision 0.9230769230769231, recall 0.9512195121951219
2019-03-03T13:26:51.615880: step 111, loss 0.36153, accuracy 0.828125, precision 0.8132530120481928, recall 0.9121621621621622
2019-03-03T13:26:52.143978: step 112, loss 0.350457, accuracy 0.820312, precision 0.8588235294117647, recall 0.8690476190476191
2019-03-03T13:26:52.632673: step 113, loss 0.33584, accuracy 0.851562, precision 0.9056603773584906, recall 0.8622754491017964
2019-03-03T13:26:53.122518: step 114, loss 0.290728, accuracy 0.863281, precision 0.9135135135135135, recall 0.898936170212766
2019-03-03T13:26:53.608220: step 115, loss 0.320421, accuracy 0.890625, precision 0.9294117647058824, recall 0.9080459770114943
2019-03-03T13:26:54.129060: step 116, loss 0.37116, accuracy 0.84375, precision 0.9349112426035503, recall 0.8449197860962567
2019-03-03T13:26:54.612955: step 117, loss 0.322479, accuracy 0.855469, precision 0.9310344827586207, recall 0.8663101604278075
2019-03-03T13:26:55.095662: step 118, loss 0.26775, accuracy 0.882812, precision 0.9408284023668639, recall 0.888268156424581
2019-03-03T13:26:55.578393: step 119, loss 0.344843, accuracy 0.863281, precision 0.9515151515151515, recall 0.8532608695652174
2019-03-03T13:26:56.098510: step 120, loss 0.299875, accuracy 0.878906, precision 0.92, recall 0.9044943820224719
2019-03-03T13:26:56.577222: step 121, loss 0.27743, accuracy 0.890625, precision 0.9130434782608695, recall 0.9130434782608695
2019-03-03T13:26:57.051974: step 122, loss 0.392017, accuracy 0.824219, precision 0.8313253012048193, recall 0.8903225806451613
2019-03-03T13:26:57.409040: step 123, loss 0.366274, accuracy 0.817647, precision 0.8373983739837398, recall 0.9035087719298246
2019-03-03T13:26:57.916222: step 124, loss 0.243423, accuracy 0.902344, precision 0.9401197604790419, recall 0.9127906976744186
2019-03-03T13:26:58.409904: step 125, loss 0.256572, accuracy 0.886719, precision 0.9222222222222223, recall 0.9171270718232044
2019-03-03T13:26:58.887877: step 126, loss 0.240811, accuracy 0.914062, precision 0.9653179190751445, recall 0.912568306010929
2019-03-03T13:26:59.351221: step 127, loss 0.288352, accuracy 0.890625, precision 0.9245283018867925, recall 0.901840490797546
2019-03-03T13:26:59.826483: step 128, loss 0.191818, accuracy 0.933594, precision 0.9709302325581395, recall 0.9329608938547486
2019-03-03T13:27:00.334326: step 129, loss 0.200353, accuracy 0.921875, precision 0.9818181818181818, recall 0.9050279329608939
2019-03-03T13:27:00.797115: step 130, loss 0.188315, accuracy 0.925781, precision 0.9695121951219512, recall 0.9190751445086706
2019-03-03T13:27:01.261776: step 131, loss 0.243231, accuracy 0.90625, precision 0.9542857142857143, recall 0.912568306010929
2019-03-03T13:27:01.724042: step 132, loss 0.218021, accuracy 0.929688, precision 0.9770114942528736, recall 0.9239130434782609
2019-03-03T13:27:02.216734: step 133, loss 0.207958, accuracy 0.933594, precision 0.9325842696629213, recall 0.9707602339181286
2019-03-03T13:27:02.674884: step 134, loss 0.233127, accuracy 0.886719, precision 0.8830409356725146, recall 0.94375
2019-03-03T13:27:03.239278: step 135, loss 0.258192, accuracy 0.898438, precision 0.9171974522292994, recall 0.9171974522292994
2019-03-03T13:27:03.939918: step 136, loss 0.215334, accuracy 0.914062, precision 0.9130434782608695, recall 0.9483870967741935
2019-03-03T13:27:04.594304: step 137, loss 0.213348, accuracy 0.914062, precision 0.9503105590062112, recall 0.9161676646706587
2019-03-03T13:27:05.229144: step 138, loss 0.220328, accuracy 0.921875, precision 0.9704142011834319, recall 0.9162011173184358
2019-03-03T13:27:05.869980: step 139, loss 0.273959, accuracy 0.894531, precision 0.9457831325301205, recall 0.8971428571428571
2019-03-03T13:27:06.490321: step 140, loss 0.226699, accuracy 0.910156, precision 0.9418604651162791, recall 0.9257142857142857
2019-03-03T13:27:07.093727: step 141, loss 0.223481, accuracy 0.933594, precision 0.9767441860465116, recall 0.9281767955801105
2019-03-03T13:27:07.700107: step 142, loss 0.212297, accuracy 0.933594, precision 0.9580838323353293, recall 0.9411764705882353
2019-03-03T13:27:08.334411: step 143, loss 0.256806, accuracy 0.898438, precision 0.9156626506024096, recall 0.926829268292683
2019-03-03T13:27:08.928334: step 144, loss 0.183863, accuracy 0.925781, precision 0.9588235294117647, recall 0.9314285714285714
2019-03-03T13:27:09.505126: step 145, loss 0.169179, accuracy 0.9375, precision 0.9685863874345549, recall 0.9487179487179487
2019-03-03T13:27:10.079630: step 146, loss 0.278982, accuracy 0.871094, precision 0.9130434782608695, recall 0.8855421686746988
2019-03-03T13:27:10.646624: step 147, loss 0.198622, accuracy 0.933594, precision 0.9532163742690059, recall 0.9476744186046512
2019-03-03T13:27:11.253999: step 148, loss 0.280824, accuracy 0.882812, precision 0.9204545454545454, recall 0.9101123595505618
2019-03-03T13:27:11.803613: step 149, loss 0.198622, accuracy 0.9375, precision 0.9562841530054644, recall 0.9562841530054644
2019-03-03T13:27:12.396050: step 150, loss 0.189391, accuracy 0.925781, precision 0.949438202247191, recall 0.9441340782122905
2019-03-03T13:27:12.938796: step 151, loss 0.214678, accuracy 0.917969, precision 0.9754601226993865, recall 0.9034090909090909
2019-03-03T13:27:13.476857: step 152, loss 0.27412, accuracy 0.882812, precision 0.9112426035502958, recall 0.9112426035502958
2019-03-03T13:27:14.033368: step 153, loss 0.220992, accuracy 0.910156, precision 0.9593023255813954, recall 0.9116022099447514
2019-03-03T13:27:14.595404: step 154, loss 0.230036, accuracy 0.910156, precision 0.9515151515151515, recall 0.9127906976744186
2019-03-03T13:27:15.126330: step 155, loss 0.247676, accuracy 0.902344, precision 0.9272727272727272, recall 0.9216867469879518
2019-03-03T13:27:15.656932: step 156, loss 0.198047, accuracy 0.929688, precision 0.967741935483871, recall 0.9202453987730062
2019-03-03T13:27:16.213305: step 157, loss 0.181301, accuracy 0.933594, precision 0.9415204678362573, recall 0.9583333333333334
2019-03-03T13:27:16.748041: step 158, loss 0.262525, accuracy 0.890625, precision 0.8975903614457831, recall 0.93125
2019-03-03T13:27:17.266676: step 159, loss 0.235012, accuracy 0.890625, precision 0.9011627906976745, recall 0.9337349397590361
2019-03-03T13:27:17.772323: step 160, loss 0.212581, accuracy 0.929688, precision 0.9378531073446328, recall 0.9595375722543352
2019-03-03T13:27:18.311480: step 161, loss 0.208481, accuracy 0.925781, precision 0.9696969696969697, recall 0.9195402298850575
2019-03-03T13:27:18.833596: step 162, loss 0.201377, accuracy 0.921875, precision 0.9502762430939227, recall 0.9398907103825137
2019-03-03T13:27:19.346620: step 163, loss 0.27461, accuracy 0.875, precision 0.9156626506024096, recall 0.8941176470588236
2019-03-03T13:27:19.726117: step 164, loss 0.194794, accuracy 0.923529, precision 0.954954954954955, recall 0.9298245614035088
2019-03-03T13:27:20.283627: step 165, loss 0.253725, accuracy 0.902344, precision 0.9341317365269461, recall 0.9176470588235294
2019-03-03T13:27:20.791311: step 166, loss 0.151245, accuracy 0.945312, precision 0.9826589595375722, recall 0.9392265193370166
2019-03-03T13:27:21.308280: step 167, loss 0.161032, accuracy 0.945312, precision 0.9683544303797469, recall 0.9444444444444444
2019-03-03T13:27:21.819932: step 168, loss 0.147153, accuracy 0.957031, precision 0.9759036144578314, recall 0.9585798816568047
2019-03-03T13:27:22.387923: step 169, loss 0.17547, accuracy 0.941406, precision 0.9401197604790419, recall 0.9691358024691358
2019-03-03T13:27:22.910170: step 170, loss 0.160804, accuracy 0.949219, precision 0.949685534591195, recall 0.967948717948718
2019-03-03T13:27:23.438777: step 171, loss 0.148014, accuracy 0.949219, precision 0.9597701149425287, recall 0.9653179190751445
2019-03-03T13:27:23.975865: step 172, loss 0.129105, accuracy 0.964844, precision 0.9662921348314607, recall 0.9828571428571429
2019-03-03T13:27:24.520409: step 173, loss 0.156748, accuracy 0.960938, precision 0.9634146341463414, recall 0.9753086419753086
2019-03-03T13:27:25.008133: step 174, loss 0.156986, accuracy 0.949219, precision 0.9634146341463414, recall 0.9575757575757575
2019-03-03T13:27:25.495822: step 175, loss 0.173247, accuracy 0.945312, precision 0.9625, recall 0.9506172839506173
2019-03-03T13:27:25.983580: step 176, loss 0.1394, accuracy 0.972656, precision 0.9832402234636871, recall 0.9777777777777777
2019-03-03T13:27:26.499205: step 177, loss 0.152479, accuracy 0.949219, precision 0.9672131147540983, recall 0.9619565217391305
2019-03-03T13:27:26.966860: step 178, loss 0.164213, accuracy 0.9375, precision 0.9753086419753086, recall 0.9294117647058824
2019-03-03T13:27:27.441795: step 179, loss 0.16991, accuracy 0.9375, precision 0.9593023255813954, recall 0.9482758620689655
2019-03-03T13:27:27.911950: step 180, loss 0.14911, accuracy 0.949219, precision 0.9824561403508771, recall 0.9438202247191011
2019-03-03T13:27:28.438729: step 181, loss 0.166706, accuracy 0.949219, precision 0.9548387096774194, recall 0.961038961038961
2019-03-03T13:27:28.925948: step 182, loss 0.154233, accuracy 0.953125, precision 0.9777777777777777, recall 0.9565217391304348
2019-03-03T13:27:29.401158: step 183, loss 0.16882, accuracy 0.945312, precision 0.9513513513513514, recall 0.9723756906077348
2019-03-03T13:27:29.873191: step 184, loss 0.164542, accuracy 0.960938, precision 0.9580838323353293, recall 0.9815950920245399
2019-03-03T13:27:30.385820: step 185, loss 0.168323, accuracy 0.953125, precision 0.9503105590062112, recall 0.9745222929936306
2019-03-03T13:27:30.853569: step 186, loss 0.136149, accuracy 0.957031, precision 0.9766081871345029, recall 0.9597701149425287
2019-03-03T13:27:31.332308: step 187, loss 0.152692, accuracy 0.953125, precision 0.9719101123595506, recall 0.9611111111111111
2019-03-03T13:27:31.797594: step 188, loss 0.181127, accuracy 0.929688, precision 0.9352941176470588, recall 0.9578313253012049
2019-03-03T13:27:32.321703: step 189, loss 0.158385, accuracy 0.933594, precision 0.9467455621301775, recall 0.9523809523809523
2019-03-03T13:27:32.788455: step 190, loss 0.17231, accuracy 0.949219, precision 0.994413407821229, recall 0.9368421052631579
2019-03-03T13:27:33.250248: step 191, loss 0.162984, accuracy 0.925781, precision 0.9685534591194969, recall 0.9166666666666666
2019-03-03T13:27:33.715005: step 192, loss 0.161659, accuracy 0.945312, precision 0.9565217391304348, recall 0.967032967032967
2019-03-03T13:27:34.241597: step 193, loss 0.163468, accuracy 0.949219, precision 0.9655172413793104, recall 0.96
2019-03-03T13:27:34.714359: step 194, loss 0.194027, accuracy 0.929688, precision 0.976878612716763, recall 0.9234972677595629
2019-03-03T13:27:35.188135: step 195, loss 0.150105, accuracy 0.945312, precision 0.9627329192546584, recall 0.950920245398773
2019-03-03T13:27:35.659893: step 196, loss 0.170066, accuracy 0.941406, precision 0.9636363636363636, recall 0.9464285714285714
2019-03-03T13:27:36.156703: step 197, loss 0.153497, accuracy 0.953125, precision 0.96045197740113, recall 0.9714285714285714
2019-03-03T13:27:36.626886: step 198, loss 0.156588, accuracy 0.960938, precision 0.9738562091503268, recall 0.9612903225806452
2019-03-03T13:27:37.087174: step 199, loss 0.173359, accuracy 0.945312, precision 0.9770114942528736, recall 0.9444444444444444
2019-03-03T13:27:37.555861: step 200, loss 0.12046, accuracy 0.96875, precision 0.9764705882352941, recall 0.9764705882352941
2019-03-03T13:27:38.021615: step 201, loss 0.160842, accuracy 0.9375, precision 0.9540229885057471, recall 0.9540229885057471
2019-03-03T13:27:38.501313: step 202, loss 0.167317, accuracy 0.933594, precision 0.949438202247191, recall 0.9548022598870056
2019-03-03T13:27:38.977072: step 203, loss 0.136803, accuracy 0.957031, precision 0.9702380952380952, recall 0.9644970414201184
2019-03-03T13:27:39.461318: step 204, loss 0.153191, accuracy 0.957031, precision 0.9647058823529412, recall 0.9704142011834319
2019-03-03T13:27:39.829355: step 205, loss 0.166419, accuracy 0.911765, precision 0.9238095238095239, recall 0.9326923076923077
2019-03-03T13:27:40.343977: step 206, loss 0.116771, accuracy 0.957031, precision 0.9700598802395209, recall 0.9642857142857143
2019-03-03T13:27:40.813747: step 207, loss 0.136857, accuracy 0.949219, precision 0.9748427672955975, recall 0.9451219512195121
2019-03-03T13:27:41.286171: step 208, loss 0.135539, accuracy 0.957031, precision 0.9651162790697675, recall 0.9707602339181286
2019-03-03T13:27:41.766909: step 209, loss 0.157707, accuracy 0.945312, precision 0.975609756097561, recall 0.9411764705882353
2019-03-03T13:27:42.325417: step 210, loss 0.140414, accuracy 0.949219, precision 0.9751552795031055, recall 0.9457831325301205
2019-03-03T13:27:42.810031: step 211, loss 0.102889, accuracy 0.960938, precision 0.9666666666666667, recall 0.9775280898876404
2019-03-03T13:27:43.295496: step 212, loss 0.132142, accuracy 0.949219, precision 0.9497206703910615, recall 0.9770114942528736
2019-03-03T13:27:43.785208: step 213, loss 0.142568, accuracy 0.949219, precision 0.9520958083832335, recall 0.9695121951219512
2019-03-03T13:27:44.309808: step 214, loss 0.15554, accuracy 0.929688, precision 0.9425287356321839, recall 0.9534883720930233
2019-03-03T13:27:44.797538: step 215, loss 0.0960703, accuracy 0.984375, precision 0.9759036144578314, recall 1.0
2019-03-03T13:27:45.287193: step 216, loss 0.116033, accuracy 0.957031, precision 0.9823529411764705, recall 0.9542857142857143
2019-03-03T13:27:45.764917: step 217, loss 0.126712, accuracy 0.953125, precision 0.9588235294117647, recall 0.9702380952380952
2019-03-03T13:27:46.284401: step 218, loss 0.122989, accuracy 0.960938, precision 0.9943181818181818, recall 0.9510869565217391
2019-03-03T13:27:46.766140: step 219, loss 0.109301, accuracy 0.96875, precision 0.9827586206896551, recall 0.9715909090909091
2019-03-03T13:27:47.237958: step 220, loss 0.0929432, accuracy 0.960938, precision 0.9879518072289156, recall 0.9534883720930233
2019-03-03T13:27:47.705997: step 221, loss 0.125377, accuracy 0.964844, precision 0.9763313609467456, recall 0.9705882352941176
2019-03-03T13:27:48.203828: step 222, loss 0.0751222, accuracy 0.992188, precision 1.0, recall 0.9882352941176471
2019-03-03T13:27:48.672926: step 223, loss 0.11918, accuracy 0.957031, precision 0.9661016949152542, recall 0.9715909090909091
2019-03-03T13:27:49.137218: step 224, loss 0.117374, accuracy 0.960938, precision 0.9751552795031055, recall 0.9631901840490797
2019-03-03T13:27:49.618958: step 225, loss 0.118295, accuracy 0.960938, precision 0.9441340782122905, recall 1.0
2019-03-03T13:27:50.131589: step 226, loss 0.146819, accuracy 0.957031, precision 0.9558011049723757, recall 0.9829545454545454
2019-03-03T13:27:50.634772: step 227, loss 0.102436, accuracy 0.976562, precision 0.9881656804733728, recall 0.9766081871345029
2019-03-03T13:27:51.120210: step 228, loss 0.125496, accuracy 0.964844, precision 0.9937106918238994, recall 0.9518072289156626
2019-03-03T13:27:51.591947: step 229, loss 0.128175, accuracy 0.957031, precision 0.9710982658959537, recall 0.9655172413793104
2019-03-03T13:27:52.093534: step 230, loss 0.136653, accuracy 0.957031, precision 0.9567901234567902, recall 0.9748427672955975
2019-03-03T13:27:52.590896: step 231, loss 0.100989, accuracy 0.960938, precision 0.9822485207100592, recall 0.9595375722543352
2019-03-03T13:27:53.071958: step 232, loss 0.156147, accuracy 0.9375, precision 0.9824561403508771, recall 0.9281767955801105
2019-03-03T13:27:53.539234: step 233, loss 0.122305, accuracy 0.96875, precision 0.9823529411764705, recall 0.9709302325581395
2019-03-03T13:27:54.023938: step 234, loss 0.108166, accuracy 0.964844, precision 0.9814814814814815, recall 0.9636363636363636
2019-03-03T13:27:54.505602: step 235, loss 0.125183, accuracy 0.953125, precision 0.9556962025316456, recall 0.967948717948718
2019-03-03T13:27:54.984846: step 236, loss 0.10028, accuracy 0.972656, precision 0.9775280898876404, recall 0.9830508474576272
2019-03-03T13:27:55.477515: step 237, loss 0.113442, accuracy 0.957031, precision 0.9642857142857143, recall 0.9700598802395209
2019-03-03T13:27:55.957232: step 238, loss 0.115445, accuracy 0.964844, precision 0.9542857142857143, recall 0.9940476190476191
2019-03-03T13:27:56.457737: step 239, loss 0.117223, accuracy 0.957031, precision 0.9662921348314607, recall 0.9717514124293786
2019-03-03T13:27:56.928478: step 240, loss 0.120636, accuracy 0.953125, precision 0.9661016949152542, recall 0.9661016949152542
2019-03-03T13:27:57.392749: step 241, loss 0.136252, accuracy 0.949219, precision 0.9647058823529412, recall 0.9590643274853801
2019-03-03T13:27:57.858035: step 242, loss 0.146749, accuracy 0.945312, precision 0.9636363636363636, recall 0.9520958083832335
2019-03-03T13:27:58.361203: step 243, loss 0.101946, accuracy 0.964844, precision 0.9935483870967742, recall 0.9506172839506173
2019-03-03T13:27:58.837946: step 244, loss 0.129245, accuracy 0.953125, precision 0.9661016949152542, recall 0.9661016949152542
2019-03-03T13:27:59.314671: step 245, loss 0.117319, accuracy 0.953125, precision 0.9698795180722891, recall 0.9583333333333334
2019-03-03T13:27:59.678698: step 246, loss 0.0806987, accuracy 0.982353, precision 0.9739130434782609, recall 1.0
2019-03-03T13:28:00.204221: step 247, loss 0.0987276, accuracy 0.96875, precision 0.9731543624161074, recall 0.9731543624161074
2019-03-03T13:28:00.682942: step 248, loss 0.10651, accuracy 0.964844, precision 0.9760479041916168, recall 0.9702380952380952
2019-03-03T13:28:01.154686: step 249, loss 0.102924, accuracy 0.964844, precision 0.9668508287292817, recall 0.9831460674157303
2019-03-03T13:28:01.625446: step 250, loss 0.100673, accuracy 0.964844, precision 0.9642857142857143, recall 0.9818181818181818

Evaluation:
[[657  80]
 [ 96 323]]
2019-03-03T13:28:02.202902: step 250, loss 0.347249, accuracy 0.847751, precision 0.8914518317503393, recall 0.8725099601593626

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551641136\checkpoints\model-250

2019-03-03T13:28:02.957941: step 251, loss 0.0730747, accuracy 0.992188, precision 0.9883040935672515, recall 1.0
2019-03-03T13:28:03.436201: step 252, loss 0.0824503, accuracy 0.976562, precision 0.976878612716763, recall 0.9883040935672515
2019-03-03T13:28:03.920419: step 253, loss 0.0755171, accuracy 0.988281, precision 1.0, recall 0.9829545454545454
2019-03-03T13:28:04.429076: step 254, loss 0.0764988, accuracy 0.976562, precision 0.9761904761904762, recall 0.9879518072289156
2019-03-03T13:28:04.919819: step 255, loss 0.0748143, accuracy 0.980469, precision 0.9830508474576272, recall 0.9886363636363636
2019-03-03T13:28:05.396069: step 256, loss 0.0858296, accuracy 0.96875, precision 0.975609756097561, recall 0.975609756097561
2019-03-03T13:28:05.877310: step 257, loss 0.0949419, accuracy 0.976562, precision 0.9739583333333334, recall 0.9946808510638298
2019-03-03T13:28:06.384955: step 258, loss 0.0825912, accuracy 0.964844, precision 0.9759036144578314, recall 0.9700598802395209
2019-03-03T13:28:06.860681: step 259, loss 0.100014, accuracy 0.953125, precision 0.9651162790697675, recall 0.9651162790697675
2019-03-03T13:28:07.347417: step 260, loss 0.0713505, accuracy 0.984375, precision 0.9942196531791907, recall 0.9828571428571429
2019-03-03T13:28:07.838614: step 261, loss 0.0817283, accuracy 0.96875, precision 0.9832402234636871, recall 0.9723756906077348
2019-03-03T13:28:08.364714: step 262, loss 0.0790376, accuracy 0.976562, precision 0.9818181818181818, recall 0.9818181818181818
2019-03-03T13:28:08.853416: step 263, loss 0.0987332, accuracy 0.964844, precision 0.9559748427672956, recall 0.987012987012987
2019-03-03T13:28:09.335143: step 264, loss 0.0778779, accuracy 0.988281, precision 0.9878787878787879, recall 0.9939024390243902
2019-03-03T13:28:09.815324: step 265, loss 0.0794651, accuracy 0.972656, precision 0.972972972972973, recall 0.9795918367346939
2019-03-03T13:28:10.340454: step 266, loss 0.0668487, accuracy 0.984375, precision 0.9941520467836257, recall 0.9826589595375722
2019-03-03T13:28:10.819195: step 267, loss 0.109504, accuracy 0.957031, precision 0.9695121951219512, recall 0.9636363636363636
2019-03-03T13:28:11.295999: step 268, loss 0.0674527, accuracy 0.988281, precision 0.9887005649717514, recall 0.9943181818181818
2019-03-03T13:28:11.766760: step 269, loss 0.0873265, accuracy 0.96875, precision 0.9761904761904762, recall 0.9761904761904762
2019-03-03T13:28:12.274016: step 270, loss 0.0895446, accuracy 0.96875, precision 0.9828571428571429, recall 0.9717514124293786
2019-03-03T13:28:12.727307: step 271, loss 0.0759473, accuracy 0.976562, precision 0.9814814814814815, recall 0.9814814814814815
2019-03-03T13:28:13.204029: step 272, loss 0.0878515, accuracy 0.972656, precision 0.9615384615384616, recall 1.0
2019-03-03T13:28:13.675286: step 273, loss 0.0834391, accuracy 0.976562, precision 0.9887640449438202, recall 0.9777777777777777
2019-03-03T13:28:14.203402: step 274, loss 0.12701, accuracy 0.949219, precision 0.9748427672955975, recall 0.9451219512195121
2019-03-03T13:28:14.681149: step 275, loss 0.0827818, accuracy 0.976562, precision 0.9828571428571429, recall 0.9828571428571429
2019-03-03T13:28:15.169085: step 276, loss 0.0869057, accuracy 0.972656, precision 0.9722222222222222, recall 0.9887005649717514
2019-03-03T13:28:15.657827: step 277, loss 0.0949314, accuracy 0.972656, precision 0.9879518072289156, recall 0.9704142011834319
2019-03-03T13:28:16.181934: step 278, loss 0.0913516, accuracy 0.972656, precision 0.9880239520958084, recall 0.9705882352941176
2019-03-03T13:28:16.657173: step 279, loss 0.0919348, accuracy 0.964844, precision 0.9772727272727273, recall 0.9717514124293786
2019-03-03T13:28:17.140436: step 280, loss 0.0838228, accuracy 0.96875, precision 0.9819277108433735, recall 0.9702380952380952
2019-03-03T13:28:17.616185: step 281, loss 0.075377, accuracy 0.980469, precision 0.9719101123595506, recall 1.0
2019-03-03T13:28:18.112861: step 282, loss 0.0934423, accuracy 0.984375, precision 0.9815950920245399, recall 0.9937888198757764
2019-03-03T13:28:18.593120: step 283, loss 0.112387, accuracy 0.957031, precision 0.9715909090909091, recall 0.9661016949152542
2019-03-03T13:28:19.071908: step 284, loss 0.0947881, accuracy 0.976562, precision 0.9700598802395209, recall 0.9938650306748467
2019-03-03T13:28:19.540189: step 285, loss 0.119794, accuracy 0.949219, precision 0.9640718562874252, recall 0.9583333333333334
2019-03-03T13:28:20.059889: step 286, loss 0.0962935, accuracy 0.972656, precision 0.9696969696969697, recall 0.9876543209876543
2019-03-03T13:28:20.447852: step 287, loss 0.0901685, accuracy 0.982353, precision 0.9904761904761905, recall 0.9811320754716981
2019-03-03T13:28:20.930274: step 288, loss 0.0543381, accuracy 0.996094, precision 0.9945054945054945, recall 1.0
2019-03-03T13:28:21.408527: step 289, loss 0.0551481, accuracy 0.992188, precision 0.9939024390243902, recall 0.9939024390243902
2019-03-03T13:28:21.899267: step 290, loss 0.0629145, accuracy 0.980469, precision 0.9884393063583815, recall 0.9827586206896551
2019-03-03T13:28:22.398953: step 291, loss 0.0800545, accuracy 0.980469, precision 0.994475138121547, recall 0.9782608695652174
2019-03-03T13:28:22.879207: step 292, loss 0.0503505, accuracy 0.984375, precision 0.9877300613496932, recall 0.9877300613496932
2019-03-03T13:28:23.354467: step 293, loss 0.0613904, accuracy 0.980469, precision 0.9761904761904762, recall 0.9939393939393939
2019-03-03T13:28:23.831286: step 294, loss 0.0886317, accuracy 0.964844, precision 0.9493670886075949, recall 0.9933774834437086
2019-03-03T13:28:24.350408: step 295, loss 0.0688751, accuracy 0.980469, precision 0.9832402234636871, recall 0.9887640449438202
2019-03-03T13:28:24.824146: step 296, loss 0.0815978, accuracy 0.976562, precision 0.9876543209876543, recall 0.975609756097561
2019-03-03T13:28:25.288933: step 297, loss 0.055487, accuracy 0.992188, precision 0.9878048780487805, recall 1.0
2019-03-03T13:28:25.768245: step 298, loss 0.0495524, accuracy 0.996094, precision 0.9939393939393939, recall 1.0
2019-03-03T13:28:26.280901: step 299, loss 0.0624422, accuracy 0.980469, precision 0.9886363636363636, recall 0.9830508474576272
2019-03-03T13:28:26.744661: step 300, loss 0.0721336, accuracy 0.980469, precision 0.9883040935672515, recall 0.9825581395348837
2019-03-03T13:28:27.209938: step 301, loss 0.0573658, accuracy 0.988281, precision 0.9883040935672515, recall 0.9941176470588236
2019-03-03T13:28:27.671238: step 302, loss 0.055129, accuracy 0.980469, precision 0.9810126582278481, recall 0.9872611464968153
2019-03-03T13:28:28.165938: step 303, loss 0.0655437, accuracy 0.980469, precision 0.9707602339181286, recall 1.0
2019-03-03T13:28:28.628239: step 304, loss 0.0740982, accuracy 0.980469, precision 0.9880952380952381, recall 0.9822485207100592
2019-03-03T13:28:29.097005: step 305, loss 0.0527859, accuracy 0.992188, precision 0.9941520467836257, recall 0.9941520467836257
2019-03-03T13:28:29.556383: step 306, loss 0.0451516, accuracy 0.992188, precision 0.9940119760479041, recall 0.9940119760479041
2019-03-03T13:28:30.039093: step 307, loss 0.066202, accuracy 0.984375, precision 0.9935897435897436, recall 0.9810126582278481
2019-03-03T13:28:30.515846: step 308, loss 0.0636504, accuracy 0.980469, precision 0.9885057471264368, recall 0.9828571428571429
2019-03-03T13:28:30.981018: step 309, loss 0.0905393, accuracy 0.976562, precision 0.9655172413793104, recall 1.0
2019-03-03T13:28:31.429149: step 310, loss 0.0567078, accuracy 0.984375, precision 0.9828571428571429, recall 0.9942196531791907
2019-03-03T13:28:31.909912: step 311, loss 0.0907469, accuracy 0.964844, precision 0.9634146341463414, recall 0.9813664596273292
2019-03-03T13:28:32.408892: step 312, loss 0.0635611, accuracy 0.984375, precision 0.9836956521739131, recall 0.9945054945054945
2019-03-03T13:28:32.900832: step 313, loss 0.0730895, accuracy 0.964844, precision 0.9759036144578314, recall 0.9700598802395209
2019-03-03T13:28:33.387652: step 314, loss 0.074874, accuracy 0.976562, precision 0.9887005649717514, recall 0.9776536312849162
2019-03-03T13:28:33.878827: step 315, loss 0.0598516, accuracy 0.984375, precision 1.0, recall 0.9771428571428571
2019-03-03T13:28:34.392454: step 316, loss 0.106345, accuracy 0.953125, precision 0.9704142011834319, recall 0.9590643274853801
2019-03-03T13:28:34.885156: step 317, loss 0.0515503, accuracy 0.980469, precision 0.9891304347826086, recall 0.9837837837837838
2019-03-03T13:28:35.368394: step 318, loss 0.0930429, accuracy 0.964844, precision 0.9876543209876543, recall 0.9580838323353293
2019-03-03T13:28:35.879054: step 319, loss 0.0691616, accuracy 0.988281, precision 0.9880952380952381, recall 0.9940119760479041
2019-03-03T13:28:36.392496: step 320, loss 0.0603741, accuracy 0.984375, precision 0.9937106918238994, recall 0.9813664596273292
2019-03-03T13:28:36.874738: step 321, loss 0.0623425, accuracy 0.980469, precision 0.9893048128342246, recall 0.9840425531914894
2019-03-03T13:28:37.358342: step 322, loss 0.0768675, accuracy 0.96875, precision 0.9705882352941176, recall 0.9821428571428571
2019-03-03T13:28:37.840848: step 323, loss 0.0612973, accuracy 0.980469, precision 0.9938650306748467, recall 0.9759036144578314
2019-03-03T13:28:38.337548: step 324, loss 0.0521184, accuracy 0.984375, precision 0.9875776397515528, recall 0.9875776397515528
2019-03-03T13:28:38.809872: step 325, loss 0.0675004, accuracy 0.96875, precision 0.975609756097561, recall 0.975609756097561
2019-03-03T13:28:39.287595: step 326, loss 0.0659826, accuracy 0.980469, precision 0.9770114942528736, recall 0.9941520467836257
2019-03-03T13:28:39.787160: step 327, loss 0.0774228, accuracy 0.976562, precision 0.9885057471264368, recall 0.9772727272727273
2019-03-03T13:28:40.190104: step 328, loss 0.105578, accuracy 0.970588, precision 0.963302752293578, recall 0.9905660377358491
2019-03-03T13:28:40.684171: step 329, loss 0.0667353, accuracy 0.972656, precision 0.9696969696969697, recall 0.9876543209876543
2019-03-03T13:28:41.181376: step 330, loss 0.0437617, accuracy 0.988281, precision 0.9943181818181818, recall 0.9887005649717514
2019-03-03T13:28:41.665078: step 331, loss 0.0540574, accuracy 0.992188, precision 1.0, recall 0.9876543209876543
2019-03-03T13:28:42.212150: step 332, loss 0.0413937, accuracy 0.996094, precision 0.9943181818181818, recall 1.0
2019-03-03T13:28:42.729926: step 333, loss 0.050818, accuracy 0.984375, precision 0.9939759036144579, recall 0.9821428571428571
2019-03-03T13:28:43.221217: step 334, loss 0.0546443, accuracy 0.988281, precision 0.9878048780487805, recall 0.9938650306748467
2019-03-03T13:28:43.713918: step 335, loss 0.0333448, accuracy 0.992188, precision 0.9947089947089947, recall 0.9947089947089947
2019-03-03T13:28:44.235382: step 336, loss 0.0866686, accuracy 0.980469, precision 0.9803921568627451, recall 0.9868421052631579
2019-03-03T13:28:44.750536: step 337, loss 0.0525488, accuracy 0.988281, precision 1.0, recall 0.9824561403508771
2019-03-03T13:28:45.454673: step 338, loss 0.0541339, accuracy 0.984375, precision 0.9814814814814815, recall 0.99375
2019-03-03T13:28:46.237581: step 339, loss 0.041718, accuracy 0.988281, precision 0.9808917197452229, recall 1.0
2019-03-03T13:28:46.955661: step 340, loss 0.0572817, accuracy 0.984375, precision 0.9815950920245399, recall 0.9937888198757764
2019-03-03T13:28:47.640828: step 341, loss 0.0494091, accuracy 0.984375, precision 0.9890710382513661, recall 0.9890710382513661
2019-03-03T13:28:48.382871: step 342, loss 0.0472923, accuracy 0.992188, precision 1.0, recall 0.9886363636363636
2019-03-03T13:28:49.045070: step 343, loss 0.0617353, accuracy 0.988281, precision 0.9827586206896551, recall 1.0
2019-03-03T13:28:49.701853: step 344, loss 0.0456834, accuracy 0.992188, precision 1.0, recall 0.988950276243094
2019-03-03T13:28:50.395402: step 345, loss 0.0468403, accuracy 0.984375, precision 0.978021978021978, recall 1.0
2019-03-03T13:28:51.047199: step 346, loss 0.0540893, accuracy 0.988281, precision 0.9937106918238994, recall 0.9875
2019-03-03T13:28:51.677262: step 347, loss 0.0321524, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:28:52.342516: step 348, loss 0.0554231, accuracy 0.980469, precision 0.9885714285714285, recall 0.9829545454545454
2019-03-03T13:28:52.937912: step 349, loss 0.0326411, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:28:53.528084: step 350, loss 0.0513834, accuracy 0.988281, precision 0.9815950920245399, recall 1.0
2019-03-03T13:28:54.174379: step 351, loss 0.0465992, accuracy 0.988281, precision 0.98125, recall 1.0
2019-03-03T13:28:54.768313: step 352, loss 0.0625022, accuracy 0.976562, precision 0.978494623655914, recall 0.9891304347826086
2019-03-03T13:28:55.347088: step 353, loss 0.063524, accuracy 0.976562, precision 0.9942857142857143, recall 0.9720670391061452
2019-03-03T13:28:55.941035: step 354, loss 0.0583376, accuracy 0.984375, precision 0.9883040935672515, recall 0.9883040935672515
2019-03-03T13:28:56.532452: step 355, loss 0.0512163, accuracy 0.984375, precision 0.9876543209876543, recall 0.9876543209876543
2019-03-03T13:28:57.091978: step 356, loss 0.0537767, accuracy 0.988281, precision 0.9942857142857143, recall 0.9886363636363636
2019-03-03T13:28:57.627590: step 357, loss 0.0677444, accuracy 0.980469, precision 0.9891304347826086, recall 0.9837837837837838
2019-03-03T13:28:58.223509: step 358, loss 0.053198, accuracy 0.984375, precision 0.9823529411764705, recall 0.9940476190476191
2019-03-03T13:28:58.755564: step 359, loss 0.0674597, accuracy 0.976562, precision 0.98125, recall 0.98125
2019-03-03T13:28:59.287117: step 360, loss 0.0618939, accuracy 0.976562, precision 0.98125, recall 0.98125
2019-03-03T13:28:59.840636: step 361, loss 0.04784, accuracy 0.988281, precision 1.0, recall 0.9821428571428571
2019-03-03T13:29:00.411847: step 362, loss 0.0401604, accuracy 0.984375, precision 0.9942857142857143, recall 0.9830508474576272
2019-03-03T13:29:00.935952: step 363, loss 0.0488367, accuracy 0.980469, precision 0.9832402234636871, recall 0.9887640449438202
2019-03-03T13:29:01.446587: step 364, loss 0.0527664, accuracy 0.992188, precision 0.99375, recall 0.99375
2019-03-03T13:29:02.008666: step 365, loss 0.0445662, accuracy 0.992188, precision 0.9878787878787879, recall 1.0
2019-03-03T13:29:02.548235: step 366, loss 0.0539656, accuracy 0.988281, precision 0.9880952380952381, recall 0.9940119760479041
2019-03-03T13:29:03.065804: step 367, loss 0.0473736, accuracy 0.988281, precision 0.9878048780487805, recall 0.9938650306748467
2019-03-03T13:29:03.580951: step 368, loss 0.0436041, accuracy 0.984375, precision 0.9826589595375722, recall 0.9941520467836257
2019-03-03T13:29:03.989475: step 369, loss 0.0486498, accuracy 0.982353, precision 0.9824561403508771, recall 0.9911504424778761
2019-03-03T13:29:04.511101: step 370, loss 0.0361401, accuracy 0.988281, precision 0.9942857142857143, recall 0.9886363636363636
2019-03-03T13:29:05.024257: step 371, loss 0.0533848, accuracy 0.984375, precision 0.9761904761904762, recall 1.0
2019-03-03T13:29:05.527911: step 372, loss 0.0233127, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:06.062534: step 373, loss 0.0698554, accuracy 0.96875, precision 0.9761904761904762, recall 0.9761904761904762
2019-03-03T13:29:06.568981: step 374, loss 0.0514834, accuracy 0.984375, precision 0.9820359281437125, recall 0.9939393939393939
2019-03-03T13:29:07.084602: step 375, loss 0.041731, accuracy 0.988281, precision 0.9939024390243902, recall 0.9878787878787879
2019-03-03T13:29:07.585248: step 376, loss 0.0343097, accuracy 0.992188, precision 0.9878048780487805, recall 1.0
2019-03-03T13:29:08.133314: step 377, loss 0.0526011, accuracy 0.992188, precision 0.9943502824858758, recall 0.9943502824858758
2019-03-03T13:29:08.624999: step 378, loss 0.0298298, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:09.106710: step 379, loss 0.0436864, accuracy 0.988281, precision 0.9882352941176471, recall 0.9940828402366864
2019-03-03T13:29:09.583207: step 380, loss 0.0763586, accuracy 0.96875, precision 0.9880952380952381, recall 0.9651162790697675
2019-03-03T13:29:10.060950: step 381, loss 0.0475225, accuracy 0.988281, precision 0.9939024390243902, recall 0.9878787878787879
2019-03-03T13:29:10.542663: step 382, loss 0.0267994, accuracy 0.996094, precision 1.0, recall 0.9939393939393939
2019-03-03T13:29:11.050207: step 383, loss 0.0573164, accuracy 0.984375, precision 0.9880239520958084, recall 0.9880239520958084
2019-03-03T13:29:11.530943: step 384, loss 0.0404884, accuracy 0.984375, precision 0.9828571428571429, recall 0.9942196531791907
2019-03-03T13:29:12.032428: step 385, loss 0.0349849, accuracy 0.996094, precision 1.0, recall 0.9940828402366864
2019-03-03T13:29:12.541096: step 386, loss 0.06461, accuracy 0.972656, precision 0.9649122807017544, recall 0.9939759036144579
2019-03-03T13:29:13.020817: step 387, loss 0.0361815, accuracy 0.996094, precision 0.9940476190476191, recall 1.0
2019-03-03T13:29:13.496760: step 388, loss 0.0415483, accuracy 0.996094, precision 0.9938650306748467, recall 1.0
2019-03-03T13:29:13.979490: step 389, loss 0.0525602, accuracy 0.980469, precision 0.9824561403508771, recall 0.9882352941176471
2019-03-03T13:29:14.498967: step 390, loss 0.0239211, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:14.972720: step 391, loss 0.0567321, accuracy 0.976562, precision 0.9746835443037974, recall 0.9871794871794872
2019-03-03T13:29:15.443533: step 392, loss 0.0330141, accuracy 0.996094, precision 1.0, recall 0.9939024390243902
2019-03-03T13:29:15.910306: step 393, loss 0.0417029, accuracy 0.996094, precision 1.0, recall 0.9943181818181818
2019-03-03T13:29:16.413571: step 394, loss 0.0474273, accuracy 0.984375, precision 0.9940828402366864, recall 0.9824561403508771
2019-03-03T13:29:16.883337: step 395, loss 0.0497534, accuracy 0.988281, precision 0.9940119760479041, recall 0.9880952380952381
2019-03-03T13:29:17.392901: step 396, loss 0.0305578, accuracy 0.992188, precision 0.9885057471264368, recall 1.0
2019-03-03T13:29:17.852671: step 397, loss 0.0512048, accuracy 0.980469, precision 0.9814814814814815, recall 0.9875776397515528
2019-03-03T13:29:18.347395: step 398, loss 0.0250394, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:18.798589: step 399, loss 0.0396037, accuracy 0.996094, precision 0.9945945945945946, recall 1.0
2019-03-03T13:29:19.264853: step 400, loss 0.0504161, accuracy 0.976562, precision 0.9881656804733728, recall 0.9766081871345029
2019-03-03T13:29:19.724136: step 401, loss 0.0545492, accuracy 0.984375, precision 0.9883720930232558, recall 0.9883720930232558
2019-03-03T13:29:20.226141: step 402, loss 0.0354523, accuracy 0.988281, precision 0.9891891891891892, recall 0.9945652173913043
2019-03-03T13:29:20.720837: step 403, loss 0.0287016, accuracy 0.996094, precision 0.99375, recall 1.0
2019-03-03T13:29:21.193574: step 404, loss 0.0443, accuracy 0.992188, precision 0.9943820224719101, recall 0.9943820224719101
2019-03-03T13:29:21.662829: step 405, loss 0.0522587, accuracy 0.980469, precision 0.9754601226993865, recall 0.99375
2019-03-03T13:29:22.156511: step 406, loss 0.0478035, accuracy 0.988281, precision 0.9887005649717514, recall 0.9943181818181818
2019-03-03T13:29:22.639231: step 407, loss 0.0408239, accuracy 0.980469, precision 0.9775280898876404, recall 0.9942857142857143
2019-03-03T13:29:23.111993: step 408, loss 0.0375363, accuracy 0.980469, precision 0.9766081871345029, recall 0.9940476190476191
2019-03-03T13:29:23.575329: step 409, loss 0.0573505, accuracy 0.980469, precision 0.9878787878787879, recall 0.9819277108433735
2019-03-03T13:29:23.925392: step 410, loss 0.07911, accuracy 0.976471, precision 0.9739130434782609, recall 0.9911504424778761
2019-03-03T13:29:24.421068: step 411, loss 0.0289599, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:24.867403: step 412, loss 0.0397076, accuracy 0.988281, precision 0.9946524064171123, recall 0.9893617021276596
2019-03-03T13:29:25.339913: step 413, loss 0.0368977, accuracy 0.992188, precision 0.9939759036144579, recall 0.9939759036144579
2019-03-03T13:29:25.802644: step 414, loss 0.0249694, accuracy 0.996094, precision 0.99375, recall 1.0
2019-03-03T13:29:26.301206: step 415, loss 0.026891, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:26.765984: step 416, loss 0.0266191, accuracy 0.996094, precision 0.9944444444444445, recall 1.0
2019-03-03T13:29:27.231030: step 417, loss 0.0252505, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:27.712255: step 418, loss 0.0498295, accuracy 0.984375, precision 0.9883040935672515, recall 0.9883040935672515
2019-03-03T13:29:28.217925: step 419, loss 0.042978, accuracy 0.988281, precision 0.9938650306748467, recall 0.9878048780487805
2019-03-03T13:29:28.696179: step 420, loss 0.028201, accuracy 0.996094, precision 1.0, recall 0.9936708860759493
2019-03-03T13:29:29.152995: step 421, loss 0.0317582, accuracy 0.988281, precision 0.9941176470588236, recall 0.9883040935672515
2019-03-03T13:29:29.621742: step 422, loss 0.036294, accuracy 0.992188, precision 0.9883720930232558, recall 1.0
2019-03-03T13:29:30.131379: step 423, loss 0.0409126, accuracy 0.988281, precision 0.9836065573770492, recall 1.0
2019-03-03T13:29:30.634036: step 424, loss 0.069857, accuracy 0.980469, precision 0.9771428571428571, recall 0.9941860465116279
2019-03-03T13:29:31.112774: step 425, loss 0.039258, accuracy 0.988281, precision 0.9821428571428571, recall 1.0
2019-03-03T13:29:31.582361: step 426, loss 0.031647, accuracy 0.988281, precision 0.9939759036144579, recall 0.9880239520958084
2019-03-03T13:29:32.065628: step 427, loss 0.030402, accuracy 0.992188, precision 0.9942528735632183, recall 0.9942528735632183
2019-03-03T13:29:32.558333: step 428, loss 0.035586, accuracy 0.988281, precision 0.9937106918238994, recall 0.9875
2019-03-03T13:29:33.027681: step 429, loss 0.0461111, accuracy 0.980469, precision 0.98125, recall 0.9874213836477987
2019-03-03T13:29:33.497448: step 430, loss 0.0292972, accuracy 0.992188, precision 0.9880239520958084, recall 1.0
2019-03-03T13:29:33.961043: step 431, loss 0.0190701, accuracy 0.996094, precision 1.0, recall 0.9941176470588236
2019-03-03T13:29:34.468944: step 432, loss 0.0244732, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:34.939207: step 433, loss 0.0297009, accuracy 0.988281, precision 0.9939759036144579, recall 0.9880239520958084
2019-03-03T13:29:35.395989: step 434, loss 0.0223354, accuracy 0.996094, precision 1.0, recall 0.9942857142857143
2019-03-03T13:29:35.885681: step 435, loss 0.0311519, accuracy 0.996094, precision 0.9938650306748467, recall 1.0
2019-03-03T13:29:36.411012: step 436, loss 0.037251, accuracy 0.980469, precision 0.9827586206896551, recall 0.9884393063583815
2019-03-03T13:29:36.897728: step 437, loss 0.0349275, accuracy 0.984375, precision 0.9822485207100592, recall 0.9940119760479041
2019-03-03T13:29:37.375740: step 438, loss 0.0459429, accuracy 0.992188, precision 0.9887005649717514, recall 1.0
2019-03-03T13:29:37.872926: step 439, loss 0.0304252, accuracy 0.988281, precision 0.9836065573770492, recall 1.0
2019-03-03T13:29:38.388548: step 440, loss 0.0514454, accuracy 0.988281, precision 1.0, recall 0.9819277108433735
2019-03-03T13:29:38.872778: step 441, loss 0.0366872, accuracy 0.992188, precision 1.0, recall 0.9877300613496932
2019-03-03T13:29:39.354113: step 442, loss 0.0778992, accuracy 0.964844, precision 0.9943181818181818, recall 0.9562841530054644
2019-03-03T13:29:39.829749: step 443, loss 0.0403801, accuracy 0.988281, precision 0.9938271604938271, recall 0.9877300613496932
2019-03-03T13:29:40.344902: step 444, loss 0.0385791, accuracy 0.988281, precision 0.9874213836477987, recall 0.9936708860759493
2019-03-03T13:29:40.812584: step 445, loss 0.0409759, accuracy 0.984375, precision 0.9879518072289156, recall 0.9879518072289156
2019-03-03T13:29:41.297802: step 446, loss 0.0494558, accuracy 0.984375, precision 0.9879518072289156, recall 0.9879518072289156
2019-03-03T13:29:41.791993: step 447, loss 0.0354866, accuracy 0.992188, precision 1.0, recall 0.988950276243094
2019-03-03T13:29:42.308610: step 448, loss 0.0300641, accuracy 0.992188, precision 0.9940476190476191, recall 0.9940476190476191
2019-03-03T13:29:42.803309: step 449, loss 0.0602341, accuracy 0.984375, precision 0.9818181818181818, recall 0.9938650306748467
2019-03-03T13:29:43.287010: step 450, loss 0.0222022, accuracy 0.996094, precision 1.0, recall 0.9941176470588236
2019-03-03T13:29:43.636607: step 451, loss 0.0292616, accuracy 0.994118, precision 1.0, recall 0.990990990990991
2019-03-03T13:29:44.149847: step 452, loss 0.0321122, accuracy 0.988281, precision 0.9817073170731707, recall 1.0
2019-03-03T13:29:44.650040: step 453, loss 0.0200152, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:45.131373: step 454, loss 0.0323029, accuracy 0.988281, precision 0.9825581395348837, recall 1.0
2019-03-03T13:29:45.598636: step 455, loss 0.0224783, accuracy 0.996094, precision 1.0, recall 0.9941176470588236
2019-03-03T13:29:46.075199: step 456, loss 0.028633, accuracy 0.992188, precision 0.9879518072289156, recall 1.0
2019-03-03T13:29:46.558927: step 457, loss 0.0221996, accuracy 0.992188, precision 0.9883720930232558, recall 1.0
2019-03-03T13:29:47.019153: step 458, loss 0.0318085, accuracy 0.988281, precision 0.9878787878787879, recall 0.9939024390243902
2019-03-03T13:29:47.479319: step 459, loss 0.0276437, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:47.966038: step 460, loss 0.0198793, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:48.473681: step 461, loss 0.0201439, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:48.953420: step 462, loss 0.0278396, accuracy 0.992188, precision 0.9942528735632183, recall 0.9942528735632183
2019-03-03T13:29:49.423186: step 463, loss 0.0293501, accuracy 0.988281, precision 0.9832402234636871, recall 1.0
2019-03-03T13:29:49.898934: step 464, loss 0.0286215, accuracy 0.992188, precision 0.9882352941176471, recall 1.0
2019-03-03T13:29:50.405579: step 465, loss 0.0399068, accuracy 0.980469, precision 0.9751552795031055, recall 0.9936708860759493
2019-03-03T13:29:50.888806: step 466, loss 0.0254323, accuracy 0.996094, precision 1.0, recall 0.9944444444444445
2019-03-03T13:29:51.357594: step 467, loss 0.0230427, accuracy 0.996094, precision 1.0, recall 0.9941176470588236
2019-03-03T13:29:51.827500: step 468, loss 0.0350042, accuracy 0.984375, precision 0.9828571428571429, recall 0.9942196531791907
2019-03-03T13:29:52.330947: step 469, loss 0.0263616, accuracy 0.996094, precision 0.9937106918238994, recall 1.0
2019-03-03T13:29:52.789247: step 470, loss 0.0241828, accuracy 0.992188, precision 1.0, recall 0.9882352941176471
2019-03-03T13:29:53.258995: step 471, loss 0.016856, accuracy 0.996094, precision 0.9942528735632183, recall 1.0
2019-03-03T13:29:53.716962: step 472, loss 0.0225585, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:54.233129: step 473, loss 0.033247, accuracy 0.992188, precision 0.9879518072289156, recall 1.0
2019-03-03T13:29:54.718951: step 474, loss 0.036187, accuracy 0.992188, precision 0.994413407821229, recall 0.994413407821229
2019-03-03T13:29:55.208165: step 475, loss 0.0199213, accuracy 0.996094, precision 1.0, recall 0.9940476190476191
2019-03-03T13:29:55.686886: step 476, loss 0.0352284, accuracy 0.984375, precision 0.9881656804733728, recall 0.9881656804733728
2019-03-03T13:29:56.201508: step 477, loss 0.0257151, accuracy 0.992188, precision 0.9894179894179894, recall 1.0
2019-03-03T13:29:56.685212: step 478, loss 0.019906, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:29:57.164951: step 479, loss 0.0311945, accuracy 0.992188, precision 1.0, recall 0.9891304347826086
2019-03-03T13:29:57.651952: step 480, loss 0.0426828, accuracy 0.988281, precision 0.9880952380952381, recall 0.9940119760479041
2019-03-03T13:29:58.160122: step 481, loss 0.0643671, accuracy 0.980469, precision 0.9883040935672515, recall 0.9825581395348837
2019-03-03T13:29:58.647845: step 482, loss 0.0449838, accuracy 0.992188, precision 0.9940476190476191, recall 0.9940476190476191
2019-03-03T13:29:59.122127: step 483, loss 0.0311766, accuracy 0.992188, precision 0.9880952380952381, recall 1.0
2019-03-03T13:29:59.587882: step 484, loss 0.0272714, accuracy 0.992188, precision 0.9934640522875817, recall 0.9934640522875817
2019-03-03T13:30:00.086058: step 485, loss 0.0171413, accuracy 1, precision 1.0, recall 1.0
2019-03-03T13:30:00.575754: step 486, loss 0.0330533, accuracy 0.988281, precision 0.988950276243094, recall 0.9944444444444445
2019-03-03T13:30:01.041017: step 487, loss 0.0334515, accuracy 0.984375, precision 0.9822485207100592, recall 0.9940119760479041
2019-03-03T13:30:01.498337: step 488, loss 0.0533765, accuracy 0.984375, precision 0.9763313609467456, recall 1.0
2019-03-03T13:30:01.972088: step 489, loss 0.023475, accuracy 0.996094, precision 0.9940476190476191, recall 1.0
2019-03-03T13:30:02.470768: step 490, loss 0.0452447, accuracy 0.984375, precision 0.9834254143646409, recall 0.994413407821229
2019-03-03T13:30:02.946200: step 491, loss 0.0388691, accuracy 0.984375, precision 0.9815950920245399, recall 0.9937888198757764
2019-03-03T13:30:03.309277: step 492, loss 0.0256383, accuracy 0.988235, precision 0.990909090909091, recall 0.990909090909091
2019-03-03T13:30:03.788056: step 493, loss 0.0375996, accuracy 0.984375, precision 0.9832402234636871, recall 0.9943502824858758
2019-03-03T13:30:04.303226: step 494, loss 0.0191858, accuracy 0.996094, precision 0.9939024390243902, recall 1.0
2019-03-03T13:30:04.772000: step 495, loss 0.0198487, accuracy 0.996094, precision 1.0, recall 0.9941860465116279
2019-03-03T13:30:05.241273: step 496, loss 0.0294541, accuracy 0.984375, precision 0.9881656804733728, recall 0.9881656804733728
2019-03-03T13:30:05.734953: step 497, loss 0.0283107, accuracy 0.988281, precision 1.0, recall 0.9828571428571429
2019-03-03T13:30:06.260549: step 498, loss 0.020692, accuracy 0.996094, precision 1.0, recall 0.9937888198757764
2019-03-03T13:30:06.735779: step 499, loss 0.0223613, accuracy 0.996094, precision 0.9940476190476191, recall 1.0
2019-03-03T13:30:07.225732: step 500, loss 0.0199819, accuracy 0.996094, precision 0.9941860465116279, recall 1.0

Evaluation:
[[666  71]
 [121 298]]
2019-03-03T13:30:07.640623: step 500, loss 0.515062, accuracy 0.83391, precision 0.903663500678426, recall 0.8462515883100381

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551641136\checkpoints\model-500


Process finished with exit code 0
