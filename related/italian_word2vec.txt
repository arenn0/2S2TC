"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
Pretrained Embedding: word2vec
Italian: True
Loading data...
4681
Max Document length: 81
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 7231
Train/Dev split: 4213/468
2019-03-03 14:10:04.030630: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:475: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551643810

2019-03-03T14:10:12.583497: step 1, loss 3.06275, accuracy 0.507812, precision 0.2830188679245283, recall 0.375
2019-03-03T14:10:13.071207: step 2, loss 2.78882, accuracy 0.507812, precision 0.6179775280898876, recall 0.3741496598639456
2019-03-03T14:10:13.561887: step 3, loss 2.30427, accuracy 0.613281, precision 0.5128205128205128, recall 0.39603960396039606
2019-03-03T14:10:14.033644: step 4, loss 1.8627, accuracy 0.734375, precision 0.4177215189873418, recall 0.6
2019-03-03T14:10:14.493941: step 5, loss 2.83159, accuracy 0.628906, precision 0.2828282828282828, recall 0.5384615384615384
2019-03-03T14:10:14.956728: step 6, loss 2.24977, accuracy 0.71875, precision 0.4583333333333333, recall 0.6875
2019-03-03T14:10:15.467648: step 7, loss 1.49096, accuracy 0.769531, precision 0.6818181818181818, recall 0.6593406593406593
2019-03-03T14:10:15.986260: step 8, loss 1.68409, accuracy 0.734375, precision 0.6458333333333334, recall 0.6458333333333334
2019-03-03T14:10:16.463492: step 9, loss 2.24105, accuracy 0.71875, precision 0.7311827956989247, recall 0.591304347826087
2019-03-03T14:10:16.936789: step 10, loss 1.78683, accuracy 0.703125, precision 0.7070707070707071, recall 0.5982905982905983
2019-03-03T14:10:17.448581: step 11, loss 1.89419, accuracy 0.726562, precision 0.7849462365591398, recall 0.5934959349593496
2019-03-03T14:10:17.964201: step 12, loss 1.97353, accuracy 0.726562, precision 0.6881720430107527, recall 0.6095238095238096
2019-03-03T14:10:18.470377: step 13, loss 1.37698, accuracy 0.769531, precision 0.7128712871287128, recall 0.7058823529411765
2019-03-03T14:10:18.947104: step 14, loss 1.16783, accuracy 0.792969, precision 0.7093023255813954, recall 0.6853932584269663
2019-03-03T14:10:19.782383: step 15, loss 1.34524, accuracy 0.761719, precision 0.6046511627906976, recall 0.6582278481012658
2019-03-03T14:10:21.818480: step 16, loss 1.26507, accuracy 0.789062, precision 0.6222222222222222, recall 0.7368421052631579
2019-03-03T14:10:22.884958: step 17, loss 1.46412, accuracy 0.803419, precision 0.6363636363636364, recall 0.8
2019-03-03T14:10:25.027564: step 18, loss 1.21261, accuracy 0.808594, precision 0.6304347826086957, recall 0.7945205479452054
2019-03-03T14:10:27.293550: step 19, loss 0.966077, accuracy 0.824219, precision 0.6629213483146067, recall 0.7972972972972973
2019-03-03T14:10:29.525583: step 20, loss 1.14347, accuracy 0.804688, precision 0.6451612903225806, recall 0.7792207792207793
2019-03-03T14:10:31.806174: step 21, loss 1.09857, accuracy 0.773438, precision 0.7254901960784313, recall 0.7115384615384616
2019-03-03T14:10:33.919663: step 22, loss 1.24486, accuracy 0.761719, precision 0.7721518987341772, recall 0.5865384615384616
2019-03-03T14:10:36.033709: step 23, loss 0.873386, accuracy 0.792969, precision 0.8426966292134831, recall 0.6578947368421053
2019-03-03T14:10:38.193980: step 24, loss 1.18439, accuracy 0.808594, precision 0.8034188034188035, recall 0.7833333333333333
2019-03-03T14:10:40.215831: step 25, loss 1.31142, accuracy 0.769531, precision 0.8433734939759037, recall 0.603448275862069
2019-03-03T14:10:42.332604: step 26, loss 1.06967, accuracy 0.773438, precision 0.7204301075268817, recall 0.6767676767676768
2019-03-03T14:10:44.442998: step 27, loss 1.3058, accuracy 0.75, precision 0.6444444444444445, recall 0.6444444444444445
2019-03-03T14:10:45.346608: step 28, loss 0.764119, accuracy 0.84375, precision 0.7244897959183674, recall 0.8452380952380952
2019-03-03T14:10:46.146470: step 29, loss 0.880568, accuracy 0.800781, precision 0.620253164556962, recall 0.7
2019-03-03T14:10:46.933437: step 30, loss 1.34175, accuracy 0.78125, precision 0.5833333333333334, recall 0.6176470588235294
2019-03-03T14:10:47.764726: step 31, loss 1.06258, accuracy 0.800781, precision 0.648936170212766, recall 0.7721518987341772
2019-03-03T14:10:48.824914: step 32, loss 1.05074, accuracy 0.785156, precision 0.5757575757575758, recall 0.8142857142857143
2019-03-03T14:10:49.957325: step 33, loss 1.25838, accuracy 0.785156, precision 0.6346153846153846, recall 0.7951807228915663
2019-03-03T14:10:50.472973: step 34, loss 0.990856, accuracy 0.794872, precision 0.6976744186046512, recall 0.7317073170731707
2019-03-03T14:10:51.493727: step 35, loss 1.17661, accuracy 0.785156, precision 0.7473684210526316, recall 0.696078431372549
2019-03-03T14:10:52.462651: step 36, loss 1.29998, accuracy 0.773438, precision 0.8247422680412371, recall 0.6611570247933884
2019-03-03T14:10:53.391169: step 37, loss 1.03775, accuracy 0.792969, precision 0.8222222222222222, recall 0.6666666666666666
2019-03-03T14:10:54.306719: step 38, loss 1.03058, accuracy 0.789062, precision 0.7439024390243902, recall 0.648936170212766
2019-03-03T14:10:55.204841: step 39, loss 0.706079, accuracy 0.820312, precision 0.8421052631578947, recall 0.7207207207207207
2019-03-03T14:10:56.069634: step 40, loss 0.807299, accuracy 0.8125, precision 0.7102803738317757, recall 0.8172043010752689
2019-03-03T14:10:56.942301: step 41, loss 0.824428, accuracy 0.839844, precision 0.7789473684210526, recall 0.7872340425531915
2019-03-03T14:10:57.791032: step 42, loss 0.837706, accuracy 0.839844, precision 0.6941176470588235, recall 0.7972972972972973
2019-03-03T14:10:58.593109: step 43, loss 1.06618, accuracy 0.808594, precision 0.6785714285714286, recall 0.7215189873417721
2019-03-03T14:10:59.343105: step 44, loss 1.01354, accuracy 0.816406, precision 0.7011494252873564, recall 0.7439024390243902
2019-03-03T14:11:00.131444: step 45, loss 0.890564, accuracy 0.816406, precision 0.6326530612244898, recall 0.8493150684931506
2019-03-03T14:11:00.894403: step 46, loss 0.799467, accuracy 0.824219, precision 0.71, recall 0.8160919540229885
2019-03-03T14:11:01.648386: step 47, loss 0.665353, accuracy 0.820312, precision 0.7938144329896907, recall 0.7475728155339806
2019-03-03T14:11:02.379440: step 48, loss 0.535793, accuracy 0.867188, precision 0.8674698795180723, recall 0.7578947368421053
2019-03-03T14:11:03.089301: step 49, loss 0.66783, accuracy 0.859375, precision 0.8586956521739131, recall 0.7745098039215687
2019-03-03T14:11:03.757515: step 50, loss 0.591601, accuracy 0.855469, precision 0.797752808988764, recall 0.7888888888888889
2019-03-03T14:11:04.103588: step 51, loss 0.672038, accuracy 0.811966, precision 0.725, recall 0.725
2019-03-03T14:11:04.787804: step 52, loss 0.902905, accuracy 0.785156, precision 0.6404494382022472, recall 0.7125
2019-03-03T14:11:05.454022: step 53, loss 0.768622, accuracy 0.835938, precision 0.8181818181818182, recall 0.7346938775510204
2019-03-03T14:11:06.152154: step 54, loss 0.662699, accuracy 0.820312, precision 0.7582417582417582, recall 0.7419354838709677
2019-03-03T14:11:06.795434: step 55, loss 0.759391, accuracy 0.816406, precision 0.691358024691358, recall 0.717948717948718
2019-03-03T14:11:07.435723: step 56, loss 0.589308, accuracy 0.859375, precision 0.74, recall 0.8809523809523809
2019-03-03T14:11:08.101942: step 57, loss 0.607114, accuracy 0.867188, precision 0.8421052631578947, recall 0.8571428571428571
2019-03-03T14:11:08.719280: step 58, loss 0.817669, accuracy 0.816406, precision 0.6705882352941176, recall 0.75
2019-03-03T14:11:09.326795: step 59, loss 0.593948, accuracy 0.867188, precision 0.8181818181818182, recall 0.8350515463917526
2019-03-03T14:11:09.928187: step 60, loss 0.506851, accuracy 0.867188, precision 0.8780487804878049, recall 0.75
2019-03-03T14:11:10.538554: step 61, loss 0.688345, accuracy 0.867188, precision 0.8522727272727273, recall 0.78125
2019-03-03T14:11:11.127999: step 62, loss 0.853565, accuracy 0.820312, precision 0.7582417582417582, recall 0.7419354838709677
2019-03-03T14:11:11.711439: step 63, loss 0.529329, accuracy 0.878906, precision 0.8850574712643678, recall 0.7857142857142857
2019-03-03T14:11:12.320815: step 64, loss 0.694449, accuracy 0.847656, precision 0.776595744680851, recall 0.8021978021978022
2019-03-03T14:11:12.921210: step 65, loss 0.616322, accuracy 0.875, precision 0.7448979591836735, recall 0.9125
2019-03-03T14:11:13.509152: step 66, loss 0.800054, accuracy 0.816406, precision 0.6923076923076923, recall 0.8804347826086957
2019-03-03T14:11:14.085616: step 67, loss 0.717494, accuracy 0.847656, precision 0.775, recall 0.7469879518072289
2019-03-03T14:11:14.362874: step 68, loss 0.617672, accuracy 0.880342, precision 0.84375, recall 0.75
2019-03-03T14:11:14.926800: step 69, loss 0.413453, accuracy 0.894531, precision 0.8292682926829268, recall 0.8395061728395061
2019-03-03T14:11:15.490319: step 70, loss 0.784436, accuracy 0.8125, precision 0.7534246575342466, recall 0.6470588235294118
2019-03-03T14:11:16.059797: step 71, loss 0.45946, accuracy 0.882812, precision 0.8207547169811321, recall 0.8877551020408163
2019-03-03T14:11:16.620516: step 72, loss 0.766195, accuracy 0.835938, precision 0.8, recall 0.7676767676767676
2019-03-03T14:11:17.187002: step 73, loss 0.35701, accuracy 0.898438, precision 0.8387096774193549, recall 0.8764044943820225
2019-03-03T14:11:17.736531: step 74, loss 0.461445, accuracy 0.855469, precision 0.780952380952381, recall 0.8541666666666666
2019-03-03T14:11:18.304529: step 75, loss 0.749273, accuracy 0.832031, precision 0.8297872340425532, recall 0.7428571428571429
2019-03-03T14:11:18.857051: step 76, loss 0.646466, accuracy 0.851562, precision 0.7731958762886598, recall 0.8241758241758241
2019-03-03T14:11:19.406099: step 77, loss 0.498805, accuracy 0.878906, precision 0.85, recall 0.8415841584158416
2019-03-03T14:11:19.969102: step 78, loss 0.526602, accuracy 0.867188, precision 0.8068181818181818, recall 0.8068181818181818
2019-03-03T14:11:20.543565: step 79, loss 0.446436, accuracy 0.878906, precision 0.8072289156626506, recall 0.8170731707317073
2019-03-03T14:11:21.079139: step 80, loss 0.542347, accuracy 0.847656, precision 0.8, recall 0.735632183908046
2019-03-03T14:11:21.603736: step 81, loss 0.612865, accuracy 0.851562, precision 0.7978723404255319, recall 0.7978723404255319
2019-03-03T14:11:22.147292: step 82, loss 0.675804, accuracy 0.828125, precision 0.7659574468085106, recall 0.7659574468085106
2019-03-03T14:11:22.672882: step 83, loss 0.640623, accuracy 0.835938, precision 0.7272727272727273, recall 0.8275862068965517
2019-03-03T14:11:23.203465: step 84, loss 0.617901, accuracy 0.867188, precision 0.8222222222222222, recall 0.8043478260869565
2019-03-03T14:11:23.457297: step 85, loss 0.628627, accuracy 0.888889, precision 0.8372093023255814, recall 0.8571428571428571
2019-03-03T14:11:23.989385: step 86, loss 0.499031, accuracy 0.863281, precision 0.8172043010752689, recall 0.8085106382978723
2019-03-03T14:11:24.531939: step 87, loss 0.546392, accuracy 0.839844, precision 0.8172043010752689, recall 0.76
2019-03-03T14:11:25.062738: step 88, loss 0.478673, accuracy 0.847656, precision 0.7717391304347826, recall 0.797752808988764
2019-03-03T14:11:25.589337: step 89, loss 0.459564, accuracy 0.875, precision 0.8350515463917526, recall 0.8350515463917526
2019-03-03T14:11:26.111360: step 90, loss 0.387631, accuracy 0.871094, precision 0.8588235294117647, recall 0.776595744680851
2019-03-03T14:11:26.637954: step 91, loss 0.394638, accuracy 0.878906, precision 0.7959183673469388, recall 0.8764044943820225
2019-03-03T14:11:27.158560: step 92, loss 0.494603, accuracy 0.894531, precision 0.8369565217391305, recall 0.8651685393258427
2019-03-03T14:11:27.670196: step 93, loss 0.485601, accuracy 0.867188, precision 0.8181818181818182, recall 0.8
2019-03-03T14:11:28.199776: step 94, loss 0.33584, accuracy 0.914062, precision 0.8865979381443299, recall 0.8865979381443299
2019-03-03T14:11:28.708433: step 95, loss 0.634798, accuracy 0.808594, precision 0.6842105263157895, recall 0.7738095238095238
2019-03-03T14:11:29.219068: step 96, loss 0.565524, accuracy 0.875, precision 0.8125, recall 0.8478260869565217
2019-03-03T14:11:29.725283: step 97, loss 0.580576, accuracy 0.851562, precision 0.8333333333333334, recall 0.7843137254901961
2019-03-03T14:11:30.244405: step 98, loss 0.716915, accuracy 0.835938, precision 0.7741935483870968, recall 0.7741935483870968
2019-03-03T14:11:30.738084: step 99, loss 0.488169, accuracy 0.875, precision 0.8666666666666667, recall 0.7959183673469388
2019-03-03T14:11:31.249228: step 100, loss 0.418036, accuracy 0.894531, precision 0.8554216867469879, recall 0.8255813953488372
2019-03-03T14:11:31.751996: step 101, loss 0.467466, accuracy 0.882812, precision 0.8, recall 0.8205128205128205
2019-03-03T14:11:31.996834: step 102, loss 0.518747, accuracy 0.846154, precision 0.7708333333333334, recall 0.8409090909090909
2019-03-03T14:11:32.508985: step 103, loss 0.387404, accuracy 0.886719, precision 0.8461538461538461, recall 0.8369565217391305
2019-03-03T14:11:33.020610: step 104, loss 0.495076, accuracy 0.867188, precision 0.7850467289719626, recall 0.8842105263157894
2019-03-03T14:11:33.503317: step 105, loss 0.484252, accuracy 0.875, precision 0.8764044943820225, recall 0.7878787878787878
2019-03-03T14:11:33.994010: step 106, loss 0.415869, accuracy 0.902344, precision 0.81, recall 0.9310344827586207
2019-03-03T14:11:34.497659: step 107, loss 0.456147, accuracy 0.863281, precision 0.7790697674418605, recall 0.8072289156626506
2019-03-03T14:11:35.005236: step 108, loss 0.538422, accuracy 0.863281, precision 0.8620689655172413, recall 0.7653061224489796
2019-03-03T14:11:35.500829: step 109, loss 0.385759, accuracy 0.878906, precision 0.8850574712643678, recall 0.7857142857142857
2019-03-03T14:11:35.991522: step 110, loss 0.374289, accuracy 0.878906, precision 0.7717391304347826, recall 0.8765432098765432
2019-03-03T14:11:36.483467: step 111, loss 0.445416, accuracy 0.875, precision 0.8426966292134831, recall 0.8064516129032258
2019-03-03T14:11:36.985128: step 112, loss 0.487126, accuracy 0.867188, precision 0.84375, recall 0.81
2019-03-03T14:11:37.462889: step 113, loss 0.422143, accuracy 0.871094, precision 0.7741935483870968, recall 0.8571428571428571
2019-03-03T14:11:37.970549: step 114, loss 0.341482, accuracy 0.890625, precision 0.8311688311688312, recall 0.810126582278481
2019-03-03T14:11:38.453257: step 115, loss 0.512826, accuracy 0.855469, precision 0.7941176470588235, recall 0.8350515463917526
2019-03-03T14:11:38.940958: step 116, loss 0.305506, accuracy 0.898438, precision 0.8636363636363636, recall 0.8444444444444444
2019-03-03T14:11:39.421674: step 117, loss 0.438988, accuracy 0.894531, precision 0.8247422680412371, recall 0.8888888888888888
2019-03-03T14:11:39.907911: step 118, loss 0.285799, accuracy 0.894531, precision 0.8705882352941177, recall 0.8222222222222222
2019-03-03T14:11:40.153763: step 119, loss 0.4544, accuracy 0.854701, precision 0.76, recall 0.8837209302325582
2019-03-03T14:11:40.645962: step 120, loss 0.272843, accuracy 0.910156, precision 0.8461538461538461, recall 0.9263157894736842
2019-03-03T14:11:41.139642: step 121, loss 0.390572, accuracy 0.859375, precision 0.8369565217391305, recall 0.7857142857142857
2019-03-03T14:11:41.806883: step 122, loss 0.36372, accuracy 0.894531, precision 0.8469387755102041, recall 0.8736842105263158
2019-03-03T14:11:42.575854: step 123, loss 0.330346, accuracy 0.929688, precision 0.9270833333333334, recall 0.89
2019-03-03T14:11:43.330284: step 124, loss 0.429253, accuracy 0.894531, precision 0.898989898989899, recall 0.839622641509434
2019-03-03T14:11:44.084837: step 125, loss 0.26327, accuracy 0.90625, precision 0.9069767441860465, recall 0.8297872340425532
2019-03-03T14:11:44.781971: step 126, loss 0.270996, accuracy 0.925781, precision 0.8571428571428571, recall 0.9285714285714286
2019-03-03T14:11:45.487085: step 127, loss 0.348964, accuracy 0.894531, precision 0.8428571428571429, recall 0.7866666666666666
2019-03-03T14:11:46.195196: step 128, loss 0.387327, accuracy 0.886719, precision 0.8476190476190476, recall 0.8725490196078431
2019-03-03T14:11:46.879808: step 129, loss 0.4755, accuracy 0.851562, precision 0.7802197802197802, recall 0.797752808988764
2019-03-03T14:11:47.559310: step 130, loss 0.229015, accuracy 0.921875, precision 0.8673469387755102, recall 0.9239130434782609
2019-03-03T14:11:48.248412: step 131, loss 0.24392, accuracy 0.910156, precision 0.8409090909090909, recall 0.891566265060241
2019-03-03T14:11:48.911498: step 132, loss 0.5566, accuracy 0.851562, precision 0.8105263157894737, recall 0.7938144329896907
2019-03-03T14:11:49.555781: step 133, loss 0.422252, accuracy 0.875, precision 0.8554216867469879, recall 0.7802197802197802
2019-03-03T14:11:50.219038: step 134, loss 0.356611, accuracy 0.878906, precision 0.8235294117647058, recall 0.813953488372093
2019-03-03T14:11:50.847849: step 135, loss 0.284489, accuracy 0.898438, precision 0.872093023255814, recall 0.8333333333333334
2019-03-03T14:11:51.141572: step 136, loss 0.239845, accuracy 0.888889, precision 0.8775510204081632, recall 0.86
2019-03-03T14:11:51.746937: step 137, loss 0.342399, accuracy 0.90625, precision 0.8160919540229885, recall 0.8987341772151899
2019-03-03T14:11:52.358267: step 138, loss 0.438551, accuracy 0.875, precision 0.8181818181818182, recall 0.8181818181818182
2019-03-03T14:11:52.963648: step 139, loss 0.273088, accuracy 0.914062, precision 0.9069767441860465, recall 0.8478260869565217
2019-03-03T14:11:53.575012: step 140, loss 0.378154, accuracy 0.882812, precision 0.8192771084337349, recall 0.8192771084337349
2019-03-03T14:11:54.178400: step 141, loss 0.213659, accuracy 0.910156, precision 0.7926829268292683, recall 0.9154929577464789
2019-03-03T14:11:54.753882: step 142, loss 0.317635, accuracy 0.871094, precision 0.7835051546391752, recall 0.8636363636363636
2019-03-03T14:11:55.331632: step 143, loss 0.300658, accuracy 0.90625, precision 0.8333333333333334, recall 0.9090909090909091
2019-03-03T14:11:55.939009: step 144, loss 0.356439, accuracy 0.871094, precision 0.8191489361702128, recall 0.8279569892473119
2019-03-03T14:11:56.500980: step 145, loss 0.193505, accuracy 0.9375, precision 0.9230769230769231, recall 0.9032258064516129
2019-03-03T14:11:57.043530: step 146, loss 0.40051, accuracy 0.875, precision 0.86, recall 0.8269230769230769
2019-03-03T14:11:57.616000: step 147, loss 0.28718, accuracy 0.914062, precision 0.8645833333333334, recall 0.9021739130434783
2019-03-03T14:11:58.191466: step 148, loss 0.247062, accuracy 0.910156, precision 0.8936170212765957, recall 0.865979381443299
2019-03-03T14:11:58.738000: step 149, loss 0.342566, accuracy 0.898438, precision 0.8352941176470589, recall 0.8554216867469879
2019-03-03T14:11:59.287828: step 150, loss 0.423607, accuracy 0.894531, precision 0.8446601941747572, recall 0.8877551020408163
2019-03-03T14:11:59.861290: step 151, loss 0.2821, accuracy 0.894531, precision 0.8543689320388349, recall 0.88
2019-03-03T14:12:00.413325: step 152, loss 0.312191, accuracy 0.902344, precision 0.9090909090909091, recall 0.8247422680412371
2019-03-03T14:12:00.676620: step 153, loss 0.309644, accuracy 0.905983, precision 0.9302325581395349, recall 0.8333333333333334
2019-03-03T14:12:01.220166: step 154, loss 0.296537, accuracy 0.890625, precision 0.9239130434782609, recall 0.8018867924528302
2019-03-03T14:12:01.776678: step 155, loss 0.209119, accuracy 0.914062, precision 0.9166666666666666, recall 0.8627450980392157
2019-03-03T14:12:02.322286: step 156, loss 0.332813, accuracy 0.882812, precision 0.8020833333333334, recall 0.875
2019-03-03T14:12:02.866516: step 157, loss 0.294597, accuracy 0.898438, precision 0.8295454545454546, recall 0.8690476190476191
2019-03-03T14:12:03.415048: step 158, loss 0.239504, accuracy 0.902344, precision 0.8105263157894737, recall 0.9166666666666666
2019-03-03T14:12:03.958601: step 159, loss 0.217048, accuracy 0.914062, precision 0.8686868686868687, recall 0.9052631578947369
2019-03-03T14:12:04.492140: step 160, loss 0.201714, accuracy 0.914062, precision 0.8979591836734694, recall 0.88
2019-03-03T14:12:05.018849: step 161, loss 0.261247, accuracy 0.898438, precision 0.8494623655913979, recall 0.8681318681318682
2019-03-03T14:12:05.566405: step 162, loss 0.34785, accuracy 0.917969, precision 0.8571428571428571, recall 0.9069767441860465
2019-03-03T14:12:06.086522: step 163, loss 0.281831, accuracy 0.914062, precision 0.8617021276595744, recall 0.9
2019-03-03T14:12:06.606134: step 164, loss 0.234317, accuracy 0.9375, precision 0.8888888888888888, recall 0.946236559139785
2019-03-03T14:12:07.148685: step 165, loss 0.265297, accuracy 0.894531, precision 0.8846153846153846, recall 0.7931034482758621
2019-03-03T14:12:07.706194: step 166, loss 0.238259, accuracy 0.902344, precision 0.851063829787234, recall 0.8791208791208791
2019-03-03T14:12:08.229306: step 167, loss 0.342381, accuracy 0.910156, precision 0.9540229885057471, recall 0.8137254901960784
2019-03-03T14:12:08.754902: step 168, loss 0.229411, accuracy 0.929688, precision 0.8876404494382022, recall 0.9080459770114943
2019-03-03T14:12:09.290894: step 169, loss 0.200419, accuracy 0.925781, precision 0.8987341772151899, recall 0.8658536585365854
2019-03-03T14:12:09.545220: step 170, loss 0.25919, accuracy 0.91453, precision 0.9375, recall 0.8653846153846154
2019-03-03T14:12:10.139630: step 171, loss 0.366923, accuracy 0.894531, precision 0.8598130841121495, recall 0.8846153846153846
2019-03-03T14:12:10.674154: step 172, loss 0.303713, accuracy 0.894531, precision 0.8404255319148937, recall 0.8681318681318682
2019-03-03T14:12:11.186785: step 173, loss 0.180448, accuracy 0.933594, precision 0.8829787234042553, recall 0.9325842696629213
2019-03-03T14:12:11.711410: step 174, loss 0.32542, accuracy 0.890625, precision 0.7790697674418605, recall 0.881578947368421
2019-03-03T14:12:12.234002: step 175, loss 0.239966, accuracy 0.925781, precision 0.8723404255319149, recall 0.9213483146067416
2019-03-03T14:12:12.729908: step 176, loss 0.148783, accuracy 0.945312, precision 0.868421052631579, recall 0.9428571428571428
2019-03-03T14:12:13.253793: step 177, loss 0.240304, accuracy 0.929688, precision 0.8351648351648352, recall 0.9620253164556962
2019-03-03T14:12:13.755463: step 178, loss 0.214148, accuracy 0.90625, precision 0.875, recall 0.875
2019-03-03T14:12:14.259117: step 179, loss 0.317032, accuracy 0.902344, precision 0.8809523809523809, recall 0.8314606741573034
2019-03-03T14:12:14.752864: step 180, loss 0.23176, accuracy 0.902344, precision 0.9080459770114943, recall 0.8229166666666666
2019-03-03T14:12:15.267487: step 181, loss 0.261936, accuracy 0.890625, precision 0.89, recall 0.839622641509434
2019-03-03T14:12:15.787098: step 182, loss 0.273078, accuracy 0.890625, precision 0.9222222222222223, recall 0.7980769230769231
2019-03-03T14:12:16.276309: step 183, loss 0.30239, accuracy 0.90625, precision 0.8494623655913979, recall 0.8876404494382022
2019-03-03T14:12:16.765003: step 184, loss 0.151484, accuracy 0.941406, precision 0.9166666666666666, recall 0.9058823529411765
2019-03-03T14:12:17.283615: step 185, loss 0.212807, accuracy 0.902344, precision 0.8817204301075269, recall 0.8541666666666666
2019-03-03T14:12:17.760851: step 186, loss 0.255717, accuracy 0.902344, precision 0.8333333333333334, recall 0.9139784946236559
2019-03-03T14:12:17.996222: step 187, loss 0.255001, accuracy 0.897436, precision 0.7777777777777778, recall 0.9459459459459459
2019-03-03T14:12:18.492899: step 188, loss 0.209528, accuracy 0.910156, precision 0.85, recall 0.8607594936708861
2019-03-03T14:12:19.008515: step 189, loss 0.238152, accuracy 0.917969, precision 0.8709677419354839, recall 0.9
2019-03-03T14:12:19.696898: step 190, loss 0.166853, accuracy 0.941406, precision 0.9247311827956989, recall 0.9148936170212766
2019-03-03T14:12:20.496270: step 191, loss 0.218435, accuracy 0.921875, precision 0.9504950495049505, recall 0.8648648648648649
2019-03-03T14:12:21.295133: step 192, loss 0.181492, accuracy 0.914062, precision 0.8928571428571429, recall 0.8522727272727273
2019-03-03T14:12:22.574714: step 193, loss 0.232867, accuracy 0.949219, precision 0.9456521739130435, recall 0.9157894736842105
2019-03-03T14:12:23.659811: step 194, loss 0.143146, accuracy 0.941406, precision 0.8888888888888888, recall 0.9230769230769231
2019-03-03T14:12:24.494642: step 195, loss 0.178361, accuracy 0.941406, precision 0.9183673469387755, recall 0.9278350515463918
2019-03-03T14:12:25.237656: step 196, loss 0.224406, accuracy 0.921875, precision 0.8390804597701149, recall 0.9240506329113924
2019-03-03T14:12:25.979671: step 197, loss 0.266709, accuracy 0.898438, precision 0.8425925925925926, recall 0.91
2019-03-03T14:12:26.677817: step 198, loss 0.198388, accuracy 0.933594, precision 0.8977272727272727, recall 0.9080459770114943
2019-03-03T14:12:27.349053: step 199, loss 0.214382, accuracy 0.921875, precision 0.8571428571428571, recall 0.9333333333333333
2019-03-03T14:12:27.936887: step 200, loss 0.224892, accuracy 0.90625, precision 0.8571428571428571, recall 0.8936170212765957
2019-03-03T14:12:28.610278: step 201, loss 0.282398, accuracy 0.902344, precision 0.8695652173913043, recall 0.8602150537634409
2019-03-03T14:12:29.255059: step 202, loss 0.271573, accuracy 0.890625, precision 0.9052631578947369, recall 0.819047619047619
2019-03-03T14:12:29.865413: step 203, loss 0.227042, accuracy 0.914062, precision 0.8765432098765432, recall 0.8554216867469879
2019-03-03T14:12:30.165124: step 204, loss 0.328025, accuracy 0.897436, precision 0.9148936170212766, recall 0.8431372549019608
2019-03-03T14:12:30.781982: step 205, loss 0.186488, accuracy 0.925781, precision 0.8901098901098901, recall 0.9
2019-03-03T14:12:31.382379: step 206, loss 0.263232, accuracy 0.882812, precision 0.8252427184466019, recall 0.8762886597938144
2019-03-03T14:12:32.002719: step 207, loss 0.167808, accuracy 0.925781, precision 0.8645833333333334, recall 0.9325842696629213
2019-03-03T14:12:32.617097: step 208, loss 0.208997, accuracy 0.929688, precision 0.8695652173913043, recall 0.9302325581395349
2019-03-03T14:12:33.206199: step 209, loss 0.245906, accuracy 0.917969, precision 0.8804347826086957, recall 0.8901098901098901
2019-03-03T14:12:33.789749: step 210, loss 0.198887, accuracy 0.902344, precision 0.8764044943820225, recall 0.8478260869565217
2019-03-03T14:12:34.358225: step 211, loss 0.170854, accuracy 0.945312, precision 0.9354838709677419, recall 0.9157894736842105
2019-03-03T14:12:34.948651: step 212, loss 0.269898, accuracy 0.917969, precision 0.9292929292929293, recall 0.8679245283018868
2019-03-03T14:12:35.526107: step 213, loss 0.215942, accuracy 0.921875, precision 0.8941176470588236, recall 0.8735632183908046
2019-03-03T14:12:36.105557: step 214, loss 0.234302, accuracy 0.917969, precision 0.8681318681318682, recall 0.8977272727272727
2019-03-03T14:12:36.734386: step 215, loss 0.147042, accuracy 0.9375, precision 0.8987341772151899, recall 0.8987341772151899
2019-03-03T14:12:37.411088: step 216, loss 0.271852, accuracy 0.902344, precision 0.8170731707317073, recall 0.8701298701298701
2019-03-03T14:12:37.999030: step 217, loss 0.280345, accuracy 0.910156, precision 0.8991596638655462, recall 0.9067796610169492
2019-03-03T14:12:38.575487: step 218, loss 0.141339, accuracy 0.9375, precision 0.8953488372093024, recall 0.9166666666666666
2019-03-03T14:12:39.154451: step 219, loss 0.180576, accuracy 0.941406, precision 0.9157894736842105, recall 0.925531914893617
2019-03-03T14:12:39.770321: step 220, loss 0.19237, accuracy 0.945312, precision 0.9294117647058824, recall 0.9080459770114943
2019-03-03T14:12:40.056556: step 221, loss 0.212941, accuracy 0.91453, precision 0.8205128205128205, recall 0.9142857142857143
2019-03-03T14:12:40.630041: step 222, loss 0.187254, accuracy 0.945312, precision 0.9090909090909091, recall 0.9302325581395349
2019-03-03T14:12:41.176580: step 223, loss 0.156298, accuracy 0.933594, precision 0.9428571428571428, recall 0.9
2019-03-03T14:12:41.693204: step 224, loss 0.169837, accuracy 0.960938, precision 0.9382716049382716, recall 0.9382716049382716
2019-03-03T14:12:42.236376: step 225, loss 0.130754, accuracy 0.933594, precision 0.9148936170212766, recall 0.9052631578947369
2019-03-03T14:12:42.782937: step 226, loss 0.170855, accuracy 0.941406, precision 0.945054945054945, recall 0.8958333333333334
2019-03-03T14:12:43.322222: step 227, loss 0.161931, accuracy 0.949219, precision 0.9560439560439561, recall 0.90625
2019-03-03T14:12:43.843827: step 228, loss 0.134724, accuracy 0.957031, precision 0.9456521739130435, recall 0.9354838709677419
2019-03-03T14:12:44.365439: step 229, loss 0.159437, accuracy 0.949219, precision 0.9555555555555556, recall 0.9052631578947369
2019-03-03T14:12:44.911490: step 230, loss 0.260761, accuracy 0.871094, precision 0.83, recall 0.8383838383838383
2019-03-03T14:12:45.462019: step 231, loss 0.163005, accuracy 0.933594, precision 0.9032258064516129, recall 0.9130434782608695
2019-03-03T14:12:46.019526: step 232, loss 0.217991, accuracy 0.914062, precision 0.8214285714285714, recall 0.9078947368421053
2019-03-03T14:12:46.569058: step 233, loss 0.153114, accuracy 0.945312, precision 0.8690476190476191, recall 0.9605263157894737
2019-03-03T14:12:47.096434: step 234, loss 0.12267, accuracy 0.945312, precision 0.94, recall 0.9215686274509803
2019-03-03T14:12:47.601084: step 235, loss 0.24973, accuracy 0.925781, precision 0.8888888888888888, recall 0.9166666666666666
2019-03-03T14:12:48.096269: step 236, loss 0.129638, accuracy 0.945312, precision 0.9032258064516129, recall 0.9438202247191011
2019-03-03T14:12:48.613887: step 237, loss 0.185031, accuracy 0.929688, precision 0.9111111111111111, recall 0.8913043478260869
2019-03-03T14:12:48.861225: step 238, loss 0.145547, accuracy 0.940171, precision 0.8780487804878049, recall 0.9473684210526315
2019-03-03T14:12:49.376852: step 239, loss 0.154491, accuracy 0.933594, precision 0.8831168831168831, recall 0.8947368421052632
2019-03-03T14:12:49.896456: step 240, loss 0.154849, accuracy 0.9375, precision 0.8928571428571429, recall 0.9146341463414634
2019-03-03T14:12:50.391133: step 241, loss 0.126137, accuracy 0.953125, precision 0.9418604651162791, recall 0.9204545454545454
2019-03-03T14:12:50.902765: step 242, loss 0.191407, accuracy 0.910156, precision 0.8804347826086957, recall 0.8709677419354839
2019-03-03T14:12:51.406419: step 243, loss 0.129053, accuracy 0.9375, precision 0.9222222222222223, recall 0.9021739130434783
2019-03-03T14:12:51.903091: step 244, loss 0.129406, accuracy 0.941406, precision 0.8947368421052632, recall 0.9066666666666666
2019-03-03T14:12:52.386798: step 245, loss 0.0948992, accuracy 0.960938, precision 0.9263157894736842, recall 0.967032967032967
2019-03-03T14:12:52.893443: step 246, loss 0.117829, accuracy 0.941406, precision 0.8877551020408163, recall 0.9560439560439561
2019-03-03T14:12:53.432010: step 247, loss 0.148061, accuracy 0.933594, precision 0.8888888888888888, recall 0.9195402298850575
2019-03-03T14:12:53.930008: step 248, loss 0.110662, accuracy 0.960938, precision 0.9222222222222223, recall 0.9651162790697675
2019-03-03T14:12:54.439179: step 249, loss 0.104682, accuracy 0.964844, precision 0.9381443298969072, recall 0.9680851063829787
2019-03-03T14:12:54.939372: step 250, loss 0.116009, accuracy 0.953125, precision 0.9333333333333333, recall 0.9333333333333333

Evaluation:
[[150  27]
 [ 27 264]]
2019-03-03T14:12:55.903730: step 250, loss 0.312175, accuracy 0.884615, precision 0.847457627118644, recall 0.847457627118644

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551643810\checkpoints\model-250

2019-03-03T14:12:56.691160: step 251, loss 0.167575, accuracy 0.921875, precision 0.90625, recall 0.8877551020408163
2019-03-03T14:12:57.436169: step 252, loss 0.162433, accuracy 0.9375, precision 0.912621359223301, recall 0.9306930693069307
2019-03-03T14:12:58.173200: step 253, loss 0.172015, accuracy 0.933594, precision 0.9081632653061225, recall 0.9175257731958762
2019-03-03T14:12:58.927184: step 254, loss 0.192481, accuracy 0.941406, precision 0.9158878504672897, recall 0.9423076923076923
2019-03-03T14:12:59.270289: step 255, loss 0.188219, accuracy 0.948718, precision 0.9574468085106383, recall 0.9183673469387755
2019-03-03T14:13:00.014069: step 256, loss 0.148869, accuracy 0.945312, precision 0.946236559139785, recall 0.9072164948453608
2019-03-03T14:13:00.748107: step 257, loss 0.0986135, accuracy 0.953125, precision 0.9404761904761905, recall 0.9186046511627907
2019-03-03T14:13:01.454737: step 258, loss 0.182391, accuracy 0.925781, precision 0.8936170212765957, recall 0.9032258064516129
2019-03-03T14:13:02.153869: step 259, loss 0.137008, accuracy 0.953125, precision 0.9175257731958762, recall 0.956989247311828
2019-03-03T14:13:02.847021: step 260, loss 0.12299, accuracy 0.949219, precision 0.9315068493150684, recall 0.8947368421052632
2019-03-03T14:13:03.504777: step 261, loss 0.152962, accuracy 0.925781, precision 0.896551724137931, recall 0.8863636363636364
2019-03-03T14:13:04.161023: step 262, loss 0.128759, accuracy 0.949219, precision 0.8804347826086957, recall 0.9759036144578314
2019-03-03T14:13:04.814277: step 263, loss 0.151231, accuracy 0.9375, precision 0.9010989010989011, recall 0.9213483146067416
2019-03-03T14:13:05.465535: step 264, loss 0.166591, accuracy 0.9375, precision 0.8846153846153846, recall 0.9078947368421053
2019-03-03T14:13:06.117307: step 265, loss 0.215915, accuracy 0.929688, precision 0.86, recall 0.9555555555555556
2019-03-03T14:13:06.742635: step 266, loss 0.221304, accuracy 0.929688, precision 0.8686868686868687, recall 0.945054945054945
2019-03-03T14:13:07.365968: step 267, loss 0.162168, accuracy 0.921875, precision 0.9058823529411765, recall 0.8651685393258427
2019-03-03T14:13:07.988210: step 268, loss 0.154883, accuracy 0.941406, precision 0.96, recall 0.897196261682243
2019-03-03T14:13:08.618038: step 269, loss 0.11315, accuracy 0.953125, precision 0.9680851063829787, recall 0.91
2019-03-03T14:13:09.208968: step 270, loss 0.174656, accuracy 0.925781, precision 0.9428571428571428, recall 0.8839285714285714
2019-03-03T14:13:09.845777: step 271, loss 0.144261, accuracy 0.945312, precision 0.9494949494949495, recall 0.912621359223301
2019-03-03T14:13:10.159958: step 272, loss 0.181833, accuracy 0.91453, precision 0.9111111111111111, recall 0.8723404255319149
2019-03-03T14:13:10.765341: step 273, loss 0.0934706, accuracy 0.960938, precision 0.9489795918367347, recall 0.9489795918367347
2019-03-03T14:13:11.398667: step 274, loss 0.0942697, accuracy 0.964844, precision 0.9230769230769231, recall 0.9767441860465116
2019-03-03T14:13:11.982109: step 275, loss 0.114831, accuracy 0.949219, precision 0.8518518518518519, recall 0.9857142857142858
2019-03-03T14:13:12.569605: step 276, loss 0.148695, accuracy 0.957031, precision 0.9340659340659341, recall 0.9444444444444444
2019-03-03T14:13:13.143069: step 277, loss 0.0770179, accuracy 0.976562, precision 0.9438202247191011, recall 0.9882352941176471
2019-03-03T14:13:13.687126: step 278, loss 0.129658, accuracy 0.945312, precision 0.92, recall 0.9387755102040817
2019-03-03T14:13:14.261591: step 279, loss 0.182042, accuracy 0.9375, precision 0.8979591836734694, recall 0.9361702127659575
2019-03-03T14:13:14.805149: step 280, loss 0.212007, accuracy 0.925781, precision 0.9052631578947369, recall 0.8958333333333334
2019-03-03T14:13:15.364648: step 281, loss 0.139015, accuracy 0.953125, precision 0.9484536082474226, recall 0.9292929292929293
2019-03-03T14:13:15.936633: step 282, loss 0.15141, accuracy 0.945312, precision 0.9313725490196079, recall 0.9313725490196079
2019-03-03T14:13:16.483172: step 283, loss 0.09134, accuracy 0.960938, precision 0.9775280898876404, recall 0.9157894736842105
2019-03-03T14:13:17.032722: step 284, loss 0.0936255, accuracy 0.964844, precision 0.975, recall 0.9176470588235294
2019-03-03T14:13:17.569287: step 285, loss 0.156264, accuracy 0.949219, precision 0.9642857142857143, recall 0.8901098901098901
2019-03-03T14:13:18.116824: step 286, loss 0.0938828, accuracy 0.957031, precision 0.9456521739130435, recall 0.9354838709677419
2019-03-03T14:13:18.634444: step 287, loss 0.138065, accuracy 0.9375, precision 0.90625, recall 0.925531914893617
2019-03-03T14:13:19.160600: step 288, loss 0.152064, accuracy 0.9375, precision 0.8723404255319149, recall 0.9534883720930233
2019-03-03T14:13:19.405186: step 289, loss 0.171746, accuracy 0.940171, precision 0.8461538461538461, recall 0.9705882352941176
2019-03-03T14:13:19.947741: step 290, loss 0.220152, accuracy 0.917969, precision 0.8469387755102041, recall 0.9325842696629213
2019-03-03T14:13:20.473845: step 291, loss 0.0933826, accuracy 0.972656, precision 0.9642857142857143, recall 0.9529411764705882
2019-03-03T14:13:20.999439: step 292, loss 0.177645, accuracy 0.925781, precision 0.8666666666666667, recall 0.9176470588235294
2019-03-03T14:13:21.539442: step 293, loss 0.130279, accuracy 0.941406, precision 0.9222222222222223, recall 0.9120879120879121
2019-03-03T14:13:22.071012: step 294, loss 0.126133, accuracy 0.945312, precision 0.948051948051948, recall 0.8795180722891566
2019-03-03T14:13:22.600597: step 295, loss 0.119008, accuracy 0.964844, precision 0.9659090909090909, recall 0.9340659340659341
2019-03-03T14:13:23.136165: step 296, loss 0.188403, accuracy 0.914062, precision 0.8888888888888888, recall 0.8888888888888888
2019-03-03T14:13:23.676743: step 297, loss 0.149404, accuracy 0.945312, precision 0.9659090909090909, recall 0.8854166666666666
2019-03-03T14:13:24.193361: step 298, loss 0.109798, accuracy 0.960938, precision 0.9518072289156626, recall 0.9294117647058824
2019-03-03T14:13:24.709502: step 299, loss 0.142567, accuracy 0.933594, precision 0.9021739130434783, recall 0.9120879120879121
2019-03-03T14:13:25.227118: step 300, loss 0.186207, accuracy 0.921875, precision 0.8681318681318682, recall 0.9080459770114943
2019-03-03T14:13:25.772177: step 301, loss 0.106884, accuracy 0.941406, precision 0.9019607843137255, recall 0.9484536082474226
2019-03-03T14:13:26.317716: step 302, loss 0.197309, accuracy 0.902344, precision 0.8, recall 0.9545454545454546
2019-03-03T14:13:26.812900: step 303, loss 0.137911, accuracy 0.957031, precision 0.9318181818181818, recall 0.9425287356321839
2019-03-03T14:13:27.298604: step 304, loss 0.15814, accuracy 0.933594, precision 0.9191919191919192, recall 0.91
2019-03-03T14:13:27.777322: step 305, loss 0.0983467, accuracy 0.964844, precision 0.9405940594059405, recall 0.9693877551020408
2019-03-03T14:13:28.010698: step 306, loss 0.090399, accuracy 0.965812, precision 0.975609756097561, recall 0.9302325581395349
2019-03-03T14:13:28.505376: step 307, loss 0.102487, accuracy 0.957031, precision 0.9506172839506173, recall 0.9166666666666666
2019-03-03T14:13:29.015014: step 308, loss 0.155184, accuracy 0.941406, precision 0.9381443298969072, recall 0.91
2019-03-03T14:13:29.534624: step 309, loss 0.178176, accuracy 0.941406, precision 0.9252336448598131, recall 0.9339622641509434
2019-03-03T14:13:30.040273: step 310, loss 0.0644818, accuracy 0.96875, precision 0.978021978021978, recall 0.9368421052631579
2019-03-03T14:13:30.552901: step 311, loss 0.0838547, accuracy 0.972656, precision 0.96875, recall 0.9587628865979382
2019-03-03T14:13:31.053563: step 312, loss 0.112657, accuracy 0.957031, precision 0.9375, recall 0.9473684210526315
2019-03-03T14:13:31.529318: step 313, loss 0.079881, accuracy 0.972656, precision 0.9901960784313726, recall 0.9439252336448598
2019-03-03T14:13:32.121732: step 314, loss 0.0643411, accuracy 0.96875, precision 0.9456521739130435, recall 0.9666666666666667
2019-03-03T14:13:32.675415: step 315, loss 0.0941139, accuracy 0.972656, precision 0.9523809523809523, recall 0.963855421686747
2019-03-03T14:13:33.222970: step 316, loss 0.125968, accuracy 0.941406, precision 0.8737864077669902, recall 0.9782608695652174
2019-03-03T14:13:33.748569: step 317, loss 0.075373, accuracy 0.964844, precision 0.9342105263157895, recall 0.9466666666666667
2019-03-03T14:13:34.300600: step 318, loss 0.160008, accuracy 0.945312, precision 0.8705882352941177, recall 0.961038961038961
2019-03-03T14:13:34.868088: step 319, loss 0.0717536, accuracy 0.96875, precision 0.9875, recall 0.9186046511627907
2019-03-03T14:13:35.434084: step 320, loss 0.145824, accuracy 0.941406, precision 0.9368421052631579, recall 0.9081632653061225
2019-03-03T14:13:35.972839: step 321, loss 0.122717, accuracy 0.945312, precision 0.9101123595505618, recall 0.9310344827586207
2019-03-03T14:13:36.523372: step 322, loss 0.096693, accuracy 0.960938, precision 0.9479166666666666, recall 0.9479166666666666
2019-03-03T14:13:36.793845: step 323, loss 0.237696, accuracy 0.940171, precision 0.8913043478260869, recall 0.9534883720930233
2019-03-03T14:13:37.334425: step 324, loss 0.0972004, accuracy 0.957031, precision 0.95, recall 0.9405940594059405
2019-03-03T14:13:37.864518: step 325, loss 0.0584644, accuracy 0.984375, precision 0.9879518072289156, recall 0.9647058823529412
2019-03-03T14:13:38.386404: step 326, loss 0.110818, accuracy 0.957031, precision 0.9310344827586207, recall 0.9418604651162791
2019-03-03T14:13:38.907523: step 327, loss 0.13653, accuracy 0.945312, precision 0.9647058823529412, recall 0.8817204301075269
2019-03-03T14:13:39.441613: step 328, loss 0.148114, accuracy 0.949219, precision 0.9263157894736842, recall 0.9361702127659575
2019-03-03T14:13:39.987664: step 329, loss 0.127549, accuracy 0.933594, precision 0.9456521739130435, recall 0.8787878787878788
2019-03-03T14:13:40.509781: step 330, loss 0.107879, accuracy 0.953125, precision 0.9775280898876404, recall 0.8969072164948454
2019-03-03T14:13:41.027402: step 331, loss 0.0917916, accuracy 0.964844, precision 0.9456521739130435, recall 0.9560439560439561
2019-03-03T14:13:41.560980: step 332, loss 0.0644811, accuracy 0.972656, precision 0.9775280898876404, recall 0.9456521739130435
2019-03-03T14:13:42.105714: step 333, loss 0.13851, accuracy 0.941406, precision 0.8712871287128713, recall 0.9777777777777777
2019-03-03T14:13:42.624606: step 334, loss 0.113, accuracy 0.945312, precision 0.9203539823008849, recall 0.9541284403669725
2019-03-03T14:13:43.167154: step 335, loss 0.10044, accuracy 0.953125, precision 0.8977272727272727, recall 0.9634146341463414
2019-03-03T14:13:43.693746: step 336, loss 0.134027, accuracy 0.953125, precision 0.9047619047619048, recall 0.95
2019-03-03T14:13:44.270204: step 337, loss 0.153433, accuracy 0.945312, precision 0.9042553191489362, recall 0.9444444444444444
2019-03-03T14:13:44.826717: step 338, loss 0.0910074, accuracy 0.957031, precision 0.9393939393939394, recall 0.9489795918367347
2019-03-03T14:13:45.400207: step 339, loss 0.137145, accuracy 0.941406, precision 0.9565217391304348, recall 0.8888888888888888
2019-03-03T14:13:45.680309: step 340, loss 0.100513, accuracy 0.948718, precision 0.9393939393939394, recall 0.8857142857142857
2019-03-03T14:13:46.270730: step 341, loss 0.129133, accuracy 0.957031, precision 0.9134615384615384, recall 0.979381443298969
2019-03-03T14:13:46.825248: step 342, loss 0.0968426, accuracy 0.972656, precision 0.9550561797752809, recall 0.9659090909090909
2019-03-03T14:13:47.382777: step 343, loss 0.0988111, accuracy 0.949219, precision 0.968421052631579, recall 0.9019607843137255
2019-03-03T14:13:47.915380: step 344, loss 0.115923, accuracy 0.96875, precision 0.967391304347826, recall 0.9468085106382979
2019-03-03T14:13:48.548146: step 345, loss 0.0914865, accuracy 0.96875, precision 0.9770114942528736, recall 0.9340659340659341
2019-03-03T14:13:49.105036: step 346, loss 0.13815, accuracy 0.953125, precision 0.9629629629629629, recall 0.896551724137931
2019-03-03T14:13:49.606593: step 347, loss 0.0888148, accuracy 0.96875, precision 0.9340659340659341, recall 0.9770114942528736
2019-03-03T14:13:50.107888: step 348, loss 0.146661, accuracy 0.945312, precision 0.9270833333333334, recall 0.9270833333333334
2019-03-03T14:13:50.606554: step 349, loss 0.0953353, accuracy 0.964844, precision 0.9418604651162791, recall 0.9529411764705882
2019-03-03T14:13:51.130019: step 350, loss 0.082767, accuracy 0.964844, precision 0.9354838709677419, recall 0.9666666666666667
2019-03-03T14:13:51.639738: step 351, loss 0.112315, accuracy 0.960938, precision 0.9148936170212766, recall 0.9772727272727273
2019-03-03T14:13:52.149338: step 352, loss 0.102293, accuracy 0.953125, precision 0.9523809523809523, recall 0.9090909090909091
2019-03-03T14:13:52.651993: step 353, loss 0.0908503, accuracy 0.96875, precision 0.9381443298969072, recall 0.978494623655914
2019-03-03T14:13:53.169638: step 354, loss 0.0730479, accuracy 0.96875, precision 0.9438202247191011, recall 0.9655172413793104
2019-03-03T14:13:53.668812: step 355, loss 0.143553, accuracy 0.949219, precision 0.9387755102040817, recall 0.9292929292929293
2019-03-03T14:13:54.176454: step 356, loss 0.0835699, accuracy 0.957031, precision 0.956989247311828, recall 0.9270833333333334
2019-03-03T14:13:54.416839: step 357, loss 0.163119, accuracy 0.940171, precision 0.9361702127659575, recall 0.9166666666666666
2019-03-03T14:13:54.923508: step 358, loss 0.0843492, accuracy 0.972656, precision 0.9444444444444444, recall 0.9770114942528736
2019-03-03T14:13:55.444114: step 359, loss 0.0751064, accuracy 0.96875, precision 0.9587628865979382, recall 0.9587628865979382
2019-03-03T14:13:55.966228: step 360, loss 0.139526, accuracy 0.925781, precision 0.8958333333333334, recall 0.9052631578947369
2019-03-03T14:13:56.501799: step 361, loss 0.0694264, accuracy 0.976562, precision 0.9574468085106383, recall 0.9782608695652174
2019-03-03T14:13:57.032377: step 362, loss 0.138395, accuracy 0.9375, precision 0.9325842696629213, recall 0.8924731182795699
2019-03-03T14:13:57.549042: step 363, loss 0.0600055, accuracy 0.984375, precision 0.9767441860465116, recall 0.9767441860465116
2019-03-03T14:13:58.128493: step 364, loss 0.0924176, accuracy 0.96875, precision 0.9518072289156626, recall 0.9518072289156626
2019-03-03T14:13:58.679020: step 365, loss 0.0708435, accuracy 0.972656, precision 0.9438202247191011, recall 0.9767441860465116
2019-03-03T14:13:59.262627: step 366, loss 0.0890718, accuracy 0.953125, precision 0.9333333333333333, recall 0.9090909090909091
2019-03-03T14:13:59.816656: step 367, loss 0.0664164, accuracy 0.980469, precision 0.9772727272727273, recall 0.9662921348314607
2019-03-03T14:14:00.373188: step 368, loss 0.172001, accuracy 0.917969, precision 0.8811881188118812, recall 0.9081632653061225
2019-03-03T14:14:00.903239: step 369, loss 0.0878308, accuracy 0.960938, precision 0.9484536082474226, recall 0.9484536082474226
2019-03-03T14:14:01.415867: step 370, loss 0.0985303, accuracy 0.960938, precision 0.9428571428571428, recall 0.9611650485436893
2019-03-03T14:14:01.965309: step 371, loss 0.0780317, accuracy 0.96875, precision 0.9473684210526315, recall 0.967741935483871
2019-03-03T14:14:02.530477: step 372, loss 0.091081, accuracy 0.957031, precision 0.9130434782608695, recall 0.9655172413793104
2019-03-03T14:14:03.136856: step 373, loss 0.155042, accuracy 0.957031, precision 0.9393939393939394, recall 0.9489795918367347
2019-03-03T14:14:03.413117: step 374, loss 0.0926952, accuracy 0.974359, precision 0.95, recall 0.9743589743589743
2019-03-03T14:14:03.996558: step 375, loss 0.105963, accuracy 0.949219, precision 0.9545454545454546, recall 0.9032258064516129
2019-03-03T14:14:04.557566: step 376, loss 0.0903409, accuracy 0.96875, precision 0.974025974025974, recall 0.9259259259259259
2019-03-03T14:14:05.113087: step 377, loss 0.12234, accuracy 0.957031, precision 0.9523809523809523, recall 0.9433962264150944
2019-03-03T14:14:05.658627: step 378, loss 0.078966, accuracy 0.96875, precision 0.9397590361445783, recall 0.9629629629629629
2019-03-03T14:14:06.211151: step 379, loss 0.126381, accuracy 0.953125, precision 0.9318181818181818, recall 0.9318181818181818
2019-03-03T14:14:06.772158: step 380, loss 0.0771261, accuracy 0.972656, precision 0.9702970297029703, recall 0.9607843137254902
2019-03-03T14:14:07.323684: step 381, loss 0.133944, accuracy 0.945312, precision 0.9340659340659341, recall 0.9139784946236559
2019-03-03T14:14:07.891166: step 382, loss 0.0400079, accuracy 0.992188, precision 0.974025974025974, recall 1.0
2019-03-03T14:14:08.441695: step 383, loss 0.127368, accuracy 0.945312, precision 0.8865979381443299, recall 0.9662921348314607
2019-03-03T14:14:08.975269: step 384, loss 0.101029, accuracy 0.941406, precision 0.8958333333333334, recall 0.945054945054945
2019-03-03T14:14:09.532778: step 385, loss 0.0947353, accuracy 0.976562, precision 0.9494949494949495, recall 0.9894736842105263
2019-03-03T14:14:10.067350: step 386, loss 0.0800084, accuracy 0.964844, precision 0.9279279279279279, recall 0.9903846153846154
2019-03-03T14:14:10.608900: step 387, loss 0.0794006, accuracy 0.972656, precision 0.967391304347826, recall 0.956989247311828
2019-03-03T14:14:11.151451: step 388, loss 0.0826077, accuracy 0.976562, precision 0.9651162790697675, recall 0.9651162790697675
2019-03-03T14:14:11.674176: step 389, loss 0.0762542, accuracy 0.964844, precision 0.9655172413793104, recall 0.9333333333333333
2019-03-03T14:14:12.223225: step 390, loss 0.090083, accuracy 0.972656, precision 0.970873786407767, recall 0.9615384615384616
2019-03-03T14:14:12.478061: step 391, loss 0.12436, accuracy 0.940171, precision 0.9714285714285714, recall 0.85
2019-03-03T14:14:13.026595: step 392, loss 0.118528, accuracy 0.949219, precision 0.9368421052631579, recall 0.9270833333333334
2019-03-03T14:14:13.573154: step 393, loss 0.0743409, accuracy 0.96875, precision 0.9603960396039604, recall 0.9603960396039604
2019-03-03T14:14:14.121723: step 394, loss 0.108664, accuracy 0.949219, precision 0.9081632653061225, recall 0.956989247311828
2019-03-03T14:14:14.669277: step 395, loss 0.0784119, accuracy 0.984375, precision 0.9642857142857143, recall 0.9878048780487805
2019-03-03T14:14:15.210819: step 396, loss 0.0755332, accuracy 0.960938, precision 0.9411764705882353, recall 0.9411764705882353
2019-03-03T14:14:15.754302: step 397, loss 0.127815, accuracy 0.933594, precision 0.9340659340659341, recall 0.8854166666666666
2019-03-03T14:14:16.286878: step 398, loss 0.0610544, accuracy 0.972656, precision 0.9770114942528736, recall 0.9444444444444444
2019-03-03T14:14:16.821449: step 399, loss 0.0735405, accuracy 0.980469, precision 0.9468085106382979, recall 1.0
2019-03-03T14:14:17.378959: step 400, loss 0.0659876, accuracy 0.972656, precision 0.9404761904761905, recall 0.9753086419753086
2019-03-03T14:14:17.933475: step 401, loss 0.111883, accuracy 0.960938, precision 0.9504950495049505, recall 0.9504950495049505
2019-03-03T14:14:18.473540: step 402, loss 0.0552932, accuracy 0.980469, precision 0.956989247311828, recall 0.9888888888888889
2019-03-03T14:14:19.028059: step 403, loss 0.0906778, accuracy 0.957031, precision 0.8913043478260869, recall 0.9879518072289156
2019-03-03T14:14:19.582087: step 404, loss 0.0833675, accuracy 0.960938, precision 0.9651162790697675, recall 0.9222222222222223
2019-03-03T14:14:20.137112: step 405, loss 0.0602025, accuracy 0.984375, precision 0.978494623655914, recall 0.978494623655914
2019-03-03T14:14:20.665212: step 406, loss 0.0682832, accuracy 0.96875, precision 0.9489795918367347, recall 0.96875
2019-03-03T14:14:21.232204: step 407, loss 0.115713, accuracy 0.957031, precision 0.9545454545454546, recall 0.9230769230769231
2019-03-03T14:14:21.509463: step 408, loss 0.0708404, accuracy 0.974359, precision 0.9782608695652174, recall 0.9574468085106383
2019-03-03T14:14:22.090907: step 409, loss 0.0932476, accuracy 0.964844, precision 0.95, recall 0.9595959595959596
2019-03-03T14:14:22.661382: step 410, loss 0.0644154, accuracy 0.976562, precision 0.96875, recall 0.96875
2019-03-03T14:14:23.225874: step 411, loss 0.0827632, accuracy 0.960938, precision 0.9473684210526315, recall 0.9473684210526315
2019-03-03T14:14:23.781390: step 412, loss 0.0833699, accuracy 0.964844, precision 0.9215686274509803, recall 0.9894736842105263
2019-03-03T14:14:24.341401: step 413, loss 0.0643859, accuracy 0.972656, precision 0.9782608695652174, recall 0.9473684210526315
2019-03-03T14:14:24.885945: step 414, loss 0.0431127, accuracy 0.992188, precision 0.9879518072289156, recall 0.9879518072289156
2019-03-03T14:14:25.441973: step 415, loss 0.0602636, accuracy 0.980469, precision 0.9811320754716981, recall 0.9719626168224299
2019-03-03T14:14:26.017944: step 416, loss 0.0838848, accuracy 0.976562, precision 0.9876543209876543, recall 0.9411764705882353
2019-03-03T14:14:26.596397: step 417, loss 0.0760092, accuracy 0.964844, precision 0.9347826086956522, recall 0.9662921348314607
2019-03-03T14:14:27.170388: step 418, loss 0.105762, accuracy 0.960938, precision 0.9113924050632911, recall 0.96
2019-03-03T14:14:27.758342: step 419, loss 0.110787, accuracy 0.960938, precision 0.9431818181818182, recall 0.9431818181818182
2019-03-03T14:14:28.322833: step 420, loss 0.0646383, accuracy 0.972656, precision 0.9468085106382979, recall 0.978021978021978
2019-03-03T14:14:28.882341: step 421, loss 0.0635785, accuracy 0.972656, precision 0.9574468085106383, recall 0.967741935483871
2019-03-03T14:14:29.434034: step 422, loss 0.074147, accuracy 0.964844, precision 0.96875, recall 0.9393939393939394
2019-03-03T14:14:29.998526: step 423, loss 0.082974, accuracy 0.976562, precision 0.98, recall 0.9607843137254902
2019-03-03T14:14:30.575930: step 424, loss 0.0854973, accuracy 0.972656, precision 0.9743589743589743, recall 0.9382716049382716
2019-03-03T14:14:30.841221: step 425, loss 0.054277, accuracy 0.974359, precision 0.95, recall 0.9743589743589743
2019-03-03T14:14:31.397528: step 426, loss 0.0676215, accuracy 0.976562, precision 0.9423076923076923, recall 1.0
2019-03-03T14:14:31.964013: step 427, loss 0.0929935, accuracy 0.960938, precision 0.9381443298969072, recall 0.9578947368421052
2019-03-03T14:14:32.546497: step 428, loss 0.0734865, accuracy 0.96875, precision 0.9574468085106383, recall 0.9574468085106383
2019-03-03T14:14:33.113980: step 429, loss 0.0295495, accuracy 0.996094, precision 0.9886363636363636, recall 1.0
2019-03-03T14:14:33.687958: step 430, loss 0.099768, accuracy 0.972656, precision 0.9565217391304348, recall 0.967032967032967
2019-03-03T14:14:34.243386: step 431, loss 0.0494404, accuracy 0.988281, precision 1.0, recall 0.9625
2019-03-03T14:14:34.858739: step 432, loss 0.0834757, accuracy 0.964844, precision 0.9489795918367347, recall 0.9587628865979382
2019-03-03T14:14:35.495855: step 433, loss 0.0722787, accuracy 0.980469, precision 0.9879518072289156, recall 0.9534883720930233
2019-03-03T14:14:36.111193: step 434, loss 0.0770021, accuracy 0.964844, precision 0.9146341463414634, recall 0.974025974025974
2019-03-03T14:14:36.721555: step 435, loss 0.0522395, accuracy 0.984375, precision 0.9904761904761905, recall 0.9719626168224299
2019-03-03T14:14:37.330926: step 436, loss 0.0770137, accuracy 0.964844, precision 0.9387755102040817, recall 0.968421052631579
2019-03-03T14:14:37.940301: step 437, loss 0.107152, accuracy 0.960938, precision 0.9294117647058824, recall 0.9518072289156626
2019-03-03T14:14:38.544196: step 438, loss 0.0701924, accuracy 0.972656, precision 0.978021978021978, recall 0.9468085106382979
2019-03-03T14:14:39.138628: step 439, loss 0.141093, accuracy 0.9375, precision 0.8941176470588236, recall 0.9156626506024096
2019-03-03T14:14:39.727075: step 440, loss 0.0882158, accuracy 0.964844, precision 0.967741935483871, recall 0.9375
2019-03-03T14:14:40.309031: step 441, loss 0.08529, accuracy 0.972656, precision 0.9795918367346939, recall 0.9504950495049505
2019-03-03T14:14:40.583298: step 442, loss 0.0690902, accuracy 0.974359, precision 0.9565217391304348, recall 0.9777777777777777
2019-03-03T14:14:41.159757: step 443, loss 0.0586087, accuracy 0.980469, precision 0.9666666666666667, recall 0.9775280898876404
2019-03-03T14:14:41.741203: step 444, loss 0.0541793, accuracy 0.984375, precision 0.9772727272727273, recall 0.9772727272727273
2019-03-03T14:14:42.309196: step 445, loss 0.0777523, accuracy 0.980469, precision 0.978021978021978, recall 0.967391304347826
2019-03-03T14:14:42.902614: step 446, loss 0.06627, accuracy 0.984375, precision 0.978021978021978, recall 0.978021978021978
2019-03-03T14:14:43.478071: step 447, loss 0.0919376, accuracy 0.976562, precision 0.9615384615384616, recall 0.9803921568627451
2019-03-03T14:14:44.042898: step 448, loss 0.0649817, accuracy 0.976562, precision 0.9888888888888889, recall 0.9468085106382979
2019-03-03T14:14:44.612376: step 449, loss 0.0631083, accuracy 0.984375, precision 0.9891304347826086, recall 0.9680851063829787
2019-03-03T14:14:45.169885: step 450, loss 0.0663312, accuracy 0.980469, precision 0.9642857142857143, recall 0.9759036144578314
2019-03-03T14:14:45.726415: step 451, loss 0.065054, accuracy 0.980469, precision 0.96875, recall 0.9789473684210527
2019-03-03T14:14:46.338161: step 452, loss 0.0486032, accuracy 0.988281, precision 0.9875, recall 0.9753086419753086
2019-03-03T14:14:46.903865: step 453, loss 0.0388808, accuracy 0.992188, precision 0.9902912621359223, recall 0.9902912621359223
2019-03-03T14:14:47.444986: step 454, loss 0.0763907, accuracy 0.960938, precision 0.941747572815534, recall 0.9603960396039604
2019-03-03T14:14:47.984561: step 455, loss 0.0782412, accuracy 0.980469, precision 0.963855421686747, recall 0.975609756097561
2019-03-03T14:14:48.504680: step 456, loss 0.0717783, accuracy 0.976562, precision 0.967741935483871, recall 0.967741935483871
2019-03-03T14:14:49.048227: step 457, loss 0.0496838, accuracy 0.980469, precision 0.9795918367346939, recall 0.9696969696969697
2019-03-03T14:14:49.588697: step 458, loss 0.157766, accuracy 0.957031, precision 0.9222222222222223, recall 0.9540229885057471
2019-03-03T14:14:49.845021: step 459, loss 0.0940972, accuracy 0.974359, precision 0.975, recall 0.9512195121951219
2019-03-03T14:14:50.366126: step 460, loss 0.118246, accuracy 0.964844, precision 0.95, recall 0.9595959595959596
2019-03-03T14:14:50.891721: step 461, loss 0.0385165, accuracy 0.984375, precision 0.9886363636363636, recall 0.9666666666666667
2019-03-03T14:14:51.421813: step 462, loss 0.0959854, accuracy 0.953125, precision 0.9285714285714286, recall 0.9479166666666666
2019-03-03T14:14:51.951926: step 463, loss 0.053056, accuracy 0.976562, precision 0.9587628865979382, recall 0.9789473684210527
2019-03-03T14:14:52.484503: step 464, loss 0.063177, accuracy 0.980469, precision 0.9696969696969697, recall 0.9795918367346939
2019-03-03T14:14:53.019107: step 465, loss 0.078055, accuracy 0.984375, precision 0.978494623655914, recall 0.978494623655914
2019-03-03T14:14:53.532737: step 466, loss 0.0620265, accuracy 0.972656, precision 0.9696969696969697, recall 0.96
2019-03-03T14:14:54.057333: step 467, loss 0.10587, accuracy 0.949219, precision 0.9270833333333334, recall 0.9368421052631579
2019-03-03T14:14:54.588913: step 468, loss 0.0546436, accuracy 0.984375, precision 1.0, recall 0.9560439560439561
2019-03-03T14:14:55.103538: step 469, loss 0.0672537, accuracy 0.980469, precision 0.9558823529411765, recall 0.9701492537313433
2019-03-03T14:14:55.622512: step 470, loss 0.0784719, accuracy 0.964844, precision 0.963302752293578, recall 0.9545454545454546
2019-03-03T14:14:56.174038: step 471, loss 0.0587309, accuracy 0.96875, precision 0.9375, recall 0.9615384615384616
2019-03-03T14:14:56.722591: step 472, loss 0.0530171, accuracy 0.980469, precision 0.96875, recall 0.9789473684210527
2019-03-03T14:14:57.254169: step 473, loss 0.0752182, accuracy 0.964844, precision 0.956989247311828, recall 0.9468085106382979
2019-03-03T14:14:57.771297: step 474, loss 0.0648759, accuracy 0.980469, precision 0.9659090909090909, recall 0.9770114942528736
2019-03-03T14:14:58.307861: step 475, loss 0.063804, accuracy 0.980469, precision 0.978021978021978, recall 0.967391304347826
2019-03-03T14:14:58.564176: step 476, loss 0.0837387, accuracy 0.974359, precision 0.9705882352941176, recall 0.9428571428571428
2019-03-03T14:14:59.095755: step 477, loss 0.0717301, accuracy 0.984375, precision 0.9767441860465116, recall 0.9767441860465116
2019-03-03T14:14:59.633318: step 478, loss 0.0720702, accuracy 0.980469, precision 0.9615384615384616, recall 0.9900990099009901
2019-03-03T14:15:00.154446: step 479, loss 0.0561921, accuracy 0.984375, precision 0.9629629629629629, recall 0.9873417721518988
2019-03-03T14:15:00.669061: step 480, loss 0.0714046, accuracy 0.96875, precision 0.9484536082474226, recall 0.968421052631579
2019-03-03T14:15:01.206402: step 481, loss 0.0622489, accuracy 0.976562, precision 1.0, recall 0.9333333333333333
2019-03-03T14:15:01.725016: step 482, loss 0.0752811, accuracy 0.964844, precision 0.9591836734693877, recall 0.9494949494949495
2019-03-03T14:15:02.245269: step 483, loss 0.0652251, accuracy 0.984375, precision 0.9787234042553191, recall 0.9787234042553191
2019-03-03T14:15:02.798787: step 484, loss 0.0661565, accuracy 0.976562, precision 0.9772727272727273, recall 0.9555555555555556
2019-03-03T14:15:03.339368: step 485, loss 0.080294, accuracy 0.980469, precision 0.9655172413793104, recall 0.9767441860465116
2019-03-03T14:15:03.875998: step 486, loss 0.0924917, accuracy 0.964844, precision 0.9795918367346939, recall 0.9320388349514563
2019-03-03T14:15:04.389944: step 487, loss 0.0517484, accuracy 0.992188, precision 0.9787234042553191, recall 1.0
2019-03-03T14:15:04.912059: step 488, loss 0.0658615, accuracy 0.976562, precision 0.9767441860465116, recall 0.9545454545454546
2019-03-03T14:15:05.422203: step 489, loss 0.096107, accuracy 0.96875, precision 0.9278350515463918, recall 0.989010989010989
2019-03-03T14:15:05.946312: step 490, loss 0.0774983, accuracy 0.96875, precision 0.9367088607594937, recall 0.961038961038961
2019-03-03T14:15:06.461935: step 491, loss 0.0885599, accuracy 0.980469, precision 0.9411764705882353, recall 1.0
2019-03-03T14:15:06.996504: step 492, loss 0.0727791, accuracy 0.976562, precision 0.9523809523809523, recall 0.9900990099009901
2019-03-03T14:15:07.248850: step 493, loss 0.0882145, accuracy 0.957265, precision 0.9245283018867925, recall 0.98
2019-03-03T14:15:07.779452: step 494, loss 0.0774916, accuracy 0.972656, precision 0.98, recall 0.9514563106796117
2019-03-03T14:15:08.315130: step 495, loss 0.0698701, accuracy 0.976562, precision 0.9607843137254902, recall 0.98
2019-03-03T14:15:08.851728: step 496, loss 0.0446797, accuracy 0.976562, precision 0.9775280898876404, recall 0.9560439560439561
2019-03-03T14:15:09.398777: step 497, loss 0.0992942, accuracy 0.960938, precision 0.989010989010989, recall 0.9090909090909091
2019-03-03T14:15:09.925394: step 498, loss 0.061183, accuracy 0.976562, precision 0.9807692307692307, recall 0.9622641509433962
2019-03-03T14:15:10.446002: step 499, loss 0.0688318, accuracy 0.96875, precision 0.989247311827957, recall 0.9292929292929293
2019-03-03T14:15:10.983085: step 500, loss 0.0461523, accuracy 0.980469, precision 0.9629629629629629, recall 0.975

Evaluation:
[[150  27]
 [ 24 267]]
2019-03-03T14:15:11.375044: step 500, loss 0.308482, accuracy 0.891026, precision 0.847457627118644, recall 0.8620689655172413

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551643810\checkpoints\model-500


Process finished with exit code 0
