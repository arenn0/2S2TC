Pretrained Embedding: ELMo
Italian: True
Loading data...
4681
Max Document length: 81
Vocabulary Size: 1
Train/Dev split: 4447/234
ELMo module loaded from tensorflow-hub
79
78
77
Writing to /home/ubuntu/Project/runs/1551758559

2019-03-05T04:02:45.339750: step 1, loss 2.67252, accuracy 0.625, precision 0.35135135135135137, recall 0.35135135135135137
2019-03-05T04:02:47.590330: step 2, loss 3.16769, accuracy 0.53125, precision 0.4782608695652174, recall 0.3793103448275862
2019-03-05T04:02:49.839347: step 3, loss 6.16358, accuracy 0.585938, precision 0.019230769230769232, recall 0.3333333333333333
2019-03-05T04:02:52.089009: step 4, loss 1.96792, accuracy 0.65625, precision 0.74, recall 0.5441176470588235
2019-03-05T04:02:54.315921: step 5, loss 4.09328, accuracy 0.570312, precision 0.8888888888888888, recall 0.4444444444444444
2019-03-05T04:02:56.564107: step 6, loss 2.3056, accuracy 0.671875, precision 0.5869565217391305, recall 0.54
2019-03-05T04:02:58.805984: step 7, loss 2.85259, accuracy 0.703125, precision 0.3409090909090909, recall 0.625
2019-03-05T04:03:01.046673: step 8, loss 3.59026, accuracy 0.65625, precision 0.23076923076923078, recall 0.75
2019-03-05T04:03:03.286766: step 9, loss 1.92167, accuracy 0.703125, precision 0.44, recall 0.6875
2019-03-05T04:03:05.523012: step 10, loss 2.28171, accuracy 0.671875, precision 0.7843137254901961, recall 0.5633802816901409
2019-03-05T04:03:07.760879: step 11, loss 4.24955, accuracy 0.523438, precision 0.8235294117647058, recall 0.3373493975903614
2019-03-05T04:03:09.986748: step 12, loss 1.4905, accuracy 0.765625, precision 0.8928571428571429, recall 0.6756756756756757
2019-03-05T04:03:12.240587: step 13, loss 2.17611, accuracy 0.679688, precision 0.42857142857142855, recall 0.6176470588235294
2019-03-05T04:03:14.488463: step 14, loss 2.36644, accuracy 0.679688, precision 0.30612244897959184, recall 0.6818181818181818
2019-03-05T04:03:16.736184: step 15, loss 2.69979, accuracy 0.671875, precision 0.38461538461538464, recall 0.6666666666666666
2019-03-05T04:03:18.969941: step 16, loss 1.40359, accuracy 0.773438, precision 0.5535714285714286, recall 0.8857142857142857
2019-03-05T04:03:21.223983: step 17, loss 1.41323, accuracy 0.75, precision 0.7291666666666666, recall 0.6481481481481481
2019-03-05T04:03:23.488771: step 18, loss 1.51538, accuracy 0.71875, precision 0.9047619047619048, recall 0.5428571428571428
2019-03-05T04:03:25.734253: step 19, loss 1.88971, accuracy 0.726562, precision 0.8947368421052632, recall 0.5230769230769231
2019-03-05T04:03:27.976845: step 20, loss 1.17342, accuracy 0.773438, precision 0.8222222222222222, recall 0.6379310344827587
2019-03-05T04:03:30.225100: step 21, loss 1.20476, accuracy 0.773438, precision 0.5405405405405406, recall 0.625
2019-03-05T04:03:32.473273: step 22, loss 2.17205, accuracy 0.679688, precision 0.3076923076923077, recall 0.7619047619047619
2019-03-05T04:03:34.743956: step 23, loss 0.979698, accuracy 0.789062, precision 0.4594594594594595, recall 0.7083333333333334
2019-03-05T04:03:37.007342: step 24, loss 1.0418, accuracy 0.78125, precision 0.6363636363636364, recall 0.7
2019-03-05T04:03:39.258115: step 25, loss 1.63854, accuracy 0.75, precision 0.6904761904761905, recall 0.6041666666666666
2019-03-05T04:03:41.541125: step 26, loss 1.2673, accuracy 0.726562, precision 0.7924528301886793, recall 0.6363636363636364
2019-03-05T04:03:43.783331: step 27, loss 1.1858, accuracy 0.734375, precision 0.8043478260869565, recall 0.5967741935483871
2019-03-05T04:03:46.043032: step 28, loss 0.821827, accuracy 0.757812, precision 0.8125, recall 0.639344262295082
2019-03-05T04:03:48.292373: step 29, loss 1.09079, accuracy 0.773438, precision 0.574468085106383, recall 0.75
2019-03-05T04:03:50.554773: step 30, loss 1.3684, accuracy 0.734375, precision 0.46, recall 0.7666666666666667
2019-03-05T04:03:52.804813: step 31, loss 1.00612, accuracy 0.742188, precision 0.49056603773584906, recall 0.8125
2019-03-05T04:03:55.048957: step 32, loss 1.25202, accuracy 0.71875, precision 0.7254901960784313, recall 0.6271186440677966
2019-03-05T04:03:57.320831: step 33, loss 0.893771, accuracy 0.78125, precision 0.8541666666666666, recall 0.6612903225806451
2019-03-05T04:03:59.569327: step 34, loss 0.951961, accuracy 0.734375, precision 0.8297872340425532, recall 0.6
2019-03-05T04:04:02.931075: step 35, loss 0.759776, accuracy 0.778947, precision 0.7666666666666667, recall 0.6216216216216216
2019-03-05T04:04:05.204754: step 36, loss 0.977431, accuracy 0.773438, precision 0.6326530612244898, recall 0.7380952380952381
2019-03-05T04:04:07.471113: step 37, loss 1.15769, accuracy 0.75, precision 0.48936170212765956, recall 0.7419354838709677
2019-03-05T04:04:09.707176: step 38, loss 1.09855, accuracy 0.695312, precision 0.5409836065573771, recall 0.75
2019-03-05T04:04:11.956123: step 39, loss 0.535212, accuracy 0.8125, precision 0.8113207547169812, recall 0.7543859649122807
2019-03-05T04:04:14.214767: step 40, loss 0.694601, accuracy 0.789062, precision 0.8222222222222222, recall 0.6607142857142857
2019-03-05T04:04:16.459724: step 41, loss 0.52688, accuracy 0.796875, precision 0.8666666666666667, recall 0.6610169491525424
2019-03-05T04:04:18.705643: step 42, loss 0.433702, accuracy 0.851562, precision 0.8333333333333334, recall 0.7446808510638298
2019-03-05T04:04:20.957022: step 43, loss 0.578903, accuracy 0.789062, precision 0.717391304347826, recall 0.7021276595744681
2019-03-05T04:04:23.218832: step 44, loss 0.850145, accuracy 0.75, precision 0.4888888888888889, recall 0.7096774193548387
2019-03-05T04:04:25.472038: step 45, loss 0.452338, accuracy 0.859375, precision 0.6444444444444445, recall 0.9354838709677419
2019-03-05T04:04:27.741778: step 46, loss 0.378616, accuracy 0.835938, precision 0.6511627906976745, recall 0.8235294117647058
2019-03-05T04:04:29.992150: step 47, loss 0.609966, accuracy 0.796875, precision 0.6486486486486487, recall 0.6486486486486487
2019-03-05T04:04:32.241854: step 48, loss 0.561108, accuracy 0.796875, precision 0.7142857142857143, recall 0.7446808510638298
2019-03-05T04:04:34.504023: step 49, loss 0.75536, accuracy 0.78125, precision 0.851063829787234, recall 0.6557377049180327
2019-03-05T04:04:36.755943: step 50, loss 0.496511, accuracy 0.851562, precision 0.8727272727272727, recall 0.8
2019-03-05T04:04:39.016692: step 51, loss 0.441114, accuracy 0.835938, precision 0.7959183673469388, recall 0.78
2019-03-05T04:04:41.266227: step 52, loss 0.507355, accuracy 0.828125, precision 0.8333333333333334, recall 0.7
2019-03-05T04:04:43.514562: step 53, loss 0.637562, accuracy 0.8125, precision 0.6326530612244898, recall 0.8378378378378378
2019-03-05T04:04:45.763683: step 54, loss 0.373195, accuracy 0.84375, precision 0.6923076923076923, recall 0.9
2019-03-05T04:04:48.028829: step 55, loss 0.401551, accuracy 0.875, precision 0.7291666666666666, recall 0.9210526315789473
2019-03-05T04:04:50.279224: step 56, loss 0.266929, accuracy 0.90625, precision 0.8444444444444444, recall 0.8837209302325582
2019-03-05T04:04:52.551278: step 57, loss 0.578421, accuracy 0.8125, precision 0.8292682926829268, recall 0.6666666666666666
2019-03-05T04:04:54.808052: step 58, loss 0.605421, accuracy 0.8125, precision 0.8571428571428571, recall 0.6122448979591837
2019-03-05T04:04:57.048106: step 59, loss 0.270702, accuracy 0.875, precision 0.7884615384615384, recall 0.8913043478260869
2019-03-05T04:04:59.311629: step 60, loss 0.459194, accuracy 0.835938, precision 0.6756756756756757, recall 0.7352941176470589
2019-03-05T04:05:01.565458: step 61, loss 0.394346, accuracy 0.859375, precision 0.6938775510204082, recall 0.918918918918919
2019-03-05T04:05:03.836221: step 62, loss 0.372195, accuracy 0.84375, precision 0.625, recall 0.9375
2019-03-05T04:05:06.094968: step 63, loss 0.477419, accuracy 0.796875, precision 0.6964285714285714, recall 0.8125
2019-03-05T04:05:08.343422: step 64, loss 0.389391, accuracy 0.859375, precision 0.8636363636363636, recall 0.76
2019-03-05T04:05:10.601212: step 65, loss 0.399121, accuracy 0.835938, precision 0.8269230769230769, recall 0.7818181818181819
2019-03-05T04:05:12.854869: step 66, loss 0.48919, accuracy 0.789062, precision 0.8333333333333334, recall 0.6363636363636364
2019-03-05T04:05:15.112230: step 67, loss 0.428968, accuracy 0.835938, precision 0.7954545454545454, recall 0.7446808510638298
2019-03-05T04:05:17.380975: step 68, loss 0.337002, accuracy 0.859375, precision 0.7959183673469388, recall 0.8297872340425532
2019-03-05T04:05:19.638323: step 69, loss 0.454019, accuracy 0.835938, precision 0.7045454545454546, recall 0.7948717948717948
2019-03-05T04:05:21.784250: step 70, loss 0.408761, accuracy 0.831579, precision 0.675, recall 0.9
2019-03-05T04:05:24.026901: step 71, loss 0.48827, accuracy 0.835938, precision 0.6739130434782609, recall 0.8378378378378378
2019-03-05T04:05:26.276262: step 72, loss 0.185011, accuracy 0.929688, precision 0.8421052631578947, recall 0.9142857142857143
2019-03-05T04:05:28.540966: step 73, loss 0.373317, accuracy 0.851562, precision 0.78, recall 0.8297872340425532
2019-03-05T04:05:30.815520: step 74, loss 0.327509, accuracy 0.851562, precision 0.8571428571428571, recall 0.7346938775510204
2019-03-05T04:05:33.079048: step 75, loss 0.320046, accuracy 0.875, precision 0.8913043478260869, recall 0.7884615384615384
2019-03-05T04:05:35.345106: step 76, loss 0.299832, accuracy 0.882812, precision 0.775, recall 0.8378378378378378
2019-03-05T04:05:37.616629: step 77, loss 0.264254, accuracy 0.84375, precision 0.6829268292682927, recall 0.8
2019-03-05T04:05:39.870410: step 78, loss 0.40887, accuracy 0.851562, precision 0.673469387755102, recall 0.9166666666666666
2019-03-05T04:05:42.097756: step 79, loss 0.343961, accuracy 0.8125, precision 0.7592592592592593, recall 0.7884615384615384
2019-03-05T04:05:44.373770: step 80, loss 0.298973, accuracy 0.851562, precision 0.6410256410256411, recall 0.8333333333333334
2019-03-05T04:05:46.632343: step 81, loss 0.239605, accuracy 0.890625, precision 0.7714285714285715, recall 0.8181818181818182
2019-03-05T04:05:48.908679: step 82, loss 0.425585, accuracy 0.8125, precision 0.8163265306122449, recall 0.7272727272727273
2019-03-05T04:05:51.174526: step 83, loss 0.225225, accuracy 0.898438, precision 0.8979591836734694, recall 0.8461538461538461
2019-03-05T04:05:53.424378: step 84, loss 0.295324, accuracy 0.898438, precision 0.782608695652174, recall 0.9230769230769231
2019-03-05T04:05:55.689341: step 85, loss 0.279785, accuracy 0.859375, precision 0.6808510638297872, recall 0.9142857142857143
2019-03-05T04:05:57.948166: step 86, loss 0.416384, accuracy 0.851562, precision 0.7857142857142857, recall 0.7674418604651163
2019-03-05T04:06:00.203431: step 87, loss 0.335378, accuracy 0.8125, precision 0.7317073170731707, recall 0.6976744186046512
2019-03-05T04:06:02.462776: step 88, loss 0.396005, accuracy 0.851562, precision 0.7, recall 0.6774193548387096
2019-03-05T04:06:04.712784: step 89, loss 0.295884, accuracy 0.875, precision 0.6981132075471698, recall 1.0
2019-03-05T04:06:06.954802: step 90, loss 0.29893, accuracy 0.882812, precision 0.7833333333333333, recall 0.9591836734693877
2019-03-05T04:06:09.192990: step 91, loss 0.265392, accuracy 0.859375, precision 0.8372093023255814, recall 0.7659574468085106
2019-03-05T04:06:11.456561: step 92, loss 0.261442, accuracy 0.875, precision 0.8541666666666666, recall 0.82
2019-03-05T04:06:13.695965: step 93, loss 0.366977, accuracy 0.84375, precision 0.7777777777777778, recall 0.7
2019-03-05T04:06:15.975775: step 94, loss 0.259554, accuracy 0.882812, precision 0.7450980392156863, recall 0.95
2019-03-05T04:06:18.229047: step 95, loss 0.409988, accuracy 0.828125, precision 0.6296296296296297, recall 0.9444444444444444
2019-03-05T04:06:20.489036: step 96, loss 0.255173, accuracy 0.890625, precision 0.8166666666666667, recall 0.9423076923076923
2019-03-05T04:06:22.744467: step 97, loss 0.243682, accuracy 0.882812, precision 0.8936170212765957, recall 0.8076923076923077
2019-03-05T04:06:24.999867: step 98, loss 0.355612, accuracy 0.851562, precision 0.8478260869565217, recall 0.7647058823529411
2019-03-05T04:06:27.270623: step 99, loss 0.380231, accuracy 0.890625, precision 0.9782608695652174, recall 0.7758620689655172
2019-03-05T04:06:29.515108: step 100, loss 0.393697, accuracy 0.84375, precision 0.8214285714285714, recall 0.8214285714285714
2019-03-05T04:06:31.771157: step 101, loss 0.274597, accuracy 0.890625, precision 0.7755102040816326, recall 0.926829268292683
2019-03-05T04:06:34.019399: step 102, loss 0.285138, accuracy 0.867188, precision 0.6530612244897959, recall 1.0
2019-03-05T04:06:36.293831: step 103, loss 0.321789, accuracy 0.851562, precision 0.6981132075471698, recall 0.925
2019-03-05T04:06:38.550866: step 104, loss 0.318355, accuracy 0.859375, precision 0.8363636363636363, recall 0.8363636363636363
2019-03-05T04:06:40.717004: step 105, loss 0.353006, accuracy 0.842105, precision 0.7297297297297297, recall 0.84375
2019-03-05T04:06:42.956895: step 106, loss 0.252492, accuracy 0.90625, precision 0.9130434782608695, recall 0.84
2019-03-05T04:06:45.207145: step 107, loss 0.252777, accuracy 0.890625, precision 0.9361702127659575, recall 0.8
2019-03-05T04:06:47.478589: step 108, loss 0.271841, accuracy 0.890625, precision 0.84, recall 0.875
2019-03-05T04:06:49.738644: step 109, loss 0.257588, accuracy 0.859375, precision 0.7962962962962963, recall 0.86
2019-03-05T04:06:52.015269: step 110, loss 0.234141, accuracy 0.90625, precision 0.8205128205128205, recall 0.8648648648648649
2019-03-05T04:06:54.260501: step 111, loss 0.185828, accuracy 0.898438, precision 0.7777777777777778, recall 0.9210526315789473
2019-03-05T04:06:56.542095: step 112, loss 0.154355, accuracy 0.921875, precision 0.7647058823529411, recall 0.9285714285714286
2019-03-05T04:06:58.808281: step 113, loss 0.407253, accuracy 0.828125, precision 0.625, recall 0.8823529411764706
2019-03-05T04:07:01.059615: step 114, loss 0.281438, accuracy 0.851562, precision 0.7307692307692307, recall 0.8837209302325582
2019-03-05T04:07:03.338711: step 115, loss 0.263089, accuracy 0.875, precision 0.8125, recall 0.8478260869565217
2019-03-05T04:07:05.585848: step 116, loss 0.22642, accuracy 0.929688, precision 0.9183673469387755, recall 0.9
2019-03-05T04:07:07.860271: step 117, loss 0.294464, accuracy 0.867188, precision 0.868421052631579, recall 0.7333333333333333
2019-03-05T04:07:10.120731: step 118, loss 0.302158, accuracy 0.867188, precision 0.8076923076923077, recall 0.8571428571428571
2019-03-05T04:07:12.362699: step 119, loss 0.42643, accuracy 0.882812, precision 0.9069767441860465, recall 0.78
2019-03-05T04:07:14.606415: step 120, loss 0.25239, accuracy 0.882812, precision 0.7297297297297297, recall 0.84375
2019-03-05T04:07:16.889864: step 121, loss 0.282183, accuracy 0.875, precision 0.7592592592592593, recall 0.9318181818181818
2019-03-05T04:07:19.161045: step 122, loss 0.282766, accuracy 0.90625, precision 0.7551020408163265, recall 1.0
2019-03-05T04:07:21.425345: step 123, loss 0.211446, accuracy 0.9375, precision 0.8333333333333334, recall 0.9375
2019-03-05T04:07:23.679562: step 124, loss 0.29167, accuracy 0.867188, precision 0.8333333333333334, recall 0.8163265306122449
2019-03-05T04:07:25.929082: step 125, loss 0.266601, accuracy 0.890625, precision 0.88, recall 0.8461538461538461
2019-03-05T04:07:28.208352: step 126, loss 0.285207, accuracy 0.875, precision 0.8867924528301887, recall 0.8245614035087719
2019-03-05T04:07:30.451644: step 127, loss 0.24636, accuracy 0.867188, precision 0.8867924528301887, recall 0.8103448275862069
2019-03-05T04:07:32.717557: step 128, loss 0.231611, accuracy 0.898438, precision 0.9090909090909091, recall 0.8620689655172413
2019-03-05T04:07:34.992175: step 129, loss 0.218529, accuracy 0.914062, precision 0.8541666666666666, recall 0.9111111111111111
2019-03-05T04:07:37.254434: step 130, loss 0.228492, accuracy 0.867188, precision 0.7254901960784313, recall 0.925
2019-03-05T04:07:39.489144: step 131, loss 0.201718, accuracy 0.914062, precision 0.8775510204081632, recall 0.8958333333333334
2019-03-05T04:07:41.751014: step 132, loss 0.293893, accuracy 0.859375, precision 0.7346938775510204, recall 0.8780487804878049
2019-03-05T04:07:44.039611: step 133, loss 0.24506, accuracy 0.890625, precision 0.8461538461538461, recall 0.8048780487804879
2019-03-05T04:07:46.290081: step 134, loss 0.288653, accuracy 0.890625, precision 0.8269230769230769, recall 0.8958333333333334
2019-03-05T04:07:48.558986: step 135, loss 0.214884, accuracy 0.9375, precision 0.8333333333333334, recall 0.9722222222222222
2019-03-05T04:07:50.817278: step 136, loss 0.243313, accuracy 0.914062, precision 0.8541666666666666, recall 0.9111111111111111
2019-03-05T04:07:53.075948: step 137, loss 0.303069, accuracy 0.859375, precision 0.75, recall 0.7894736842105263
2019-03-05T04:07:55.352317: step 138, loss 0.233582, accuracy 0.921875, precision 0.8571428571428571, recall 0.9
2019-03-05T04:07:57.609158: step 139, loss 0.319935, accuracy 0.867188, precision 0.7254901960784313, recall 0.925
2019-03-05T04:07:59.768770: step 140, loss 0.20679, accuracy 0.915789, precision 0.8611111111111112, recall 0.9117647058823529
2019-03-05T04:08:02.042144: step 141, loss 0.288418, accuracy 0.859375, precision 0.7924528301886793, recall 0.8571428571428571
2019-03-05T04:08:04.301853: step 142, loss 0.254637, accuracy 0.882812, precision 0.7674418604651163, recall 0.868421052631579
2019-03-05T04:08:06.534273: step 143, loss 0.295101, accuracy 0.875, precision 0.8333333333333334, recall 0.8653846153846154
2019-03-05T04:08:08.801315: step 144, loss 0.28384, accuracy 0.875, precision 0.8541666666666666, recall 0.82
2019-03-05T04:08:11.065113: step 145, loss 0.16616, accuracy 0.914062, precision 0.9, recall 0.8823529411764706
2019-03-05T04:08:13.322326: step 146, loss 0.35064, accuracy 0.882812, precision 0.8727272727272727, recall 0.8571428571428571
2019-03-05T04:08:15.577784: step 147, loss 0.232714, accuracy 0.9375, precision 0.8979591836734694, recall 0.9361702127659575
2019-03-05T04:08:17.831354: step 148, loss 0.301991, accuracy 0.867188, precision 0.7358490566037735, recall 0.9285714285714286
2019-03-05T04:08:20.125619: step 149, loss 0.181945, accuracy 0.9375, precision 0.7941176470588235, recall 0.9642857142857143
2019-03-05T04:08:22.379992: step 150, loss 0.207355, accuracy 0.914062, precision 0.8372093023255814, recall 0.9
2019-03-05T04:08:24.626365: step 151, loss 0.203185, accuracy 0.9375, precision 0.88, recall 0.9565217391304348
2019-03-05T04:08:26.879752: step 152, loss 0.194837, accuracy 0.9375, precision 0.8536585365853658, recall 0.9459459459459459
2019-03-05T04:08:29.142157: step 153, loss 0.218683, accuracy 0.90625, precision 0.8958333333333334, recall 0.86
2019-03-05T04:08:31.384943: step 154, loss 0.288955, accuracy 0.882812, precision 0.8545454545454545, recall 0.8703703703703703
2019-03-05T04:08:33.654403: step 155, loss 0.210511, accuracy 0.90625, precision 0.9024390243902439, recall 0.8222222222222222
2019-03-05T04:08:35.928241: step 156, loss 0.215253, accuracy 0.914062, precision 0.875, recall 0.8936170212765957
2019-03-05T04:08:38.196144: step 157, loss 0.207845, accuracy 0.921875, precision 0.896551724137931, recall 0.9285714285714286
2019-03-05T04:08:40.451735: step 158, loss 0.230236, accuracy 0.90625, precision 0.8958333333333334, recall 0.86
2019-03-05T04:08:42.693961: step 159, loss 0.238277, accuracy 0.898438, precision 0.8163265306122449, recall 0.9090909090909091
2019-03-05T04:08:44.947882: step 160, loss 0.184169, accuracy 0.90625, precision 0.8823529411764706, recall 0.8823529411764706
2019-03-05T04:08:47.224823: step 161, loss 0.157978, accuracy 0.9375, precision 0.9361702127659575, recall 0.8979591836734694
2019-03-05T04:08:49.482608: step 162, loss 0.240558, accuracy 0.882812, precision 0.7608695652173914, recall 0.8974358974358975
2019-03-05T04:08:51.730936: step 163, loss 0.251366, accuracy 0.898438, precision 0.8076923076923077, recall 0.9333333333333333
2019-03-05T04:08:53.989625: step 164, loss 0.230546, accuracy 0.929688, precision 0.9183673469387755, recall 0.9
2019-03-05T04:08:56.238196: step 165, loss 0.192721, accuracy 0.890625, precision 0.8085106382978723, recall 0.8837209302325582
2019-03-05T04:08:58.509806: step 166, loss 0.273411, accuracy 0.914062, precision 0.9090909090909091, recall 0.8928571428571429
2019-03-05T04:09:00.790182: step 167, loss 0.232529, accuracy 0.898438, precision 0.8378378378378378, recall 0.8157894736842105
2019-03-05T04:09:03.067559: step 168, loss 0.262454, accuracy 0.914062, precision 0.8636363636363636, recall 0.8837209302325582
2019-03-05T04:09:05.346518: step 169, loss 0.196497, accuracy 0.921875, precision 0.8333333333333334, recall 0.9210526315789473
2019-03-05T04:09:07.602696: step 170, loss 0.24172, accuracy 0.898438, precision 0.75, recall 0.9090909090909091
2019-03-05T04:09:09.843420: step 171, loss 0.180328, accuracy 0.90625, precision 0.7560975609756098, recall 0.9393939393939394
2019-03-05T04:09:12.112771: step 172, loss 0.241876, accuracy 0.914062, precision 0.875, recall 0.8936170212765957
2019-03-05T04:09:14.374837: step 173, loss 0.236889, accuracy 0.914062, precision 0.85, recall 0.8717948717948718
2019-03-05T04:09:16.625233: step 174, loss 0.190687, accuracy 0.914062, precision 0.8285714285714286, recall 0.8529411764705882
2019-03-05T04:09:18.771670: step 175, loss 0.276013, accuracy 0.894737, precision 0.7575757575757576, recall 0.9259259259259259
2019-03-05T04:09:21.038420: step 176, loss 0.307057, accuracy 0.84375, precision 0.7407407407407407, recall 0.8695652173913043
2019-03-05T04:09:23.314467: step 177, loss 0.165086, accuracy 0.921875, precision 0.9, recall 0.9
2019-03-05T04:09:25.552621: step 178, loss 0.18529, accuracy 0.921875, precision 0.9482758620689655, recall 0.8870967741935484
2019-03-05T04:09:27.802928: step 179, loss 0.187155, accuracy 0.921875, precision 0.9183673469387755, recall 0.8823529411764706
2019-03-05T04:09:30.049727: step 180, loss 0.16697, accuracy 0.945312, precision 0.9607843137254902, recall 0.9074074074074074
2019-03-05T04:09:32.292576: step 181, loss 0.182607, accuracy 0.921875, precision 0.8958333333333334, recall 0.8958333333333334
2019-03-05T04:09:34.573188: step 182, loss 0.138913, accuracy 0.9375, precision 0.8571428571428571, recall 0.9473684210526315
2019-03-05T04:09:36.834102: step 183, loss 0.217712, accuracy 0.914062, precision 0.7608695652173914, recall 1.0
2019-03-05T04:09:39.096024: step 184, loss 0.229555, accuracy 0.890625, precision 0.7884615384615384, recall 0.9318181818181818
2019-03-05T04:09:41.358364: step 185, loss 0.182871, accuracy 0.90625, precision 0.8541666666666666, recall 0.8913043478260869
2019-03-05T04:09:43.618415: step 186, loss 0.260425, accuracy 0.875, precision 0.8867924528301887, recall 0.8245614035087719
2019-03-05T04:09:45.868178: step 187, loss 0.29724, accuracy 0.90625, precision 0.9411764705882353, recall 0.8421052631578947
2019-03-05T04:09:48.128737: step 188, loss 0.195799, accuracy 0.890625, precision 0.9387755102040817, recall 0.8070175438596491
2019-03-05T04:09:50.394395: step 189, loss 0.209777, accuracy 0.914062, precision 0.7948717948717948, recall 0.9117647058823529
2019-03-05T04:09:52.649740: step 190, loss 0.177516, accuracy 0.9375, precision 0.8709677419354839, recall 0.8709677419354839
2019-03-05T04:09:54.904706: step 191, loss 0.236436, accuracy 0.90625, precision 0.8095238095238095, recall 0.8947368421052632
2019-03-05T04:09:57.163364: step 192, loss 0.171036, accuracy 0.9375, precision 0.8260869565217391, recall 1.0
2019-03-05T04:09:59.427541: step 193, loss 0.265943, accuracy 0.890625, precision 0.7435897435897436, recall 0.8787878787878788
2019-03-05T04:10:01.690532: step 194, loss 0.289693, accuracy 0.929688, precision 0.9285714285714286, recall 0.8666666666666667
2019-03-05T04:10:03.944167: step 195, loss 0.226009, accuracy 0.898438, precision 0.7692307692307693, recall 0.975609756097561
2019-03-05T04:10:06.206023: step 196, loss 0.272677, accuracy 0.882812, precision 0.9347826086956522, recall 0.7818181818181819
2019-03-05T04:10:08.448283: step 197, loss 0.12032, accuracy 0.96875, precision 0.9387755102040817, recall 0.9787234042553191
2019-03-05T04:10:10.731717: step 198, loss 0.152903, accuracy 0.9375, precision 0.8958333333333334, recall 0.9347826086956522
2019-03-05T04:10:12.977493: step 199, loss 0.196392, accuracy 0.914062, precision 0.8846153846153846, recall 0.9019607843137255
2019-03-05T04:10:15.241317: step 200, loss 0.288895, accuracy 0.921875, precision 0.9137931034482759, recall 0.9137931034482759
2019-03-05T04:10:17.527400: step 201, loss 0.155297, accuracy 0.929688, precision 0.9387755102040817, recall 0.8846153846153846
2019-03-05T04:10:19.769407: step 202, loss 0.15874, accuracy 0.953125, precision 0.925, recall 0.925
2019-03-05T04:10:22.020200: step 203, loss 0.170076, accuracy 0.945312, precision 0.94, recall 0.9215686274509803
2019-03-05T04:10:24.292766: step 204, loss 0.191073, accuracy 0.929688, precision 0.8666666666666667, recall 0.9285714285714286
2019-03-05T04:10:26.528738: step 205, loss 0.233599, accuracy 0.867188, precision 0.7441860465116279, recall 0.8421052631578947
2019-03-05T04:10:28.756726: step 206, loss 0.161831, accuracy 0.929688, precision 0.7631578947368421, recall 1.0
2019-03-05T04:10:31.011814: step 207, loss 0.267452, accuracy 0.90625, precision 0.782608695652174, recall 0.9473684210526315
2019-03-05T04:10:33.259745: step 208, loss 0.256713, accuracy 0.914062, precision 0.8431372549019608, recall 0.9347826086956522
2019-03-05T04:10:35.526126: step 209, loss 0.186049, accuracy 0.914062, precision 0.875, recall 0.8536585365853658
2019-03-05T04:10:37.695506: step 210, loss 0.147237, accuracy 0.947368, precision 0.9333333333333333, recall 0.9032258064516129
2019-03-05T04:10:39.965709: step 211, loss 0.176308, accuracy 0.945312, precision 0.9375, recall 0.9183673469387755
2019-03-05T04:10:42.226800: step 212, loss 0.154021, accuracy 0.976562, precision 0.9743589743589743, recall 0.95
2019-03-05T04:10:44.466301: step 213, loss 0.187579, accuracy 0.914062, precision 0.813953488372093, recall 0.9210526315789473
2019-03-05T04:10:46.703793: step 214, loss 0.109878, accuracy 0.976562, precision 0.9767441860465116, recall 0.9545454545454546
2019-03-05T04:10:48.959816: step 215, loss 0.265011, accuracy 0.867188, precision 0.7115384615384616, recall 0.9487179487179487
2019-03-05T04:10:51.232918: step 216, loss 0.21259, accuracy 0.898438, precision 0.8333333333333334, recall 0.9183673469387755
2019-03-05T04:10:53.485378: step 217, loss 0.198269, accuracy 0.929688, precision 0.92, recall 0.9019607843137255
2019-03-05T04:10:55.743763: step 218, loss 0.206909, accuracy 0.882812, precision 0.9148936170212766, recall 0.7962962962962963
2019-03-05T04:10:57.984897: step 219, loss 0.178092, accuracy 0.9375, precision 0.9444444444444444, recall 0.9107142857142857
2019-03-05T04:11:00.245567: step 220, loss 0.183835, accuracy 0.914062, precision 0.9272727272727272, recall 0.8793103448275862
2019-03-05T04:11:02.478190: step 221, loss 0.106861, accuracy 0.953125, precision 0.9361702127659575, recall 0.9361702127659575
2019-03-05T04:11:04.714334: step 222, loss 0.179192, accuracy 0.9375, precision 0.9333333333333333, recall 0.8936170212765957
2019-03-05T04:11:06.954658: step 223, loss 0.165796, accuracy 0.9375, precision 0.8333333333333334, recall 1.0
2019-03-05T04:11:09.228217: step 224, loss 0.160163, accuracy 0.914062, precision 0.7804878048780488, recall 0.9411764705882353
2019-03-05T04:11:11.492934: step 225, loss 0.197021, accuracy 0.945312, precision 0.8780487804878049, recall 0.9473684210526315
2019-03-05T04:11:13.749092: step 226, loss 0.150482, accuracy 0.96875, precision 0.9523809523809523, recall 0.9523809523809523
2019-03-05T04:11:16.016567: step 227, loss 0.149641, accuracy 0.945312, precision 0.9318181818181818, recall 0.9111111111111111
2019-03-05T04:11:18.271851: step 228, loss 0.217889, accuracy 0.914062, precision 0.9130434782608695, recall 0.8571428571428571
2019-03-05T04:11:20.517618: step 229, loss 0.207416, accuracy 0.9375, precision 0.9375, recall 0.9
2019-03-05T04:11:22.771194: step 230, loss 0.139772, accuracy 0.960938, precision 0.918918918918919, recall 0.9444444444444444
2019-03-05T04:11:25.041481: step 231, loss 0.161585, accuracy 0.945312, precision 0.8888888888888888, recall 0.9523809523809523
2019-03-05T04:11:27.321265: step 232, loss 0.196892, accuracy 0.914062, precision 0.8076923076923077, recall 0.9767441860465116
2019-03-05T04:11:29.605842: step 233, loss 0.136774, accuracy 0.945312, precision 0.88, recall 0.9777777777777777
2019-03-05T04:11:31.869823: step 234, loss 0.178651, accuracy 0.921875, precision 0.9375, recall 0.8653846153846154
2019-03-05T04:11:34.113928: step 235, loss 0.194797, accuracy 0.945312, precision 0.9333333333333333, recall 0.9130434782608695
2019-03-05T04:11:36.377669: step 236, loss 0.231604, accuracy 0.929688, precision 0.9772727272727273, recall 0.8431372549019608
2019-03-05T04:11:38.647020: step 237, loss 0.253749, accuracy 0.890625, precision 0.8412698412698413, recall 0.9298245614035088
2019-03-05T04:11:40.900386: step 238, loss 0.235478, accuracy 0.9375, precision 0.8604651162790697, recall 0.9487179487179487
2019-03-05T04:11:43.162816: step 239, loss 0.146862, accuracy 0.9375, precision 0.8846153846153846, recall 0.9583333333333334
2019-03-05T04:11:45.415405: step 240, loss 0.135177, accuracy 0.960938, precision 0.96, recall 0.9411764705882353
2019-03-05T04:11:47.663297: step 241, loss 0.126264, accuracy 0.96875, precision 0.9777777777777777, recall 0.9361702127659575
2019-03-05T04:11:49.905825: step 242, loss 0.160519, accuracy 0.929688, precision 0.9, recall 0.8780487804878049
2019-03-05T04:11:52.149913: step 243, loss 0.180723, accuracy 0.945312, precision 0.8695652173913043, recall 0.975609756097561
2019-03-05T04:11:54.389520: step 244, loss 0.138734, accuracy 0.953125, precision 0.9473684210526315, recall 0.9
2019-03-05T04:11:56.534778: step 245, loss 0.162445, accuracy 0.926316, precision 0.8571428571428571, recall 0.972972972972973
2019-03-05T04:11:58.799369: step 246, loss 0.160361, accuracy 0.9375, precision 0.8292682926829268, recall 0.9714285714285714
2019-03-05T04:12:01.049877: step 247, loss 0.128403, accuracy 0.984375, precision 0.9743589743589743, recall 0.9743589743589743
2019-03-05T04:12:03.320768: step 248, loss 0.159534, accuracy 0.929688, precision 0.8936170212765957, recall 0.9130434782608695
2019-03-05T04:12:05.579537: step 249, loss 0.130801, accuracy 0.9375, precision 0.9444444444444444, recall 0.9107142857142857
2019-03-05T04:12:07.857195: step 250, loss 0.165429, accuracy 0.945312, precision 0.9130434782608695, recall 0.9333333333333333

Evaluation:
[[ 47  19]
 [ 59 109]]
2019-03-05T04:12:13.239494: step 250, loss 0.681571, accuracy 0.666667, precision 0.7121212121212122, recall 0.44339622641509435

Saved model checkpoint to /home/ubuntu/Project/runs/1551758559/checkpoints/model-250

2019-03-05T04:12:16.247507: step 251, loss 0.213196, accuracy 0.898438, precision 0.8333333333333334, recall 0.8888888888888888
2019-03-05T04:12:18.493036: step 252, loss 0.129305, accuracy 0.976562, precision 0.9487179487179487, recall 0.9736842105263158
2019-03-05T04:12:20.742861: step 253, loss 0.155865, accuracy 0.9375, precision 0.8888888888888888, recall 0.96
2019-03-05T04:12:22.992590: step 254, loss 0.13733, accuracy 0.945312, precision 0.9130434782608695, recall 0.9333333333333333
2019-03-05T04:12:25.238415: step 255, loss 0.16384, accuracy 0.9375, precision 0.875, recall 0.9545454545454546
2019-03-05T04:12:27.484581: step 256, loss 0.161561, accuracy 0.9375, precision 0.9433962264150944, recall 0.9090909090909091
2019-03-05T04:12:29.728698: step 257, loss 0.0983232, accuracy 0.960938, precision 0.9318181818181818, recall 0.9534883720930233
2019-03-05T04:12:31.991832: step 258, loss 0.183988, accuracy 0.898438, precision 0.9090909090909091, recall 0.8163265306122449
2019-03-05T04:12:34.249552: step 259, loss 0.136899, accuracy 0.96875, precision 0.96, recall 0.96
2019-03-05T04:12:36.501069: step 260, loss 0.172296, accuracy 0.914062, precision 0.8076923076923077, recall 0.9767441860465116
2019-03-05T04:12:38.787918: step 261, loss 0.173473, accuracy 0.929688, precision 0.9302325581395349, recall 0.8695652173913043
2019-03-05T04:12:41.019672: step 262, loss 0.163552, accuracy 0.9375, precision 0.9464285714285714, recall 0.9137931034482759
2019-03-05T04:12:43.283163: step 263, loss 0.117469, accuracy 0.96875, precision 1.0, recall 0.9215686274509803
2019-03-05T04:12:45.527119: step 264, loss 0.179932, accuracy 0.921875, precision 0.925, recall 0.8409090909090909
2019-03-05T04:12:47.776586: step 265, loss 0.175593, accuracy 0.921875, precision 0.84, recall 0.9545454545454546
2019-03-05T04:12:50.031327: step 266, loss 0.207324, accuracy 0.921875, precision 0.8048780487804879, recall 0.9428571428571428
2019-03-05T04:12:52.301738: step 267, loss 0.135004, accuracy 0.9375, precision 0.8888888888888888, recall 0.9302325581395349
2019-03-05T04:12:54.568836: step 268, loss 0.22096, accuracy 0.914062, precision 0.9056603773584906, recall 0.8888888888888888
2019-03-05T04:12:56.820502: step 269, loss 0.127984, accuracy 0.9375, precision 0.88, recall 0.9565217391304348
2019-03-05T04:12:59.075382: step 270, loss 0.144655, accuracy 0.929688, precision 0.8461538461538461, recall 0.9166666666666666
2019-03-05T04:13:01.325879: step 271, loss 0.216983, accuracy 0.9375, precision 0.9245283018867925, recall 0.9245283018867925
2019-03-05T04:13:03.565618: step 272, loss 0.184203, accuracy 0.90625, precision 0.9649122807017544, recall 0.8461538461538461
2019-03-05T04:13:05.822880: step 273, loss 0.164813, accuracy 0.945312, precision 0.9534883720930233, recall 0.8913043478260869
2019-03-05T04:13:08.078335: step 274, loss 0.116677, accuracy 0.953125, precision 0.9090909090909091, recall 0.9523809523809523
2019-03-05T04:13:10.329694: step 275, loss 0.135744, accuracy 0.96875, precision 0.9534883720930233, recall 0.9534883720930233
2019-03-05T04:13:12.588913: step 276, loss 0.201085, accuracy 0.914062, precision 0.7959183673469388, recall 0.975
2019-03-05T04:13:14.841720: step 277, loss 0.130602, accuracy 0.929688, precision 0.8333333333333334, recall 0.9459459459459459
2019-03-05T04:13:17.083493: step 278, loss 0.140338, accuracy 0.960938, precision 0.9473684210526315, recall 0.9230769230769231
2019-03-05T04:13:19.321615: step 279, loss 0.15375, accuracy 0.953125, precision 0.9230769230769231, recall 0.96
2019-03-05T04:13:21.486033: step 280, loss 0.0691853, accuracy 0.978947, precision 0.972972972972973, recall 0.972972972972973
2019-03-05T04:13:23.732753: step 281, loss 0.113342, accuracy 0.96875, precision 0.9361702127659575, recall 0.9777777777777777
2019-03-05T04:13:26.003594: step 282, loss 0.134273, accuracy 0.960938, precision 0.975609756097561, recall 0.9090909090909091
2019-03-05T04:13:28.242078: step 283, loss 0.12021, accuracy 0.945312, precision 0.9642857142857143, recall 0.9152542372881356
2019-03-05T04:13:30.510360: step 284, loss 0.137942, accuracy 0.953125, precision 0.9583333333333334, recall 0.92
2019-03-05T04:13:32.756105: step 285, loss 0.120873, accuracy 0.945312, precision 0.9318181818181818, recall 0.9111111111111111
2019-03-05T04:13:35.003816: step 286, loss 0.185956, accuracy 0.9375, precision 0.8870967741935484, recall 0.9821428571428571
2019-03-05T04:13:37.247477: step 287, loss 0.141907, accuracy 0.945312, precision 0.9152542372881356, recall 0.9642857142857143
2019-03-05T04:13:39.491256: step 288, loss 0.178463, accuracy 0.921875, precision 0.9024390243902439, recall 0.8604651162790697
2019-03-05T04:13:41.737654: step 289, loss 0.138738, accuracy 0.9375, precision 0.8947368421052632, recall 0.8947368421052632
2019-03-05T04:13:43.970479: step 290, loss 0.125568, accuracy 0.953125, precision 0.875, recall 0.9722222222222222
2019-03-05T04:13:46.210737: step 291, loss 0.136856, accuracy 0.929688, precision 0.7837837837837838, recall 0.9666666666666667
2019-03-05T04:13:48.463155: step 292, loss 0.0909955, accuracy 0.96875, precision 1.0, recall 0.92
2019-03-05T04:13:50.712233: step 293, loss 0.0974444, accuracy 0.96875, precision 0.9622641509433962, recall 0.9622641509433962
2019-03-05T04:13:52.957213: step 294, loss 0.16739, accuracy 0.929688, precision 0.9166666666666666, recall 0.8461538461538461
2019-03-05T04:13:55.207685: step 295, loss 0.166549, accuracy 0.921875, precision 0.8478260869565217, recall 0.9285714285714286
2019-03-05T04:13:57.457750: step 296, loss 0.110524, accuracy 0.953125, precision 0.88, recall 1.0
2019-03-05T04:13:59.729157: step 297, loss 0.163177, accuracy 0.945312, precision 0.8823529411764706, recall 0.9782608695652174
2019-03-05T04:14:01.983609: step 298, loss 0.122226, accuracy 0.96875, precision 1.0, recall 0.9259259259259259
2019-03-05T04:14:04.229035: step 299, loss 0.157205, accuracy 0.945312, precision 1.0, recall 0.8571428571428571
2019-03-05T04:14:06.477311: step 300, loss 0.183896, accuracy 0.9375, precision 0.9534883720930233, recall 0.8723404255319149
2019-03-05T04:14:08.755038: step 301, loss 0.172482, accuracy 0.914062, precision 0.8490566037735849, recall 0.9375
2019-03-05T04:14:11.007835: step 302, loss 0.185547, accuracy 0.921875, precision 0.7674418604651163, recall 1.0
2019-03-05T04:14:13.269280: step 303, loss 0.265346, accuracy 0.882812, precision 0.7592592592592593, recall 0.9534883720930233
2019-03-05T04:14:15.544910: step 304, loss 0.144904, accuracy 0.945312, precision 0.9038461538461539, recall 0.9591836734693877
2019-03-05T04:14:17.801557: step 305, loss 0.131928, accuracy 0.96875, precision 0.9782608695652174, recall 0.9375
2019-03-05T04:14:20.066428: step 306, loss 0.240391, accuracy 0.914062, precision 0.9777777777777777, recall 0.8148148148148148
2019-03-05T04:14:22.318034: step 307, loss 0.139943, accuracy 0.953125, precision 1.0, recall 0.8666666666666667
2019-03-05T04:14:24.584227: step 308, loss 0.0801485, accuracy 0.976562, precision 0.9555555555555556, recall 0.9772727272727273
2019-03-05T04:14:26.835325: step 309, loss 0.312663, accuracy 0.867188, precision 0.6730769230769231, recall 1.0
2019-03-05T04:14:29.095673: step 310, loss 0.13992, accuracy 0.953125, precision 0.9130434782608695, recall 0.9545454545454546
2019-03-05T04:14:31.367986: step 311, loss 0.223133, accuracy 0.945312, precision 0.9, recall 0.9230769230769231
2019-03-05T04:14:33.637207: step 312, loss 0.0847442, accuracy 0.984375, precision 0.9795918367346939, recall 0.9795918367346939
2019-03-05T04:14:35.886155: step 313, loss 0.151828, accuracy 0.9375, precision 0.9444444444444444, recall 0.85
2019-03-05T04:14:38.138389: step 314, loss 0.0835505, accuracy 0.984375, precision 0.9814814814814815, recall 0.9814814814814815
2019-03-05T04:14:40.297015: step 315, loss 0.111923, accuracy 0.968421, precision 0.9534883720930233, recall 0.9761904761904762
2019-03-05T04:14:42.568740: step 316, loss 0.122715, accuracy 0.953125, precision 0.9111111111111111, recall 0.9534883720930233
2019-03-05T04:14:44.823264: step 317, loss 0.184548, accuracy 0.953125, precision 0.9795918367346939, recall 0.9056603773584906
2019-03-05T04:14:47.090743: step 318, loss 0.120011, accuracy 0.953125, precision 0.9591836734693877, recall 0.9215686274509803
2019-03-05T04:14:49.360833: step 319, loss 0.149445, accuracy 0.945312, precision 0.8717948717948718, recall 0.9444444444444444
2019-03-05T04:14:51.624156: step 320, loss 0.146029, accuracy 0.9375, precision 0.8541666666666666, recall 0.9761904761904762
2019-03-05T04:14:53.876318: step 321, loss 0.115646, accuracy 0.960938, precision 0.9130434782608695, recall 0.9767441860465116
2019-03-05T04:14:56.124829: step 322, loss 0.111107, accuracy 0.945312, precision 0.9565217391304348, recall 0.8979591836734694
2019-03-05T04:14:58.390459: step 323, loss 0.0964466, accuracy 0.96875, precision 0.96, recall 0.96
2019-03-05T04:15:00.650585: step 324, loss 0.0914241, accuracy 0.96875, precision 1.0, recall 0.9166666666666666
2019-03-05T04:15:02.891621: step 325, loss 0.214257, accuracy 0.890625, precision 0.9347826086956522, recall 0.7962962962962963
2019-03-05T04:15:05.146525: step 326, loss 0.124764, accuracy 0.9375, precision 0.8666666666666667, recall 0.9512195121951219
2019-03-05T04:15:07.395227: step 327, loss 0.188521, accuracy 0.929688, precision 0.8392857142857143, recall 1.0
2019-03-05T04:15:09.656748: step 328, loss 0.071905, accuracy 0.984375, precision 0.9803921568627451, recall 0.9803921568627451
2019-03-05T04:15:11.916760: step 329, loss 0.112604, accuracy 0.945312, precision 0.92, recall 0.9387755102040817
2019-03-05T04:15:14.166938: step 330, loss 0.0961482, accuracy 0.984375, precision 0.9607843137254902, recall 1.0
2019-03-05T04:15:16.411793: step 331, loss 0.183008, accuracy 0.9375, precision 0.9361702127659575, recall 0.8979591836734694
2019-03-05T04:15:18.670703: step 332, loss 0.096494, accuracy 0.96875, precision 0.9534883720930233, recall 0.9534883720930233
2019-03-05T04:15:20.923743: step 333, loss 0.128052, accuracy 0.96875, precision 0.9574468085106383, recall 0.9574468085106383
2019-03-05T04:15:23.180892: step 334, loss 0.125513, accuracy 0.953125, precision 0.9591836734693877, recall 0.9215686274509803
2019-03-05T04:15:25.430591: step 335, loss 0.103596, accuracy 0.976562, precision 0.96, recall 0.9795918367346939
2019-03-05T04:15:27.707040: step 336, loss 0.143678, accuracy 0.9375, precision 0.8863636363636364, recall 0.9285714285714286
2019-03-05T04:15:29.970409: step 337, loss 0.117207, accuracy 0.953125, precision 0.926829268292683, recall 0.926829268292683
2019-03-05T04:15:32.219321: step 338, loss 0.166138, accuracy 0.921875, precision 0.8085106382978723, recall 0.9743589743589743
2019-03-05T04:15:34.459964: step 339, loss 0.134354, accuracy 0.945312, precision 0.9137931034482759, recall 0.9636363636363636
2019-03-05T04:15:36.717006: step 340, loss 0.150356, accuracy 0.945312, precision 0.9767441860465116, recall 0.875
2019-03-05T04:15:38.981668: step 341, loss 0.0970128, accuracy 0.976562, precision 1.0, recall 0.9411764705882353
2019-03-05T04:15:41.235127: step 342, loss 0.122786, accuracy 0.953125, precision 0.9047619047619048, recall 0.95
2019-03-05T04:15:43.496416: step 343, loss 0.117888, accuracy 0.960938, precision 0.9111111111111111, recall 0.9761904761904762
2019-03-05T04:15:45.744836: step 344, loss 0.146645, accuracy 0.9375, precision 0.8604651162790697, recall 0.9487179487179487
2019-03-05T04:15:47.995420: step 345, loss 0.0803687, accuracy 0.976562, precision 0.9333333333333333, recall 1.0
2019-03-05T04:15:50.249012: step 346, loss 0.155573, accuracy 0.921875, precision 0.875, recall 0.875
2019-03-05T04:15:52.510776: step 347, loss 0.0881514, accuracy 0.945312, precision 0.8909090909090909, recall 0.98
2019-03-05T04:15:54.788342: step 348, loss 0.121435, accuracy 0.953125, precision 0.9795918367346939, recall 0.9056603773584906
2019-03-05T04:15:57.047240: step 349, loss 0.145665, accuracy 0.929688, precision 0.9111111111111111, recall 0.8913043478260869
2019-03-05T04:15:59.190737: step 350, loss 0.120152, accuracy 0.978947, precision 0.967741935483871, recall 0.967741935483871
2019-03-05T04:16:01.440796: step 351, loss 0.0865303, accuracy 0.96875, precision 0.95, recall 0.95
2019-03-05T04:16:03.675366: step 352, loss 0.181782, accuracy 0.914062, precision 0.8, recall 0.975609756097561
2019-03-05T04:16:05.940379: step 353, loss 0.0997768, accuracy 0.953125, precision 0.8837209302325582, recall 0.9743589743589743
2019-03-05T04:16:08.173462: step 354, loss 0.0824329, accuracy 0.953125, precision 0.9230769230769231, recall 0.96
2019-03-05T04:16:10.415120: step 355, loss 0.113811, accuracy 0.953125, precision 0.9534883720930233, recall 0.9111111111111111
2019-03-05T04:16:12.671295: step 356, loss 0.115086, accuracy 0.953125, precision 0.9761904761904762, recall 0.8913043478260869
2019-03-05T04:16:14.931617: step 357, loss 0.0653937, accuracy 0.984375, precision 1.0, recall 0.9565217391304348
2019-03-05T04:16:17.194450: step 358, loss 0.141832, accuracy 0.9375, precision 0.8333333333333334, recall 1.0
2019-03-05T04:16:19.457166: step 359, loss 0.09705, accuracy 0.953125, precision 0.8888888888888888, recall 1.0
2019-03-05T04:16:21.713689: step 360, loss 0.120856, accuracy 0.960938, precision 0.9696969696969697, recall 0.8888888888888888
2019-03-05T04:16:23.985956: step 361, loss 0.115343, accuracy 0.96875, precision 0.9347826086956522, recall 0.9772727272727273
2019-03-05T04:16:26.235682: step 362, loss 0.122175, accuracy 0.929688, precision 0.8222222222222222, recall 0.9736842105263158
2019-03-05T04:16:28.503361: step 363, loss 0.110763, accuracy 0.960938, precision 0.92, recall 0.9787234042553191
2019-03-05T04:16:30.762157: step 364, loss 0.0771763, accuracy 0.96875, precision 0.9782608695652174, recall 0.9375
2019-03-05T04:16:33.010763: step 365, loss 0.102292, accuracy 0.976562, precision 0.9615384615384616, recall 0.9803921568627451
2019-03-05T04:16:35.264351: step 366, loss 0.112335, accuracy 0.9375, precision 1.0, recall 0.8545454545454545
2019-03-05T04:16:37.540770: step 367, loss 0.0852143, accuracy 0.976562, precision 0.9736842105263158, recall 0.9487179487179487
2019-03-05T04:16:39.804983: step 368, loss 0.163, accuracy 0.9375, precision 0.8478260869565217, recall 0.975
2019-03-05T04:16:42.048697: step 369, loss 0.180003, accuracy 0.898438, precision 0.7755102040816326, recall 0.95
2019-03-05T04:16:44.298714: step 370, loss 0.0978507, accuracy 0.976562, precision 0.94, recall 1.0
2019-03-05T04:16:46.538385: step 371, loss 0.0995817, accuracy 0.96875, precision 0.9743589743589743, recall 0.926829268292683
2019-03-05T04:16:48.789665: step 372, loss 0.0936088, accuracy 0.976562, precision 0.975609756097561, recall 0.9523809523809523
2019-03-05T04:16:51.048750: step 373, loss 0.124217, accuracy 0.953125, precision 0.9803921568627451, recall 0.9090909090909091
2019-03-05T04:16:53.310540: step 374, loss 0.0822571, accuracy 0.960938, precision 0.9361702127659575, recall 0.9565217391304348
2019-03-05T04:16:55.551924: step 375, loss 0.103746, accuracy 0.960938, precision 0.9464285714285714, recall 0.9636363636363636
2019-03-05T04:16:57.795869: step 376, loss 0.086013, accuracy 0.976562, precision 0.9444444444444444, recall 1.0
2019-03-05T04:17:00.044274: step 377, loss 0.103727, accuracy 0.976562, precision 0.9558823529411765, recall 1.0
2019-03-05T04:17:02.311393: step 378, loss 0.146086, accuracy 0.945312, precision 0.9245283018867925, recall 0.9423076923076923
2019-03-05T04:17:04.562056: step 379, loss 0.197609, accuracy 0.914062, precision 0.9215686274509803, recall 0.8703703703703703
2019-03-05T04:17:06.823648: step 380, loss 0.0795168, accuracy 0.984375, precision 1.0, recall 0.9545454545454546
2019-03-05T04:17:09.068143: step 381, loss 0.179734, accuracy 0.914062, precision 1.0, recall 0.7843137254901961
2019-03-05T04:17:11.299010: step 382, loss 0.192292, accuracy 0.9375, precision 0.9565217391304348, recall 0.88
2019-03-05T04:17:13.546890: step 383, loss 0.161213, accuracy 0.914062, precision 0.7777777777777778, recall 0.9722222222222222
2019-03-05T04:17:15.808588: step 384, loss 0.140304, accuracy 0.929688, precision 0.7692307692307693, recall 1.0
2019-03-05T04:17:17.966032: step 385, loss 0.248942, accuracy 0.905263, precision 0.7567567567567568, recall 1.0
2019-03-05T04:17:20.197325: step 386, loss 0.137552, accuracy 0.953125, precision 0.8775510204081632, recall 1.0
2019-03-05T04:17:22.455283: step 387, loss 0.133291, accuracy 0.96875, precision 1.0, recall 0.9148936170212766
2019-03-05T04:17:24.709850: step 388, loss 0.17173, accuracy 0.929688, precision 1.0, recall 0.8615384615384616
2019-03-05T04:17:26.963742: step 389, loss 0.149291, accuracy 0.945312, precision 0.975, recall 0.8666666666666667
2019-03-05T04:17:29.216696: step 390, loss 0.0620435, accuracy 0.992188, precision 0.9818181818181818, recall 1.0
2019-03-05T04:17:31.455198: step 391, loss 0.0654836, accuracy 0.976562, precision 0.9318181818181818, recall 1.0
2019-03-05T04:17:33.690159: step 392, loss 0.107832, accuracy 0.960938, precision 0.9318181818181818, recall 0.9534883720930233
2019-03-05T04:17:35.949940: step 393, loss 0.13473, accuracy 0.945312, precision 0.8695652173913043, recall 0.975609756097561
2019-03-05T04:17:38.200695: step 394, loss 0.0758946, accuracy 0.984375, precision 0.9565217391304348, recall 1.0
2019-03-05T04:17:40.446784: step 395, loss 0.0829241, accuracy 0.96875, precision 0.9565217391304348, recall 0.9565217391304348
2019-03-05T04:17:42.695295: step 396, loss 0.130456, accuracy 0.945312, precision 0.9767441860465116, recall 0.875
2019-03-05T04:17:44.949076: step 397, loss 0.106184, accuracy 0.960938, precision 0.975, recall 0.9069767441860465
2019-03-05T04:17:47.197287: step 398, loss 0.14389, accuracy 0.976562, precision 0.9607843137254902, recall 0.98
2019-03-05T04:17:49.430031: step 399, loss 0.0750007, accuracy 0.976562, precision 0.9285714285714286, recall 1.0
2019-03-05T04:17:51.690949: step 400, loss 0.0914639, accuracy 0.960938, precision 0.9166666666666666, recall 0.9777777777777777
2019-03-05T04:17:53.939352: step 401, loss 0.058798, accuracy 0.984375, precision 0.9574468085106383, recall 1.0
2019-03-05T04:17:56.176768: step 402, loss 0.0808099, accuracy 0.976562, precision 0.95, recall 0.9743589743589743
2019-03-05T04:17:58.432344: step 403, loss 0.0961771, accuracy 0.960938, precision 0.9807692307692307, recall 0.9272727272727272
2019-03-05T04:18:00.679063: step 404, loss 0.107697, accuracy 0.9375, precision 0.8913043478260869, recall 0.9318181818181818
2019-03-05T04:18:02.924257: step 405, loss 0.0880084, accuracy 0.976562, precision 0.9285714285714286, recall 1.0
2019-03-05T04:18:05.170172: step 406, loss 0.0932196, accuracy 0.984375, precision 0.975609756097561, recall 0.975609756097561
2019-03-05T04:18:07.405873: step 407, loss 0.116111, accuracy 0.960938, precision 0.9375, recall 0.9574468085106383
2019-03-05T04:18:09.661934: step 408, loss 0.10452, accuracy 0.953125, precision 0.9183673469387755, recall 0.9574468085106383
2019-03-05T04:18:11.919696: step 409, loss 0.0887021, accuracy 0.96875, precision 0.9333333333333333, recall 0.9767441860465116
2019-03-05T04:18:14.160881: step 410, loss 0.0991523, accuracy 0.96875, precision 1.0, recall 0.92
2019-03-05T04:18:16.403619: step 411, loss 0.0698552, accuracy 0.976562, precision 0.9791666666666666, recall 0.9591836734693877
2019-03-05T04:18:18.663558: step 412, loss 0.0719116, accuracy 0.96875, precision 0.9411764705882353, recall 0.9795918367346939
2019-03-05T04:18:20.921530: step 413, loss 0.0839351, accuracy 0.984375, precision 0.9523809523809523, recall 1.0
2019-03-05T04:18:23.161813: step 414, loss 0.125958, accuracy 0.9375, precision 0.86, recall 0.9772727272727273
2019-03-05T04:18:25.408428: step 415, loss 0.0717272, accuracy 0.96875, precision 0.9454545454545454, recall 0.9811320754716981
2019-03-05T04:18:27.658717: step 416, loss 0.119396, accuracy 0.960938, precision 0.9230769230769231, recall 0.9795918367346939
2019-03-05T04:18:29.898309: step 417, loss 0.0747169, accuracy 0.976562, precision 0.9615384615384616, recall 0.9803921568627451
2019-03-05T04:18:32.151581: step 418, loss 0.101019, accuracy 0.953125, precision 0.9591836734693877, recall 0.9215686274509803
2019-03-05T04:18:34.382942: step 419, loss 0.109759, accuracy 0.984375, precision 1.0, recall 0.9574468085106383
2019-03-05T04:18:36.542264: step 420, loss 0.0743939, accuracy 0.968421, precision 0.9411764705882353, recall 0.9696969696969697
2019-03-05T04:18:38.792768: step 421, loss 0.0396914, accuracy 1, precision 1.0, recall 1.0
2019-03-05T04:18:41.046619: step 422, loss 0.127173, accuracy 0.953125, precision 0.88, recall 1.0
2019-03-05T04:18:43.317811: step 423, loss 0.0901836, accuracy 0.960938, precision 0.9183673469387755, recall 0.9782608695652174
2019-03-05T04:18:45.602283: step 424, loss 0.0529667, accuracy 0.984375, precision 0.9393939393939394, recall 1.0
2019-03-05T04:18:47.873807: step 425, loss 0.0841116, accuracy 0.960938, precision 0.96, recall 0.9411764705882353
2019-03-05T04:18:50.113383: step 426, loss 0.126033, accuracy 0.96875, precision 0.9318181818181818, recall 0.9761904761904762
2019-03-05T04:18:52.363719: step 427, loss 0.0864478, accuracy 0.976562, precision 0.9772727272727273, recall 0.9555555555555556
2019-03-05T04:18:54.613541: step 428, loss 0.0844953, accuracy 0.976562, precision 0.926829268292683, recall 1.0
2019-03-05T04:18:56.850834: step 429, loss 0.166468, accuracy 0.929688, precision 0.8846153846153846, recall 0.9387755102040817
2019-03-05T04:18:59.098802: step 430, loss 0.0553632, accuracy 0.984375, precision 0.9642857142857143, recall 1.0
2019-03-05T04:19:01.361355: step 431, loss 0.115191, accuracy 0.945312, precision 0.9166666666666666, recall 0.9361702127659575
2019-03-05T04:19:03.646911: step 432, loss 0.074443, accuracy 0.984375, precision 1.0, recall 0.9629629629629629
2019-03-05T04:19:05.888167: step 433, loss 0.0843197, accuracy 0.976562, precision 0.9761904761904762, recall 0.9534883720930233
2019-03-05T04:19:08.145360: step 434, loss 0.0671459, accuracy 0.984375, precision 0.9574468085106383, recall 1.0
2019-03-05T04:19:10.371404: step 435, loss 0.092287, accuracy 0.953125, precision 0.94, recall 0.94
2019-03-05T04:19:12.617739: step 436, loss 0.0714388, accuracy 0.984375, precision 1.0, recall 0.95
2019-03-05T04:19:14.861372: step 437, loss 0.0901033, accuracy 0.953125, precision 0.9090909090909091, recall 0.9523809523809523
2019-03-05T04:19:17.133460: step 438, loss 0.131493, accuracy 0.929688, precision 0.8461538461538461, recall 0.9777777777777777
2019-03-05T04:19:19.393965: step 439, loss 0.0473181, accuracy 1, precision 1.0, recall 1.0
2019-03-05T04:19:21.654263: step 440, loss 0.054398, accuracy 0.992188, precision 0.9807692307692307, recall 1.0
2019-03-05T04:19:23.899183: step 441, loss 0.0756516, accuracy 0.984375, precision 1.0, recall 0.9555555555555556
2019-03-05T04:19:26.145616: step 442, loss 0.121023, accuracy 0.976562, precision 0.9772727272727273, recall 0.9555555555555556
2019-03-05T04:19:28.389853: step 443, loss 0.0960324, accuracy 0.976562, precision 1.0, recall 0.9411764705882353
2019-03-05T04:19:30.642468: step 444, loss 0.0814547, accuracy 0.976562, precision 0.9574468085106383, recall 0.9782608695652174
2019-03-05T04:19:32.898963: step 445, loss 0.101166, accuracy 0.953125, precision 0.8837209302325582, recall 0.9743589743589743
2019-03-05T04:19:35.158274: step 446, loss 0.0609791, accuracy 0.984375, precision 0.9545454545454546, recall 1.0
2019-03-05T04:19:37.411419: step 447, loss 0.101865, accuracy 0.96875, precision 0.9795918367346939, recall 0.9411764705882353
2019-03-05T04:19:39.659342: step 448, loss 0.0889134, accuracy 0.992188, precision 1.0, recall 0.9772727272727273
2019-03-05T04:19:41.905202: step 449, loss 0.0406164, accuracy 0.992188, precision 0.9811320754716981, recall 1.0
2019-03-05T04:19:44.139871: step 450, loss 0.0838217, accuracy 0.96875, precision 0.9591836734693877, recall 0.9591836734693877
2019-03-05T04:19:46.389855: step 451, loss 0.082323, accuracy 0.953125, precision 0.9387755102040817, recall 0.9387755102040817
2019-03-05T04:19:48.643376: step 452, loss 0.0738751, accuracy 0.984375, precision 0.967741935483871, recall 0.967741935483871
2019-03-05T04:19:50.884795: step 453, loss 0.067894, accuracy 0.960938, precision 0.94, recall 0.9591836734693877
2019-03-05T04:19:53.126907: step 454, loss 0.120414, accuracy 0.960938, precision 0.9215686274509803, recall 0.9791666666666666
2019-03-05T04:19:55.294924: step 455, loss 0.0411525, accuracy 0.989474, precision 0.9714285714285714, recall 1.0
2019-03-05T04:19:57.540812: step 456, loss 0.0630434, accuracy 0.976562, precision 0.9761904761904762, recall 0.9534883720930233
2019-03-05T04:19:59.789240: step 457, loss 0.118599, accuracy 0.960938, precision 0.98, recall 0.9245283018867925
2019-03-05T04:20:02.030944: step 458, loss 0.059823, accuracy 0.976562, precision 0.9347826086956522, recall 1.0
2019-03-05T04:20:04.280014: step 459, loss 0.0804691, accuracy 0.960938, precision 0.9347826086956522, recall 0.9555555555555556
2019-03-05T04:20:06.520724: step 460, loss 0.0519694, accuracy 0.976562, precision 0.9787234042553191, recall 0.9583333333333334
2019-03-05T04:20:08.775493: step 461, loss 0.063537, accuracy 0.976562, precision 1.0, recall 0.9333333333333333
2019-03-05T04:20:11.020558: step 462, loss 0.0986813, accuracy 0.96875, precision 0.9545454545454546, recall 0.9545454545454546
2019-03-05T04:20:13.262336: step 463, loss 0.0846169, accuracy 0.976562, precision 0.9444444444444444, recall 1.0
2019-03-05T04:20:15.500353: step 464, loss 0.101293, accuracy 0.953125, precision 0.9318181818181818, recall 0.9318181818181818
2019-03-05T04:20:17.753383: step 465, loss 0.101536, accuracy 0.96875, precision 0.975, recall 0.9285714285714286
2019-03-05T04:20:20.001613: step 466, loss 0.0747031, accuracy 0.976562, precision 0.9574468085106383, recall 0.9782608695652174
2019-03-05T04:20:22.243350: step 467, loss 0.0617166, accuracy 0.976562, precision 0.9411764705882353, recall 1.0
2019-03-05T04:20:24.502914: step 468, loss 0.107615, accuracy 0.976562, precision 1.0, recall 0.9454545454545454
2019-03-05T04:20:26.745782: step 469, loss 0.0286373, accuracy 0.992188, precision 0.9714285714285714, recall 1.0
2019-03-05T04:20:28.994443: step 470, loss 0.0652333, accuracy 0.984375, precision 0.9827586206896551, recall 0.9827586206896551
2019-03-05T04:20:31.235918: step 471, loss 0.0833461, accuracy 0.96875, precision 1.0, recall 0.9166666666666666
2019-03-05T04:20:33.481696: step 472, loss 0.083559, accuracy 0.96875, precision 0.9534883720930233, recall 0.9534883720930233
2019-03-05T04:20:35.755919: step 473, loss 0.0994578, accuracy 0.96875, precision 0.9361702127659575, recall 0.9777777777777777
2019-03-05T04:20:38.000482: step 474, loss 0.0825339, accuracy 0.96875, precision 0.9591836734693877, recall 0.9591836734693877
2019-03-05T04:20:40.258471: step 475, loss 0.0501345, accuracy 0.984375, precision 0.9523809523809523, recall 1.0
2019-03-05T04:20:42.506882: step 476, loss 0.104556, accuracy 0.96875, precision 0.9347826086956522, recall 0.9772727272727273
2019-03-05T04:20:44.758135: step 477, loss 0.0670625, accuracy 0.976562, precision 0.9565217391304348, recall 0.9777777777777777
2019-03-05T04:20:47.001877: step 478, loss 0.0620672, accuracy 0.96875, precision 0.9523809523809523, recall 0.9523809523809523
2019-03-05T04:20:49.246664: step 479, loss 0.0753103, accuracy 0.96875, precision 0.9512195121951219, recall 0.9512195121951219
2019-03-05T04:20:51.503294: step 480, loss 0.0910469, accuracy 0.976562, precision 0.9629629629629629, recall 0.9811320754716981
2019-03-05T04:20:53.743223: step 481, loss 0.061631, accuracy 0.976562, precision 0.9791666666666666, recall 0.9591836734693877
2019-03-05T04:20:55.988412: step 482, loss 0.0916041, accuracy 0.976562, precision 1.0, recall 0.9423076923076923
2019-03-05T04:20:58.238851: step 483, loss 0.0687038, accuracy 0.976562, precision 0.9545454545454546, recall 0.9767441860465116
2019-03-05T04:21:00.501210: step 484, loss 0.0826087, accuracy 0.976562, precision 0.9347826086956522, recall 1.0
2019-03-05T04:21:02.746299: step 485, loss 0.113129, accuracy 0.945312, precision 0.8571428571428571, recall 1.0
2019-03-05T04:21:04.997039: step 486, loss 0.0690697, accuracy 0.992188, precision 1.0, recall 0.9811320754716981
2019-03-05T04:21:07.258153: step 487, loss 0.079797, accuracy 0.984375, precision 0.9827586206896551, recall 0.9827586206896551
2019-03-05T04:21:09.506640: step 488, loss 0.056833, accuracy 0.992188, precision 1.0, recall 0.9814814814814815
2019-03-05T04:21:11.753155: step 489, loss 0.0999097, accuracy 0.96875, precision 1.0, recall 0.9215686274509803
2019-03-05T04:21:13.901096: step 490, loss 0.106906, accuracy 0.947368, precision 0.9655172413793104, recall 0.875
2019-03-05T04:21:16.160285: step 491, loss 0.053555, accuracy 0.992188, precision 0.9787234042553191, recall 1.0
2019-03-05T04:21:18.422497: step 492, loss 0.107961, accuracy 0.945312, precision 0.8409090909090909, recall 1.0
2019-03-05T04:21:20.657172: step 493, loss 0.105791, accuracy 0.945312, precision 0.8333333333333334, recall 1.0
2019-03-05T04:21:22.902239: step 494, loss 0.116649, accuracy 0.945312, precision 0.9019607843137255, recall 0.9583333333333334
2019-03-05T04:21:25.173352: step 495, loss 0.0451778, accuracy 0.992188, precision 0.98, recall 1.0
2019-03-05T04:21:27.420234: step 496, loss 0.153334, accuracy 0.929688, precision 1.0, recall 0.847457627118644
2019-03-05T04:21:29.677386: step 497, loss 0.149745, accuracy 0.945312, precision 1.0, recall 0.8833333333333333
2019-03-05T04:21:31.932349: step 498, loss 0.102668, accuracy 0.96875, precision 0.9787234042553191, recall 0.9387755102040817
2019-03-05T04:21:34.176978: step 499, loss 0.0847891, accuracy 0.953125, precision 0.8636363636363636, recall 1.0
2019-03-05T04:21:36.427243: step 500, loss 0.113715, accuracy 0.921875, precision 0.7674418604651163, recall 1.0

Evaluation:
[[ 29  37]
 [ 25 143]]
2019-03-05T04:21:40.129531: step 500, loss 0.606286, accuracy 0.735043, precision 0.4393939393939394, recall 0.5370370370370371

Saved model checkpoint to /home/ubuntu/Project/runs/1551758559/checkpoints/model-500

