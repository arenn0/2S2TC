"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
GloVe model loaded
Pretrained Embedding: GloVe
Italian: True
Loading data...
4681
Max Document length: 81
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 1
Train/Dev split: 4213/468
2019-03-03 14:33:21.867037: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:475: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551645203

2019-03-03T14:33:24.616971: step 1, loss 0.836984, accuracy 0.578125, precision 0.379746835443038, recall 0.33707865168539325
2019-03-03T14:33:25.350017: step 2, loss 0.809437, accuracy 0.578125, precision 0.3125, recall 0.4166666666666667
2019-03-03T14:33:26.037244: step 3, loss 0.783684, accuracy 0.644531, precision 0.3373493975903614, recall 0.4375
2019-03-03T14:33:26.751356: step 4, loss 0.83308, accuracy 0.589844, precision 0.2647058823529412, recall 0.47368421052631576
2019-03-03T14:33:27.462960: step 5, loss 0.736129, accuracy 0.660156, precision 0.3106796116504854, recall 0.6666666666666666
2019-03-03T14:33:28.164595: step 6, loss 0.702135, accuracy 0.6875, precision 0.28735632183908044, recall 0.5813953488372093
2019-03-03T14:33:29.304080: step 7, loss 0.694646, accuracy 0.695312, precision 0.3, recall 0.6428571428571429
2019-03-03T14:33:32.331821: step 8, loss 0.612131, accuracy 0.75, precision 0.4, recall 0.723404255319149
2019-03-03T14:33:35.427241: step 9, loss 0.669352, accuracy 0.695312, precision 0.41237113402061853, recall 0.6557377049180327
2019-03-03T14:33:38.472892: step 10, loss 0.620753, accuracy 0.683594, precision 0.3673469387755102, recall 0.6545454545454545
2019-03-03T14:33:41.381648: step 11, loss 0.617259, accuracy 0.714844, precision 0.40425531914893614, recall 0.6909090909090909
2019-03-03T14:33:44.443970: step 12, loss 0.617994, accuracy 0.71875, precision 0.31645569620253167, recall 0.5813953488372093
2019-03-03T14:33:47.454198: step 13, loss 0.661019, accuracy 0.699219, precision 0.3684210526315789, recall 0.6730769230769231
2019-03-03T14:33:50.684592: step 14, loss 0.617368, accuracy 0.691406, precision 0.31868131868131866, recall 0.6304347826086957
2019-03-03T14:33:53.727671: step 15, loss 0.6275, accuracy 0.691406, precision 0.3, recall 0.627906976744186
2019-03-03T14:33:56.724832: step 16, loss 0.671656, accuracy 0.65625, precision 0.25806451612903225, recall 0.5581395348837209
2019-03-03T14:33:58.273219: step 17, loss 0.711121, accuracy 0.649573, precision 0.23404255319148937, recall 0.6875
2019-03-03T14:34:01.758291: step 18, loss 0.565862, accuracy 0.746094, precision 0.43434343434343436, recall 0.8269230769230769
2019-03-03T14:34:04.938788: step 19, loss 0.482038, accuracy 0.777344, precision 0.5172413793103449, recall 0.75
2019-03-03T14:34:07.112644: step 20, loss 0.517926, accuracy 0.722656, precision 0.391304347826087, recall 0.7058823529411765
2019-03-03T14:34:08.518624: step 21, loss 0.502626, accuracy 0.742188, precision 0.4431818181818182, recall 0.6964285714285714
2019-03-03T14:34:09.663562: step 22, loss 0.508239, accuracy 0.789062, precision 0.5529411764705883, recall 0.746031746031746
2019-03-03T14:34:10.742682: step 23, loss 0.487164, accuracy 0.785156, precision 0.5346534653465347, recall 0.8709677419354839
2019-03-03T14:34:12.095062: step 24, loss 0.425409, accuracy 0.816406, precision 0.6421052631578947, recall 0.8243243243243243
2019-03-03T14:34:13.565164: step 25, loss 0.450871, accuracy 0.796875, precision 0.5760869565217391, recall 0.803030303030303
2019-03-03T14:34:14.769359: step 26, loss 0.485283, accuracy 0.753906, precision 0.5360824742268041, recall 0.7428571428571429
2019-03-03T14:34:15.811900: step 27, loss 0.38638, accuracy 0.8125, precision 0.5921052631578947, recall 0.7258064516129032
2019-03-03T14:34:16.986271: step 28, loss 0.485032, accuracy 0.769531, precision 0.504950495049505, recall 0.85
2019-03-03T14:34:18.188058: step 29, loss 0.417983, accuracy 0.800781, precision 0.5764705882352941, recall 0.765625
2019-03-03T14:34:19.409790: step 30, loss 0.481045, accuracy 0.792969, precision 0.5555555555555556, recall 0.7936507936507936
2019-03-03T14:34:20.941800: step 31, loss 0.411959, accuracy 0.789062, precision 0.5555555555555556, recall 0.78125
2019-03-03T14:34:22.007302: step 32, loss 0.451212, accuracy 0.800781, precision 0.6170212765957447, recall 0.7945205479452054
2019-03-03T14:34:23.218260: step 33, loss 0.469336, accuracy 0.816406, precision 0.66, recall 0.8354430379746836
2019-03-03T14:34:23.801700: step 34, loss 0.419439, accuracy 0.820513, precision 0.5405405405405406, recall 0.8333333333333334
2019-03-03T14:34:24.818980: step 35, loss 0.385027, accuracy 0.84375, precision 0.717391304347826, recall 0.825
2019-03-03T14:34:25.869192: step 36, loss 0.347641, accuracy 0.875, precision 0.7674418604651163, recall 0.8461538461538461
2019-03-03T14:34:26.911415: step 37, loss 0.388308, accuracy 0.871094, precision 0.79, recall 0.8681318681318682
2019-03-03T14:34:28.009003: step 38, loss 0.312793, accuracy 0.886719, precision 0.8089887640449438, recall 0.8571428571428571
2019-03-03T14:34:29.032081: step 39, loss 0.336709, accuracy 0.878906, precision 0.8181818181818182, recall 0.8275862068965517
2019-03-03T14:34:30.012492: step 40, loss 0.431566, accuracy 0.84375, precision 0.7350427350427351, recall 0.9052631578947369
2019-03-03T14:34:31.017806: step 41, loss 0.36933, accuracy 0.8125, precision 0.7444444444444445, recall 0.7282608695652174
2019-03-03T14:34:32.077970: step 42, loss 0.323979, accuracy 0.859375, precision 0.7676767676767676, recall 0.8539325842696629
2019-03-03T14:34:33.099246: step 43, loss 0.345676, accuracy 0.859375, precision 0.8064516129032258, recall 0.8064516129032258
2019-03-03T14:34:34.135476: step 44, loss 0.32811, accuracy 0.867188, precision 0.7654320987654321, recall 0.8051948051948052
2019-03-03T14:34:34.988193: step 45, loss 0.327162, accuracy 0.859375, precision 0.8210526315789474, recall 0.8041237113402062
2019-03-03T14:34:35.863852: step 46, loss 0.29274, accuracy 0.875, precision 0.8690476190476191, recall 0.776595744680851
2019-03-03T14:34:36.697622: step 47, loss 0.302153, accuracy 0.890625, precision 0.7934782608695652, recall 0.8902439024390244
2019-03-03T14:34:37.546354: step 48, loss 0.321119, accuracy 0.871094, precision 0.7386363636363636, recall 0.8666666666666667
2019-03-03T14:34:38.367176: step 49, loss 0.339911, accuracy 0.855469, precision 0.7289719626168224, recall 0.9069767441860465
2019-03-03T14:34:39.184267: step 50, loss 0.252113, accuracy 0.902344, precision 0.7948717948717948, recall 0.8732394366197183
2019-03-03T14:34:39.593174: step 51, loss 0.244497, accuracy 0.923077, precision 0.8333333333333334, recall 0.8620689655172413
2019-03-03T14:34:40.416970: step 52, loss 0.227758, accuracy 0.910156, precision 0.8613861386138614, recall 0.90625
2019-03-03T14:34:41.251743: step 53, loss 0.259925, accuracy 0.882812, precision 0.7835051546391752, recall 0.8941176470588236
2019-03-03T14:34:42.208252: step 54, loss 0.270209, accuracy 0.910156, precision 0.8470588235294118, recall 0.8780487804878049
2019-03-03T14:34:43.121342: step 55, loss 0.251621, accuracy 0.902344, precision 0.8055555555555556, recall 0.9560439560439561
2019-03-03T14:34:44.131641: step 56, loss 0.246915, accuracy 0.894531, precision 0.7625, recall 0.8840579710144928
2019-03-03T14:34:44.963945: step 57, loss 0.270128, accuracy 0.902344, precision 0.8117647058823529, recall 0.8846153846153846
2019-03-03T14:34:45.971258: step 58, loss 0.257187, accuracy 0.890625, precision 0.8095238095238095, recall 0.85
2019-03-03T14:34:47.035406: step 59, loss 0.214954, accuracy 0.921875, precision 0.9223300970873787, recall 0.8878504672897196
2019-03-03T14:34:47.959935: step 60, loss 0.278126, accuracy 0.882812, precision 0.8705882352941177, recall 0.7956989247311828
2019-03-03T14:34:49.020691: step 61, loss 0.267995, accuracy 0.894531, precision 0.8636363636363636, recall 0.8878504672897196
2019-03-03T14:34:50.106787: step 62, loss 0.275254, accuracy 0.914062, precision 0.8390804597701149, recall 0.9012345679012346
2019-03-03T14:34:51.085239: step 63, loss 0.275246, accuracy 0.902344, precision 0.8488372093023255, recall 0.8588235294117647
2019-03-03T14:34:52.267020: step 64, loss 0.253599, accuracy 0.898438, precision 0.8666666666666667, recall 0.8478260869565217
2019-03-03T14:34:53.901682: step 65, loss 0.258164, accuracy 0.90625, precision 0.8041237113402062, recall 0.9397590361445783
2019-03-03T14:34:55.360788: step 66, loss 0.221522, accuracy 0.910156, precision 0.8352941176470589, recall 0.8875
2019-03-03T14:34:56.778999: step 67, loss 0.245498, accuracy 0.898438, precision 0.7934782608695652, recall 0.9125
2019-03-03T14:34:57.389358: step 68, loss 0.217673, accuracy 0.91453, precision 0.8823529411764706, recall 0.8333333333333334
2019-03-03T14:34:58.613086: step 69, loss 0.200307, accuracy 0.921875, precision 0.8631578947368421, recall 0.9213483146067416
2019-03-03T14:34:59.759550: step 70, loss 0.261311, accuracy 0.882812, precision 0.8241758241758241, recall 0.8426966292134831
2019-03-03T14:35:00.819948: step 71, loss 0.177825, accuracy 0.921875, precision 0.8241758241758241, recall 0.9493670886075949
2019-03-03T14:35:01.937471: step 72, loss 0.179134, accuracy 0.941406, precision 0.8850574712643678, recall 0.9390243902439024
2019-03-03T14:35:03.000957: step 73, loss 0.180055, accuracy 0.933594, precision 0.8372093023255814, recall 0.96
2019-03-03T14:35:04.077104: step 74, loss 0.180936, accuracy 0.921875, precision 0.8333333333333334, recall 0.9210526315789473
2019-03-03T14:35:05.105866: step 75, loss 0.185073, accuracy 0.917969, precision 0.8409090909090909, recall 0.9135802469135802
2019-03-03T14:35:06.169022: step 76, loss 0.186111, accuracy 0.929688, precision 0.8807339449541285, recall 0.9504950495049505
2019-03-03T14:35:07.176842: step 77, loss 0.223282, accuracy 0.914062, precision 0.8666666666666667, recall 0.8863636363636364
2019-03-03T14:35:08.332270: step 78, loss 0.206214, accuracy 0.910156, precision 0.8723404255319149, recall 0.8817204301075269
2019-03-03T14:35:09.345733: step 79, loss 0.198133, accuracy 0.925781, precision 0.8842105263157894, recall 0.9130434782608695
2019-03-03T14:35:10.281668: step 80, loss 0.191205, accuracy 0.921875, precision 0.9215686274509803, recall 0.8867924528301887
2019-03-03T14:35:11.287916: step 81, loss 0.196453, accuracy 0.917969, precision 0.9186046511627907, recall 0.8494623655913979
2019-03-03T14:35:12.296240: step 82, loss 0.249535, accuracy 0.90625, precision 0.9213483146067416, recall 0.8282828282828283
2019-03-03T14:35:13.273702: step 83, loss 0.212104, accuracy 0.910156, precision 0.8604651162790697, recall 0.8705882352941177
2019-03-03T14:35:14.204289: step 84, loss 0.182906, accuracy 0.921875, precision 0.8775510204081632, recall 0.9148936170212766
2019-03-03T14:35:14.636668: step 85, loss 0.231442, accuracy 0.931624, precision 0.8421052631578947, recall 0.9411764705882353
2019-03-03T14:35:15.533273: step 86, loss 0.192161, accuracy 0.929688, precision 0.8636363636363636, recall 0.926829268292683
2019-03-03T14:35:16.402467: step 87, loss 0.147613, accuracy 0.941406, precision 0.9390243902439024, recall 0.8850574712643678
2019-03-03T14:35:17.298073: step 88, loss 0.236611, accuracy 0.902344, precision 0.8235294117647058, recall 0.875
2019-03-03T14:35:18.160766: step 89, loss 0.19857, accuracy 0.917969, precision 0.8404255319148937, recall 0.9294117647058824
2019-03-03T14:35:19.046398: step 90, loss 0.135006, accuracy 0.949219, precision 0.9021739130434783, recall 0.9540229885057471
2019-03-03T14:35:19.911145: step 91, loss 0.180273, accuracy 0.933594, precision 0.8953488372093024, recall 0.9058823529411765
2019-03-03T14:35:20.758879: step 92, loss 0.163224, accuracy 0.941406, precision 0.9247311827956989, recall 0.9148936170212766
2019-03-03T14:35:21.630585: step 93, loss 0.222825, accuracy 0.910156, precision 0.845360824742268, recall 0.9111111111111111
2019-03-03T14:35:22.476150: step 94, loss 0.178947, accuracy 0.9375, precision 0.897196261682243, recall 0.9504950495049505
2019-03-03T14:35:23.319671: step 95, loss 0.149452, accuracy 0.945312, precision 0.9239130434782609, recall 0.9239130434782609
2019-03-03T14:35:24.182874: step 96, loss 0.13433, accuracy 0.941406, precision 0.8762886597938144, recall 0.9659090909090909
2019-03-03T14:35:25.031623: step 97, loss 0.170166, accuracy 0.9375, precision 0.91, recall 0.9285714285714286
2019-03-03T14:35:25.907281: step 98, loss 0.164395, accuracy 0.933594, precision 0.900990099009901, recall 0.9285714285714286
2019-03-03T14:35:26.733091: step 99, loss 0.124359, accuracy 0.957031, precision 0.925, recall 0.9367088607594937
2019-03-03T14:35:27.553280: step 100, loss 0.187272, accuracy 0.9375, precision 0.9354838709677419, recall 0.8969072164948454
2019-03-03T14:35:28.372108: step 101, loss 0.159797, accuracy 0.949219, precision 0.9367088607594937, recall 0.9024390243902439
2019-03-03T14:35:28.772060: step 102, loss 0.216004, accuracy 0.905983, precision 0.9069767441860465, recall 0.8478260869565217
2019-03-03T14:35:30.094548: step 103, loss 0.194895, accuracy 0.917969, precision 0.8958333333333334, recall 0.8865979381443299
2019-03-03T14:35:31.505783: step 104, loss 0.116681, accuracy 0.976562, precision 0.967391304347826, recall 0.967391304347826
2019-03-03T14:35:32.709565: step 105, loss 0.105268, accuracy 0.964844, precision 0.9230769230769231, recall 0.9767441860465116
2019-03-03T14:35:33.872413: step 106, loss 0.142464, accuracy 0.9375, precision 0.8651685393258427, recall 0.9506172839506173
2019-03-03T14:35:35.037313: step 107, loss 0.119578, accuracy 0.976562, precision 0.9777777777777777, recall 0.9565217391304348
2019-03-03T14:35:36.141938: step 108, loss 0.134619, accuracy 0.945312, precision 0.9166666666666666, recall 0.9361702127659575
2019-03-03T14:35:37.208088: step 109, loss 0.14376, accuracy 0.949219, precision 0.9230769230769231, recall 0.9333333333333333
2019-03-03T14:35:38.251298: step 110, loss 0.128156, accuracy 0.949219, precision 0.9259259259259259, recall 0.9146341463414634
2019-03-03T14:35:39.297464: step 111, loss 0.106476, accuracy 0.957031, precision 0.9157894736842105, recall 0.9666666666666667
2019-03-03T14:35:40.294798: step 112, loss 0.109986, accuracy 0.953125, precision 0.8924731182795699, recall 0.9764705882352941
2019-03-03T14:35:41.307608: step 113, loss 0.117255, accuracy 0.964844, precision 0.946236559139785, recall 0.9565217391304348
2019-03-03T14:35:42.279011: step 114, loss 0.141509, accuracy 0.941406, precision 0.9021739130434783, recall 0.9325842696629213
2019-03-03T14:35:43.235495: step 115, loss 0.143775, accuracy 0.941406, precision 0.898876404494382, recall 0.9302325581395349
2019-03-03T14:35:44.192422: step 116, loss 0.11045, accuracy 0.960938, precision 0.9479166666666666, recall 0.9479166666666666
2019-03-03T14:35:45.198120: step 117, loss 0.135591, accuracy 0.945312, precision 0.875, recall 0.9625
2019-03-03T14:35:46.109217: step 118, loss 0.0944427, accuracy 0.972656, precision 0.9555555555555556, recall 0.9662921348314607
2019-03-03T14:35:46.568988: step 119, loss 0.182589, accuracy 0.940171, precision 0.9148936170212766, recall 0.9347826086956522
2019-03-03T14:35:47.479553: step 120, loss 0.0935379, accuracy 0.964844, precision 0.95, recall 0.9595959595959596
2019-03-03T14:35:48.396047: step 121, loss 0.117199, accuracy 0.945312, precision 0.9139784946236559, recall 0.9340659340659341
2019-03-03T14:35:49.301645: step 122, loss 0.081041, accuracy 0.984375, precision 0.989010989010989, recall 0.967741935483871
2019-03-03T14:35:50.202745: step 123, loss 0.0988087, accuracy 0.972656, precision 0.9882352941176471, recall 0.9333333333333333
2019-03-03T14:35:51.093253: step 124, loss 0.11155, accuracy 0.957031, precision 0.9705882352941176, recall 0.9252336448598131
2019-03-03T14:35:51.975579: step 125, loss 0.136219, accuracy 0.9375, precision 0.9042553191489362, recall 0.9239130434782609
2019-03-03T14:35:52.867032: step 126, loss 0.112898, accuracy 0.964844, precision 0.9523809523809523, recall 0.9615384615384616
2019-03-03T14:35:53.742201: step 127, loss 0.133553, accuracy 0.957031, precision 0.9130434782608695, recall 0.9655172413793104
2019-03-03T14:35:54.620852: step 128, loss 0.13978, accuracy 0.941406, precision 0.9431818181818182, recall 0.8924731182795699
2019-03-03T14:35:55.483216: step 129, loss 0.101912, accuracy 0.964844, precision 0.9545454545454546, recall 0.9438202247191011
2019-03-03T14:35:56.343935: step 130, loss 0.105214, accuracy 0.953125, precision 0.9183673469387755, recall 0.9574468085106383
2019-03-03T14:35:57.189247: step 131, loss 0.137803, accuracy 0.949219, precision 0.9125, recall 0.9240506329113924
2019-03-03T14:35:58.034361: step 132, loss 0.134937, accuracy 0.949219, precision 0.9444444444444444, recall 0.8831168831168831
2019-03-03T14:35:58.927389: step 133, loss 0.0940816, accuracy 0.972656, precision 0.9506172839506173, recall 0.9625
2019-03-03T14:35:59.849860: step 134, loss 0.0925372, accuracy 0.980469, precision 0.9764705882352941, recall 0.9651162790697675
2019-03-03T14:36:00.742980: step 135, loss 0.122918, accuracy 0.953125, precision 0.9142857142857143, recall 0.9696969696969697
2019-03-03T14:36:01.142355: step 136, loss 0.104318, accuracy 0.957265, precision 0.92, recall 0.9787234042553191
2019-03-03T14:36:02.090283: step 137, loss 0.0973778, accuracy 0.957031, precision 0.9438202247191011, recall 0.9333333333333333
2019-03-03T14:36:03.079123: step 138, loss 0.121241, accuracy 0.957031, precision 0.9130434782608695, recall 0.9655172413793104
2019-03-03T14:36:03.996048: step 139, loss 0.0924644, accuracy 0.964844, precision 0.9468085106382979, recall 0.956989247311828
2019-03-03T14:36:04.931724: step 140, loss 0.0992843, accuracy 0.964844, precision 0.9313725490196079, recall 0.979381443298969
2019-03-03T14:36:05.795926: step 141, loss 0.135573, accuracy 0.953125, precision 0.9438202247191011, recall 0.9230769230769231
2019-03-03T14:36:06.663946: step 142, loss 0.121101, accuracy 0.964844, precision 0.946236559139785, recall 0.9565217391304348
2019-03-03T14:36:07.494726: step 143, loss 0.0833977, accuracy 0.976562, precision 0.9565217391304348, recall 0.9777777777777777
2019-03-03T14:36:08.352594: step 144, loss 0.0790117, accuracy 0.976562, precision 0.968421052631579, recall 0.968421052631579
2019-03-03T14:36:09.201663: step 145, loss 0.103931, accuracy 0.964844, precision 0.9578947368421052, recall 0.9479166666666666
2019-03-03T14:36:10.031969: step 146, loss 0.0868528, accuracy 0.964844, precision 0.9595959595959596, recall 0.95
2019-03-03T14:36:10.850518: step 147, loss 0.104552, accuracy 0.960938, precision 0.9886363636363636, recall 0.90625
2019-03-03T14:36:11.664383: step 148, loss 0.0787857, accuracy 0.972656, precision 0.9259259259259259, recall 0.9868421052631579
2019-03-03T14:36:12.485993: step 149, loss 0.0743505, accuracy 0.972656, precision 0.9534883720930233, recall 0.9647058823529412
2019-03-03T14:36:13.303755: step 150, loss 0.0824419, accuracy 0.980469, precision 0.9560439560439561, recall 0.9886363636363636
2019-03-03T14:36:14.142549: step 151, loss 0.0878601, accuracy 0.972656, precision 0.9545454545454546, recall 0.9655172413793104
2019-03-03T14:36:14.964889: step 152, loss 0.0757323, accuracy 0.972656, precision 0.9375, recall 0.989010989010989
2019-03-03T14:36:15.352826: step 153, loss 0.068494, accuracy 0.974359, precision 0.9743589743589743, recall 0.95
2019-03-03T14:36:16.191095: step 154, loss 0.0702916, accuracy 0.980469, precision 0.9787234042553191, recall 0.968421052631579
2019-03-03T14:36:17.164061: step 155, loss 0.0884516, accuracy 0.972656, precision 0.9423076923076923, recall 0.98989898989899
2019-03-03T14:36:18.422754: step 156, loss 0.102969, accuracy 0.960938, precision 0.9101123595505618, recall 0.9759036144578314
2019-03-03T14:36:19.670978: step 157, loss 0.0907487, accuracy 0.976562, precision 0.9696969696969697, recall 0.9696969696969697
2019-03-03T14:36:20.870801: step 158, loss 0.0643894, accuracy 0.976562, precision 0.967032967032967, recall 0.967032967032967
2019-03-03T14:36:22.065548: step 159, loss 0.0492284, accuracy 0.992188, precision 0.989010989010989, recall 0.989010989010989
2019-03-03T14:36:23.189563: step 160, loss 0.0761584, accuracy 0.980469, precision 0.9775280898876404, recall 0.9666666666666667
2019-03-03T14:36:24.302149: step 161, loss 0.0898653, accuracy 0.96875, precision 0.9387755102040817, recall 0.9787234042553191
2019-03-03T14:36:25.375283: step 162, loss 0.088678, accuracy 0.96875, precision 0.9069767441860465, recall 1.0
2019-03-03T14:36:26.427143: step 163, loss 0.0448679, accuracy 0.996094, precision 0.9876543209876543, recall 1.0
2019-03-03T14:36:27.451435: step 164, loss 0.0713187, accuracy 0.976562, precision 0.9583333333333334, recall 0.9787234042553191
2019-03-03T14:36:28.465739: step 165, loss 0.0779849, accuracy 0.972656, precision 0.9560439560439561, recall 0.9666666666666667
2019-03-03T14:36:29.464089: step 166, loss 0.0758604, accuracy 0.972656, precision 0.9529411764705882, recall 0.9642857142857143
2019-03-03T14:36:30.432014: step 167, loss 0.0825863, accuracy 0.972656, precision 0.956989247311828, recall 0.967391304347826
2019-03-03T14:36:31.395944: step 168, loss 0.0932855, accuracy 0.960938, precision 0.9393939393939394, recall 0.9587628865979382
2019-03-03T14:36:32.487494: step 169, loss 0.0733698, accuracy 0.984375, precision 0.9879518072289156, recall 0.9647058823529412
2019-03-03T14:36:33.157704: step 170, loss 0.107928, accuracy 0.957265, precision 0.925, recall 0.9487179487179487
2019-03-03T14:36:34.455742: step 171, loss 0.0706365, accuracy 0.996094, precision 1.0, recall 0.9886363636363636
2019-03-03T14:36:35.476015: step 172, loss 0.0702298, accuracy 0.972656, precision 0.9662921348314607, recall 0.9555555555555556
2019-03-03T14:36:36.483322: step 173, loss 0.0672142, accuracy 0.984375, precision 0.9879518072289156, recall 0.9647058823529412
2019-03-03T14:36:37.401872: step 174, loss 0.0810636, accuracy 0.976562, precision 0.9803921568627451, recall 0.9615384615384616
2019-03-03T14:36:38.314942: step 175, loss 0.0660492, accuracy 0.976562, precision 0.9565217391304348, recall 0.9777777777777777
2019-03-03T14:36:39.259734: step 176, loss 0.0600087, accuracy 0.984375, precision 0.9666666666666667, recall 0.9886363636363636
2019-03-03T14:36:40.165314: step 177, loss 0.0558081, accuracy 0.984375, precision 0.9787234042553191, recall 0.9787234042553191
2019-03-03T14:36:41.059926: step 178, loss 0.0899862, accuracy 0.972656, precision 0.945054945054945, recall 0.9772727272727273
2019-03-03T14:36:41.947797: step 179, loss 0.0527031, accuracy 0.980469, precision 0.9655172413793104, recall 0.9767441860465116
2019-03-03T14:36:42.819975: step 180, loss 0.0666114, accuracy 0.984375, precision 0.9659090909090909, recall 0.9883720930232558
2019-03-03T14:36:43.681685: step 181, loss 0.0885625, accuracy 0.972656, precision 0.9537037037037037, recall 0.9809523809523809
2019-03-03T14:36:44.537358: step 182, loss 0.108621, accuracy 0.957031, precision 0.9069767441860465, recall 0.9629629629629629
2019-03-03T14:36:45.391075: step 183, loss 0.0654846, accuracy 0.980469, precision 0.9555555555555556, recall 0.9885057471264368
2019-03-03T14:36:46.243795: step 184, loss 0.0749727, accuracy 0.96875, precision 0.96, recall 0.96
2019-03-03T14:36:47.069869: step 185, loss 0.0848218, accuracy 0.964844, precision 0.9368421052631579, recall 0.967391304347826
2019-03-03T14:36:47.907631: step 186, loss 0.0600967, accuracy 0.976562, precision 0.9759036144578314, recall 0.9529411764705882
2019-03-03T14:36:48.317540: step 187, loss 0.0595547, accuracy 0.982906, precision 0.9545454545454546, recall 1.0
2019-03-03T14:36:49.141320: step 188, loss 0.0489622, accuracy 0.988281, precision 0.9782608695652174, recall 0.989010989010989
2019-03-03T14:36:49.990468: step 189, loss 0.050721, accuracy 0.992188, precision 0.989010989010989, recall 0.989010989010989
2019-03-03T14:36:50.865031: step 190, loss 0.0473372, accuracy 0.996094, precision 0.9883720930232558, recall 1.0
2019-03-03T14:36:51.681848: step 191, loss 0.0527242, accuracy 0.980469, precision 0.9787234042553191, recall 0.968421052631579
2019-03-03T14:36:52.492681: step 192, loss 0.0529243, accuracy 0.992188, precision 0.9879518072289156, recall 0.9879518072289156
2019-03-03T14:36:53.328446: step 193, loss 0.0562518, accuracy 0.984375, precision 0.9777777777777777, recall 0.9777777777777777
2019-03-03T14:36:54.172721: step 194, loss 0.0569498, accuracy 0.980469, precision 0.9902912621359223, recall 0.9622641509433962
2019-03-03T14:36:55.016936: step 195, loss 0.0433091, accuracy 0.992188, precision 0.9893617021276596, recall 0.9893617021276596
2019-03-03T14:36:55.858685: step 196, loss 0.0650035, accuracy 0.976562, precision 0.9565217391304348, recall 0.9777777777777777
2019-03-03T14:36:56.690461: step 197, loss 0.0537401, accuracy 0.984375, precision 0.9603960396039604, recall 1.0
2019-03-03T14:36:57.512041: step 198, loss 0.063466, accuracy 0.984375, precision 0.9764705882352941, recall 0.9764705882352941
2019-03-03T14:36:58.378256: step 199, loss 0.0609483, accuracy 0.980469, precision 1.0, recall 0.945054945054945
2019-03-03T14:36:59.223013: step 200, loss 0.123469, accuracy 0.964844, precision 0.9431818181818182, recall 0.9540229885057471
2019-03-03T14:37:00.094992: step 201, loss 0.0792496, accuracy 0.964844, precision 0.9425287356321839, recall 0.9534883720930233
2019-03-03T14:37:00.942725: step 202, loss 0.0550001, accuracy 0.984375, precision 0.9702970297029703, recall 0.98989898989899
2019-03-03T14:37:01.799435: step 203, loss 0.062469, accuracy 0.976562, precision 0.968421052631579, recall 0.968421052631579
2019-03-03T14:37:02.211375: step 204, loss 0.0532878, accuracy 0.982906, precision 0.9512195121951219, recall 1.0
2019-03-03T14:37:03.016386: step 205, loss 0.0496698, accuracy 0.984375, precision 0.9705882352941176, recall 0.99
2019-03-03T14:37:03.830231: step 206, loss 0.041469, accuracy 0.988281, precision 0.9803921568627451, recall 0.9900990099009901
2019-03-03T14:37:04.692902: step 207, loss 0.056261, accuracy 0.976562, precision 0.9770114942528736, recall 0.9550561797752809
2019-03-03T14:37:05.529685: step 208, loss 0.0470074, accuracy 0.980469, precision 0.9550561797752809, recall 0.9883720930232558
2019-03-03T14:37:06.364035: step 209, loss 0.028924, accuracy 0.996094, precision 1.0, recall 0.989010989010989
2019-03-03T14:37:07.179374: step 210, loss 0.0539261, accuracy 0.988281, precision 0.9772727272727273, recall 0.9885057471264368
2019-03-03T14:37:08.058042: step 211, loss 0.0404582, accuracy 0.996094, precision 1.0, recall 0.9880952380952381
2019-03-03T14:37:08.967610: step 212, loss 0.045301, accuracy 0.984375, precision 0.975609756097561, recall 0.975609756097561
2019-03-03T14:37:09.862219: step 213, loss 0.0458215, accuracy 0.992188, precision 0.9878048780487805, recall 0.9878048780487805
2019-03-03T14:37:11.140394: step 214, loss 0.0448534, accuracy 0.992188, precision 0.9797979797979798, recall 1.0
2019-03-03T14:37:12.434932: step 215, loss 0.0625557, accuracy 0.976562, precision 0.96875, recall 0.96875
2019-03-03T14:37:13.628746: step 216, loss 0.0583014, accuracy 0.976562, precision 0.9381443298969072, recall 1.0
2019-03-03T14:37:14.823469: step 217, loss 0.0651881, accuracy 0.980469, precision 0.9655172413793104, recall 0.9767441860465116
2019-03-03T14:37:15.957912: step 218, loss 0.0437951, accuracy 0.988281, precision 0.9789473684210527, recall 0.9893617021276596
2019-03-03T14:37:17.069473: step 219, loss 0.0317202, accuracy 0.992188, precision 1.0, recall 0.9795918367346939
2019-03-03T14:37:18.145614: step 220, loss 0.0510471, accuracy 0.984375, precision 0.968421052631579, recall 0.989247311827957
2019-03-03T14:37:18.656286: step 221, loss 0.0651527, accuracy 0.974359, precision 0.9487179487179487, recall 0.9736842105263158
2019-03-03T14:37:19.706022: step 222, loss 0.0457607, accuracy 0.984375, precision 0.9642857142857143, recall 0.9878048780487805
2019-03-03T14:37:20.720310: step 223, loss 0.0356615, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:37:21.707179: step 224, loss 0.0447266, accuracy 0.988281, precision 0.9767441860465116, recall 0.9882352941176471
2019-03-03T14:37:22.709518: step 225, loss 0.0599307, accuracy 0.972656, precision 0.9603960396039604, recall 0.97
2019-03-03T14:37:23.656986: step 226, loss 0.0471686, accuracy 0.992188, precision 0.9795918367346939, recall 1.0
2019-03-03T14:37:24.624907: step 227, loss 0.0518745, accuracy 0.988281, precision 0.9759036144578314, recall 0.9878048780487805
2019-03-03T14:37:25.565391: step 228, loss 0.0549392, accuracy 0.976562, precision 0.9651162790697675, recall 0.9651162790697675
2019-03-03T14:37:26.510480: step 229, loss 0.0425575, accuracy 0.988281, precision 0.9885057471264368, recall 0.9772727272727273
2019-03-03T14:37:27.434906: step 230, loss 0.0334637, accuracy 0.984375, precision 0.9789473684210527, recall 0.9789473684210527
2019-03-03T14:37:28.367925: step 231, loss 0.0470046, accuracy 0.984375, precision 0.9873417721518988, recall 0.9629629629629629
2019-03-03T14:37:29.273499: step 232, loss 0.0551521, accuracy 0.976562, precision 0.9397590361445783, recall 0.9873417721518988
2019-03-03T14:37:30.176605: step 233, loss 0.0518811, accuracy 0.984375, precision 0.9642857142857143, recall 0.9878048780487805
2019-03-03T14:37:31.074445: step 234, loss 0.0359744, accuracy 0.984375, precision 0.9908256880733946, recall 0.972972972972973
2019-03-03T14:37:31.959609: step 235, loss 0.0326409, accuracy 0.996094, precision 1.0, recall 0.9903846153846154
2019-03-03T14:37:32.858712: step 236, loss 0.0437028, accuracy 0.988281, precision 0.9789473684210527, recall 0.9893617021276596
2019-03-03T14:37:33.727898: step 237, loss 0.0374519, accuracy 0.984375, precision 0.9906542056074766, recall 0.9724770642201835
2019-03-03T14:37:34.147289: step 238, loss 0.0445504, accuracy 0.982906, precision 0.9807692307692307, recall 0.9807692307692307
2019-03-03T14:37:35.021953: step 239, loss 0.0548422, accuracy 0.980469, precision 0.9759036144578314, recall 0.9642857142857143
2019-03-03T14:37:35.873674: step 240, loss 0.0324586, accuracy 0.992188, precision 0.9896907216494846, recall 0.9896907216494846
2019-03-03T14:37:36.730382: step 241, loss 0.0430665, accuracy 0.980469, precision 0.9696969696969697, recall 0.9795918367346939
2019-03-03T14:37:37.591101: step 242, loss 0.0269805, accuracy 0.992188, precision 0.989010989010989, recall 0.989010989010989
2019-03-03T14:37:38.443848: step 243, loss 0.0324843, accuracy 0.988281, precision 0.9777777777777777, recall 0.9887640449438202
2019-03-03T14:37:39.279133: step 244, loss 0.0399941, accuracy 0.988281, precision 0.9775280898876404, recall 0.9886363636363636
2019-03-03T14:37:40.126377: step 245, loss 0.0299337, accuracy 0.996094, precision 0.98989898989899, recall 1.0
2019-03-03T14:37:40.966845: step 246, loss 0.0351878, accuracy 0.988281, precision 0.9801980198019802, recall 0.99
2019-03-03T14:37:41.790642: step 247, loss 0.0607022, accuracy 0.972656, precision 0.9456521739130435, recall 0.9775280898876404
2019-03-03T14:37:42.649516: step 248, loss 0.0293249, accuracy 0.996094, precision 0.9880952380952381, recall 1.0
2019-03-03T14:37:43.454868: step 249, loss 0.0372933, accuracy 0.992188, precision 0.9782608695652174, recall 1.0
2019-03-03T14:37:44.285648: step 250, loss 0.0542424, accuracy 0.988281, precision 0.9873417721518988, recall 0.975

Evaluation:
[[158  26]
 [ 22 262]]
2019-03-03T14:37:44.804176: step 250, loss 0.350134, accuracy 0.897436, precision 0.8586956521739131, recall 0.8777777777777778

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551645203\checkpoints\model-250

2019-03-03T14:37:45.790722: step 251, loss 0.0368051, accuracy 0.988281, precision 1.0, recall 0.9680851063829787
2019-03-03T14:37:46.643508: step 252, loss 0.0544092, accuracy 0.972656, precision 0.9607843137254902, recall 0.9702970297029703
2019-03-03T14:37:47.486257: step 253, loss 0.0482187, accuracy 0.984375, precision 0.9770114942528736, recall 0.9770114942528736
2019-03-03T14:37:48.322070: step 254, loss 0.0231723, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:37:48.714403: step 255, loss 0.021749, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:37:49.524270: step 256, loss 0.0648326, accuracy 0.980469, precision 0.9764705882352941, recall 0.9651162790697675
2019-03-03T14:37:50.373998: step 257, loss 0.0265739, accuracy 0.996094, precision 0.9893617021276596, recall 1.0
2019-03-03T14:37:51.196798: step 258, loss 0.0471102, accuracy 0.984375, precision 0.9782608695652174, recall 0.9782608695652174
2019-03-03T14:37:52.070470: step 259, loss 0.0442499, accuracy 0.992188, precision 0.9897959183673469, recall 0.9897959183673469
2019-03-03T14:37:52.981803: step 260, loss 0.0273861, accuracy 0.996094, precision 0.989247311827957, recall 1.0
2019-03-03T14:37:53.876894: step 261, loss 0.026799, accuracy 0.992188, precision 0.9795918367346939, recall 1.0
2019-03-03T14:37:54.781943: step 262, loss 0.0338454, accuracy 0.988281, precision 0.9642857142857143, recall 1.0
2019-03-03T14:37:55.648583: step 263, loss 0.0361881, accuracy 0.984375, precision 0.9896907216494846, recall 0.9696969696969697
2019-03-03T14:37:56.918488: step 264, loss 0.0345884, accuracy 0.988281, precision 0.9891304347826086, recall 0.978494623655914
2019-03-03T14:37:58.282362: step 265, loss 0.0274066, accuracy 0.988281, precision 0.97, recall 1.0
2019-03-03T14:37:59.678476: step 266, loss 0.0273064, accuracy 0.996094, precision 0.99, recall 1.0
2019-03-03T14:38:01.240658: step 267, loss 0.0316451, accuracy 0.992188, precision 0.9801980198019802, recall 1.0
2019-03-03T14:38:02.802483: step 268, loss 0.0522201, accuracy 0.980469, precision 0.9555555555555556, recall 0.9885057471264368
2019-03-03T14:38:04.030122: step 269, loss 0.0447038, accuracy 0.988281, precision 0.987012987012987, recall 0.9743589743589743
2019-03-03T14:38:05.190125: step 270, loss 0.0465095, accuracy 0.984375, precision 0.9885057471264368, recall 0.9662921348314607
2019-03-03T14:38:06.279746: step 271, loss 0.0399882, accuracy 0.988281, precision 0.9620253164556962, recall 1.0
2019-03-03T14:38:06.763052: step 272, loss 0.0233604, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:38:07.788275: step 273, loss 0.0383665, accuracy 0.988281, precision 0.975, recall 0.9873417721518988
2019-03-03T14:38:08.830828: step 274, loss 0.0612314, accuracy 0.976562, precision 0.9900990099009901, recall 0.9523809523809523
2019-03-03T14:38:09.797776: step 275, loss 0.0286656, accuracy 0.996094, precision 1.0, recall 0.9891304347826086
2019-03-03T14:38:10.770192: step 276, loss 0.0244598, accuracy 0.992188, precision 0.9895833333333334, recall 0.9895833333333334
2019-03-03T14:38:11.698276: step 277, loss 0.0201103, accuracy 0.996094, precision 0.9883720930232558, recall 1.0
2019-03-03T14:38:12.649750: step 278, loss 0.031354, accuracy 0.992188, precision 0.9882352941176471, recall 0.9882352941176471
2019-03-03T14:38:13.598478: step 279, loss 0.0330642, accuracy 0.988281, precision 0.9882352941176471, recall 0.9767441860465116
2019-03-03T14:38:14.522516: step 280, loss 0.0220753, accuracy 0.996094, precision 0.9875, recall 1.0
2019-03-03T14:38:15.421113: step 281, loss 0.0282172, accuracy 0.996094, precision 1.0, recall 0.98989898989899
2019-03-03T14:38:16.332677: step 282, loss 0.0283133, accuracy 0.992188, precision 0.9880952380952381, recall 0.9880952380952381
2019-03-03T14:38:17.216315: step 283, loss 0.0554799, accuracy 0.976562, precision 0.9696969696969697, recall 0.9696969696969697
2019-03-03T14:38:18.116907: step 284, loss 0.0215614, accuracy 0.992188, precision 0.9775280898876404, recall 1.0
2019-03-03T14:38:19.009840: step 285, loss 0.0241593, accuracy 0.996094, precision 1.0, recall 0.9912280701754386
2019-03-03T14:38:19.873556: step 286, loss 0.0340088, accuracy 0.988281, precision 0.9888888888888889, recall 0.978021978021978
2019-03-03T14:38:20.744738: step 287, loss 0.0293272, accuracy 0.996094, precision 0.9895833333333334, recall 1.0
2019-03-03T14:38:21.595488: step 288, loss 0.0294265, accuracy 0.996094, precision 0.9887640449438202, recall 1.0
2019-03-03T14:38:22.045308: step 289, loss 0.0641838, accuracy 0.974359, precision 0.9574468085106383, recall 0.9782608695652174
2019-03-03T14:38:22.983819: step 290, loss 0.0287318, accuracy 0.992188, precision 0.9746835443037974, recall 1.0
2019-03-03T14:38:23.941768: step 291, loss 0.0273535, accuracy 0.992188, precision 0.9879518072289156, recall 0.9879518072289156
2019-03-03T14:38:24.924649: step 292, loss 0.0293237, accuracy 0.996094, precision 0.9888888888888889, recall 1.0
2019-03-03T14:38:25.841198: step 293, loss 0.0302897, accuracy 0.988281, precision 0.9777777777777777, recall 0.9887640449438202
2019-03-03T14:38:26.800745: step 294, loss 0.0241581, accuracy 0.996094, precision 0.9879518072289156, recall 1.0
2019-03-03T14:38:27.687893: step 295, loss 0.033038, accuracy 0.992188, precision 0.9886363636363636, recall 0.9886363636363636
2019-03-03T14:38:28.597461: step 296, loss 0.0310415, accuracy 0.996094, precision 0.9900990099009901, recall 1.0
2019-03-03T14:38:29.521941: step 297, loss 0.0407727, accuracy 0.980469, precision 0.98, recall 0.9702970297029703
2019-03-03T14:38:30.452473: step 298, loss 0.0247952, accuracy 0.996094, precision 0.9891304347826086, recall 1.0
2019-03-03T14:38:31.364046: step 299, loss 0.0324668, accuracy 0.992188, precision 1.0, recall 0.9803921568627451
2019-03-03T14:38:32.346953: step 300, loss 0.0336272, accuracy 0.988281, precision 0.9690721649484536, recall 1.0
2019-03-03T14:38:33.161284: step 301, loss 0.0547912, accuracy 0.984375, precision 0.9605263157894737, recall 0.9864864864864865
2019-03-03T14:38:34.002015: step 302, loss 0.0392289, accuracy 0.976562, precision 0.9680851063829787, recall 0.9680851063829787
2019-03-03T14:38:34.831795: step 303, loss 0.0231265, accuracy 0.996094, precision 0.9883720930232558, recall 1.0
2019-03-03T14:38:35.640634: step 304, loss 0.0507475, accuracy 0.984375, precision 0.9789473684210527, recall 0.9789473684210527
2019-03-03T14:38:36.484455: step 305, loss 0.0333632, accuracy 0.992188, precision 0.9906542056074766, recall 0.9906542056074766
2019-03-03T14:38:36.878402: step 306, loss 0.0386726, accuracy 0.974359, precision 0.9375, recall 1.0
2019-03-03T14:38:37.681264: step 307, loss 0.0385122, accuracy 0.984375, precision 0.9659090909090909, recall 0.9883720930232558
2019-03-03T14:38:38.502115: step 308, loss 0.0341614, accuracy 0.988281, precision 0.9894736842105263, recall 0.9791666666666666
2019-03-03T14:38:39.301977: step 309, loss 0.0225139, accuracy 0.996094, precision 1.0, recall 0.9904761904761905
2019-03-03T14:38:40.151215: step 310, loss 0.0397519, accuracy 0.992188, precision 1.0, recall 0.978494623655914
2019-03-03T14:38:40.965582: step 311, loss 0.0330006, accuracy 0.992188, precision 0.9777777777777777, recall 1.0
2019-03-03T14:38:41.751972: step 312, loss 0.0236108, accuracy 0.996094, precision 0.9886363636363636, recall 1.0
2019-03-03T14:38:42.570800: step 313, loss 0.0171939, accuracy 0.996094, precision 1.0, recall 0.9883720930232558
2019-03-03T14:38:43.400583: step 314, loss 0.0292128, accuracy 0.992188, precision 0.9789473684210527, recall 1.0
2019-03-03T14:38:44.209933: step 315, loss 0.0362129, accuracy 0.988281, precision 1.0, recall 0.9690721649484536
2019-03-03T14:38:45.034722: step 316, loss 0.019786, accuracy 0.996094, precision 1.0, recall 0.9885057471264368
2019-03-03T14:38:45.822615: step 317, loss 0.0236518, accuracy 0.996094, precision 1.0, recall 0.9885057471264368
2019-03-03T14:38:46.658380: step 318, loss 0.0366744, accuracy 0.988281, precision 0.9897959183673469, recall 0.9797979797979798
2019-03-03T14:38:47.493264: step 319, loss 0.021135, accuracy 0.996094, precision 1.0, recall 0.9894736842105263
2019-03-03T14:38:48.379412: step 320, loss 0.0277138, accuracy 0.992188, precision 0.9893617021276596, recall 0.9893617021276596
2019-03-03T14:38:49.234200: step 321, loss 0.0241334, accuracy 0.992188, precision 0.9777777777777777, recall 1.0
2019-03-03T14:38:50.068969: step 322, loss 0.035631, accuracy 0.996094, precision 0.9878048780487805, recall 1.0
2019-03-03T14:38:50.515961: step 323, loss 0.0199338, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:38:51.491375: step 324, loss 0.0364032, accuracy 0.980469, precision 0.9534883720930233, recall 0.9879518072289156
2019-03-03T14:38:52.499210: step 325, loss 0.0257013, accuracy 0.988281, precision 0.9702970297029703, recall 1.0
2019-03-03T14:38:53.671611: step 326, loss 0.0362013, accuracy 0.992188, precision 0.9761904761904762, recall 1.0
2019-03-03T14:38:55.400985: step 327, loss 0.0183506, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:38:56.929937: step 328, loss 0.0178239, accuracy 0.996094, precision 0.9876543209876543, recall 1.0
2019-03-03T14:38:58.508708: step 329, loss 0.0262406, accuracy 0.996094, precision 1.0, recall 0.9883720930232558
2019-03-03T14:38:59.907555: step 330, loss 0.0402628, accuracy 0.984375, precision 1.0, recall 0.9603960396039604
2019-03-03T14:39:01.366167: step 331, loss 0.0115701, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:02.923938: step 332, loss 0.0264156, accuracy 0.992188, precision 0.9789473684210527, recall 1.0
2019-03-03T14:39:04.363614: step 333, loss 0.0182935, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:05.515558: step 334, loss 0.0169391, accuracy 0.996094, precision 1.0, recall 0.9888888888888889
2019-03-03T14:39:06.670477: step 335, loss 0.0140807, accuracy 0.996094, precision 0.9894736842105263, recall 1.0
2019-03-03T14:39:07.699601: step 336, loss 0.0309267, accuracy 0.988281, precision 1.0, recall 0.9714285714285714
2019-03-03T14:39:08.761629: step 337, loss 0.0271537, accuracy 0.992188, precision 0.978494623655914, recall 1.0
2019-03-03T14:39:09.750805: step 338, loss 0.017394, accuracy 0.996094, precision 1.0, recall 0.9885057471264368
2019-03-03T14:39:10.725153: step 339, loss 0.0302382, accuracy 0.992188, precision 0.9795918367346939, recall 1.0
2019-03-03T14:39:11.200917: step 340, loss 0.0178895, accuracy 0.991453, precision 0.9736842105263158, recall 1.0
2019-03-03T14:39:12.138410: step 341, loss 0.0145584, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:13.046026: step 342, loss 0.0268619, accuracy 0.992188, precision 0.9888888888888889, recall 0.9888888888888889
2019-03-03T14:39:14.119157: step 343, loss 0.0359154, accuracy 0.992188, precision 0.9888888888888889, recall 0.9888888888888889
2019-03-03T14:39:15.144824: step 344, loss 0.0258231, accuracy 0.992188, precision 0.9897959183673469, recall 0.9897959183673469
2019-03-03T14:39:16.154025: step 345, loss 0.0192955, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:17.089983: step 346, loss 0.0136613, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:18.031491: step 347, loss 0.0179655, accuracy 0.996094, precision 0.9896907216494846, recall 1.0
2019-03-03T14:39:18.934074: step 348, loss 0.0206024, accuracy 0.996094, precision 0.9883720930232558, recall 1.0
2019-03-03T14:39:19.861969: step 349, loss 0.0219869, accuracy 0.996094, precision 0.9887640449438202, recall 1.0
2019-03-03T14:39:20.841568: step 350, loss 0.0297951, accuracy 0.988281, precision 0.9882352941176471, recall 0.9767441860465116
2019-03-03T14:39:21.735606: step 351, loss 0.0201372, accuracy 0.996094, precision 1.0, recall 0.9887640449438202
2019-03-03T14:39:22.791109: step 352, loss 0.0169889, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:23.674747: step 353, loss 0.0259024, accuracy 0.988281, precision 0.9696969696969697, recall 1.0
2019-03-03T14:39:24.645685: step 354, loss 0.0202697, accuracy 0.992188, precision 0.9888888888888889, recall 0.9888888888888889
2019-03-03T14:39:25.571164: step 355, loss 0.025684, accuracy 0.996094, precision 1.0, recall 0.9891304347826086
2019-03-03T14:39:26.582757: step 356, loss 0.0223638, accuracy 0.996094, precision 1.0, recall 0.99
2019-03-03T14:39:27.034064: step 357, loss 0.0202521, accuracy 0.991453, precision 1.0, recall 0.9777777777777777
2019-03-03T14:39:27.935654: step 358, loss 0.0137379, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:28.907058: step 359, loss 0.0191033, accuracy 0.996094, precision 1.0, recall 0.9904761904761905
2019-03-03T14:39:29.827964: step 360, loss 0.0219889, accuracy 0.992188, precision 0.9883720930232558, recall 0.9883720930232558
2019-03-03T14:39:30.692013: step 361, loss 0.0140959, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:31.545731: step 362, loss 0.0233819, accuracy 0.992188, precision 0.9887640449438202, recall 0.9887640449438202
2019-03-03T14:39:32.396025: step 363, loss 0.0165273, accuracy 0.996094, precision 0.9886363636363636, recall 1.0
2019-03-03T14:39:33.294136: step 364, loss 0.0317619, accuracy 0.988281, precision 0.9893617021276596, recall 0.9789473684210527
2019-03-03T14:39:34.216513: step 365, loss 0.023205, accuracy 0.992188, precision 0.9787234042553191, recall 1.0
2019-03-03T14:39:35.407322: step 366, loss 0.0248969, accuracy 0.992188, precision 0.989247311827957, recall 0.989247311827957
2019-03-03T14:39:36.310906: step 367, loss 0.0302499, accuracy 0.992188, precision 0.9782608695652174, recall 1.0
2019-03-03T14:39:37.199983: step 368, loss 0.019637, accuracy 0.996094, precision 1.0, recall 0.99
2019-03-03T14:39:38.317006: step 369, loss 0.030002, accuracy 0.992188, precision 0.978494623655914, recall 1.0
2019-03-03T14:39:39.652950: step 370, loss 0.0132246, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:40.817839: step 371, loss 0.0208179, accuracy 0.996094, precision 0.9882352941176471, recall 1.0
2019-03-03T14:39:42.112497: step 372, loss 0.0224473, accuracy 0.996094, precision 1.0, recall 0.99
2019-03-03T14:39:43.430800: step 373, loss 0.0136087, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:44.029328: step 374, loss 0.00990895, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:45.076869: step 375, loss 0.014398, accuracy 0.996094, precision 1.0, recall 0.9894736842105263
2019-03-03T14:39:46.122095: step 376, loss 0.0215302, accuracy 0.996094, precision 1.0, recall 0.9887640449438202
2019-03-03T14:39:47.100496: step 377, loss 0.0214286, accuracy 0.992188, precision 1.0, recall 0.98
2019-03-03T14:39:48.250551: step 378, loss 0.0172884, accuracy 0.996094, precision 1.0, recall 0.9875
2019-03-03T14:39:49.299252: step 379, loss 0.0270412, accuracy 0.996094, precision 1.0, recall 0.9886363636363636
2019-03-03T14:39:50.370353: step 380, loss 0.00859958, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:51.744898: step 381, loss 0.0185061, accuracy 0.996094, precision 1.0, recall 0.9891304347826086
2019-03-03T14:39:52.728777: step 382, loss 0.0102861, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:39:53.745949: step 383, loss 0.0358986, accuracy 0.992188, precision 0.9813084112149533, recall 1.0
2019-03-03T14:39:54.894876: step 384, loss 0.0137372, accuracy 0.996094, precision 0.989247311827957, recall 1.0
2019-03-03T14:39:55.913187: step 385, loss 0.0147736, accuracy 0.996094, precision 0.9871794871794872, recall 1.0
2019-03-03T14:39:56.836232: step 386, loss 0.0212746, accuracy 0.988281, precision 0.967032967032967, recall 1.0
2019-03-03T14:39:57.671033: step 387, loss 0.0204788, accuracy 0.992188, precision 0.978494623655914, recall 1.0
2019-03-03T14:39:58.520279: step 388, loss 0.0330599, accuracy 0.988281, precision 0.9714285714285714, recall 1.0
2019-03-03T14:39:59.448317: step 389, loss 0.0362286, accuracy 0.984375, precision 0.9801980198019802, recall 0.9801980198019802
2019-03-03T14:40:00.394769: step 390, loss 0.0146997, accuracy 0.996094, precision 1.0, recall 0.9894736842105263
2019-03-03T14:40:00.812159: step 391, loss 0.00558184, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:01.791540: step 392, loss 0.0260638, accuracy 0.992188, precision 1.0, recall 0.9772727272727273
2019-03-03T14:40:02.867174: step 393, loss 0.0202459, accuracy 0.996094, precision 0.989010989010989, recall 1.0
2019-03-03T14:40:03.835586: step 394, loss 0.020955, accuracy 0.992188, precision 0.9885057471264368, recall 0.9885057471264368
2019-03-03T14:40:04.901735: step 395, loss 0.0191583, accuracy 0.992188, precision 0.9893617021276596, recall 0.9893617021276596
2019-03-03T14:40:06.020748: step 396, loss 0.0119288, accuracy 0.996094, precision 0.9893617021276596, recall 1.0
2019-03-03T14:40:07.042047: step 397, loss 0.0267809, accuracy 0.992188, precision 1.0, recall 0.9797979797979798
2019-03-03T14:40:07.949739: step 398, loss 0.0203213, accuracy 0.996094, precision 1.0, recall 0.9894736842105263
2019-03-03T14:40:08.815936: step 399, loss 0.012894, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:09.568101: step 400, loss 0.0130067, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:10.336066: step 401, loss 0.018785, accuracy 0.996094, precision 1.0, recall 0.9882352941176471
2019-03-03T14:40:11.215224: step 402, loss 0.0128151, accuracy 0.996094, precision 1.0, recall 0.989247311827957
2019-03-03T14:40:12.020072: step 403, loss 0.00734053, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:12.849853: step 404, loss 0.0350546, accuracy 0.988281, precision 0.9711538461538461, recall 1.0
2019-03-03T14:40:13.606830: step 405, loss 0.0291373, accuracy 0.992188, precision 1.0, recall 0.9772727272727273
2019-03-03T14:40:14.514084: step 406, loss 0.021152, accuracy 0.992188, precision 0.9885057471264368, recall 0.9885057471264368
2019-03-03T14:40:15.475029: step 407, loss 0.0179552, accuracy 0.996094, precision 0.9878048780487805, recall 1.0
2019-03-03T14:40:15.928169: step 408, loss 0.0108273, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:16.972923: step 409, loss 0.0249394, accuracy 0.988281, precision 0.9770114942528736, recall 0.9883720930232558
2019-03-03T14:40:17.995213: step 410, loss 0.0419865, accuracy 0.988281, precision 0.98, recall 0.98989898989899
2019-03-03T14:40:18.998532: step 411, loss 0.011369, accuracy 0.996094, precision 0.9878048780487805, recall 1.0
2019-03-03T14:40:19.954974: step 412, loss 0.0208377, accuracy 0.996094, precision 1.0, recall 0.9900990099009901
2019-03-03T14:40:20.917794: step 413, loss 0.0210723, accuracy 0.992188, precision 0.9857142857142858, recall 0.9857142857142858
2019-03-03T14:40:21.961513: step 414, loss 0.0114972, accuracy 0.996094, precision 0.989247311827957, recall 1.0
2019-03-03T14:40:22.828197: step 415, loss 0.0189982, accuracy 0.996094, precision 1.0, recall 0.99
2019-03-03T14:40:23.838838: step 416, loss 0.01428, accuracy 0.996094, precision 1.0, recall 0.9886363636363636
2019-03-03T14:40:24.663158: step 417, loss 0.0127983, accuracy 0.996094, precision 1.0, recall 0.9863013698630136
2019-03-03T14:40:25.642258: step 418, loss 0.0176172, accuracy 0.996094, precision 0.9904761904761905, recall 1.0
2019-03-03T14:40:26.474042: step 419, loss 0.0212034, accuracy 0.992188, precision 0.989010989010989, recall 0.989010989010989
2019-03-03T14:40:27.352685: step 420, loss 0.00989007, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:28.339054: step 421, loss 0.0206345, accuracy 0.992188, precision 0.98989898989899, recall 0.98989898989899
2019-03-03T14:40:29.222776: step 422, loss 0.014522, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:30.102425: step 423, loss 0.0195741, accuracy 0.996094, precision 1.0, recall 0.9900990099009901
2019-03-03T14:40:31.024804: step 424, loss 0.0135399, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:31.484572: step 425, loss 0.00987388, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:32.434053: step 426, loss 0.01377, accuracy 0.996094, precision 1.0, recall 0.9880952380952381
2019-03-03T14:40:33.438888: step 427, loss 0.0181015, accuracy 0.992188, precision 0.989010989010989, recall 0.989010989010989
2019-03-03T14:40:34.407816: step 428, loss 0.012199, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:35.292968: step 429, loss 0.0158798, accuracy 0.996094, precision 0.9896907216494846, recall 1.0
2019-03-03T14:40:36.216498: step 430, loss 0.0153516, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:37.360460: step 431, loss 0.0172251, accuracy 0.996094, precision 0.9888888888888889, recall 1.0
2019-03-03T14:40:38.831063: step 432, loss 0.0198122, accuracy 0.996094, precision 1.0, recall 0.9901960784313726
2019-03-03T14:40:40.210413: step 433, loss 0.0214806, accuracy 0.992188, precision 0.9782608695652174, recall 1.0
2019-03-03T14:40:41.542872: step 434, loss 0.0182025, accuracy 0.996094, precision 1.0, recall 0.9886363636363636
2019-03-03T14:40:42.799576: step 435, loss 0.0102692, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:44.086135: step 436, loss 0.0179225, accuracy 0.996094, precision 1.0, recall 0.9886363636363636
2019-03-03T14:40:45.231601: step 437, loss 0.0197596, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:46.357017: step 438, loss 0.0143585, accuracy 0.996094, precision 1.0, recall 0.989010989010989
2019-03-03T14:40:47.495972: step 439, loss 0.0259088, accuracy 0.992188, precision 0.9805825242718447, recall 1.0
2019-03-03T14:40:48.559688: step 440, loss 0.00601564, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:49.592437: step 441, loss 0.023558, accuracy 0.996094, precision 1.0, recall 0.9888888888888889
2019-03-03T14:40:50.096525: step 442, loss 0.0146991, accuracy 0.991453, precision 0.9761904761904762, recall 1.0
2019-03-03T14:40:51.121203: step 443, loss 0.021577, accuracy 0.988281, precision 0.9897959183673469, recall 0.9797979797979798
2019-03-03T14:40:52.297574: step 444, loss 0.0126494, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:53.323615: step 445, loss 0.0125664, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:54.484894: step 446, loss 0.012359, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:55.625845: step 447, loss 0.00627252, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:56.843588: step 448, loss 0.0145232, accuracy 0.996094, precision 0.9891304347826086, recall 1.0
2019-03-03T14:40:57.743234: step 449, loss 0.0177073, accuracy 0.992188, precision 0.9875, recall 0.9875
2019-03-03T14:40:58.646331: step 450, loss 0.00837446, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:40:59.566886: step 451, loss 0.0138716, accuracy 0.996094, precision 1.0, recall 0.9882352941176471
2019-03-03T14:41:00.494919: step 452, loss 0.0130711, accuracy 0.996094, precision 0.9895833333333334, recall 1.0
2019-03-03T14:41:01.370630: step 453, loss 0.0163691, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:02.329067: step 454, loss 0.0171518, accuracy 0.992188, precision 0.9895833333333334, recall 0.9895833333333334
2019-03-03T14:41:03.245615: step 455, loss 0.0147059, accuracy 0.996094, precision 0.9891304347826086, recall 1.0
2019-03-03T14:41:04.122273: step 456, loss 0.0103792, accuracy 0.996094, precision 1.0, recall 0.9887640449438202
2019-03-03T14:41:04.985007: step 457, loss 0.00944988, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:05.826265: step 458, loss 0.0290524, accuracy 0.988281, precision 0.9809523809523809, recall 0.9903846153846154
2019-03-03T14:41:06.259108: step 459, loss 0.0124817, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:07.111828: step 460, loss 0.0174302, accuracy 0.992188, precision 0.9811320754716981, recall 1.0
2019-03-03T14:41:07.948102: step 461, loss 0.009506, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:08.834598: step 462, loss 0.0114085, accuracy 0.996094, precision 1.0, recall 0.9891304347826086
2019-03-03T14:41:09.745678: step 463, loss 0.0101881, accuracy 0.996094, precision 0.989247311827957, recall 1.0
2019-03-03T14:41:10.678185: step 464, loss 0.00877131, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:11.537048: step 465, loss 0.00734579, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:12.473602: step 466, loss 0.0211276, accuracy 0.988281, precision 1.0, recall 0.9651162790697675
2019-03-03T14:41:13.356239: step 467, loss 0.0139379, accuracy 0.996094, precision 1.0, recall 0.989247311827957
2019-03-03T14:41:14.204482: step 468, loss 0.0101019, accuracy 0.996094, precision 0.9894736842105263, recall 1.0
2019-03-03T14:41:15.055724: step 469, loss 0.00986765, accuracy 0.996094, precision 0.9886363636363636, recall 1.0
2019-03-03T14:41:15.951403: step 470, loss 0.0210887, accuracy 0.996094, precision 0.9904761904761905, recall 1.0
2019-03-03T14:41:16.689008: step 471, loss 0.0128423, accuracy 0.996094, precision 0.9883720930232558, recall 1.0
2019-03-03T14:41:17.419053: step 472, loss 0.0083226, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:18.438228: step 473, loss 0.0184344, accuracy 0.992188, precision 0.9893617021276596, recall 0.9893617021276596
2019-03-03T14:41:19.731880: step 474, loss 0.0185411, accuracy 0.996094, precision 1.0, recall 0.989010989010989
2019-03-03T14:41:20.880809: step 475, loss 0.0143558, accuracy 0.996094, precision 0.9871794871794872, recall 1.0
2019-03-03T14:41:21.391971: step 476, loss 0.0127718, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:22.664081: step 477, loss 0.0124943, accuracy 0.996094, precision 0.9882352941176471, recall 1.0
2019-03-03T14:41:23.903774: step 478, loss 0.0134828, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:25.014258: step 479, loss 0.00824844, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:26.144236: step 480, loss 0.00868155, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:27.198919: step 481, loss 0.0100393, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:28.246956: step 482, loss 0.0177778, accuracy 0.992188, precision 0.9891304347826086, recall 0.9891304347826086
2019-03-03T14:41:29.322486: step 483, loss 0.0110455, accuracy 0.996094, precision 0.989247311827957, recall 1.0
2019-03-03T14:41:30.354614: step 484, loss 0.0116957, accuracy 0.996094, precision 1.0, recall 0.9887640449438202
2019-03-03T14:41:31.431244: step 485, loss 0.0143302, accuracy 0.996094, precision 0.9880952380952381, recall 1.0
2019-03-03T14:41:32.396103: step 486, loss 0.00661524, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:33.371865: step 487, loss 0.0155584, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:34.349779: step 488, loss 0.012772, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:35.340129: step 489, loss 0.0126693, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:36.308540: step 490, loss 0.0128701, accuracy 0.996094, precision 0.9879518072289156, recall 1.0
2019-03-03T14:41:37.473001: step 491, loss 0.0174596, accuracy 0.992188, precision 0.9902912621359223, recall 0.9902912621359223
2019-03-03T14:41:38.509287: step 492, loss 0.0139027, accuracy 0.996094, precision 0.9886363636363636, recall 1.0
2019-03-03T14:41:39.032995: step 493, loss 0.00577739, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:40.258718: step 494, loss 0.0122506, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:41.264972: step 495, loss 0.00586259, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:42.163594: step 496, loss 0.0122813, accuracy 0.996094, precision 1.0, recall 0.98989898989899
2019-03-03T14:41:43.010331: step 497, loss 0.0167291, accuracy 0.996094, precision 1.0, recall 0.9882352941176471
2019-03-03T14:41:43.911921: step 498, loss 0.00729424, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:44.790024: step 499, loss 0.010069, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:41:45.657725: step 500, loss 0.0115226, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[155  29]
 [ 24 260]]
2019-03-03T14:41:46.097065: step 500, loss 0.482417, accuracy 0.886752, precision 0.842391304347826, recall 0.8659217877094972

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551645203\checkpoints\model-500


Process finished with exit code 0
