"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
No Pretrained Embedding
Italian: True
Loading data...
4681
Max Document length: 81
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 7231
Train/Dev split: 4213/468
2019-03-03 14:59:57.569979: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_with_embeddings.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551646798

2019-03-03T14:59:59.045831: step 1, loss 2.5487, accuracy 0.433594, precision 0.8125, recall 0.3804878048780488
2019-03-03T14:59:59.358998: step 2, loss 1.7957, accuracy 0.496094, precision 0.5591397849462365, recall 0.37142857142857144
2019-03-03T14:59:59.696093: step 3, loss 1.54141, accuracy 0.625, precision 0.3764705882352941, recall 0.4266666666666667
2019-03-03T15:00:00.014242: step 4, loss 1.77475, accuracy 0.648438, precision 0.19047619047619047, recall 0.42105263157894735
2019-03-03T15:00:00.308455: step 5, loss 2.13346, accuracy 0.613281, precision 0.14285714285714285, recall 0.625
2019-03-03T15:00:00.635582: step 6, loss 1.89251, accuracy 0.640625, precision 0.16494845360824742, recall 0.5925925925925926
2019-03-03T15:00:00.938770: step 7, loss 1.73801, accuracy 0.625, precision 0.1956521739130435, recall 0.45
2019-03-03T15:00:01.253927: step 8, loss 1.44823, accuracy 0.695312, precision 0.42528735632183906, recall 0.5692307692307692
2019-03-03T15:00:01.565096: step 9, loss 1.4237, accuracy 0.660156, precision 0.4606741573033708, recall 0.5125
2019-03-03T15:00:01.864296: step 10, loss 1.11337, accuracy 0.6875, precision 0.632183908045977, recall 0.5339805825242718
2019-03-03T15:00:02.177459: step 11, loss 0.942255, accuracy 0.71875, precision 0.7, recall 0.5833333333333334
2019-03-03T15:00:02.474664: step 12, loss 1.38277, accuracy 0.648438, precision 0.6195652173913043, recall 0.5089285714285714
2019-03-03T15:00:02.783838: step 13, loss 1.36197, accuracy 0.644531, precision 0.7692307692307693, recall 0.54421768707483
2019-03-03T15:00:03.071070: step 14, loss 1.29897, accuracy 0.652344, precision 0.7037037037037037, recall 0.5714285714285714
2019-03-03T15:00:03.412159: step 15, loss 1.5181, accuracy 0.6875, precision 0.7444444444444445, recall 0.5403225806451613
2019-03-03T15:00:03.763220: step 16, loss 0.986445, accuracy 0.730469, precision 0.7126436781609196, recall 0.5849056603773585
2019-03-03T15:00:03.956701: step 17, loss 1.25123, accuracy 0.717949, precision 0.6, recall 0.39473684210526316
2019-03-03T15:00:04.359626: step 18, loss 0.908114, accuracy 0.753906, precision 0.6363636363636364, recall 0.6436781609195402
2019-03-03T15:00:04.715674: step 19, loss 1.01104, accuracy 0.730469, precision 0.6428571428571429, recall 0.5806451612903226
2019-03-03T15:00:05.081695: step 20, loss 0.689661, accuracy 0.773438, precision 0.686046511627907, recall 0.6555555555555556
2019-03-03T15:00:05.446719: step 21, loss 0.797807, accuracy 0.761719, precision 0.6407766990291263, recall 0.7333333333333333
2019-03-03T15:00:05.787807: step 22, loss 0.71322, accuracy 0.785156, precision 0.6981132075471698, recall 0.7628865979381443
2019-03-03T15:00:06.141861: step 23, loss 0.79819, accuracy 0.789062, precision 0.6483516483516484, recall 0.7283950617283951
2019-03-03T15:00:06.518854: step 24, loss 0.879326, accuracy 0.753906, precision 0.5348837209302325, recall 0.6666666666666666
2019-03-03T15:00:06.984609: step 25, loss 1.09754, accuracy 0.746094, precision 0.6, recall 0.6785714285714286
2019-03-03T15:00:07.515188: step 26, loss 0.72514, accuracy 0.808594, precision 0.6395348837209303, recall 0.7534246575342466
2019-03-03T15:00:08.022831: step 27, loss 0.850155, accuracy 0.769531, precision 0.5862068965517241, recall 0.6891891891891891
2019-03-03T15:00:08.410794: step 28, loss 0.846526, accuracy 0.765625, precision 0.6145833333333334, recall 0.7195121951219512
2019-03-03T15:00:08.813717: step 29, loss 0.762712, accuracy 0.773438, precision 0.6666666666666666, recall 0.6666666666666666
2019-03-03T15:00:09.195696: step 30, loss 0.853546, accuracy 0.785156, precision 0.6521739130434783, recall 0.7228915662650602
2019-03-03T15:00:09.545759: step 31, loss 0.978839, accuracy 0.730469, precision 0.6095238095238096, recall 0.6956521739130435
2019-03-03T15:00:09.880863: step 32, loss 0.631133, accuracy 0.800781, precision 0.6703296703296703, recall 0.7439024390243902
2019-03-03T15:00:10.329662: step 33, loss 0.87255, accuracy 0.765625, precision 0.6966292134831461, recall 0.6526315789473685
2019-03-03T15:00:10.518159: step 34, loss 0.531437, accuracy 0.846154, precision 0.8205128205128205, recall 0.7441860465116279
2019-03-03T15:00:10.874207: step 35, loss 0.611227, accuracy 0.816406, precision 0.8315789473684211, recall 0.7181818181818181
2019-03-03T15:00:11.202331: step 36, loss 0.563815, accuracy 0.796875, precision 0.8076923076923077, recall 0.63
2019-03-03T15:00:11.572340: step 37, loss 0.904546, accuracy 0.773438, precision 0.7021276595744681, recall 0.6875
2019-03-03T15:00:11.910437: step 38, loss 0.628779, accuracy 0.792969, precision 0.7857142857142857, recall 0.7064220183486238
2019-03-03T15:00:12.254517: step 39, loss 0.739356, accuracy 0.828125, precision 0.8586956521739131, recall 0.7181818181818181
2019-03-03T15:00:12.592613: step 40, loss 0.753757, accuracy 0.808594, precision 0.7840909090909091, recall 0.696969696969697
2019-03-03T15:00:12.917744: step 41, loss 0.626701, accuracy 0.804688, precision 0.7052631578947368, recall 0.7528089887640449
2019-03-03T15:00:13.277781: step 42, loss 0.689937, accuracy 0.808594, precision 0.75, recall 0.7575757575757576
2019-03-03T15:00:13.612885: step 43, loss 0.548457, accuracy 0.847656, precision 0.7586206896551724, recall 0.7857142857142857
2019-03-03T15:00:13.984891: step 44, loss 0.69804, accuracy 0.835938, precision 0.7608695652173914, recall 0.7777777777777778
2019-03-03T15:00:14.337948: step 45, loss 0.618345, accuracy 0.820312, precision 0.7216494845360825, recall 0.7865168539325843
2019-03-03T15:00:14.686016: step 46, loss 0.677288, accuracy 0.785156, precision 0.5853658536585366, recall 0.6956521739130435
2019-03-03T15:00:15.015136: step 47, loss 0.591776, accuracy 0.789062, precision 0.6470588235294118, recall 0.7857142857142857
2019-03-03T15:00:15.366198: step 48, loss 0.601166, accuracy 0.851562, precision 0.7604166666666666, recall 0.8295454545454546
2019-03-03T15:00:15.709280: step 49, loss 0.583249, accuracy 0.824219, precision 0.6904761904761905, recall 0.7532467532467533
2019-03-03T15:00:16.020449: step 50, loss 0.541334, accuracy 0.816406, precision 0.6823529411764706, recall 0.7435897435897436
2019-03-03T15:00:16.179025: step 51, loss 0.397512, accuracy 0.880342, precision 0.8260869565217391, recall 0.8636363636363636
2019-03-03T15:00:16.494182: step 52, loss 0.574999, accuracy 0.808594, precision 0.7272727272727273, recall 0.7191011235955056
2019-03-03T15:00:16.861200: step 53, loss 0.648482, accuracy 0.816406, precision 0.7291666666666666, recall 0.7692307692307693
2019-03-03T15:00:17.216252: step 54, loss 0.613838, accuracy 0.828125, precision 0.8444444444444444, recall 0.7169811320754716
2019-03-03T15:00:17.560331: step 55, loss 0.490436, accuracy 0.832031, precision 0.8275862068965517, recall 0.72
2019-03-03T15:00:17.891447: step 56, loss 0.355727, accuracy 0.898438, precision 0.9032258064516129, recall 0.8316831683168316
2019-03-03T15:00:18.237520: step 57, loss 0.491347, accuracy 0.847656, precision 0.813953488372093, recall 0.7526881720430108
2019-03-03T15:00:18.586587: step 58, loss 0.462754, accuracy 0.820312, precision 0.75, recall 0.75
2019-03-03T15:00:18.932661: step 59, loss 0.510013, accuracy 0.84375, precision 0.8152173913043478, recall 0.7653061224489796
2019-03-03T15:00:19.282726: step 60, loss 0.683897, accuracy 0.8125, precision 0.7474747474747475, recall 0.7628865979381443
2019-03-03T15:00:19.681659: step 61, loss 0.523877, accuracy 0.867188, precision 0.7831325301204819, recall 0.8024691358024691
2019-03-03T15:00:20.700175: step 62, loss 0.473898, accuracy 0.859375, precision 0.7872340425531915, recall 0.8222222222222222
2019-03-03T15:00:22.095908: step 63, loss 0.702331, accuracy 0.816406, precision 0.7525773195876289, recall 0.7604166666666666
2019-03-03T15:00:23.513944: step 64, loss 0.512665, accuracy 0.84375, precision 0.7752808988764045, recall 0.7752808988764045
2019-03-03T15:00:25.021745: step 65, loss 0.404927, accuracy 0.878906, precision 0.7816091954022989, recall 0.85
2019-03-03T15:00:26.218809: step 66, loss 0.475956, accuracy 0.851562, precision 0.7872340425531915, recall 0.8043478260869565
2019-03-03T15:00:27.247057: step 67, loss 0.673939, accuracy 0.828125, precision 0.75, recall 0.8125
2019-03-03T15:00:27.778635: step 68, loss 0.584123, accuracy 0.811966, precision 0.725, recall 0.725
2019-03-03T15:00:28.610409: step 69, loss 0.538449, accuracy 0.832031, precision 0.7752808988764045, recall 0.75
2019-03-03T15:00:29.388330: step 70, loss 0.41812, accuracy 0.855469, precision 0.7625, recall 0.7721518987341772
2019-03-03T15:00:30.207144: step 71, loss 0.464722, accuracy 0.859375, precision 0.8229166666666666, recall 0.8061224489795918
2019-03-03T15:00:30.903304: step 72, loss 0.351803, accuracy 0.871094, precision 0.8383838383838383, recall 0.83
2019-03-03T15:00:31.610414: step 73, loss 0.755514, accuracy 0.796875, precision 0.6395348837209303, recall 0.7236842105263158
2019-03-03T15:00:32.233747: step 74, loss 0.366346, accuracy 0.875, precision 0.8351648351648352, recall 0.8172043010752689
2019-03-03T15:00:32.918212: step 75, loss 0.452736, accuracy 0.839844, precision 0.79, recall 0.797979797979798
2019-03-03T15:00:33.558013: step 76, loss 0.431216, accuracy 0.855469, precision 0.8673469387755102, recall 0.7798165137614679
2019-03-03T15:00:34.208929: step 77, loss 0.574944, accuracy 0.824219, precision 0.7473684210526316, recall 0.7717391304347826
2019-03-03T15:00:34.802190: step 78, loss 0.455851, accuracy 0.851562, precision 0.7640449438202247, recall 0.8
2019-03-03T15:00:35.374656: step 79, loss 0.460908, accuracy 0.851562, precision 0.8352941176470589, recall 0.7473684210526316
2019-03-03T15:00:35.970863: step 80, loss 0.506021, accuracy 0.8125, precision 0.7333333333333333, recall 0.7333333333333333
2019-03-03T15:00:36.541319: step 81, loss 0.604923, accuracy 0.851562, precision 0.8105263157894737, recall 0.7938144329896907
2019-03-03T15:00:37.044486: step 82, loss 0.35598, accuracy 0.882812, precision 0.7956989247311828, recall 0.8705882352941177
2019-03-03T15:00:37.547145: step 83, loss 0.479201, accuracy 0.867188, precision 0.84, recall 0.8235294117647058
2019-03-03T15:00:38.108175: step 84, loss 0.323356, accuracy 0.886719, precision 0.8024691358024691, recall 0.8333333333333334
2019-03-03T15:00:38.396407: step 85, loss 0.270322, accuracy 0.923077, precision 0.8181818181818182, recall 0.972972972972973
2019-03-03T15:00:38.965218: step 86, loss 0.594652, accuracy 0.832031, precision 0.8037383177570093, recall 0.7962962962962963
2019-03-03T15:00:39.509509: step 87, loss 0.396583, accuracy 0.878906, precision 0.782608695652174, recall 0.8674698795180723
2019-03-03T15:00:40.087502: step 88, loss 0.389847, accuracy 0.882812, precision 0.8604651162790697, recall 0.8043478260869565
2019-03-03T15:00:40.601166: step 89, loss 0.38138, accuracy 0.863281, precision 0.8085106382978723, recall 0.8172043010752689
2019-03-03T15:00:41.115791: step 90, loss 0.380293, accuracy 0.886719, precision 0.8767123287671232, recall 0.7619047619047619
2019-03-03T15:00:41.612513: step 91, loss 0.470411, accuracy 0.851562, precision 0.8152173913043478, recall 0.78125
2019-03-03T15:00:42.174285: step 92, loss 0.39002, accuracy 0.867188, precision 0.7792207792207793, recall 0.7792207792207793
2019-03-03T15:00:42.702251: step 93, loss 0.480687, accuracy 0.871094, precision 0.8229166666666666, recall 0.8315789473684211
2019-03-03T15:00:43.189959: step 94, loss 0.42553, accuracy 0.847656, precision 0.7802197802197802, recall 0.7888888888888889
2019-03-03T15:00:43.675660: step 95, loss 0.410061, accuracy 0.878906, precision 0.8686868686868687, recall 0.8269230769230769
2019-03-03T15:00:44.184300: step 96, loss 0.406761, accuracy 0.875, precision 0.8125, recall 0.8478260869565217
2019-03-03T15:00:44.661563: step 97, loss 0.443862, accuracy 0.863281, precision 0.776595744680851, recall 0.8390804597701149
2019-03-03T15:00:45.146267: step 98, loss 0.320404, accuracy 0.910156, precision 0.8414634146341463, recall 0.8734177215189873
2019-03-03T15:00:45.603557: step 99, loss 0.409409, accuracy 0.851562, precision 0.7127659574468085, recall 0.8589743589743589
2019-03-03T15:00:46.151117: step 100, loss 0.283402, accuracy 0.898438, precision 0.8020833333333334, recall 0.9166666666666666
2019-03-03T15:00:46.620861: step 101, loss 0.429121, accuracy 0.851562, precision 0.78125, recall 0.8152173913043478
2019-03-03T15:00:46.845265: step 102, loss 0.484393, accuracy 0.854701, precision 0.8478260869565217, recall 0.7959183673469388
2019-03-03T15:00:47.307027: step 103, loss 0.382054, accuracy 0.863281, precision 0.8295454545454546, recall 0.7849462365591398
2019-03-03T15:00:47.765802: step 104, loss 0.485248, accuracy 0.851562, precision 0.8, recall 0.7228915662650602
2019-03-03T15:00:48.240531: step 105, loss 0.31351, accuracy 0.90625, precision 0.8854166666666666, recall 0.8673469387755102
2019-03-03T15:00:48.686365: step 106, loss 0.269421, accuracy 0.914062, precision 0.9278350515463918, recall 0.8571428571428571
2019-03-03T15:00:49.106772: step 107, loss 0.260813, accuracy 0.898438, precision 0.9148936170212766, recall 0.8269230769230769
2019-03-03T15:00:49.537134: step 108, loss 0.331049, accuracy 0.886719, precision 0.8613861386138614, recall 0.8529411764705882
2019-03-03T15:00:50.010865: step 109, loss 0.247025, accuracy 0.914062, precision 0.8571428571428571, recall 0.896551724137931
2019-03-03T15:00:50.431255: step 110, loss 0.294603, accuracy 0.890625, precision 0.813953488372093, recall 0.8536585365853658
2019-03-03T15:00:50.861051: step 111, loss 0.276591, accuracy 0.894531, precision 0.8681318681318682, recall 0.8404255319148937
2019-03-03T15:00:51.282438: step 112, loss 0.418077, accuracy 0.851562, precision 0.8144329896907216, recall 0.797979797979798
2019-03-03T15:00:51.735226: step 113, loss 0.366417, accuracy 0.894531, precision 0.797752808988764, recall 0.8875
2019-03-03T15:00:52.237901: step 114, loss 0.359498, accuracy 0.894531, precision 0.8571428571428571, recall 0.8275862068965517
2019-03-03T15:00:52.740945: step 115, loss 0.259089, accuracy 0.90625, precision 0.8068181818181818, recall 0.9102564102564102
2019-03-03T15:00:53.325678: step 116, loss 0.297142, accuracy 0.898438, precision 0.8349514563106796, recall 0.9052631578947369
2019-03-03T15:00:53.909118: step 117, loss 0.346733, accuracy 0.871094, precision 0.78125, recall 0.8620689655172413
2019-03-03T15:00:54.571349: step 118, loss 0.374616, accuracy 0.890625, precision 0.7741935483870968, recall 0.9113924050632911
2019-03-03T15:00:54.845613: step 119, loss 0.281723, accuracy 0.923077, precision 0.9285714285714286, recall 0.8666666666666667
2019-03-03T15:00:55.592618: step 120, loss 0.377813, accuracy 0.875, precision 0.7848101265822784, recall 0.8051948051948052
2019-03-03T15:00:56.119208: step 121, loss 0.250611, accuracy 0.917969, precision 0.8888888888888888, recall 0.8791208791208791
2019-03-03T15:00:56.607208: step 122, loss 0.334493, accuracy 0.882812, precision 0.8854166666666666, recall 0.8173076923076923
2019-03-03T15:00:57.042049: step 123, loss 0.218028, accuracy 0.933594, precision 0.9117647058823529, recall 0.9207920792079208
2019-03-03T15:00:57.543217: step 124, loss 0.277685, accuracy 0.914062, precision 0.8961038961038961, recall 0.8313253012048193
2019-03-03T15:00:58.129834: step 125, loss 0.257612, accuracy 0.890625, precision 0.88, recall 0.8461538461538461
2019-03-03T15:00:58.634776: step 126, loss 0.280575, accuracy 0.914062, precision 0.8315789473684211, recall 0.9294117647058824
2019-03-03T15:00:59.130451: step 127, loss 0.358251, accuracy 0.886719, precision 0.8829787234042553, recall 0.8217821782178217
2019-03-03T15:00:59.629116: step 128, loss 0.328068, accuracy 0.890625, precision 0.8490566037735849, recall 0.8823529411764706
2019-03-03T15:01:00.079911: step 129, loss 0.227846, accuracy 0.917969, precision 0.8488372093023255, recall 0.9012345679012346
2019-03-03T15:01:00.584562: step 130, loss 0.347826, accuracy 0.882812, precision 0.8526315789473684, recall 0.8350515463917526
2019-03-03T15:01:01.035896: step 131, loss 0.270219, accuracy 0.90625, precision 0.8541666666666666, recall 0.8913043478260869
2019-03-03T15:01:01.521597: step 132, loss 0.240311, accuracy 0.886719, precision 0.8804347826086957, recall 0.8181818181818182
2019-03-03T15:01:01.948456: step 133, loss 0.249388, accuracy 0.90625, precision 0.8461538461538461, recall 0.8461538461538461
2019-03-03T15:01:02.390276: step 134, loss 0.193858, accuracy 0.921875, precision 0.8409090909090909, recall 0.925
2019-03-03T15:01:02.857062: step 135, loss 0.301017, accuracy 0.898438, precision 0.8404255319148937, recall 0.8777777777777778
2019-03-03T15:01:03.096423: step 136, loss 0.319467, accuracy 0.871795, precision 0.813953488372093, recall 0.8333333333333334
2019-03-03T15:01:03.545448: step 137, loss 0.350509, accuracy 0.875, precision 0.7471264367816092, recall 0.8666666666666667
2019-03-03T15:01:03.996237: step 138, loss 0.236467, accuracy 0.917969, precision 0.8777777777777778, recall 0.8876404494382022
2019-03-03T15:01:04.445038: step 139, loss 0.16629, accuracy 0.929688, precision 0.9021739130434783, recall 0.9021739130434783
2019-03-03T15:01:04.957676: step 140, loss 0.293639, accuracy 0.90625, precision 0.9010989010989011, recall 0.845360824742268
2019-03-03T15:01:05.568037: step 141, loss 0.272243, accuracy 0.914062, precision 0.8514851485148515, recall 0.9247311827956989
2019-03-03T15:01:06.058729: step 142, loss 0.320067, accuracy 0.890625, precision 0.8555555555555555, recall 0.8369565217391305
2019-03-03T15:01:06.521000: step 143, loss 0.281075, accuracy 0.902344, precision 0.9047619047619048, recall 0.8172043010752689
2019-03-03T15:01:07.008208: step 144, loss 0.267579, accuracy 0.921875, precision 0.927710843373494, recall 0.8461538461538461
2019-03-03T15:01:07.443822: step 145, loss 0.271853, accuracy 0.914062, precision 0.8958333333333334, recall 0.8775510204081632
2019-03-03T15:01:07.938905: step 146, loss 0.331146, accuracy 0.871094, precision 0.8095238095238095, recall 0.8
2019-03-03T15:01:08.464500: step 147, loss 0.309128, accuracy 0.882812, precision 0.819047619047619, recall 0.8865979381443299
2019-03-03T15:01:08.913300: step 148, loss 0.315507, accuracy 0.894531, precision 0.7816091954022989, recall 0.8947368421052632
2019-03-03T15:01:09.352133: step 149, loss 0.249518, accuracy 0.917969, precision 0.8787878787878788, recall 0.90625
2019-03-03T15:01:09.786477: step 150, loss 0.286447, accuracy 0.894531, precision 0.8444444444444444, recall 0.8539325842696629
2019-03-03T15:01:10.181993: step 151, loss 0.318387, accuracy 0.894531, precision 0.8709677419354839, recall 0.84375
2019-03-03T15:01:10.658720: step 152, loss 0.283192, accuracy 0.894531, precision 0.8369565217391305, recall 0.8651685393258427
2019-03-03T15:01:10.853556: step 153, loss 0.211412, accuracy 0.888889, precision 0.9574468085106383, recall 0.8035714285714286
2019-03-03T15:01:11.237527: step 154, loss 0.190446, accuracy 0.914062, precision 0.8865979381443299, recall 0.8865979381443299
2019-03-03T15:01:11.614035: step 155, loss 0.162578, accuracy 0.925781, precision 0.8901098901098901, recall 0.9
2019-03-03T15:01:11.984046: step 156, loss 0.26671, accuracy 0.914062, precision 0.8260869565217391, recall 0.926829268292683
2019-03-03T15:01:12.408910: step 157, loss 0.14758, accuracy 0.929688, precision 0.8876404494382022, recall 0.9080459770114943
2019-03-03T15:01:12.795394: step 158, loss 0.177061, accuracy 0.9375, precision 0.9024390243902439, recall 0.9024390243902439
2019-03-03T15:01:13.223766: step 159, loss 0.26478, accuracy 0.902344, precision 0.8586956521739131, recall 0.8681318681318682
2019-03-03T15:01:13.650035: step 160, loss 0.229198, accuracy 0.917969, precision 0.9411764705882353, recall 0.8333333333333334
2019-03-03T15:01:14.055741: step 161, loss 0.22408, accuracy 0.898438, precision 0.8505747126436781, recall 0.8505747126436781
2019-03-03T15:01:14.510686: step 162, loss 0.236526, accuracy 0.902344, precision 0.8297872340425532, recall 0.896551724137931
2019-03-03T15:01:14.928597: step 163, loss 0.239491, accuracy 0.917969, precision 0.8723404255319149, recall 0.9010989010989011
2019-03-03T15:01:15.327529: step 164, loss 0.268058, accuracy 0.90625, precision 0.851063829787234, recall 0.8888888888888888
2019-03-03T15:01:15.725979: step 165, loss 0.335221, accuracy 0.902344, precision 0.89, recall 0.8640776699029126
2019-03-03T15:01:16.140862: step 166, loss 0.221757, accuracy 0.921875, precision 0.8522727272727273, recall 0.9146341463414634
2019-03-03T15:01:16.554758: step 167, loss 0.23731, accuracy 0.914062, precision 0.91, recall 0.875
2019-03-03T15:01:16.953689: step 168, loss 0.187978, accuracy 0.917969, precision 0.8764044943820225, recall 0.8863636363636364
2019-03-03T15:01:17.331198: step 169, loss 0.238507, accuracy 0.902344, precision 0.8556701030927835, recall 0.8829787234042553
2019-03-03T15:01:17.537646: step 170, loss 0.252605, accuracy 0.923077, precision 0.875, recall 0.8974358974358975
2019-03-03T15:01:17.963271: step 171, loss 0.23835, accuracy 0.929688, precision 0.9375, recall 0.8522727272727273
2019-03-03T15:01:18.389222: step 172, loss 0.21188, accuracy 0.929688, precision 0.9375, recall 0.8823529411764706
2019-03-03T15:01:18.801474: step 173, loss 0.119937, accuracy 0.960938, precision 0.9176470588235294, recall 0.9629629629629629
2019-03-03T15:01:19.205394: step 174, loss 0.0959527, accuracy 0.964844, precision 0.9620253164556962, recall 0.926829268292683
2019-03-03T15:01:19.625545: step 175, loss 0.19988, accuracy 0.921875, precision 0.8791208791208791, recall 0.898876404494382
2019-03-03T15:01:20.038442: step 176, loss 0.103969, accuracy 0.957031, precision 0.9325842696629213, recall 0.9431818181818182
2019-03-03T15:01:20.485758: step 177, loss 0.222942, accuracy 0.90625, precision 0.8876404494382022, recall 0.8494623655913979
2019-03-03T15:01:20.854770: step 178, loss 0.213887, accuracy 0.921875, precision 0.9042553191489362, recall 0.8854166666666666
2019-03-03T15:01:21.243243: step 179, loss 0.207739, accuracy 0.933594, precision 0.9642857142857143, recall 0.8526315789473684
2019-03-03T15:01:21.623229: step 180, loss 0.252229, accuracy 0.910156, precision 0.8375, recall 0.8701298701298701
2019-03-03T15:01:22.041174: step 181, loss 0.238725, accuracy 0.945312, precision 0.8602150537634409, recall 0.9876543209876543
2019-03-03T15:01:22.420160: step 182, loss 0.195731, accuracy 0.933594, precision 0.900990099009901, recall 0.9285714285714286
2019-03-03T15:01:22.787179: step 183, loss 0.223689, accuracy 0.925781, precision 0.8725490196078431, recall 0.9368421052631579
2019-03-03T15:01:23.144225: step 184, loss 0.17282, accuracy 0.925781, precision 0.86, recall 0.945054945054945
2019-03-03T15:01:23.509321: step 185, loss 0.246701, accuracy 0.910156, precision 0.8245614035087719, recall 0.9690721649484536
2019-03-03T15:01:23.892507: step 186, loss 0.239176, accuracy 0.921875, precision 0.8913043478260869, recall 0.8913043478260869
2019-03-03T15:01:24.076017: step 187, loss 0.230886, accuracy 0.931624, precision 0.9523809523809523, recall 0.8695652173913043
2019-03-03T15:01:24.468966: step 188, loss 0.115219, accuracy 0.957031, precision 0.9, recall 0.96
2019-03-03T15:01:24.825015: step 189, loss 0.14621, accuracy 0.933594, precision 0.9139784946236559, recall 0.9042553191489362
2019-03-03T15:01:25.224008: step 190, loss 0.168079, accuracy 0.941406, precision 0.967391304347826, recall 0.8811881188118812
2019-03-03T15:01:25.553128: step 191, loss 0.151137, accuracy 0.949219, precision 0.9375, recall 0.9278350515463918
2019-03-03T15:01:25.921144: step 192, loss 0.199511, accuracy 0.929688, precision 0.9204545454545454, recall 0.8804347826086957
2019-03-03T15:01:26.275197: step 193, loss 0.222489, accuracy 0.914062, precision 0.8947368421052632, recall 0.8762886597938144
2019-03-03T15:01:26.610324: step 194, loss 0.19063, accuracy 0.925781, precision 0.9157894736842105, recall 0.8877551020408163
2019-03-03T15:01:26.942436: step 195, loss 0.214746, accuracy 0.921875, precision 0.8842105263157894, recall 0.9032258064516129
2019-03-03T15:01:27.276545: step 196, loss 0.142467, accuracy 0.945312, precision 0.9387755102040817, recall 0.92
2019-03-03T15:01:27.639576: step 197, loss 0.17061, accuracy 0.945312, precision 0.9405940594059405, recall 0.9223300970873787
2019-03-03T15:01:28.005111: step 198, loss 0.200152, accuracy 0.9375, precision 0.9381443298969072, recall 0.900990099009901
2019-03-03T15:01:28.388132: step 199, loss 0.133499, accuracy 0.941406, precision 0.8947368421052632, recall 0.9444444444444444
2019-03-03T15:01:28.772129: step 200, loss 0.126822, accuracy 0.945312, precision 0.9397590361445783, recall 0.896551724137931
2019-03-03T15:01:29.166075: step 201, loss 0.143518, accuracy 0.953125, precision 0.9166666666666666, recall 0.9390243902439024
2019-03-03T15:01:29.521144: step 202, loss 0.204829, accuracy 0.917969, precision 0.8723404255319149, recall 0.9010989010989011
2019-03-03T15:01:29.882179: step 203, loss 0.132925, accuracy 0.941406, precision 0.9456521739130435, recall 0.8969072164948454
2019-03-03T15:01:30.071187: step 204, loss 0.19397, accuracy 0.923077, precision 0.8787878787878788, recall 0.8529411764705882
2019-03-03T15:01:30.506024: step 205, loss 0.172679, accuracy 0.914062, precision 0.84, recall 0.9333333333333333
2019-03-03T15:01:30.885030: step 206, loss 0.208826, accuracy 0.941406, precision 0.9473684210526315, recall 0.9
2019-03-03T15:01:31.230664: step 207, loss 0.154471, accuracy 0.957031, precision 0.9387755102040817, recall 0.9484536082474226
2019-03-03T15:01:31.571261: step 208, loss 0.12049, accuracy 0.949219, precision 0.9186046511627907, recall 0.9294117647058824
2019-03-03T15:01:31.921334: step 209, loss 0.140643, accuracy 0.9375, precision 0.8829787234042553, recall 0.9431818181818182
2019-03-03T15:01:32.278381: step 210, loss 0.127232, accuracy 0.945312, precision 0.925531914893617, recall 0.925531914893617
2019-03-03T15:01:32.649471: step 211, loss 0.200238, accuracy 0.914062, precision 0.8532110091743119, recall 0.9393939393939394
2019-03-03T15:01:33.020989: step 212, loss 0.18941, accuracy 0.933594, precision 0.8953488372093024, recall 0.9058823529411765
2019-03-03T15:01:33.406957: step 213, loss 0.202024, accuracy 0.921875, precision 0.8586956521739131, recall 0.9186046511627907
2019-03-03T15:01:33.827833: step 214, loss 0.213004, accuracy 0.921875, precision 0.8829787234042553, recall 0.9021739130434783
2019-03-03T15:01:34.211039: step 215, loss 0.120402, accuracy 0.949219, precision 0.9397590361445783, recall 0.9069767441860465
2019-03-03T15:01:34.603498: step 216, loss 0.139162, accuracy 0.941406, precision 0.9438202247191011, recall 0.8936170212765957
2019-03-03T15:01:35.047313: step 217, loss 0.176339, accuracy 0.933594, precision 0.9647058823529412, recall 0.8541666666666666
2019-03-03T15:01:35.437270: step 218, loss 0.198436, accuracy 0.9375, precision 0.9418604651162791, recall 0.8804347826086957
2019-03-03T15:01:35.850194: step 219, loss 0.183121, accuracy 0.929688, precision 0.9166666666666666, recall 0.875
2019-03-03T15:01:36.240666: step 220, loss 0.212066, accuracy 0.921875, precision 0.9175257731958762, recall 0.8811881188118812
2019-03-03T15:01:36.429161: step 221, loss 0.123677, accuracy 0.965812, precision 0.9487179487179487, recall 0.9487179487179487
2019-03-03T15:01:36.826100: step 222, loss 0.108304, accuracy 0.960938, precision 0.9186046511627907, recall 0.9634146341463414
2019-03-03T15:01:37.246973: step 223, loss 0.140083, accuracy 0.953125, precision 0.9411764705882353, recall 0.9195402298850575
2019-03-03T15:01:37.625490: step 224, loss 0.138228, accuracy 0.949219, precision 0.875, recall 0.958904109589041
2019-03-03T15:01:37.999491: step 225, loss 0.203568, accuracy 0.929688, precision 0.8202247191011236, recall 0.9733333333333334
2019-03-03T15:01:38.366533: step 226, loss 0.144589, accuracy 0.929688, precision 0.8488372093023255, recall 0.9358974358974359
2019-03-03T15:01:38.743523: step 227, loss 0.201142, accuracy 0.929688, precision 0.84375, recall 0.9642857142857143
2019-03-03T15:01:39.132012: step 228, loss 0.0964475, accuracy 0.964844, precision 0.9468085106382979, recall 0.956989247311828
2019-03-03T15:01:39.522968: step 229, loss 0.225383, accuracy 0.914062, precision 0.8823529411764706, recall 0.9
2019-03-03T15:01:39.907961: step 230, loss 0.0944232, accuracy 0.960938, precision 0.9347826086956522, recall 0.9555555555555556
2019-03-03T15:01:40.292850: step 231, loss 0.156692, accuracy 0.941406, precision 0.9166666666666666, recall 0.9263157894736842
2019-03-03T15:01:40.683803: step 232, loss 0.235034, accuracy 0.925781, precision 0.9777777777777777, recall 0.8380952380952381
2019-03-03T15:01:41.074271: step 233, loss 0.116463, accuracy 0.957031, precision 0.9484536082474226, recall 0.9387755102040817
2019-03-03T15:01:41.433387: step 234, loss 0.144517, accuracy 0.949219, precision 0.9895833333333334, recall 0.8878504672897196
2019-03-03T15:01:41.761605: step 235, loss 0.108202, accuracy 0.933594, precision 0.9587628865979382, recall 0.8773584905660378
2019-03-03T15:01:42.132883: step 236, loss 0.122394, accuracy 0.957031, precision 0.9278350515463918, recall 0.9574468085106383
2019-03-03T15:01:42.461029: step 237, loss 0.114269, accuracy 0.964844, precision 0.9431818181818182, recall 0.9540229885057471
2019-03-03T15:01:42.635071: step 238, loss 0.056703, accuracy 0.991453, precision 0.975, recall 1.0
2019-03-03T15:01:42.972170: step 239, loss 0.0753321, accuracy 0.96875, precision 0.9545454545454546, recall 0.9545454545454546
2019-03-03T15:01:43.332208: step 240, loss 0.122355, accuracy 0.953125, precision 0.918918918918919, recall 0.918918918918919
2019-03-03T15:01:43.729197: step 241, loss 0.126277, accuracy 0.960938, precision 0.9512195121951219, recall 0.9285714285714286
2019-03-03T15:01:44.136332: step 242, loss 0.131425, accuracy 0.941406, precision 0.8777777777777778, recall 0.9518072289156626
2019-03-03T15:01:44.577195: step 243, loss 0.176882, accuracy 0.929688, precision 0.8709677419354839, recall 0.9310344827586207
2019-03-03T15:01:44.942744: step 244, loss 0.144364, accuracy 0.945312, precision 0.8673469387755102, recall 0.9883720930232558
2019-03-03T15:01:45.313288: step 245, loss 0.0584883, accuracy 0.964844, precision 0.9146341463414634, recall 0.974025974025974
2019-03-03T15:01:45.694268: step 246, loss 0.182418, accuracy 0.9375, precision 0.8969072164948454, recall 0.9354838709677419
2019-03-03T15:01:46.072258: step 247, loss 0.0882617, accuracy 0.957031, precision 0.9247311827956989, recall 0.9555555555555556
2019-03-03T15:01:46.462216: step 248, loss 0.124893, accuracy 0.960938, precision 0.9607843137254902, recall 0.9423076923076923
2019-03-03T15:01:46.848183: step 249, loss 0.183743, accuracy 0.949219, precision 0.9545454545454546, recall 0.9032258064516129
2019-03-03T15:01:47.258088: step 250, loss 0.110575, accuracy 0.949219, precision 0.9494949494949495, recall 0.9215686274509803

Evaluation:
[[152  30]
 [ 16 270]]
2019-03-03T15:01:47.645055: step 250, loss 0.313929, accuracy 0.901709, precision 0.8351648351648352, recall 0.9047619047619048

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551646798\checkpoints\model-250

2019-03-03T15:01:48.215349: step 251, loss 0.0867523, accuracy 0.957031, precision 0.9333333333333333, recall 0.9607843137254902
2019-03-03T15:01:48.593342: step 252, loss 0.111591, accuracy 0.953125, precision 0.9595959595959596, recall 0.9223300970873787
2019-03-03T15:01:48.939272: step 253, loss 0.104419, accuracy 0.957031, precision 0.945054945054945, recall 0.9347826086956522
2019-03-03T15:01:49.297311: step 254, loss 0.0937329, accuracy 0.960938, precision 0.9347826086956522, recall 0.9555555555555556
2019-03-03T15:01:49.469851: step 255, loss 0.102549, accuracy 0.940171, precision 0.9473684210526315, recall 0.8780487804878049
2019-03-03T15:01:49.803015: step 256, loss 0.0949491, accuracy 0.953125, precision 0.9473684210526315, recall 0.9278350515463918
2019-03-03T15:01:50.132649: step 257, loss 0.0738911, accuracy 0.976562, precision 0.9611650485436893, recall 0.9801980198019802
2019-03-03T15:01:50.475733: step 258, loss 0.173543, accuracy 0.929688, precision 0.9113924050632911, recall 0.8674698795180723
2019-03-03T15:01:50.831780: step 259, loss 0.0915274, accuracy 0.957031, precision 0.9342105263157895, recall 0.922077922077922
2019-03-03T15:01:51.184836: step 260, loss 0.202647, accuracy 0.945312, precision 0.9238095238095239, recall 0.941747572815534
2019-03-03T15:01:51.534900: step 261, loss 0.114611, accuracy 0.953125, precision 0.9315068493150684, recall 0.9066666666666666
2019-03-03T15:01:51.906906: step 262, loss 0.152457, accuracy 0.945312, precision 0.9354838709677419, recall 0.9157894736842105
2019-03-03T15:01:52.251496: step 263, loss 0.0859229, accuracy 0.972656, precision 0.9560439560439561, recall 0.9666666666666667
2019-03-03T15:01:52.623502: step 264, loss 0.0686091, accuracy 0.96875, precision 0.9310344827586207, recall 0.9759036144578314
2019-03-03T15:01:52.982544: step 265, loss 0.149003, accuracy 0.949219, precision 0.9117647058823529, recall 0.9587628865979382
2019-03-03T15:01:53.328654: step 266, loss 0.162339, accuracy 0.929688, precision 0.8709677419354839, recall 0.9310344827586207
2019-03-03T15:01:53.667748: step 267, loss 0.149478, accuracy 0.933594, precision 0.90625, recall 0.9157894736842105
2019-03-03T15:01:53.999884: step 268, loss 0.0930885, accuracy 0.960938, precision 0.9117647058823529, recall 0.9893617021276596
2019-03-03T15:01:54.389160: step 269, loss 0.123751, accuracy 0.957031, precision 0.8936170212765957, recall 0.9882352941176471
2019-03-03T15:01:54.727255: step 270, loss 0.11388, accuracy 0.957031, precision 0.9166666666666666, recall 0.9506172839506173
2019-03-03T15:01:55.119207: step 271, loss 0.0786607, accuracy 0.96875, precision 0.9494949494949495, recall 0.9690721649484536
2019-03-03T15:01:55.326653: step 272, loss 0.108249, accuracy 0.965812, precision 0.9487179487179487, recall 0.9487179487179487
2019-03-03T15:01:55.681704: step 273, loss 0.126688, accuracy 0.945312, precision 0.9615384615384616, recall 0.9090909090909091
2019-03-03T15:01:56.028312: step 274, loss 0.11876, accuracy 0.960938, precision 0.9651162790697675, recall 0.9222222222222223
2019-03-03T15:01:56.391342: step 275, loss 0.129159, accuracy 0.964844, precision 0.935064935064935, recall 0.9473684210526315
2019-03-03T15:01:56.752985: step 276, loss 0.111432, accuracy 0.957031, precision 0.9574468085106383, recall 0.9278350515463918
2019-03-03T15:01:57.109540: step 277, loss 0.144976, accuracy 0.945312, precision 0.9777777777777777, recall 0.88
2019-03-03T15:01:57.456612: step 278, loss 0.1463, accuracy 0.957031, precision 0.9680851063829787, recall 0.9191919191919192
2019-03-03T15:01:57.796307: step 279, loss 0.0877337, accuracy 0.957031, precision 0.9595959595959596, recall 0.9313725490196079
2019-03-03T15:01:58.135403: step 280, loss 0.0770823, accuracy 0.964844, precision 0.9770114942528736, recall 0.9239130434782609
2019-03-03T15:01:58.520771: step 281, loss 0.0862418, accuracy 0.972656, precision 0.9696969696969697, recall 0.96
2019-03-03T15:01:58.905952: step 282, loss 0.094755, accuracy 0.96875, precision 0.9285714285714286, recall 0.975
2019-03-03T15:01:59.334820: step 283, loss 0.0415303, accuracy 0.988281, precision 0.9696969696969697, recall 1.0
2019-03-03T15:01:59.789603: step 284, loss 0.108335, accuracy 0.941406, precision 0.9230769230769231, recall 0.9130434782608695
2019-03-03T15:02:00.173597: step 285, loss 0.0886266, accuracy 0.96875, precision 0.9247311827956989, recall 0.9885057471264368
2019-03-03T15:02:00.624899: step 286, loss 0.110693, accuracy 0.957031, precision 0.9411764705882353, recall 0.9302325581395349
2019-03-03T15:02:01.032808: step 287, loss 0.0954022, accuracy 0.960938, precision 0.9024390243902439, recall 0.9736842105263158
2019-03-03T15:02:01.452795: step 288, loss 0.0712024, accuracy 0.96875, precision 0.94, recall 0.9791666666666666
2019-03-03T15:02:01.668066: step 289, loss 0.145014, accuracy 0.948718, precision 0.9361702127659575, recall 0.9361702127659575
2019-03-03T15:02:02.047582: step 290, loss 0.149052, accuracy 0.957031, precision 0.9693877551020408, recall 0.9223300970873787
2019-03-03T15:02:02.455513: step 291, loss 0.0845684, accuracy 0.976562, precision 0.9887640449438202, recall 0.946236559139785
2019-03-03T15:02:02.869406: step 292, loss 0.0895456, accuracy 0.964844, precision 0.9702970297029703, recall 0.9423076923076923
2019-03-03T15:02:03.243406: step 293, loss 0.0782535, accuracy 0.972656, precision 0.9479166666666666, recall 0.978494623655914
2019-03-03T15:02:03.656302: step 294, loss 0.0706022, accuracy 0.976562, precision 0.9634146341463414, recall 0.9634146341463414
2019-03-03T15:02:04.067204: step 295, loss 0.123488, accuracy 0.957031, precision 0.9325842696629213, recall 0.9431818181818182
2019-03-03T15:02:04.487081: step 296, loss 0.111039, accuracy 0.960938, precision 0.9529411764705882, recall 0.9310344827586207
2019-03-03T15:02:04.898492: step 297, loss 0.0599343, accuracy 0.976562, precision 0.9767441860465116, recall 0.9545454545454546
2019-03-03T15:02:05.294476: step 298, loss 0.106005, accuracy 0.976562, precision 0.9545454545454546, recall 0.9767441860465116
2019-03-03T15:02:05.700391: step 299, loss 0.0439799, accuracy 0.984375, precision 0.9882352941176471, recall 0.9655172413793104
2019-03-03T15:02:06.108303: step 300, loss 0.0614194, accuracy 0.976562, precision 0.9529411764705882, recall 0.9759036144578314
2019-03-03T15:02:06.525185: step 301, loss 0.0800853, accuracy 0.972656, precision 0.9473684210526315, recall 0.9782608695652174
2019-03-03T15:02:06.918152: step 302, loss 0.0930062, accuracy 0.957031, precision 0.9473684210526315, recall 0.9375
2019-03-03T15:02:07.303647: step 303, loss 0.0781105, accuracy 0.953125, precision 0.9238095238095239, recall 0.9603960396039604
2019-03-03T15:02:07.709561: step 304, loss 0.0985785, accuracy 0.957031, precision 0.9230769230769231, recall 0.9545454545454546
2019-03-03T15:02:08.125450: step 305, loss 0.0528376, accuracy 0.984375, precision 0.9662921348314607, recall 0.9885057471264368
2019-03-03T15:02:08.325913: step 306, loss 0.0914466, accuracy 0.948718, precision 0.9230769230769231, recall 0.96
2019-03-03T15:02:08.731827: step 307, loss 0.0821044, accuracy 0.960938, precision 0.9803921568627451, recall 0.9259259259259259
2019-03-03T15:02:09.164284: step 308, loss 0.0570432, accuracy 0.972656, precision 0.9587628865979382, recall 0.96875
2019-03-03T15:02:09.609615: step 309, loss 0.087529, accuracy 0.960938, precision 0.963855421686747, recall 0.9195402298850575
2019-03-03T15:02:10.054424: step 310, loss 0.0532892, accuracy 0.980469, precision 0.9761904761904762, recall 0.9647058823529412
2019-03-03T15:02:10.518184: step 311, loss 0.09017, accuracy 0.976562, precision 0.9647058823529412, recall 0.9647058823529412
2019-03-03T15:02:10.939059: step 312, loss 0.063891, accuracy 0.96875, precision 0.9659090909090909, recall 0.9444444444444444
2019-03-03T15:02:11.355476: step 313, loss 0.0983291, accuracy 0.957031, precision 0.9404761904761905, recall 0.9294117647058824
2019-03-03T15:02:11.852327: step 314, loss 0.0583339, accuracy 0.96875, precision 0.9603960396039604, recall 0.9603960396039604
2019-03-03T15:02:12.316597: step 315, loss 0.117375, accuracy 0.964844, precision 0.926829268292683, recall 0.9620253164556962
2019-03-03T15:02:12.750459: step 316, loss 0.140806, accuracy 0.953125, precision 0.9578947368421052, recall 0.9191919191919192
2019-03-03T15:02:13.143408: step 317, loss 0.0751867, accuracy 0.976562, precision 0.9791666666666666, recall 0.9591836734693877
2019-03-03T15:02:13.510426: step 318, loss 0.0727484, accuracy 0.964844, precision 0.9489795918367347, recall 0.9587628865979382
2019-03-03T15:02:13.908291: step 319, loss 0.0475921, accuracy 0.984375, precision 0.967032967032967, recall 0.9887640449438202
2019-03-03T15:02:14.331595: step 320, loss 0.153601, accuracy 0.960938, precision 0.9230769230769231, recall 0.9655172413793104
2019-03-03T15:02:14.729306: step 321, loss 0.12624, accuracy 0.964844, precision 0.9555555555555556, recall 0.945054945054945
2019-03-03T15:02:15.090337: step 322, loss 0.111395, accuracy 0.96875, precision 0.956989247311828, recall 0.956989247311828
2019-03-03T15:02:15.255790: step 323, loss 0.181398, accuracy 0.931624, precision 0.8823529411764706, recall 0.9574468085106383
2019-03-03T15:02:15.609853: step 324, loss 0.0590994, accuracy 0.980469, precision 0.9775280898876404, recall 0.9666666666666667
2019-03-03T15:02:16.014266: step 325, loss 0.0598521, accuracy 0.984375, precision 0.9893617021276596, recall 0.96875
2019-03-03T15:02:16.390259: step 326, loss 0.059582, accuracy 0.976562, precision 0.9565217391304348, recall 0.9777777777777777
2019-03-03T15:02:16.773348: step 327, loss 0.083719, accuracy 0.96875, precision 0.9693877551020408, recall 0.95
2019-03-03T15:02:17.142321: step 328, loss 0.0642562, accuracy 0.96875, precision 1.0, recall 0.9272727272727272
2019-03-03T15:02:17.536269: step 329, loss 0.0910157, accuracy 0.957031, precision 0.9494949494949495, recall 0.94
2019-03-03T15:02:17.943694: step 330, loss 0.074698, accuracy 0.976562, precision 0.979381443298969, recall 0.9595959595959596
2019-03-03T15:02:18.332654: step 331, loss 0.109067, accuracy 0.960938, precision 0.9479166666666666, recall 0.9479166666666666
2019-03-03T15:02:18.793422: step 332, loss 0.0983645, accuracy 0.957031, precision 0.9578947368421052, recall 0.9285714285714286
2019-03-03T15:02:19.239768: step 333, loss 0.10023, accuracy 0.949219, precision 0.8817204301075269, recall 0.9761904761904762
2019-03-03T15:02:19.656654: step 334, loss 0.0597391, accuracy 0.964844, precision 0.9310344827586207, recall 0.9642857142857143
2019-03-03T15:02:20.068837: step 335, loss 0.0491349, accuracy 0.984375, precision 0.9620253164556962, recall 0.987012987012987
2019-03-03T15:02:20.467769: step 336, loss 0.0978929, accuracy 0.964844, precision 0.926829268292683, recall 0.9620253164556962
2019-03-03T15:02:20.836783: step 337, loss 0.11429, accuracy 0.964844, precision 0.9418604651162791, recall 0.9529411764705882
2019-03-03T15:02:21.166901: step 338, loss 0.093475, accuracy 0.972656, precision 0.9555555555555556, recall 0.9662921348314607
2019-03-03T15:02:21.576321: step 339, loss 0.0933458, accuracy 0.964844, precision 0.9555555555555556, recall 0.945054945054945
2019-03-03T15:02:21.772796: step 340, loss 0.188129, accuracy 0.931624, precision 0.9047619047619048, recall 0.9047619047619048
2019-03-03T15:02:22.183728: step 341, loss 0.0943866, accuracy 0.953125, precision 0.9347826086956522, recall 0.9347826086956522
2019-03-03T15:02:22.568240: step 342, loss 0.117521, accuracy 0.960938, precision 0.9529411764705882, recall 0.9310344827586207
2019-03-03T15:02:22.947311: step 343, loss 0.079323, accuracy 0.960938, precision 0.9494949494949495, recall 0.9494949494949495
2019-03-03T15:02:23.319317: step 344, loss 0.0706787, accuracy 0.964844, precision 0.9431818181818182, recall 0.9540229885057471
2019-03-03T15:02:23.739194: step 345, loss 0.048141, accuracy 0.980469, precision 0.9425287356321839, recall 1.0
2019-03-03T15:02:24.123697: step 346, loss 0.06651, accuracy 0.972656, precision 0.9659090909090909, recall 0.9550561797752809
2019-03-03T15:02:24.495722: step 347, loss 0.0767377, accuracy 0.984375, precision 0.9787234042553191, recall 0.9787234042553191
2019-03-03T15:02:24.902634: step 348, loss 0.0751025, accuracy 0.976562, precision 0.9493670886075949, recall 0.974025974025974
2019-03-03T15:02:25.302076: step 349, loss 0.103782, accuracy 0.960938, precision 0.9302325581395349, recall 0.9523809523809523
2019-03-03T15:02:25.691280: step 350, loss 0.0656333, accuracy 0.976562, precision 0.96875, recall 0.96875
2019-03-03T15:02:26.077120: step 351, loss 0.0783627, accuracy 0.972656, precision 0.9711538461538461, recall 0.9619047619047619
2019-03-03T15:02:26.455620: step 352, loss 0.0987628, accuracy 0.96875, precision 0.9444444444444444, recall 0.9659090909090909
2019-03-03T15:02:26.834605: step 353, loss 0.10459, accuracy 0.964844, precision 0.9666666666666667, recall 0.9354838709677419
2019-03-03T15:02:27.180716: step 354, loss 0.0622706, accuracy 0.976562, precision 0.9381443298969072, recall 1.0
2019-03-03T15:02:27.533768: step 355, loss 0.0474088, accuracy 0.976562, precision 0.9693877551020408, recall 0.9693877551020408
2019-03-03T15:02:27.877849: step 356, loss 0.0711591, accuracy 0.96875, precision 0.96, recall 0.96
2019-03-03T15:02:28.060361: step 357, loss 0.0337235, accuracy 0.982906, precision 1.0, recall 0.95
2019-03-03T15:02:28.411791: step 358, loss 0.0948774, accuracy 0.957031, precision 0.9662921348314607, recall 0.9148936170212766
2019-03-03T15:02:28.779834: step 359, loss 0.0435445, accuracy 0.992188, precision 0.9782608695652174, recall 1.0
2019-03-03T15:02:29.124419: step 360, loss 0.0287473, accuracy 0.992188, precision 0.9761904761904762, recall 1.0
2019-03-03T15:02:29.462516: step 361, loss 0.045322, accuracy 0.988281, precision 0.9901960784313726, recall 0.9805825242718447
2019-03-03T15:02:29.801610: step 362, loss 0.0659693, accuracy 0.96875, precision 0.9468085106382979, recall 0.967391304347826
2019-03-03T15:02:30.145690: step 363, loss 0.0659888, accuracy 0.972656, precision 0.9411764705882353, recall 0.975609756097561
2019-03-03T15:02:30.495196: step 364, loss 0.12974, accuracy 0.945312, precision 0.9310344827586207, recall 0.9101123595505618
2019-03-03T15:02:30.853239: step 365, loss 0.0622646, accuracy 0.96875, precision 0.9361702127659575, recall 0.9777777777777777
2019-03-03T15:02:31.188857: step 366, loss 0.069621, accuracy 0.976562, precision 0.9591836734693877, recall 0.9791666666666666
2019-03-03T15:02:31.544420: step 367, loss 0.0407807, accuracy 0.988281, precision 0.9871794871794872, recall 0.9746835443037974
2019-03-03T15:02:31.889495: step 368, loss 0.0406733, accuracy 0.984375, precision 0.9896907216494846, recall 0.9696969696969697
2019-03-03T15:02:32.237090: step 369, loss 0.0857309, accuracy 0.964844, precision 0.9615384615384616, recall 0.9523809523809523
2019-03-03T15:02:32.592298: step 370, loss 0.043004, accuracy 0.984375, precision 0.9565217391304348, recall 1.0
2019-03-03T15:02:32.956653: step 371, loss 0.0782141, accuracy 0.980469, precision 0.978494623655914, recall 0.9680851063829787
2019-03-03T15:02:33.305258: step 372, loss 0.0848696, accuracy 0.972656, precision 0.9306930693069307, recall 1.0
2019-03-03T15:02:33.652330: step 373, loss 0.0759282, accuracy 0.960938, precision 0.9036144578313253, recall 0.974025974025974
2019-03-03T15:02:33.836940: step 374, loss 0.0829004, accuracy 0.965812, precision 0.9736842105263158, recall 0.925
2019-03-03T15:02:34.160588: step 375, loss 0.0924479, accuracy 0.96875, precision 0.9893617021276596, recall 0.93
2019-03-03T15:02:34.512157: step 376, loss 0.0540783, accuracy 0.988281, precision 0.979381443298969, recall 0.9895833333333334
2019-03-03T15:02:34.881171: step 377, loss 0.0344898, accuracy 0.984375, precision 0.9655172413793104, recall 0.9882352941176471
2019-03-03T15:02:35.234246: step 378, loss 0.0567388, accuracy 0.964844, precision 0.9560439560439561, recall 0.9456521739130435
2019-03-03T15:02:35.594306: step 379, loss 0.0560771, accuracy 0.976562, precision 0.9574468085106383, recall 0.9782608695652174
2019-03-03T15:02:35.947402: step 380, loss 0.083224, accuracy 0.96875, precision 0.945054945054945, recall 0.9662921348314607
2019-03-03T15:02:36.296981: step 381, loss 0.0970124, accuracy 0.964844, precision 0.9368421052631579, recall 0.967391304347826
2019-03-03T15:02:36.657018: step 382, loss 0.0470313, accuracy 0.980469, precision 0.9518072289156626, recall 0.9875
2019-03-03T15:02:37.001604: step 383, loss 0.104372, accuracy 0.976562, precision 0.9619047619047619, recall 0.9805825242718447
2019-03-03T15:02:37.360644: step 384, loss 0.0719591, accuracy 0.976562, precision 0.9767441860465116, recall 0.9545454545454546
2019-03-03T15:02:37.699742: step 385, loss 0.0605949, accuracy 0.980469, precision 0.9690721649484536, recall 0.9791666666666666
2019-03-03T15:02:38.047806: step 386, loss 0.0886044, accuracy 0.960938, precision 0.9318181818181818, recall 0.9534883720930233
2019-03-03T15:02:38.410347: step 387, loss 0.0581611, accuracy 0.972656, precision 0.935064935064935, recall 0.972972972972973
2019-03-03T15:02:38.799307: step 388, loss 0.0833153, accuracy 0.980469, precision 0.9743589743589743, recall 0.9620253164556962
2019-03-03T15:02:39.141907: step 389, loss 0.039611, accuracy 0.980469, precision 0.98, recall 0.9702970297029703
2019-03-03T15:02:39.475017: step 390, loss 0.0656303, accuracy 0.980469, precision 0.967391304347826, recall 0.978021978021978
2019-03-03T15:02:39.645561: step 391, loss 0.0261683, accuracy 0.991453, precision 1.0, recall 0.9824561403508771
2019-03-03T15:02:39.966733: step 392, loss 0.098969, accuracy 0.964844, precision 0.9743589743589743, recall 0.9156626506024096
2019-03-03T15:02:40.318303: step 393, loss 0.0595228, accuracy 0.976562, precision 0.9885057471264368, recall 0.945054945054945
2019-03-03T15:02:40.666373: step 394, loss 0.103561, accuracy 0.972656, precision 0.9797979797979798, recall 0.9509803921568627
2019-03-03T15:02:41.040881: step 395, loss 0.0708583, accuracy 0.960938, precision 0.9418604651162791, recall 0.9418604651162791
2019-03-03T15:02:41.379040: step 396, loss 0.0339206, accuracy 0.980469, precision 1.0, recall 0.9438202247191011
2019-03-03T15:02:41.709343: step 397, loss 0.0836547, accuracy 0.96875, precision 0.9620253164556962, recall 0.9382716049382716
2019-03-03T15:02:42.032551: step 398, loss 0.0420055, accuracy 0.988281, precision 0.9894736842105263, recall 0.9791666666666666
2019-03-03T15:02:42.366660: step 399, loss 0.0682882, accuracy 0.976562, precision 0.9574468085106383, recall 0.9782608695652174
2019-03-03T15:02:42.701696: step 400, loss 0.0671361, accuracy 0.96875, precision 0.9354838709677419, recall 0.9775280898876404
2019-03-03T15:02:43.041661: step 401, loss 0.060146, accuracy 0.976562, precision 0.9493670886075949, recall 0.974025974025974
2019-03-03T15:02:43.386738: step 402, loss 0.0464927, accuracy 0.980469, precision 0.9484536082474226, recall 1.0
2019-03-03T15:02:43.727826: step 403, loss 0.03655, accuracy 0.988281, precision 0.9803921568627451, recall 0.9900990099009901
2019-03-03T15:02:44.070957: step 404, loss 0.089, accuracy 0.972656, precision 0.9263157894736842, recall 1.0
2019-03-03T15:02:44.420918: step 405, loss 0.0326821, accuracy 0.988281, precision 0.9811320754716981, recall 0.9904761904761905
2019-03-03T15:02:44.758147: step 406, loss 0.0638016, accuracy 0.972656, precision 0.9615384615384616, recall 0.970873786407767
2019-03-03T15:02:45.111080: step 407, loss 0.040779, accuracy 0.988281, precision 0.9791666666666666, recall 0.9894736842105263
2019-03-03T15:02:45.282633: step 408, loss 0.0557288, accuracy 0.974359, precision 0.972972972972973, recall 0.9473684210526315
2019-03-03T15:02:45.631705: step 409, loss 0.0202453, accuracy 0.996094, precision 1.0, recall 0.9887640449438202
2019-03-03T15:02:45.964328: step 410, loss 0.0312324, accuracy 0.992188, precision 0.9879518072289156, recall 0.9879518072289156
2019-03-03T15:02:46.294978: step 411, loss 0.0530872, accuracy 0.984375, precision 0.9902912621359223, recall 0.9714285714285714
2019-03-03T15:02:46.634078: step 412, loss 0.0489523, accuracy 0.984375, precision 1.0, recall 0.956989247311828
2019-03-03T15:02:47.001102: step 413, loss 0.0718157, accuracy 0.972656, precision 0.9895833333333334, recall 0.9405940594059405
2019-03-03T15:02:47.326351: step 414, loss 0.0663067, accuracy 0.972656, precision 0.9787234042553191, recall 0.9484536082474226
2019-03-03T15:02:47.656768: step 415, loss 0.0337495, accuracy 0.984375, precision 1.0, recall 0.9611650485436893
2019-03-03T15:02:47.997857: step 416, loss 0.0336051, accuracy 0.992188, precision 0.989247311827957, recall 0.989247311827957
2019-03-03T15:02:48.332961: step 417, loss 0.0420594, accuracy 0.984375, precision 0.9772727272727273, recall 0.9772727272727273
2019-03-03T15:02:48.665072: step 418, loss 0.0498165, accuracy 0.976562, precision 0.9418604651162791, recall 0.9878048780487805
2019-03-03T15:02:49.029101: step 419, loss 0.0703248, accuracy 0.980469, precision 0.9714285714285714, recall 0.9807692307692307
2019-03-03T15:02:49.371239: step 420, loss 0.0805684, accuracy 0.976562, precision 0.9555555555555556, recall 0.9772727272727273
2019-03-03T15:02:49.704333: step 421, loss 0.0535671, accuracy 0.976562, precision 0.9375, recall 1.0
2019-03-03T15:02:50.027469: step 422, loss 0.0464937, accuracy 0.980469, precision 0.9887640449438202, recall 0.9565217391304348
2019-03-03T15:02:50.364715: step 423, loss 0.0265675, accuracy 0.996094, precision 0.9878048780487805, recall 1.0
2019-03-03T15:02:50.717791: step 424, loss 0.0261464, accuracy 0.992188, precision 1.0, recall 0.9767441860465116
2019-03-03T15:02:50.912272: step 425, loss 0.0711903, accuracy 0.982906, precision 0.9782608695652174, recall 0.9782608695652174
2019-03-03T15:02:51.248374: step 426, loss 0.0509369, accuracy 0.980469, precision 0.9711538461538461, recall 0.9805825242718447
2019-03-03T15:02:51.590458: step 427, loss 0.0577027, accuracy 0.976562, precision 0.9560439560439561, recall 0.9775280898876404
2019-03-03T15:02:51.923590: step 428, loss 0.0524008, accuracy 0.976562, precision 0.9711538461538461, recall 0.9711538461538461
2019-03-03T15:02:52.259712: step 429, loss 0.0436745, accuracy 0.988281, precision 0.9782608695652174, recall 0.989010989010989
2019-03-03T15:02:52.610775: step 430, loss 0.0779221, accuracy 0.960938, precision 0.9368421052631579, recall 0.956989247311828
2019-03-03T15:02:52.957846: step 431, loss 0.03035, accuracy 0.984375, precision 0.9651162790697675, recall 0.9880952380952381
2019-03-03T15:02:53.301764: step 432, loss 0.0554597, accuracy 0.992188, precision 1.0, recall 0.9767441860465116
2019-03-03T15:02:53.648312: step 433, loss 0.0507942, accuracy 0.976562, precision 0.975609756097561, recall 0.9523809523809523
2019-03-03T15:02:53.986409: step 434, loss 0.0368104, accuracy 0.988281, precision 0.9883720930232558, recall 0.9770114942528736
2019-03-03T15:02:54.329001: step 435, loss 0.036993, accuracy 0.984375, precision 0.9655172413793104, recall 0.9882352941176471
2019-03-03T15:02:54.660116: step 436, loss 0.054605, accuracy 0.980469, precision 0.9764705882352941, recall 0.9651162790697675
2019-03-03T15:02:55.011278: step 437, loss 0.0633766, accuracy 0.976562, precision 0.9782608695652174, recall 0.9574468085106383
2019-03-03T15:02:55.397582: step 438, loss 0.0405522, accuracy 0.984375, precision 1.0, recall 0.9587628865979382
2019-03-03T15:02:55.788048: step 439, loss 0.0332353, accuracy 0.980469, precision 0.967032967032967, recall 0.9777777777777777
2019-03-03T15:02:56.138113: step 440, loss 0.0438025, accuracy 0.988281, precision 1.0, recall 0.9711538461538461
2019-03-03T15:02:56.522093: step 441, loss 0.0513265, accuracy 0.984375, precision 0.989247311827957, recall 0.968421052631579
2019-03-03T15:02:56.715575: step 442, loss 0.0797302, accuracy 0.957265, precision 0.9777777777777777, recall 0.9166666666666666
2019-03-03T15:02:57.121496: step 443, loss 0.0286684, accuracy 0.996094, precision 1.0, recall 0.989247311827957
2019-03-03T15:02:57.492120: step 444, loss 0.0562205, accuracy 0.976562, precision 0.9578947368421052, recall 0.978494623655914
2019-03-03T15:02:57.878597: step 445, loss 0.0634612, accuracy 0.980469, precision 0.9555555555555556, recall 0.9885057471264368
2019-03-03T15:02:58.269553: step 446, loss 0.0533014, accuracy 0.984375, precision 0.9565217391304348, recall 1.0
2019-03-03T15:02:58.625635: step 447, loss 0.065039, accuracy 0.980469, precision 0.956989247311828, recall 0.9888888888888889
2019-03-03T15:02:58.997240: step 448, loss 0.0579936, accuracy 0.976562, precision 0.9545454545454546, recall 0.9767441860465116
2019-03-03T15:02:59.335358: step 449, loss 0.0441595, accuracy 0.980469, precision 0.9680851063829787, recall 0.978494623655914
2019-03-03T15:02:59.726333: step 450, loss 0.0323516, accuracy 0.984375, precision 0.978021978021978, recall 0.978021978021978
2019-03-03T15:03:00.173139: step 451, loss 0.0522472, accuracy 0.972656, precision 0.9381443298969072, recall 0.9891304347826086
2019-03-03T15:03:00.648907: step 452, loss 0.0420205, accuracy 0.984375, precision 0.9642857142857143, recall 0.9878048780487805
2019-03-03T15:03:01.131616: step 453, loss 0.0376515, accuracy 0.996094, precision 1.0, recall 0.9871794871794872
2019-03-03T15:03:01.593449: step 454, loss 0.0639877, accuracy 0.976562, precision 0.9767441860465116, recall 0.9545454545454546
2019-03-03T15:03:02.044754: step 455, loss 0.0351339, accuracy 0.984375, precision 0.9893617021276596, recall 0.96875
2019-03-03T15:03:02.508158: step 456, loss 0.0466243, accuracy 0.992188, precision 1.0, recall 0.9797979797979798
2019-03-03T15:03:03.062555: step 457, loss 0.032956, accuracy 0.980469, precision 0.9903846153846154, recall 0.9626168224299065
2019-03-03T15:03:03.546279: step 458, loss 0.0452763, accuracy 0.980469, precision 0.96875, recall 0.9789473684210527
2019-03-03T15:03:03.783644: step 459, loss 0.0293301, accuracy 0.991453, precision 1.0, recall 0.9761904761904762
2019-03-03T15:03:04.217486: step 460, loss 0.0495946, accuracy 0.984375, precision 1.0, recall 0.9545454545454546
2019-03-03T15:03:04.668797: step 461, loss 0.049082, accuracy 0.992188, precision 0.9886363636363636, recall 0.9886363636363636
2019-03-03T15:03:05.071722: step 462, loss 0.0381561, accuracy 0.980469, precision 0.9891304347826086, recall 0.9578947368421052
2019-03-03T15:03:05.485858: step 463, loss 0.0375707, accuracy 0.988281, precision 0.9761904761904762, recall 0.9879518072289156
2019-03-03T15:03:05.888782: step 464, loss 0.0529633, accuracy 0.980469, precision 0.967032967032967, recall 0.9777777777777777
2019-03-03T15:03:06.311309: step 465, loss 0.0383886, accuracy 0.984375, precision 0.9886363636363636, recall 0.9666666666666667
2019-03-03T15:03:06.733064: step 466, loss 0.0570481, accuracy 0.976562, precision 0.9509803921568627, recall 0.9897959183673469
2019-03-03T15:03:07.174542: step 467, loss 0.0241796, accuracy 0.992188, precision 0.9887640449438202, recall 0.9887640449438202
2019-03-03T15:03:07.674207: step 468, loss 0.0620076, accuracy 0.980469, precision 0.9791666666666666, recall 0.9690721649484536
2019-03-03T15:03:08.106051: step 469, loss 0.0262702, accuracy 0.992188, precision 0.9883720930232558, recall 0.9883720930232558
2019-03-03T15:03:08.529918: step 470, loss 0.056695, accuracy 0.984375, precision 0.9696969696969697, recall 0.9896907216494846
2019-03-03T15:03:08.987695: step 471, loss 0.0176046, accuracy 0.996094, precision 0.9902912621359223, recall 1.0
2019-03-03T15:03:09.406575: step 472, loss 0.0530173, accuracy 0.980469, precision 0.9787234042553191, recall 0.968421052631579
2019-03-03T15:03:09.844692: step 473, loss 0.0404065, accuracy 0.984375, precision 0.9647058823529412, recall 0.9879518072289156
2019-03-03T15:03:10.226184: step 474, loss 0.0229556, accuracy 0.996094, precision 1.0, recall 0.9888888888888889
2019-03-03T15:03:10.588746: step 475, loss 0.0744914, accuracy 0.96875, precision 0.9514563106796117, recall 0.9702970297029703
2019-03-03T15:03:10.802176: step 476, loss 0.0122631, accuracy 1, precision 1.0, recall 1.0
2019-03-03T15:03:11.226579: step 477, loss 0.0616441, accuracy 0.980469, precision 0.968421052631579, recall 0.9787234042553191
2019-03-03T15:03:11.620533: step 478, loss 0.0530776, accuracy 0.980469, precision 0.989247311827957, recall 0.9583333333333334
2019-03-03T15:03:12.023340: step 479, loss 0.0489187, accuracy 0.980469, precision 0.9509803921568627, recall 1.0
2019-03-03T15:03:12.448203: step 480, loss 0.0346524, accuracy 0.988281, precision 0.9797979797979798, recall 0.9897959183673469
2019-03-03T15:03:12.883355: step 481, loss 0.0343383, accuracy 0.984375, precision 0.9795918367346939, recall 0.9795918367346939
2019-03-03T15:03:13.304530: step 482, loss 0.0640739, accuracy 0.980469, precision 0.9647058823529412, recall 0.9761904761904762
2019-03-03T15:03:13.710048: step 483, loss 0.0527311, accuracy 0.980469, precision 0.9666666666666667, recall 0.9775280898876404
2019-03-03T15:03:14.118956: step 484, loss 0.0415769, accuracy 0.980469, precision 0.967741935483871, recall 0.9782608695652174
2019-03-03T15:03:14.550801: step 485, loss 0.0399279, accuracy 0.988281, precision 1.0, recall 0.96875
2019-03-03T15:03:14.996610: step 486, loss 0.0541068, accuracy 0.976562, precision 0.9651162790697675, recall 0.9651162790697675
2019-03-03T15:03:15.403524: step 487, loss 0.0396183, accuracy 0.984375, precision 0.9795918367346939, recall 0.9795918367346939
2019-03-03T15:03:15.815753: step 488, loss 0.0430642, accuracy 0.984375, precision 0.9775280898876404, recall 0.9775280898876404
2019-03-03T15:03:16.250591: step 489, loss 0.0303934, accuracy 0.988281, precision 0.98, recall 0.98989898989899
2019-03-03T15:03:16.645635: step 490, loss 0.0702857, accuracy 0.976562, precision 0.95, recall 0.9743589743589743
2019-03-03T15:03:17.034127: step 491, loss 0.0435263, accuracy 0.988281, precision 0.9875, recall 0.9753086419753086
2019-03-03T15:03:17.405641: step 492, loss 0.102421, accuracy 0.960938, precision 0.9484536082474226, recall 0.9484536082474226
2019-03-03T15:03:17.600122: step 493, loss 0.103888, accuracy 0.965812, precision 0.9393939393939394, recall 0.9393939393939394
2019-03-03T15:03:17.980133: step 494, loss 0.0195796, accuracy 0.992188, precision 0.9873417721518988, recall 0.9873417721518988
2019-03-03T15:03:18.345158: step 495, loss 0.0403419, accuracy 0.988281, precision 0.979381443298969, recall 0.9895833333333334
2019-03-03T15:03:18.731147: step 496, loss 0.0214501, accuracy 0.988281, precision 1.0, recall 0.9716981132075472
2019-03-03T15:03:19.143456: step 497, loss 0.067571, accuracy 0.972656, precision 0.9423076923076923, recall 0.98989898989899
2019-03-03T15:03:19.530424: step 498, loss 0.065589, accuracy 0.972656, precision 0.9693877551020408, recall 0.9595959595959596
2019-03-03T15:03:19.912543: step 499, loss 0.0363979, accuracy 0.992188, precision 1.0, recall 0.9797979797979798
2019-03-03T15:03:20.320450: step 500, loss 0.0334572, accuracy 0.984375, precision 0.98989898989899, recall 0.9702970297029703

Evaluation:
[[149  33]
 [ 15 271]]
2019-03-03T15:03:20.509944: step 500, loss 0.370382, accuracy 0.897436, precision 0.8186813186813187, recall 0.9085365853658537

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551646798\checkpoints\model-500


Process finished with exit code 0
