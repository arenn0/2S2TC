Pretrained Embedding: ELMo
Italian: True
Loading data...
4681
Max Document length: 81
Vocabulary Size: 1
Train/Dev split: 4354/327
ELMo module loaded from tensorflow-hub
38
37
36
Writing to /home/ubuntu/Project/runs/1551760451

2019-03-05T04:34:17.168295: step 1, loss 2.70754, accuracy 0.570312, precision 0.3673469387755102, recall 0.42857142857142855
2019-03-05T04:34:18.320300: step 2, loss 9.09794, accuracy 0.382812, precision 1.0, recall 0.373015873015873
2019-03-05T04:34:19.441132: step 3, loss 3.10639, accuracy 0.632812, precision 0.24444444444444444, recall 0.4583333333333333
2019-03-05T04:34:20.560471: step 4, loss 7.12452, accuracy 0.59375, precision 0.0, recall 0.0
2019-03-05T04:34:21.687458: step 5, loss 4.37003, accuracy 0.695312, precision 0.07317073170731707, recall 0.75
2019-03-05T04:34:22.811678: step 6, loss 1.67585, accuracy 0.789062, precision 0.5961538461538461, recall 0.8378378378378378
2019-03-05T04:34:23.942122: step 7, loss 2.57454, accuracy 0.664062, precision 0.8536585365853658, recall 0.4861111111111111
2019-03-05T04:34:25.070887: step 8, loss 4.67069, accuracy 0.578125, precision 0.9787234042553191, recall 0.46464646464646464
2019-03-05T04:34:26.197212: step 9, loss 2.94333, accuracy 0.601562, precision 0.8181818181818182, recall 0.45569620253164556
2019-03-05T04:34:27.319683: step 10, loss 1.82056, accuracy 0.703125, precision 0.47058823529411764, recall 0.4444444444444444
2019-03-05T04:34:28.449080: step 11, loss 4.02506, accuracy 0.632812, precision 0.17307692307692307, recall 0.6923076923076923
2019-03-05T04:34:29.586618: step 12, loss 3.93057, accuracy 0.648438, precision 0.22448979591836735, recall 0.6111111111111112
2019-03-05T04:34:30.713765: step 13, loss 2.51325, accuracy 0.703125, precision 0.25, recall 0.8571428571428571
2019-03-05T04:34:31.830955: step 14, loss 1.79099, accuracy 0.765625, precision 0.45714285714285713, recall 0.5925925925925926
2019-03-05T04:34:32.965467: step 15, loss 1.765, accuracy 0.703125, precision 0.7755102040816326, recall 0.5846153846153846
2019-03-05T04:34:34.089451: step 16, loss 2.2475, accuracy 0.617188, precision 0.7894736842105263, recall 0.4225352112676056
2019-03-05T04:34:35.217068: step 17, loss 1.75875, accuracy 0.703125, precision 0.8723404255319149, recall 0.5616438356164384
2019-03-05T04:34:36.342290: step 18, loss 1.74322, accuracy 0.695312, precision 0.717391304347826, recall 0.559322033898305
2019-03-05T04:34:37.460498: step 19, loss 1.29683, accuracy 0.75, precision 0.6, recall 0.6585365853658537
2019-03-05T04:34:38.577346: step 20, loss 1.96779, accuracy 0.726562, precision 0.45098039215686275, recall 0.7666666666666667
2019-03-05T04:34:39.702900: step 21, loss 1.64325, accuracy 0.742188, precision 0.3877551020408163, recall 0.8636363636363636
2019-03-05T04:34:40.832899: step 22, loss 1.07694, accuracy 0.820312, precision 0.5128205128205128, recall 0.8333333333333334
2019-03-05T04:34:41.953931: step 23, loss 1.32445, accuracy 0.734375, precision 0.6170212765957447, recall 0.6444444444444445
2019-03-05T04:34:43.076730: step 24, loss 1.40299, accuracy 0.65625, precision 0.56, recall 0.56
2019-03-05T04:34:44.200232: step 25, loss 1.83035, accuracy 0.640625, precision 0.7, recall 0.5303030303030303
2019-03-05T04:34:45.337802: step 26, loss 1.81036, accuracy 0.65625, precision 0.8421052631578947, recall 0.45714285714285713
2019-03-05T04:34:46.473295: step 27, loss 1.44792, accuracy 0.726562, precision 0.7291666666666666, recall 0.6140350877192983
2019-03-05T04:34:47.604282: step 28, loss 1.26397, accuracy 0.703125, precision 0.5365853658536586, recall 0.5365853658536586
2019-03-05T04:34:48.738485: step 29, loss 1.30449, accuracy 0.773438, precision 0.44, recall 0.9565217391304348
2019-03-05T04:34:49.884349: step 30, loss 1.09191, accuracy 0.742188, precision 0.3829787234042553, recall 0.8181818181818182
2019-03-05T04:34:51.008871: step 31, loss 1.46559, accuracy 0.726562, precision 0.5283018867924528, recall 0.7368421052631579
2019-03-05T04:34:52.136946: step 32, loss 0.942476, accuracy 0.765625, precision 0.7169811320754716, recall 0.7169811320754716
2019-03-05T04:34:53.272282: step 33, loss 0.845161, accuracy 0.757812, precision 0.7333333333333333, recall 0.6346153846153846
2019-03-05T04:34:54.418938: step 34, loss 1.37615, accuracy 0.742188, precision 0.8, recall 0.5614035087719298
2019-03-05T04:34:55.616113: step 35, loss 0.00718727, accuracy 1, precision 1.0, recall 1.0
2019-03-05T04:34:56.741009: step 36, loss 1.11656, accuracy 0.726562, precision 0.875, recall 0.5384615384615384
2019-03-05T04:34:57.872510: step 37, loss 0.5501, accuracy 0.828125, precision 0.7872340425531915, recall 0.7551020408163265
2019-03-05T04:34:59.004389: step 38, loss 0.54558, accuracy 0.828125, precision 0.7, recall 0.8333333333333334
2019-03-05T04:35:00.146915: step 39, loss 0.622239, accuracy 0.835938, precision 0.5813953488372093, recall 0.8928571428571429
2019-03-05T04:35:01.272477: step 40, loss 1.05271, accuracy 0.765625, precision 0.43902439024390244, recall 0.72
2019-03-05T04:35:02.408804: step 41, loss 0.933467, accuracy 0.726562, precision 0.4318181818181818, recall 0.6551724137931034
2019-03-05T04:35:03.544799: step 42, loss 0.835423, accuracy 0.75, precision 0.6, recall 0.7674418604651163
2019-03-05T04:35:04.679192: step 43, loss 0.725472, accuracy 0.789062, precision 0.78, recall 0.7090909090909091
2019-03-05T04:35:05.819765: step 44, loss 1.05016, accuracy 0.671875, precision 0.7872340425531915, recall 0.5362318840579711
2019-03-05T04:35:06.967403: step 45, loss 0.877429, accuracy 0.742188, precision 0.9230769230769231, recall 0.6233766233766234
2019-03-05T04:35:08.091862: step 46, loss 0.849917, accuracy 0.757812, precision 0.8372093023255814, recall 0.6
2019-03-05T04:35:09.212025: step 47, loss 0.605335, accuracy 0.773438, precision 0.7111111111111111, recall 0.6666666666666666
2019-03-05T04:35:10.359746: step 48, loss 0.686118, accuracy 0.773438, precision 0.5882352941176471, recall 0.7894736842105263
2019-03-05T04:35:11.489145: step 49, loss 0.544249, accuracy 0.84375, precision 0.48484848484848486, recall 0.8421052631578947
2019-03-05T04:35:12.634395: step 50, loss 0.766955, accuracy 0.78125, precision 0.5192307692307693, recall 0.9
2019-03-05T04:35:13.764379: step 51, loss 0.479426, accuracy 0.867188, precision 0.6590909090909091, recall 0.9354838709677419
2019-03-05T04:35:14.916166: step 52, loss 0.399929, accuracy 0.851562, precision 0.75, recall 0.7692307692307693
2019-03-05T04:35:16.038616: step 53, loss 0.704625, accuracy 0.835938, precision 0.8431372549019608, recall 0.7678571428571429
2019-03-05T04:35:17.177572: step 54, loss 0.724488, accuracy 0.8125, precision 0.9285714285714286, recall 0.65
2019-03-05T04:35:18.325917: step 55, loss 0.534737, accuracy 0.804688, precision 0.813953488372093, recall 0.6730769230769231
2019-03-05T04:35:19.474671: step 56, loss 0.595086, accuracy 0.78125, precision 0.7954545454545454, recall 0.6481481481481481
2019-03-05T04:35:20.604323: step 57, loss 0.564705, accuracy 0.765625, precision 0.5714285714285714, recall 0.8421052631578947
2019-03-05T04:35:21.740448: step 58, loss 0.538896, accuracy 0.8125, precision 0.5609756097560976, recall 0.7931034482758621
2019-03-05T04:35:22.871722: step 59, loss 0.497057, accuracy 0.828125, precision 0.5813953488372093, recall 0.8620689655172413
2019-03-05T04:35:24.007252: step 60, loss 0.487883, accuracy 0.789062, precision 0.6595744680851063, recall 0.7380952380952381
2019-03-05T04:35:25.147839: step 61, loss 0.428047, accuracy 0.820312, precision 0.74, recall 0.7872340425531915
2019-03-05T04:35:26.297457: step 62, loss 0.420275, accuracy 0.84375, precision 0.9130434782608695, recall 0.7241379310344828
2019-03-05T04:35:27.430917: step 63, loss 0.558051, accuracy 0.8125, precision 0.8367346938775511, recall 0.7192982456140351
2019-03-05T04:35:28.561621: step 64, loss 0.407866, accuracy 0.835938, precision 0.7631578947368421, recall 0.7073170731707317
2019-03-05T04:35:29.706918: step 65, loss 0.314893, accuracy 0.882812, precision 0.7777777777777778, recall 0.9333333333333333
2019-03-05T04:35:30.835724: step 66, loss 0.456461, accuracy 0.867188, precision 0.7441860465116279, recall 0.8421052631578947
2019-03-05T04:35:31.957462: step 67, loss 0.485512, accuracy 0.820312, precision 0.6428571428571429, recall 0.7714285714285715
2019-03-05T04:35:33.102291: step 68, loss 0.377276, accuracy 0.859375, precision 0.74, recall 0.8809523809523809
2019-03-05T04:35:34.238303: step 69, loss 0.327232, accuracy 0.882812, precision 0.7777777777777778, recall 0.875
2019-03-05T04:35:34.474665: step 70, loss 0.017098, accuracy 1, precision 1.0, recall 1.0
2019-03-05T04:35:35.603917: step 71, loss 0.531117, accuracy 0.78125, precision 0.8571428571428571, recall 0.6666666666666666
2019-03-05T04:35:36.747691: step 72, loss 0.22485, accuracy 0.898438, precision 0.9019607843137255, recall 0.8518518518518519
2019-03-05T04:35:37.892524: step 73, loss 0.324683, accuracy 0.859375, precision 0.84, recall 0.8076923076923077
2019-03-05T04:35:39.040427: step 74, loss 0.393726, accuracy 0.851562, precision 0.696969696969697, recall 0.71875
2019-03-05T04:35:40.168913: step 75, loss 0.351686, accuracy 0.84375, precision 0.7551020408163265, recall 0.8222222222222222
2019-03-05T04:35:41.305838: step 76, loss 0.270533, accuracy 0.890625, precision 0.7872340425531915, recall 0.9024390243902439
2019-03-05T04:35:42.431525: step 77, loss 0.473327, accuracy 0.773438, precision 0.5686274509803921, recall 0.8055555555555556
2019-03-05T04:35:43.570923: step 78, loss 0.432829, accuracy 0.828125, precision 0.5952380952380952, recall 0.8333333333333334
2019-03-05T04:35:44.711796: step 79, loss 0.213844, accuracy 0.9375, precision 0.8863636363636364, recall 0.9285714285714286
2019-03-05T04:35:45.847613: step 80, loss 0.285937, accuracy 0.851562, precision 0.7872340425531915, recall 0.8043478260869565
2019-03-05T04:35:46.983288: step 81, loss 0.281274, accuracy 0.875, precision 0.8695652173913043, recall 0.8
2019-03-05T04:35:48.116706: step 82, loss 0.321084, accuracy 0.835938, precision 0.7708333333333334, recall 0.7872340425531915
2019-03-05T04:35:49.249969: step 83, loss 0.397753, accuracy 0.835938, precision 0.82, recall 0.7735849056603774
2019-03-05T04:35:50.381337: step 84, loss 0.318875, accuracy 0.882812, precision 0.8181818181818182, recall 0.8372093023255814
2019-03-05T04:35:51.523514: step 85, loss 0.305863, accuracy 0.890625, precision 0.9183673469387755, recall 0.8181818181818182
2019-03-05T04:35:52.651325: step 86, loss 0.293542, accuracy 0.890625, precision 0.75, recall 0.9166666666666666
2019-03-05T04:35:53.784719: step 87, loss 0.276282, accuracy 0.84375, precision 0.7142857142857143, recall 0.8536585365853658
2019-03-05T04:35:54.924375: step 88, loss 0.309997, accuracy 0.867188, precision 0.6944444444444444, recall 0.8064516129032258
2019-03-05T04:35:56.058734: step 89, loss 0.427549, accuracy 0.84375, precision 0.6538461538461539, recall 0.9444444444444444
2019-03-05T04:35:57.187106: step 90, loss 0.497821, accuracy 0.796875, precision 0.6530612244897959, recall 0.7804878048780488
2019-03-05T04:35:58.324124: step 91, loss 0.367506, accuracy 0.875, precision 0.8478260869565217, recall 0.8125
2019-03-05T04:35:59.451596: step 92, loss 0.245324, accuracy 0.90625, precision 0.8, recall 0.9230769230769231
2019-03-05T04:36:00.589730: step 93, loss 0.389678, accuracy 0.820312, precision 0.75, recall 0.7959183673469388
2019-03-05T04:36:01.732018: step 94, loss 0.303688, accuracy 0.890625, precision 0.975609756097561, recall 0.7547169811320755
2019-03-05T04:36:02.881119: step 95, loss 0.39815, accuracy 0.835938, precision 0.8958333333333334, recall 0.7288135593220338
2019-03-05T04:36:04.027573: step 96, loss 0.231939, accuracy 0.90625, precision 0.875, recall 0.9074074074074074
2019-03-05T04:36:05.168995: step 97, loss 0.295913, accuracy 0.867188, precision 0.6571428571428571, recall 0.8214285714285714
2019-03-05T04:36:06.298358: step 98, loss 0.363815, accuracy 0.820312, precision 0.68, recall 0.8292682926829268
2019-03-05T04:36:07.435711: step 99, loss 0.367287, accuracy 0.859375, precision 0.6578947368421053, recall 0.8333333333333334
2019-03-05T04:36:08.573656: step 100, loss 0.29453, accuracy 0.882812, precision 0.782608695652174, recall 0.8780487804878049
2019-03-05T04:36:09.718008: step 101, loss 0.361785, accuracy 0.84375, precision 0.6, recall 0.9310344827586207
2019-03-05T04:36:10.843776: step 102, loss 0.352004, accuracy 0.859375, precision 0.717391304347826, recall 0.868421052631579
2019-03-05T04:36:11.975437: step 103, loss 0.313649, accuracy 0.859375, precision 0.8157894736842105, recall 0.7380952380952381
2019-03-05T04:36:13.110872: step 104, loss 0.188986, accuracy 0.929688, precision 0.9130434782608695, recall 0.8936170212765957
2019-03-05T04:36:13.347817: step 105, loss 0.041515, accuracy 1, precision nan, recall nan
2019-03-05T04:36:14.489671: step 106, loss 0.281619, accuracy 0.90625, precision 0.8863636363636364, recall 0.8478260869565217
2019-03-05T04:36:15.635321: step 107, loss 0.219619, accuracy 0.914062, precision 0.8709677419354839, recall 0.7941176470588235
2019-03-05T04:36:16.786493: step 108, loss 0.288782, accuracy 0.867188, precision 0.7592592592592593, recall 0.9111111111111111
2019-03-05T04:36:17.917739: step 109, loss 0.376317, accuracy 0.820312, precision 0.7083333333333334, recall 0.7906976744186046
2019-03-05T04:36:19.064570: step 110, loss 0.287319, accuracy 0.875, precision 0.7735849056603774, recall 0.9111111111111111
2019-03-05T04:36:20.193062: step 111, loss 0.259119, accuracy 0.898438, precision 0.8113207547169812, recall 0.9347826086956522
2019-03-05T04:36:21.324362: step 112, loss 0.329982, accuracy 0.796875, precision 0.660377358490566, recall 0.813953488372093
2019-03-05T04:36:22.449190: step 113, loss 0.217204, accuracy 0.90625, precision 0.8979591836734694, recall 0.8627450980392157
2019-03-05T04:36:23.590520: step 114, loss 0.241501, accuracy 0.90625, precision 0.9272727272727272, recall 0.864406779661017
2019-03-05T04:36:24.718945: step 115, loss 0.285286, accuracy 0.90625, precision 0.9411764705882353, recall 0.8421052631578947
2019-03-05T04:36:25.848934: step 116, loss 0.257365, accuracy 0.914062, precision 0.9245283018867925, recall 0.875
2019-03-05T04:36:26.987244: step 117, loss 0.346428, accuracy 0.875, precision 0.875, recall 0.8076923076923077
2019-03-05T04:36:28.126249: step 118, loss 0.325578, accuracy 0.875, precision 0.717948717948718, recall 0.8484848484848485
2019-03-05T04:36:29.250597: step 119, loss 0.325072, accuracy 0.835938, precision 0.8723404255319149, recall 0.7321428571428571
2019-03-05T04:36:30.385271: step 120, loss 0.230016, accuracy 0.890625, precision 0.7027027027027027, recall 0.896551724137931
2019-03-05T04:36:31.537087: step 121, loss 0.394691, accuracy 0.851562, precision 0.6981132075471698, recall 0.925
2019-03-05T04:36:32.669978: step 122, loss 0.289351, accuracy 0.875, precision 0.7333333333333333, recall 0.8918918918918919
2019-03-05T04:36:33.798545: step 123, loss 0.379102, accuracy 0.820312, precision 0.68, recall 0.8292682926829268
2019-03-05T04:36:34.944177: step 124, loss 0.184254, accuracy 0.9375, precision 1.0, recall 0.8367346938775511
2019-03-05T04:36:36.096926: step 125, loss 0.268702, accuracy 0.882812, precision 0.9696969696969697, recall 0.6956521739130435
2019-03-05T04:36:37.232669: step 126, loss 0.263019, accuracy 0.929688, precision 0.9019607843137255, recall 0.92
2019-03-05T04:36:38.378048: step 127, loss 0.243344, accuracy 0.882812, precision 0.875, recall 0.7777777777777778
2019-03-05T04:36:39.525741: step 128, loss 0.23469, accuracy 0.90625, precision 0.8297872340425532, recall 0.9069767441860465
2019-03-05T04:36:40.664333: step 129, loss 0.311974, accuracy 0.875, precision 0.74, recall 0.925
2019-03-05T04:36:41.810140: step 130, loss 0.252663, accuracy 0.90625, precision 0.7708333333333334, recall 0.9736842105263158
2019-03-05T04:36:42.941843: step 131, loss 0.277959, accuracy 0.859375, precision 0.782608695652174, recall 0.8181818181818182
2019-03-05T04:36:44.076246: step 132, loss 0.274365, accuracy 0.875, precision 0.8846153846153846, recall 0.8214285714285714
2019-03-05T04:36:45.207112: step 133, loss 0.268904, accuracy 0.890625, precision 0.8857142857142857, recall 0.7560975609756098
2019-03-05T04:36:46.339790: step 134, loss 0.230971, accuracy 0.914062, precision 0.851063829787234, recall 0.9090909090909091
2019-03-05T04:36:47.474418: step 135, loss 0.329839, accuracy 0.867188, precision 0.8043478260869565, recall 0.8222222222222222
2019-03-05T04:36:48.623453: step 136, loss 0.250233, accuracy 0.859375, precision 0.7954545454545454, recall 0.7954545454545454
2019-03-05T04:36:49.752253: step 137, loss 0.248942, accuracy 0.898438, precision 0.7714285714285715, recall 0.84375
2019-03-05T04:36:50.884898: step 138, loss 0.225778, accuracy 0.875, precision 0.6888888888888889, recall 0.9393939393939394
2019-03-05T04:36:52.022506: step 139, loss 0.185467, accuracy 0.9375, precision 0.8157894736842105, recall 0.96875
2019-03-05T04:36:52.258448: step 140, loss 0.0483467, accuracy 1, precision 1.0, recall 1.0
2019-03-05T04:36:53.388641: step 141, loss 0.143629, accuracy 0.945312, precision 0.8913043478260869, recall 0.9534883720930233
2019-03-05T04:36:54.526805: step 142, loss 0.273001, accuracy 0.867188, precision 0.76, recall 0.8837209302325582
2019-03-05T04:36:55.653707: step 143, loss 0.243604, accuracy 0.898438, precision 0.8113207547169812, recall 0.9347826086956522
2019-03-05T04:36:56.792417: step 144, loss 0.190821, accuracy 0.914062, precision 0.8571428571428571, recall 0.8780487804878049
2019-03-05T04:36:57.928508: step 145, loss 0.308088, accuracy 0.882812, precision 0.9183673469387755, recall 0.8035714285714286
2019-03-05T04:36:59.057530: step 146, loss 0.221436, accuracy 0.90625, precision 0.9111111111111111, recall 0.8367346938775511
2019-03-05T04:37:00.201876: step 147, loss 0.253462, accuracy 0.890625, precision 0.84, recall 0.875
2019-03-05T04:37:01.326859: step 148, loss 0.189406, accuracy 0.914062, precision 0.8222222222222222, recall 0.925
2019-03-05T04:37:02.471689: step 149, loss 0.194865, accuracy 0.929688, precision 0.8157894736842105, recall 0.9393939393939394
2019-03-05T04:37:03.621927: step 150, loss 0.252649, accuracy 0.867188, precision 0.7358490566037735, recall 0.9285714285714286
2019-03-05T04:37:04.766080: step 151, loss 0.234012, accuracy 0.90625, precision 0.8181818181818182, recall 0.8181818181818182
2019-03-05T04:37:05.890589: step 152, loss 0.301761, accuracy 0.90625, precision 0.8085106382978723, recall 0.926829268292683
2019-03-05T04:37:07.021592: step 153, loss 0.250179, accuracy 0.890625, precision 0.82, recall 0.8913043478260869
2019-03-05T04:37:08.161443: step 154, loss 0.203436, accuracy 0.921875, precision 0.9, recall 0.8571428571428571
2019-03-05T04:37:09.292847: step 155, loss 0.152381, accuracy 0.929688, precision 0.8846153846153846, recall 0.9387755102040817
2019-03-05T04:37:10.431121: step 156, loss 0.199926, accuracy 0.929688, precision 0.9056603773584906, recall 0.9230769230769231
2019-03-05T04:37:11.573730: step 157, loss 0.209496, accuracy 0.914062, precision 0.8979591836734694, recall 0.88
2019-03-05T04:37:12.720738: step 158, loss 0.296378, accuracy 0.875, precision 0.8536585365853658, recall 0.7777777777777778
2019-03-05T04:37:13.854015: step 159, loss 0.255481, accuracy 0.882812, precision 0.8085106382978723, recall 0.8636363636363636
2019-03-05T04:37:14.986943: step 160, loss 0.232454, accuracy 0.914062, precision 0.8048780487804879, recall 0.9166666666666666
2019-03-05T04:37:16.113752: step 161, loss 0.238881, accuracy 0.890625, precision 0.8, recall 0.8780487804878049
2019-03-05T04:37:17.256413: step 162, loss 0.116225, accuracy 0.960938, precision 0.8979591836734694, recall 1.0
2019-03-05T04:37:18.404203: step 163, loss 0.227776, accuracy 0.90625, precision 0.8490566037735849, recall 0.9183673469387755
2019-03-05T04:37:19.546934: step 164, loss 0.17922, accuracy 0.914062, precision 0.8653846153846154, recall 0.9183673469387755
2019-03-05T04:37:20.688932: step 165, loss 0.265908, accuracy 0.867188, precision 0.9019607843137255, recall 0.7931034482758621
2019-03-05T04:37:21.827698: step 166, loss 0.405595, accuracy 0.851562, precision 0.8055555555555556, recall 0.7073170731707317
2019-03-05T04:37:22.965381: step 167, loss 0.194957, accuracy 0.914062, precision 0.8604651162790697, recall 0.8809523809523809
2019-03-05T04:37:24.106259: step 168, loss 0.266355, accuracy 0.882812, precision 0.7872340425531915, recall 0.8809523809523809
2019-03-05T04:37:25.237745: step 169, loss 0.226528, accuracy 0.90625, precision 0.8333333333333334, recall 0.875
2019-03-05T04:37:26.384242: step 170, loss 0.336529, accuracy 0.875, precision 0.7818181818181819, recall 0.9148936170212766
2019-03-05T04:37:27.534999: step 171, loss 0.216932, accuracy 0.90625, precision 0.7906976744186046, recall 0.918918918918919
2019-03-05T04:37:28.675316: step 172, loss 0.162188, accuracy 0.921875, precision 0.8723404255319149, recall 0.9111111111111111
2019-03-05T04:37:29.797710: step 173, loss 0.205229, accuracy 0.921875, precision 0.8181818181818182, recall 0.8709677419354839
2019-03-05T04:37:30.929218: step 174, loss 0.238915, accuracy 0.90625, precision 0.8780487804878049, recall 0.8372093023255814
2019-03-05T04:37:31.168142: step 175, loss 0.005199, accuracy 1, precision 1.0, recall 1.0
2019-03-05T04:37:32.318950: step 176, loss 0.219519, accuracy 0.90625, precision 0.8604651162790697, recall 0.8604651162790697
2019-03-05T04:37:33.464580: step 177, loss 0.285481, accuracy 0.882812, precision 0.8125, recall 0.8666666666666667
2019-03-05T04:37:34.607273: step 178, loss 0.101055, accuracy 0.960938, precision 0.8823529411764706, recall 0.967741935483871
2019-03-05T04:37:35.746682: step 179, loss 0.130023, accuracy 0.945312, precision 0.8979591836734694, recall 0.9565217391304348
2019-03-05T04:37:36.894201: step 180, loss 0.162184, accuracy 0.921875, precision 0.8478260869565217, recall 0.9285714285714286
2019-03-05T04:37:38.028880: step 181, loss 0.119784, accuracy 0.953125, precision 0.9473684210526315, recall 0.9
2019-03-05T04:37:39.164554: step 182, loss 0.188104, accuracy 0.90625, precision 0.7647058823529411, recall 0.8666666666666667
2019-03-05T04:37:40.294320: step 183, loss 0.166253, accuracy 0.929688, precision 0.8947368421052632, recall 0.8717948717948718
2019-03-05T04:37:41.430617: step 184, loss 0.258803, accuracy 0.890625, precision 0.7727272727272727, recall 0.8947368421052632
2019-03-05T04:37:42.566481: step 185, loss 0.161245, accuracy 0.945312, precision 0.8958333333333334, recall 0.9555555555555556
2019-03-05T04:37:43.704135: step 186, loss 0.209789, accuracy 0.914062, precision 0.8627450980392157, recall 0.9166666666666666
2019-03-05T04:37:44.837722: step 187, loss 0.189797, accuracy 0.9375, precision 0.9565217391304348, recall 0.88
2019-03-05T04:37:45.984004: step 188, loss 0.139773, accuracy 0.96875, precision 0.9629629629629629, recall 0.9629629629629629
2019-03-05T04:37:47.134829: step 189, loss 0.194021, accuracy 0.898438, precision 0.8688524590163934, recall 0.9137931034482759
2019-03-05T04:37:48.267286: step 190, loss 0.147523, accuracy 0.953125, precision 0.9285714285714286, recall 0.9629629629629629
2019-03-05T04:37:49.401579: step 191, loss 0.310236, accuracy 0.875, precision 0.8666666666666667, recall 0.7959183673469388
2019-03-05T04:37:50.525050: step 192, loss 0.254464, accuracy 0.867188, precision 0.7894736842105263, recall 0.9
2019-03-05T04:37:51.662451: step 193, loss 0.236468, accuracy 0.882812, precision 0.8333333333333334, recall 0.7692307692307693
2019-03-05T04:37:52.804774: step 194, loss 0.165593, accuracy 0.9375, precision 0.8604651162790697, recall 0.9487179487179487
2019-03-05T04:37:53.938154: step 195, loss 0.217446, accuracy 0.890625, precision 0.7105263157894737, recall 0.9
2019-03-05T04:37:55.066033: step 196, loss 0.167946, accuracy 0.929688, precision 0.8541666666666666, recall 0.9534883720930233
2019-03-05T04:37:56.212608: step 197, loss 0.197962, accuracy 0.9375, precision 0.8409090909090909, recall 0.9736842105263158
2019-03-05T04:37:57.338950: step 198, loss 0.208396, accuracy 0.921875, precision 0.9302325581395349, recall 0.851063829787234
2019-03-05T04:37:58.472843: step 199, loss 0.208349, accuracy 0.921875, precision 0.9069767441860465, recall 0.8666666666666667
2019-03-05T04:37:59.604869: step 200, loss 0.184496, accuracy 0.914062, precision 0.9347826086956522, recall 0.8431372549019608
2019-03-05T04:38:00.752473: step 201, loss 0.228932, accuracy 0.914062, precision 0.8780487804878049, recall 0.8571428571428571
2019-03-05T04:38:01.907617: step 202, loss 0.154653, accuracy 0.945312, precision 0.8727272727272727, recall 1.0
2019-03-05T04:38:03.043595: step 203, loss 0.250646, accuracy 0.914062, precision 0.7959183673469388, recall 0.975
2019-03-05T04:38:04.187010: step 204, loss 0.220153, accuracy 0.875, precision 0.75, recall 0.868421052631579
2019-03-05T04:38:05.330878: step 205, loss 0.31245, accuracy 0.90625, precision 0.8863636363636364, recall 0.8478260869565217
2019-03-05T04:38:06.462580: step 206, loss 0.245312, accuracy 0.890625, precision 0.8444444444444444, recall 0.8444444444444444
2019-03-05T04:38:07.615169: step 207, loss 0.210014, accuracy 0.921875, precision 0.875, recall 0.9130434782608695
2019-03-05T04:38:08.764028: step 208, loss 0.156716, accuracy 0.929688, precision 0.9433962264150944, recall 0.8928571428571429
2019-03-05T04:38:09.895746: step 209, loss 0.184016, accuracy 0.921875, precision 0.86, recall 0.9347826086956522
2019-03-05T04:38:10.133593: step 210, loss 0.0808455, accuracy 1, precision nan, recall nan
2019-03-05T04:38:11.271617: step 211, loss 0.16294, accuracy 0.9375, precision 0.8571428571428571, recall 0.9767441860465116
2019-03-05T04:38:12.420721: step 212, loss 0.131989, accuracy 0.960938, precision 0.9215686274509803, recall 0.9791666666666666
2019-03-05T04:38:13.562506: step 213, loss 0.196955, accuracy 0.9375, precision 0.9069767441860465, recall 0.9069767441860465
2019-03-05T04:38:14.700410: step 214, loss 0.12401, accuracy 0.953125, precision 0.9069767441860465, recall 0.9512195121951219
2019-03-05T04:38:15.857443: step 215, loss 0.162433, accuracy 0.945312, precision 0.9347826086956522, recall 0.9148936170212766
2019-03-05T04:38:16.985924: step 216, loss 0.296813, accuracy 0.875, precision 0.6976744186046512, recall 0.9090909090909091
2019-03-05T04:38:18.115306: step 217, loss 0.137608, accuracy 0.960938, precision 0.9347826086956522, recall 0.9555555555555556
2019-03-05T04:38:19.252005: step 218, loss 0.183699, accuracy 0.9375, precision 0.9629629629629629, recall 0.896551724137931
2019-03-05T04:38:20.394287: step 219, loss 0.101262, accuracy 0.96875, precision 0.975609756097561, recall 0.9302325581395349
2019-03-05T04:38:21.535655: step 220, loss 0.291219, accuracy 0.875, precision 0.8409090909090909, recall 0.8043478260869565
2019-03-05T04:38:22.674784: step 221, loss 0.159024, accuracy 0.9375, precision 0.8636363636363636, recall 0.95
2019-03-05T04:38:23.811041: step 222, loss 0.154784, accuracy 0.9375, precision 0.8541666666666666, recall 0.9761904761904762
2019-03-05T04:38:24.958359: step 223, loss 0.187727, accuracy 0.914062, precision 0.8043478260869565, recall 0.9487179487179487
2019-03-05T04:38:26.097920: step 224, loss 0.130927, accuracy 0.960938, precision 0.92, recall 0.9787234042553191
2019-03-05T04:38:27.228162: step 225, loss 0.180949, accuracy 0.953125, precision 0.9772727272727273, recall 0.8958333333333334
2019-03-05T04:38:28.362439: step 226, loss 0.262848, accuracy 0.898438, precision 0.8717948717948718, recall 0.8095238095238095
2019-03-05T04:38:29.514849: step 227, loss 0.153139, accuracy 0.945312, precision 0.875, recall 0.9459459459459459
2019-03-05T04:38:30.650843: step 228, loss 0.252856, accuracy 0.921875, precision 0.8958333333333334, recall 0.8958333333333334
2019-03-05T04:38:31.795208: step 229, loss 0.204112, accuracy 0.914062, precision 0.851063829787234, recall 0.9090909090909091
2019-03-05T04:38:32.925909: step 230, loss 0.217819, accuracy 0.898438, precision 0.7916666666666666, recall 0.926829268292683
2019-03-05T04:38:34.056663: step 231, loss 0.13632, accuracy 0.9375, precision 0.9361702127659575, recall 0.8979591836734694
2019-03-05T04:38:35.202964: step 232, loss 0.146484, accuracy 0.960938, precision 0.9487179487179487, recall 0.925
2019-03-05T04:38:36.333675: step 233, loss 0.184712, accuracy 0.9375, precision 0.9487179487179487, recall 0.8604651162790697
2019-03-05T04:38:37.455986: step 234, loss 0.158346, accuracy 0.929688, precision 0.8478260869565217, recall 0.9512195121951219
2019-03-05T04:38:38.591792: step 235, loss 0.203028, accuracy 0.921875, precision 0.8717948717948718, recall 0.8717948717948718
2019-03-05T04:38:39.736613: step 236, loss 0.170646, accuracy 0.921875, precision 0.8703703703703703, recall 0.94
2019-03-05T04:38:40.866913: step 237, loss 0.127246, accuracy 0.929688, precision 0.9090909090909091, recall 0.8888888888888888
2019-03-05T04:38:42.000286: step 238, loss 0.178861, accuracy 0.914062, precision 0.8846153846153846, recall 0.9019607843137255
2019-03-05T04:38:43.146156: step 239, loss 0.229691, accuracy 0.90625, precision 0.7692307692307693, recall 0.9090909090909091
2019-03-05T04:38:44.287855: step 240, loss 0.183111, accuracy 0.9375, precision 0.8666666666666667, recall 0.9512195121951219
2019-03-05T04:38:45.426207: step 241, loss 0.173831, accuracy 0.929688, precision 0.8653846153846154, recall 0.9574468085106383
2019-03-05T04:38:46.561480: step 242, loss 0.194165, accuracy 0.914062, precision 0.9411764705882353, recall 0.8571428571428571
2019-03-05T04:38:47.707836: step 243, loss 0.180776, accuracy 0.914062, precision 0.8867924528301887, recall 0.9038461538461539
2019-03-05T04:38:48.840565: step 244, loss 0.184506, accuracy 0.929688, precision 0.8695652173913043, recall 0.9302325581395349
2019-03-05T04:38:49.082389: step 245, loss 0.247955, accuracy 1, precision 1.0, recall 1.0
2019-03-05T04:38:50.229100: step 246, loss 0.196297, accuracy 0.921875, precision 0.9583333333333334, recall 0.8518518518518519
2019-03-05T04:38:51.357631: step 247, loss 0.184168, accuracy 0.914062, precision 0.9743589743589743, recall 0.7916666666666666
2019-03-05T04:38:52.499688: step 248, loss 0.194417, accuracy 0.90625, precision 0.9655172413793104, recall 0.8484848484848485
2019-03-05T04:38:53.639293: step 249, loss 0.216444, accuracy 0.921875, precision 0.9705882352941176, recall 0.7857142857142857
2019-03-05T04:38:54.790369: step 250, loss 0.250819, accuracy 0.929688, precision 0.8431372549019608, recall 0.9772727272727273

Evaluation:
