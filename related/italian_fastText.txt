"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
fastText model loaded
Pretrained Embedding: fastText
Italian: True
Loading data...
4681
Max Document length: 81
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:82: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Vocabulary Size: 1
Train/Dev split: 4213/468
2019-03-03 14:53:28.748339: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:475: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551646409

2019-03-03T14:53:30.534337: step 1, loss 0.897567, accuracy 0.484375, precision 0.8061224489795918, recall 0.4114583333333333
2019-03-03T14:53:31.129746: step 2, loss 0.788599, accuracy 0.546875, precision 0.40217391304347827, recall 0.37755102040816324
2019-03-03T14:53:31.721163: step 3, loss 0.694521, accuracy 0.640625, precision 0.30526315789473685, recall 0.5272727272727272
2019-03-03T14:53:32.308593: step 4, loss 0.674801, accuracy 0.707031, precision 0.29347826086956524, recall 0.7297297297297297
2019-03-03T14:53:32.896022: step 5, loss 0.639959, accuracy 0.710938, precision 0.29411764705882354, recall 0.6410256410256411
2019-03-03T14:53:33.493425: step 6, loss 0.570315, accuracy 0.722656, precision 0.42718446601941745, recall 0.7857142857142857
2019-03-03T14:53:34.084844: step 7, loss 0.654782, accuracy 0.683594, precision 0.4380952380952381, recall 0.6764705882352942
2019-03-03T14:53:34.671276: step 8, loss 0.562793, accuracy 0.757812, precision 0.5465116279069767, recall 0.6714285714285714
2019-03-03T14:53:35.265687: step 9, loss 0.64242, accuracy 0.667969, precision 0.5154639175257731, recall 0.5681818181818182
2019-03-03T14:53:35.854114: step 10, loss 0.489252, accuracy 0.792969, precision 0.7368421052631579, recall 0.7142857142857143
2019-03-03T14:53:36.444535: step 11, loss 0.603461, accuracy 0.726562, precision 0.6216216216216216, recall 0.5227272727272727
2019-03-03T14:53:37.036951: step 12, loss 0.559117, accuracy 0.769531, precision 0.75, recall 0.6407766990291263
2019-03-03T14:53:37.624381: step 13, loss 0.500204, accuracy 0.773438, precision 0.6739130434782609, recall 0.6888888888888889
2019-03-03T14:53:38.213807: step 14, loss 0.515741, accuracy 0.738281, precision 0.5058823529411764, recall 0.6323529411764706
2019-03-03T14:53:38.809213: step 15, loss 0.457143, accuracy 0.773438, precision 0.5520833333333334, recall 0.7794117647058824
2019-03-03T14:53:39.397639: step 16, loss 0.504041, accuracy 0.765625, precision 0.5050505050505051, recall 0.819672131147541
2019-03-03T14:53:39.692851: step 17, loss 0.590883, accuracy 0.692308, precision 0.3333333333333333, recall 0.5652173913043478
2019-03-03T14:53:40.285268: step 18, loss 0.411209, accuracy 0.804688, precision 0.5955056179775281, recall 0.7910447761194029
2019-03-03T14:53:40.877682: step 19, loss 0.364824, accuracy 0.847656, precision 0.6976744186046512, recall 0.821917808219178
2019-03-03T14:53:41.470099: step 20, loss 0.405432, accuracy 0.816406, precision 0.7010309278350515, recall 0.7906976744186046
2019-03-03T14:53:42.058526: step 21, loss 0.37173, accuracy 0.824219, precision 0.7023809523809523, recall 0.7468354430379747
2019-03-03T14:53:42.643960: step 22, loss 0.380069, accuracy 0.8125, precision 0.7156862745098039, recall 0.7934782608695652
2019-03-03T14:53:43.238373: step 23, loss 0.324217, accuracy 0.824219, precision 0.7669902912621359, recall 0.79
2019-03-03T14:53:43.840760: step 24, loss 0.373176, accuracy 0.820312, precision 0.86, recall 0.7288135593220338
2019-03-03T14:53:44.437167: step 25, loss 0.418803, accuracy 0.8125, precision 0.8, recall 0.7407407407407407
2019-03-03T14:53:45.031577: step 26, loss 0.357963, accuracy 0.8125, precision 0.8, recall 0.7238095238095238
2019-03-03T14:53:45.622995: step 27, loss 0.329485, accuracy 0.851562, precision 0.8314606741573034, recall 0.7628865979381443
2019-03-03T14:53:46.216410: step 28, loss 0.32069, accuracy 0.855469, precision 0.7789473684210526, recall 0.8222222222222222
2019-03-03T14:53:46.807828: step 29, loss 0.352212, accuracy 0.871094, precision 0.8229166666666666, recall 0.8315789473684211
2019-03-03T14:53:47.405231: step 30, loss 0.29407, accuracy 0.875, precision 0.8333333333333334, recall 0.7954545454545454
2019-03-03T14:53:48.002633: step 31, loss 0.245957, accuracy 0.902344, precision 0.8064516129032258, recall 0.9146341463414634
2019-03-03T14:53:48.598042: step 32, loss 0.349327, accuracy 0.839844, precision 0.7209302325581395, recall 0.7848101265822784
2019-03-03T14:53:49.200432: step 33, loss 0.273537, accuracy 0.875, precision 0.7341772151898734, recall 0.8405797101449275
2019-03-03T14:53:49.497664: step 34, loss 0.292906, accuracy 0.871795, precision 0.7441860465116279, recall 0.8888888888888888
2019-03-03T14:53:50.094041: step 35, loss 0.263156, accuracy 0.878906, precision 0.7674418604651163, recall 0.8571428571428571
2019-03-03T14:53:50.738319: step 36, loss 0.265356, accuracy 0.878906, precision 0.7391304347826086, recall 0.9066666666666666
2019-03-03T14:53:52.670155: step 37, loss 0.269048, accuracy 0.882812, precision 0.7916666666666666, recall 0.8837209302325582
2019-03-03T14:53:55.035830: step 38, loss 0.211992, accuracy 0.910156, precision 0.8170731707317073, recall 0.8933333333333333
2019-03-03T14:53:56.844992: step 39, loss 0.267968, accuracy 0.894531, precision 0.8202247191011236, recall 0.8690476190476191
2019-03-03T14:53:58.435740: step 40, loss 0.246612, accuracy 0.894531, precision 0.81, recall 0.9101123595505618
2019-03-03T14:53:59.667447: step 41, loss 0.237466, accuracy 0.894531, precision 0.8235294117647058, recall 0.8536585365853658
2019-03-03T14:54:00.956002: step 42, loss 0.308335, accuracy 0.871094, precision 0.8021978021978022, recall 0.8295454545454546
2019-03-03T14:54:02.198679: step 43, loss 0.189579, accuracy 0.9375, precision 0.9368421052631579, recall 0.898989898989899
2019-03-03T14:54:03.231917: step 44, loss 0.262579, accuracy 0.878906, precision 0.8478260869565217, recall 0.8210526315789474
2019-03-03T14:54:04.322999: step 45, loss 0.245667, accuracy 0.90625, precision 0.8602150537634409, recall 0.8791208791208791
2019-03-03T14:54:05.350252: step 46, loss 0.169515, accuracy 0.945312, precision 0.9387755102040817, recall 0.92
2019-03-03T14:54:06.357560: step 47, loss 0.230188, accuracy 0.921875, precision 0.89, recall 0.9081632653061225
2019-03-03T14:54:07.339933: step 48, loss 0.226696, accuracy 0.914062, precision 0.8554216867469879, recall 0.8765432098765432
2019-03-03T14:54:08.301364: step 49, loss 0.240834, accuracy 0.917969, precision 0.8780487804878049, recall 0.8674698795180723
2019-03-03T14:54:09.237859: step 50, loss 0.187264, accuracy 0.917969, precision 0.8918918918918919, recall 0.9166666666666666
2019-03-03T14:54:09.700621: step 51, loss 0.258812, accuracy 0.923077, precision 0.8913043478260869, recall 0.9111111111111111
2019-03-03T14:54:10.615177: step 52, loss 0.19016, accuracy 0.921875, precision 0.8809523809523809, recall 0.8809523809523809
2019-03-03T14:54:11.503800: step 53, loss 0.192654, accuracy 0.9375, precision 0.891566265060241, recall 0.9135802469135802
2019-03-03T14:54:12.389433: step 54, loss 0.157438, accuracy 0.949219, precision 0.9318181818181818, recall 0.9213483146067416
2019-03-03T14:54:13.257113: step 55, loss 0.188661, accuracy 0.929688, precision 0.8469387755102041, recall 0.9651162790697675
2019-03-03T14:54:14.108838: step 56, loss 0.178701, accuracy 0.917969, precision 0.8571428571428571, recall 0.9230769230769231
2019-03-03T14:54:14.958566: step 57, loss 0.208602, accuracy 0.929688, precision 0.8942307692307693, recall 0.93
2019-03-03T14:54:15.788346: step 58, loss 0.147369, accuracy 0.945312, precision 0.9130434782608695, recall 0.9333333333333333
2019-03-03T14:54:16.603168: step 59, loss 0.138122, accuracy 0.953125, precision 0.9333333333333333, recall 0.9514563106796117
2019-03-03T14:54:17.407018: step 60, loss 0.168158, accuracy 0.933594, precision 0.9, recall 0.9278350515463918
2019-03-03T14:54:18.194912: step 61, loss 0.153802, accuracy 0.953125, precision 0.9540229885057471, recall 0.9120879120879121
2019-03-03T14:54:18.976822: step 62, loss 0.156231, accuracy 0.949219, precision 0.9456521739130435, recall 0.9157894736842105
2019-03-03T14:54:19.764714: step 63, loss 0.173499, accuracy 0.957031, precision 0.9880952380952381, recall 0.8924731182795699
2019-03-03T14:54:20.534657: step 64, loss 0.147414, accuracy 0.949219, precision 0.9230769230769231, recall 0.9113924050632911
2019-03-03T14:54:21.297616: step 65, loss 0.180386, accuracy 0.941406, precision 0.9595959595959596, recall 0.8962264150943396
2019-03-03T14:54:22.049605: step 66, loss 0.142916, accuracy 0.96875, precision 0.9550561797752809, recall 0.9550561797752809
2019-03-03T14:54:22.803590: step 67, loss 0.166076, accuracy 0.925781, precision 0.8817204301075269, recall 0.9111111111111111
2019-03-03T14:54:23.174598: step 68, loss 0.129388, accuracy 0.957265, precision 0.9361702127659575, recall 0.9565217391304348
2019-03-03T14:54:23.908636: step 69, loss 0.170544, accuracy 0.929688, precision 0.8723404255319149, recall 0.9318181818181818
2019-03-03T14:54:24.654640: step 70, loss 0.152782, accuracy 0.941406, precision 0.9108910891089109, recall 0.9387755102040817
2019-03-03T14:54:25.383691: step 71, loss 0.116458, accuracy 0.957031, precision 0.9213483146067416, recall 0.9534883720930233
2019-03-03T14:54:26.109751: step 72, loss 0.133789, accuracy 0.96875, precision 0.9696969696969697, recall 0.9504950495049505
2019-03-03T14:54:26.832817: step 73, loss 0.125605, accuracy 0.964844, precision 0.9468085106382979, recall 0.956989247311828
2019-03-03T14:54:27.547905: step 74, loss 0.153055, accuracy 0.953125, precision 0.9680851063829787, recall 0.91
2019-03-03T14:54:28.252025: step 75, loss 0.119104, accuracy 0.964844, precision 0.968421052631579, recall 0.9387755102040817
2019-03-03T14:54:28.952151: step 76, loss 0.151266, accuracy 0.949219, precision 0.9058823529411765, recall 0.9390243902439024
2019-03-03T14:54:29.655272: step 77, loss 0.113491, accuracy 0.960938, precision 0.9583333333333334, recall 0.9387755102040817
2019-03-03T14:54:30.350413: step 78, loss 0.139612, accuracy 0.957031, precision 0.9393939393939394, recall 0.9489795918367347
2019-03-03T14:54:31.044557: step 79, loss 0.129051, accuracy 0.964844, precision 0.9772727272727273, recall 0.9247311827956989
2019-03-03T14:54:31.737702: step 80, loss 0.140536, accuracy 0.953125, precision 0.9361702127659575, recall 0.9361702127659575
2019-03-03T14:54:32.425863: step 81, loss 0.10898, accuracy 0.96875, precision 0.9494949494949495, recall 0.9690721649484536
2019-03-03T14:54:33.105048: step 82, loss 0.129972, accuracy 0.945312, precision 0.9146341463414634, recall 0.9146341463414634
2019-03-03T14:54:33.787225: step 83, loss 0.101432, accuracy 0.96875, precision 0.972972972972973, recall 0.9230769230769231
2019-03-03T14:54:34.473389: step 84, loss 0.112513, accuracy 0.96875, precision 0.9615384615384616, recall 0.9615384615384616
2019-03-03T14:54:34.817468: step 85, loss 0.0678942, accuracy 0.991453, precision 0.9705882352941176, recall 1.0
2019-03-03T14:54:35.489673: step 86, loss 0.108421, accuracy 0.960938, precision 0.9240506329113924, recall 0.948051948051948
2019-03-03T14:54:36.153897: step 87, loss 0.0911678, accuracy 0.976562, precision 0.9770114942528736, recall 0.9550561797752809
2019-03-03T14:54:36.820115: step 88, loss 0.0796344, accuracy 0.976562, precision 0.9444444444444444, recall 0.9883720930232558
2019-03-03T14:54:37.482344: step 89, loss 0.0995677, accuracy 0.960938, precision 0.9302325581395349, recall 0.9523809523809523
2019-03-03T14:54:38.153550: step 90, loss 0.102895, accuracy 0.976562, precision 1.0, recall 0.9333333333333333
2019-03-03T14:54:38.809795: step 91, loss 0.0882201, accuracy 0.972656, precision 0.9444444444444444, recall 0.9770114942528736
2019-03-03T14:54:39.475016: step 92, loss 0.0720112, accuracy 0.980469, precision 0.9809523809523809, recall 0.9716981132075472
2019-03-03T14:54:40.139241: step 93, loss 0.0900896, accuracy 0.972656, precision 0.9473684210526315, recall 0.9782608695652174
2019-03-03T14:54:40.787509: step 94, loss 0.121759, accuracy 0.953125, precision 0.9375, recall 0.9375
2019-03-03T14:54:41.437770: step 95, loss 0.108428, accuracy 0.972656, precision 0.9595959595959596, recall 0.9693877551020408
2019-03-03T14:54:42.087034: step 96, loss 0.0884056, accuracy 0.984375, precision 0.9894736842105263, recall 0.9690721649484536
2019-03-03T14:54:42.730314: step 97, loss 0.0720002, accuracy 0.984375, precision 0.989247311827957, recall 0.968421052631579
2019-03-03T14:54:43.374592: step 98, loss 0.0826333, accuracy 0.96875, precision 0.967391304347826, recall 0.9468085106382979
2019-03-03T14:54:44.016873: step 99, loss 0.0785052, accuracy 0.96875, precision 0.9587628865979382, recall 0.9587628865979382
2019-03-03T14:54:44.656166: step 100, loss 0.119581, accuracy 0.945312, precision 0.9310344827586207, recall 0.9101123595505618
2019-03-03T14:54:45.304431: step 101, loss 0.0913574, accuracy 0.972656, precision 0.967741935483871, recall 0.9574468085106383
2019-03-03T14:54:45.622582: step 102, loss 0.0994528, accuracy 0.957265, precision 0.9433962264150944, recall 0.9615384615384616
2019-03-03T14:54:46.256884: step 103, loss 0.0799024, accuracy 0.972656, precision 0.9680851063829787, recall 0.9578947368421052
2019-03-03T14:54:46.887200: step 104, loss 0.0709035, accuracy 0.988281, precision 0.9894736842105263, recall 0.9791666666666666
2019-03-03T14:54:47.517513: step 105, loss 0.0609209, accuracy 0.988281, precision 0.9642857142857143, recall 1.0
2019-03-03T14:54:48.148826: step 106, loss 0.0805492, accuracy 0.976562, precision 0.9809523809523809, recall 0.9626168224299065
2019-03-03T14:54:48.792107: step 107, loss 0.0667413, accuracy 0.980469, precision 0.9885057471264368, recall 0.9555555555555556
2019-03-03T14:54:49.438378: step 108, loss 0.0833122, accuracy 0.980469, precision 0.9545454545454546, recall 0.9882352941176471
2019-03-03T14:54:50.090635: step 109, loss 0.0524565, accuracy 0.980469, precision 0.9789473684210527, recall 0.96875
2019-03-03T14:54:50.732917: step 110, loss 0.0914802, accuracy 0.960938, precision 0.9393939393939394, recall 0.9587628865979382
2019-03-03T14:54:51.372207: step 111, loss 0.0547765, accuracy 0.984375, precision 0.98, recall 0.98
2019-03-03T14:54:52.003521: step 112, loss 0.0739502, accuracy 0.972656, precision 0.9705882352941176, recall 0.9611650485436893
2019-03-03T14:54:52.642810: step 113, loss 0.0683707, accuracy 0.96875, precision 0.956989247311828, recall 0.956989247311828
2019-03-03T14:54:53.291078: step 114, loss 0.0688028, accuracy 0.976562, precision 0.9787234042553191, recall 0.9583333333333334
2019-03-03T14:54:53.942337: step 115, loss 0.0653466, accuracy 0.972656, precision 0.9770114942528736, recall 0.9444444444444444
2019-03-03T14:54:54.613543: step 116, loss 0.0467011, accuracy 0.988281, precision 0.9791666666666666, recall 0.9894736842105263
2019-03-03T14:54:55.298710: step 117, loss 0.05735, accuracy 0.980469, precision 0.9605263157894737, recall 0.9733333333333334
2019-03-03T14:54:55.938001: step 118, loss 0.0811717, accuracy 0.980469, precision 0.9885057471264368, recall 0.9555555555555556
2019-03-03T14:54:56.263132: step 119, loss 0.0780594, accuracy 0.982906, precision 1.0, recall 0.9512195121951219
2019-03-03T14:54:56.891453: step 120, loss 0.0674275, accuracy 0.976562, precision 0.9764705882352941, recall 0.9540229885057471
2019-03-03T14:54:57.524758: step 121, loss 0.0460122, accuracy 0.988281, precision 0.968421052631579, recall 1.0
2019-03-03T14:54:58.191975: step 122, loss 0.0567856, accuracy 0.988281, precision 0.98, recall 0.98989898989899
2019-03-03T14:54:58.845229: step 123, loss 0.0764955, accuracy 0.980469, precision 0.978021978021978, recall 0.967391304347826
2019-03-03T14:54:59.500477: step 124, loss 0.0689841, accuracy 0.976562, precision 0.9651162790697675, recall 0.9651162790697675
2019-03-03T14:55:00.158716: step 125, loss 0.053255, accuracy 0.976562, precision 0.96, recall 0.9795918367346939
2019-03-03T14:55:00.814963: step 126, loss 0.04485, accuracy 0.992188, precision 0.9887640449438202, recall 0.9887640449438202
2019-03-03T14:55:01.469213: step 127, loss 0.0463558, accuracy 0.980469, precision 0.9767441860465116, recall 0.9655172413793104
2019-03-03T14:55:02.122467: step 128, loss 0.0564036, accuracy 0.980469, precision 0.9565217391304348, recall 0.9887640449438202
2019-03-03T14:55:02.767742: step 129, loss 0.0431886, accuracy 0.988281, precision 0.99, recall 0.9801980198019802
2019-03-03T14:55:03.411021: step 130, loss 0.0637856, accuracy 0.984375, precision 0.9901960784313726, recall 0.9711538461538461
2019-03-03T14:55:04.054301: step 131, loss 0.089309, accuracy 0.960938, precision 0.92, recall 0.9787234042553191
2019-03-03T14:55:04.689603: step 132, loss 0.0488093, accuracy 0.976562, precision 0.9642857142857143, recall 0.9642857142857143
2019-03-03T14:55:05.348839: step 133, loss 0.0525959, accuracy 0.980469, precision 0.9880952380952381, recall 0.9540229885057471
2019-03-03T14:55:06.004087: step 134, loss 0.0434441, accuracy 0.988281, precision 0.9888888888888889, recall 0.978021978021978
2019-03-03T14:55:06.662328: step 135, loss 0.0732824, accuracy 0.96875, precision 0.9886363636363636, recall 0.925531914893617
2019-03-03T14:55:06.986462: step 136, loss 0.049838, accuracy 0.982906, precision 0.9591836734693877, recall 1.0
2019-03-03T14:55:07.638717: step 137, loss 0.0362712, accuracy 0.988281, precision 1.0, recall 0.967032967032967
2019-03-03T14:55:08.286984: step 138, loss 0.043717, accuracy 0.996094, precision 0.9897959183673469, recall 1.0
2019-03-03T14:55:08.931262: step 139, loss 0.0397242, accuracy 0.996094, precision 0.987012987012987, recall 1.0
2019-03-03T14:55:09.590499: step 140, loss 0.0402144, accuracy 0.992188, precision 0.990909090909091, recall 0.990909090909091
2019-03-03T14:55:10.233779: step 141, loss 0.0257495, accuracy 0.996094, precision 1.0, recall 0.989010989010989
2019-03-03T14:55:10.873070: step 142, loss 0.0472881, accuracy 0.980469, precision 0.9797979797979798, recall 0.97
2019-03-03T14:55:11.512362: step 143, loss 0.0440066, accuracy 0.992188, precision 0.9876543209876543, recall 0.9876543209876543
2019-03-03T14:55:12.167609: step 144, loss 0.050996, accuracy 0.972656, precision 0.9519230769230769, recall 0.9801980198019802
2019-03-03T14:55:12.818868: step 145, loss 0.0268709, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:13.476111: step 146, loss 0.0324307, accuracy 0.996094, precision 0.9882352941176471, recall 1.0
2019-03-03T14:55:14.132356: step 147, loss 0.029926, accuracy 0.988281, precision 0.9875, recall 0.9753086419753086
2019-03-03T14:55:14.774640: step 148, loss 0.0543162, accuracy 0.984375, precision 0.9882352941176471, recall 0.9655172413793104
2019-03-03T14:55:15.422906: step 149, loss 0.040604, accuracy 0.992188, precision 0.9893617021276596, recall 0.9893617021276596
2019-03-03T14:55:16.071172: step 150, loss 0.0402902, accuracy 0.992188, precision 0.9894736842105263, recall 0.9894736842105263
2019-03-03T14:55:16.710463: step 151, loss 0.0449008, accuracy 0.992188, precision 0.9897959183673469, recall 0.9897959183673469
2019-03-03T14:55:17.355738: step 152, loss 0.0541811, accuracy 0.984375, precision 0.9789473684210527, recall 0.9789473684210527
2019-03-03T14:55:17.674885: step 153, loss 0.0204993, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:18.323151: step 154, loss 0.0338102, accuracy 0.996094, precision 0.98989898989899, recall 1.0
2019-03-03T14:55:18.965434: step 155, loss 0.0388267, accuracy 0.988281, precision 0.979381443298969, recall 0.9895833333333334
2019-03-03T14:55:19.625669: step 156, loss 0.0421723, accuracy 0.992188, precision 1.0, recall 0.9761904761904762
2019-03-03T14:55:20.271941: step 157, loss 0.0251604, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:20.930181: step 158, loss 0.0290674, accuracy 0.996094, precision 1.0, recall 0.9893617021276596
2019-03-03T14:55:21.584432: step 159, loss 0.0409473, accuracy 0.992188, precision 0.9764705882352941, recall 1.0
2019-03-03T14:55:22.246661: step 160, loss 0.0326374, accuracy 0.988281, precision 0.9782608695652174, recall 0.989010989010989
2019-03-03T14:55:22.908891: step 161, loss 0.027624, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:23.574113: step 162, loss 0.0337909, accuracy 0.988281, precision 1.0, recall 0.9711538461538461
2019-03-03T14:55:24.233349: step 163, loss 0.0306937, accuracy 0.992188, precision 0.9893617021276596, recall 0.9893617021276596
2019-03-03T14:55:24.889595: step 164, loss 0.0392594, accuracy 0.992188, precision 0.99, recall 0.99
2019-03-03T14:55:25.558806: step 165, loss 0.0188093, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:26.218044: step 166, loss 0.0214334, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:26.865313: step 167, loss 0.0451577, accuracy 0.980469, precision 0.9714285714285714, recall 0.9807692307692307
2019-03-03T14:55:27.518568: step 168, loss 0.0410627, accuracy 0.988281, precision 0.9894736842105263, recall 0.9791666666666666
2019-03-03T14:55:28.158854: step 169, loss 0.0512915, accuracy 0.972656, precision 0.9880952380952381, recall 0.9325842696629213
2019-03-03T14:55:28.489969: step 170, loss 0.0215868, accuracy 0.991453, precision 1.0, recall 0.9761904761904762
2019-03-03T14:55:29.138236: step 171, loss 0.0328065, accuracy 0.992188, precision 0.98989898989899, recall 0.98989898989899
2019-03-03T14:55:29.780520: step 172, loss 0.0175175, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:30.428787: step 173, loss 0.030625, accuracy 0.996094, precision 0.9888888888888889, recall 1.0
2019-03-03T14:55:31.083037: step 174, loss 0.0204976, accuracy 0.992188, precision 0.989247311827957, recall 0.989247311827957
2019-03-03T14:55:31.745267: step 175, loss 0.0230311, accuracy 0.996094, precision 0.9901960784313726, recall 1.0
2019-03-03T14:55:32.390540: step 176, loss 0.0287282, accuracy 0.988281, precision 0.9893617021276596, recall 0.9789473684210527
2019-03-03T14:55:33.037810: step 177, loss 0.0214324, accuracy 0.996094, precision 0.989010989010989, recall 1.0
2019-03-03T14:55:33.685081: step 178, loss 0.0238864, accuracy 0.992188, precision 0.978494623655914, recall 1.0
2019-03-03T14:55:34.329357: step 179, loss 0.0373864, accuracy 0.992188, precision 0.989010989010989, recall 0.989010989010989
2019-03-03T14:55:34.976627: step 180, loss 0.0277943, accuracy 0.992188, precision 0.9764705882352941, recall 1.0
2019-03-03T14:55:35.623895: step 181, loss 0.0245784, accuracy 0.996094, precision 1.0, recall 0.9886363636363636
2019-03-03T14:55:36.269171: step 182, loss 0.0428113, accuracy 0.988281, precision 0.9885057471264368, recall 0.9772727272727273
2019-03-03T14:55:36.918436: step 183, loss 0.0240978, accuracy 0.992188, precision 0.9897959183673469, recall 0.9897959183673469
2019-03-03T14:55:37.578670: step 184, loss 0.0179045, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:38.232921: step 185, loss 0.0316118, accuracy 0.984375, precision 0.96875, recall 0.9893617021276596
2019-03-03T14:55:38.880189: step 186, loss 0.0282753, accuracy 0.996094, precision 0.9894736842105263, recall 1.0
2019-03-03T14:55:39.208314: step 187, loss 0.0690599, accuracy 0.974359, precision 0.972972972972973, recall 0.9473684210526315
2019-03-03T14:55:39.863562: step 188, loss 0.0239688, accuracy 0.992188, precision 1.0, recall 0.9761904761904762
2019-03-03T14:55:40.501854: step 189, loss 0.0190094, accuracy 0.996094, precision 1.0, recall 0.989247311827957
2019-03-03T14:55:41.156106: step 190, loss 0.0229805, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:41.795397: step 191, loss 0.020091, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:42.434688: step 192, loss 0.022269, accuracy 0.992188, precision 0.9767441860465116, recall 1.0
2019-03-03T14:55:43.072979: step 193, loss 0.0232991, accuracy 0.996094, precision 0.9896907216494846, recall 1.0
2019-03-03T14:55:43.704292: step 194, loss 0.0157938, accuracy 0.996094, precision 1.0, recall 0.9878048780487805
2019-03-03T14:55:44.355551: step 195, loss 0.0147629, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:44.996837: step 196, loss 0.0164396, accuracy 0.996094, precision 1.0, recall 0.9883720930232558
2019-03-03T14:55:45.662057: step 197, loss 0.0164191, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:46.318303: step 198, loss 0.0148379, accuracy 0.996094, precision 1.0, recall 0.9896907216494846
2019-03-03T14:55:46.956596: step 199, loss 0.0225361, accuracy 0.996094, precision 1.0, recall 0.9907407407407407
2019-03-03T14:55:47.597883: step 200, loss 0.0229479, accuracy 0.996094, precision 1.0, recall 0.9905660377358491
2019-03-03T14:55:48.235179: step 201, loss 0.0342758, accuracy 0.984375, precision 0.9764705882352941, recall 0.9764705882352941
2019-03-03T14:55:48.892422: step 202, loss 0.0255704, accuracy 0.992188, precision 0.9895833333333334, recall 0.9895833333333334
2019-03-03T14:55:49.547670: step 203, loss 0.0282826, accuracy 0.988281, precision 1.0, recall 0.9680851063829787
2019-03-03T14:55:49.871804: step 204, loss 0.0592767, accuracy 0.982906, precision 0.975, recall 0.975
2019-03-03T14:55:50.526055: step 205, loss 0.032206, accuracy 0.996094, precision 1.0, recall 0.9905660377358491
2019-03-03T14:55:51.190277: step 206, loss 0.0120682, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:51.841537: step 207, loss 0.0172305, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:52.494789: step 208, loss 0.0212662, accuracy 0.996094, precision 1.0, recall 0.9861111111111112
2019-03-03T14:55:53.140065: step 209, loss 0.0136779, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:53.785339: step 210, loss 0.0230931, accuracy 0.996094, precision 1.0, recall 0.99
2019-03-03T14:55:54.425627: step 211, loss 0.0146461, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:55:55.057938: step 212, loss 0.0161883, accuracy 0.996094, precision 0.989010989010989, recall 1.0
2019-03-03T14:55:55.690247: step 213, loss 0.0192302, accuracy 0.996094, precision 0.9882352941176471, recall 1.0
2019-03-03T14:55:56.345494: step 214, loss 0.0136108, accuracy 0.996094, precision 0.9900990099009901, recall 1.0
2019-03-03T14:55:56.994758: step 215, loss 0.0241555, accuracy 0.996094, precision 1.0, recall 0.9878048780487805
2019-03-03T14:55:57.646019: step 216, loss 0.0135019, accuracy 0.996094, precision 1.0, recall 0.9896907216494846
2019-03-03T14:55:58.298275: step 217, loss 0.0154109, accuracy 0.996094, precision 1.0, recall 0.99
2019-03-03T14:55:58.945544: step 218, loss 0.0227729, accuracy 0.992188, precision 0.975609756097561, recall 1.0
2019-03-03T14:55:59.587825: step 219, loss 0.0107602, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:00.241079: step 220, loss 0.0177844, accuracy 0.996094, precision 1.0, recall 0.9887640449438202
2019-03-03T14:56:00.559228: step 221, loss 0.0113131, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:01.202508: step 222, loss 0.0159415, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:01.842798: step 223, loss 0.0199467, accuracy 0.996094, precision 0.9887640449438202, recall 1.0
2019-03-03T14:56:02.474110: step 224, loss 0.0127519, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:03.122375: step 225, loss 0.0303435, accuracy 0.992188, precision 1.0, recall 0.9805825242718447
2019-03-03T14:56:03.771640: step 226, loss 0.0163123, accuracy 0.996094, precision 0.9896907216494846, recall 1.0
2019-03-03T14:56:04.426888: step 227, loss 0.0335913, accuracy 0.992188, precision 1.0, recall 0.9807692307692307
2019-03-03T14:56:05.068175: step 228, loss 0.0133405, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:05.714445: step 229, loss 0.0172173, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:06.360718: step 230, loss 0.0200251, accuracy 0.992188, precision 1.0, recall 0.9764705882352941
2019-03-03T14:56:07.003000: step 231, loss 0.0182488, accuracy 0.996094, precision 1.0, recall 0.9883720930232558
2019-03-03T14:56:07.645283: step 232, loss 0.0105987, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:08.289562: step 233, loss 0.0150718, accuracy 0.996094, precision 1.0, recall 0.9888888888888889
2019-03-03T14:56:08.916884: step 234, loss 0.0168506, accuracy 0.996094, precision 1.0, recall 0.9896907216494846
2019-03-03T14:56:09.561161: step 235, loss 0.00506554, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:10.215412: step 236, loss 0.0243956, accuracy 0.988281, precision 0.978494623655914, recall 0.9891304347826086
2019-03-03T14:56:10.865674: step 237, loss 0.00709968, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:11.189806: step 238, loss 0.0121107, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:11.876970: step 239, loss 0.0220785, accuracy 0.988281, precision 0.9878048780487805, recall 0.9759036144578314
2019-03-03T14:56:12.535210: step 240, loss 0.00618986, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:13.186469: step 241, loss 0.013771, accuracy 0.992188, precision 0.9809523809523809, recall 1.0
2019-03-03T14:56:13.830746: step 242, loss 0.0275793, accuracy 0.992188, precision 0.9777777777777777, recall 1.0
2019-03-03T14:56:14.487988: step 243, loss 0.0177357, accuracy 0.992188, precision 0.9803921568627451, recall 1.0
2019-03-03T14:56:15.150217: step 244, loss 0.00837715, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:15.810454: step 245, loss 0.0126406, accuracy 0.996094, precision 0.9887640449438202, recall 1.0
2019-03-03T14:56:16.462709: step 246, loss 0.0201408, accuracy 0.992188, precision 0.9891304347826086, recall 0.9891304347826086
2019-03-03T14:56:17.125935: step 247, loss 0.012938, accuracy 0.996094, precision 1.0, recall 0.9900990099009901
2019-03-03T14:56:17.788165: step 248, loss 0.0142957, accuracy 0.996094, precision 0.98989898989899, recall 1.0
2019-03-03T14:56:18.443413: step 249, loss 0.029076, accuracy 0.984375, precision 0.9534883720930233, recall 1.0
2019-03-03T14:56:19.103649: step 250, loss 0.0163456, accuracy 0.996094, precision 1.0, recall 0.9880952380952381

Evaluation:
[[151  21]
 [ 18 278]]
2019-03-03T14:56:19.540480: step 250, loss 0.268206, accuracy 0.916667, precision 0.877906976744186, recall 0.893491124260355

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551646409\checkpoints\model-250

2019-03-03T14:56:20.427110: step 251, loss 0.0121166, accuracy 0.996094, precision 0.9903846153846154, recall 1.0
2019-03-03T14:56:21.088343: step 252, loss 0.0114827, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:21.747580: step 253, loss 0.0279393, accuracy 0.992188, precision 1.0, recall 0.9787234042553191
2019-03-03T14:56:22.414795: step 254, loss 0.0127411, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:22.740923: step 255, loss 0.01239, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:23.397169: step 256, loss 0.0269088, accuracy 0.992188, precision 1.0, recall 0.9782608695652174
2019-03-03T14:56:24.043441: step 257, loss 0.0119019, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:24.698690: step 258, loss 0.00995186, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:25.337980: step 259, loss 0.00973664, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:25.977273: step 260, loss 0.00968074, accuracy 0.996094, precision 0.9897959183673469, recall 1.0
2019-03-03T14:56:26.616562: step 261, loss 0.0091213, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:27.255852: step 262, loss 0.0112627, accuracy 0.996094, precision 1.0, recall 0.9879518072289156
2019-03-03T14:56:27.906113: step 263, loss 0.00730729, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:28.566348: step 264, loss 0.0141709, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:29.213619: step 265, loss 0.00749762, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:29.863880: step 266, loss 0.0185549, accuracy 0.996094, precision 0.9896907216494846, recall 1.0
2019-03-03T14:56:30.507160: step 267, loss 0.00851004, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:31.155426: step 268, loss 0.0176496, accuracy 0.992188, precision 0.9795918367346939, recall 1.0
2019-03-03T14:56:31.828626: step 269, loss 0.0154664, accuracy 0.996094, precision 0.9893617021276596, recall 1.0
2019-03-03T14:56:32.506813: step 270, loss 0.0094995, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:33.154083: step 271, loss 0.0194465, accuracy 0.988281, precision 0.978021978021978, recall 0.9888888888888889
2019-03-03T14:56:33.492179: step 272, loss 0.00935512, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:34.150419: step 273, loss 0.00423586, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:34.800680: step 274, loss 0.0114301, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:35.451939: step 275, loss 0.00923644, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:36.096216: step 276, loss 0.0114547, accuracy 0.996094, precision 1.0, recall 0.989010989010989
2019-03-03T14:56:36.735507: step 277, loss 0.01161, accuracy 0.996094, precision 1.0, recall 0.9896907216494846
2019-03-03T14:56:37.408708: step 278, loss 0.00676479, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:38.049993: step 279, loss 0.00977904, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:38.693273: step 280, loss 0.00617704, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:39.329572: step 281, loss 0.00945272, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:39.960885: step 282, loss 0.00820268, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:40.613141: step 283, loss 0.00762237, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:41.260410: step 284, loss 0.00651653, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:41.906681: step 285, loss 0.0104306, accuracy 0.996094, precision 0.99, recall 1.0
2019-03-03T14:56:42.563925: step 286, loss 0.00982708, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:43.205210: step 287, loss 0.00736444, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:43.849488: step 288, loss 0.0199455, accuracy 0.996094, precision 1.0, recall 0.9879518072289156
2019-03-03T14:56:44.170628: step 289, loss 0.00322796, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:44.815904: step 290, loss 0.0119772, accuracy 0.996094, precision 1.0, recall 0.9886363636363636
2019-03-03T14:56:45.466166: step 291, loss 0.00574584, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:46.120416: step 292, loss 0.00875935, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:46.767684: step 293, loss 0.00983885, accuracy 0.996094, precision 1.0, recall 0.990990990990991
2019-03-03T14:56:47.431909: step 294, loss 0.00618112, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:48.075190: step 295, loss 0.00526447, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:48.721462: step 296, loss 0.00691314, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:49.363745: step 297, loss 0.00701667, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:49.998050: step 298, loss 0.00509715, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:50.647314: step 299, loss 0.0101894, accuracy 0.996094, precision 1.0, recall 0.9895833333333334
2019-03-03T14:56:51.281617: step 300, loss 0.0127496, accuracy 0.996094, precision 0.9891304347826086, recall 1.0
2019-03-03T14:56:51.909936: step 301, loss 0.00693173, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:52.552220: step 302, loss 0.00722051, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:53.190513: step 303, loss 0.0206367, accuracy 0.988281, precision 0.9702970297029703, recall 1.0
2019-03-03T14:56:53.818834: step 304, loss 0.0140268, accuracy 0.996094, precision 0.9891304347826086, recall 1.0
2019-03-03T14:56:54.456129: step 305, loss 0.00856567, accuracy 0.996094, precision 1.0, recall 0.9888888888888889
2019-03-03T14:56:54.829132: step 306, loss 0.00538898, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:55.507318: step 307, loss 0.0108602, accuracy 0.996094, precision 0.9906542056074766, recall 1.0
2019-03-03T14:56:56.145611: step 308, loss 0.00789324, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:56.786898: step 309, loss 0.00842721, accuracy 0.996094, precision 0.9879518072289156, recall 1.0
2019-03-03T14:56:57.402253: step 310, loss 0.00568035, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:58.024589: step 311, loss 0.00746626, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:58.653906: step 312, loss 0.00661593, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:59.338077: step 313, loss 0.00384355, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:56:59.998312: step 314, loss 0.00586797, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:00.657549: step 315, loss 0.0041369, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:01.302824: step 316, loss 0.0050387, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:01.955081: step 317, loss 0.00908474, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:02.633266: step 318, loss 0.00792718, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:03.291507: step 319, loss 0.00375972, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:03.951743: step 320, loss 0.0155877, accuracy 0.988281, precision 0.9880952380952381, recall 0.9764705882352941
2019-03-03T14:57:04.606991: step 321, loss 0.0157379, accuracy 0.996094, precision 1.0, recall 0.9880952380952381
2019-03-03T14:57:05.270217: step 322, loss 0.00759164, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:05.594351: step 323, loss 0.0127906, accuracy 0.991453, precision 1.0, recall 0.9795918367346939
2019-03-03T14:57:06.241619: step 324, loss 0.00318858, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:06.880910: step 325, loss 0.010441, accuracy 0.996094, precision 0.9900990099009901, recall 1.0
2019-03-03T14:57:07.531172: step 326, loss 0.00314268, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:08.172458: step 327, loss 0.016548, accuracy 0.996094, precision 0.9876543209876543, recall 1.0
2019-03-03T14:57:08.813743: step 328, loss 0.00989214, accuracy 0.996094, precision 1.0, recall 0.9893617021276596
2019-03-03T14:57:09.454031: step 329, loss 0.00627016, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:10.088337: step 330, loss 0.00617233, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:10.724634: step 331, loss 0.0065479, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:11.362926: step 332, loss 0.00813433, accuracy 0.996094, precision 0.9894736842105263, recall 1.0
2019-03-03T14:57:12.012191: step 333, loss 0.00390508, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:12.667439: step 334, loss 0.00293008, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:13.309722: step 335, loss 0.00621435, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:13.958986: step 336, loss 0.00270354, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:14.603264: step 337, loss 0.00692209, accuracy 0.996094, precision 1.0, recall 0.9886363636363636
2019-03-03T14:57:15.242555: step 338, loss 0.00563064, accuracy 0.996094, precision 0.9896907216494846, recall 1.0
2019-03-03T14:57:15.902789: step 339, loss 0.00491483, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:16.235900: step 340, loss 0.00897566, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:16.897131: step 341, loss 0.00598454, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:17.552380: step 342, loss 0.00240337, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:18.210618: step 343, loss 0.00280367, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:18.859887: step 344, loss 0.00339031, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:19.505157: step 345, loss 0.0157749, accuracy 0.996094, precision 1.0, recall 0.9896907216494846
2019-03-03T14:57:20.146444: step 346, loss 0.00513306, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:20.785734: step 347, loss 0.00429733, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:21.426022: step 348, loss 0.00535521, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:22.072294: step 349, loss 0.00299117, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:22.738512: step 350, loss 0.00728084, accuracy 0.996094, precision 0.9900990099009901, recall 1.0
2019-03-03T14:57:23.402738: step 351, loss 0.00491744, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:24.052002: step 352, loss 0.00317419, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:24.702262: step 353, loss 0.00466136, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:25.352525: step 354, loss 0.0192236, accuracy 0.992188, precision 0.9891304347826086, recall 0.9891304347826086
2019-03-03T14:57:25.993810: step 355, loss 0.00451788, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:26.637091: step 356, loss 0.00615661, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:26.961224: step 357, loss 0.0125183, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:27.600513: step 358, loss 0.00262366, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:28.253768: step 359, loss 0.00612121, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:28.892062: step 360, loss 0.00632939, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:29.542324: step 361, loss 0.00355302, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:30.188595: step 362, loss 0.00351187, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:30.848829: step 363, loss 0.00654235, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:31.495100: step 364, loss 0.00660604, accuracy 0.996094, precision 0.989247311827957, recall 1.0
2019-03-03T14:57:32.140376: step 365, loss 0.00488265, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:32.793630: step 366, loss 0.0105606, accuracy 0.996094, precision 1.0, recall 0.9901960784313726
2019-03-03T14:57:33.440899: step 367, loss 0.00268121, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:34.077198: step 368, loss 0.0108895, accuracy 0.996094, precision 1.0, recall 0.9876543209876543
2019-03-03T14:57:34.717488: step 369, loss 0.00297542, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:35.371737: step 370, loss 0.00736402, accuracy 0.996094, precision 0.9893617021276596, recall 1.0
2019-03-03T14:57:36.032969: step 371, loss 0.00482224, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:36.692207: step 372, loss 0.0140018, accuracy 0.996094, precision 0.9900990099009901, recall 1.0
2019-03-03T14:57:37.341471: step 373, loss 0.00228109, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:37.666601: step 374, loss 0.00351766, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:38.323844: step 375, loss 0.00519882, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:38.977097: step 376, loss 0.00614876, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:39.620377: step 377, loss 0.00347461, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:40.263657: step 378, loss 0.00345521, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:40.895971: step 379, loss 0.00242426, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:41.539247: step 380, loss 0.0122951, accuracy 0.996094, precision 0.99, recall 1.0
2019-03-03T14:57:42.175545: step 381, loss 0.00749924, accuracy 0.996094, precision 1.0, recall 0.989010989010989
2019-03-03T14:57:42.821818: step 382, loss 0.00809057, accuracy 0.996094, precision 1.0, recall 0.9885057471264368
2019-03-03T14:57:43.493024: step 383, loss 0.0112023, accuracy 0.996094, precision 0.9882352941176471, recall 1.0
2019-03-03T14:57:44.143284: step 384, loss 0.00385167, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:44.784571: step 385, loss 0.00697262, accuracy 0.996094, precision 1.0, recall 0.9895833333333334
2019-03-03T14:57:45.425855: step 386, loss 0.0042851, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:46.077115: step 387, loss 0.0195187, accuracy 0.992188, precision 0.9904761904761905, recall 0.9904761904761905
2019-03-03T14:57:46.714411: step 388, loss 0.00443022, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:47.371654: step 389, loss 0.00712341, accuracy 0.996094, precision 1.0, recall 0.9883720930232558
2019-03-03T14:57:48.021914: step 390, loss 0.00459145, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:48.362005: step 391, loss 0.00313976, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:49.019249: step 392, loss 0.00440207, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:49.668515: step 393, loss 0.00414836, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:50.316779: step 394, loss 0.00419362, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:50.961056: step 395, loss 0.00263344, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:51.607330: step 396, loss 0.00944813, accuracy 0.996094, precision 0.9879518072289156, recall 1.0
2019-03-03T14:57:52.251606: step 397, loss 0.00917223, accuracy 0.996094, precision 0.9897959183673469, recall 1.0
2019-03-03T14:57:52.900870: step 398, loss 0.00551816, accuracy 0.996094, precision 0.9897959183673469, recall 1.0
2019-03-03T14:57:53.547143: step 399, loss 0.0035373, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:54.198401: step 400, loss 0.00441246, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:54.843676: step 401, loss 0.00682583, accuracy 0.996094, precision 0.9888888888888889, recall 1.0
2019-03-03T14:57:55.502913: step 402, loss 0.00491135, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:56.173122: step 403, loss 0.0280865, accuracy 0.988281, precision 1.0, recall 0.9714285714285714
2019-03-03T14:57:56.820391: step 404, loss 0.0064797, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:57.466664: step 405, loss 0.00459233, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:58.110941: step 406, loss 0.0165721, accuracy 0.996094, precision 0.9879518072289156, recall 1.0
2019-03-03T14:57:58.763199: step 407, loss 0.0034215, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:59.087330: step 408, loss 0.00235919, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:57:59.735597: step 409, loss 0.00273398, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:00.375886: step 410, loss 0.00290148, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:01.021161: step 411, loss 0.00271302, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:01.670424: step 412, loss 0.0154088, accuracy 0.996094, precision 1.0, recall 0.9893617021276596
2019-03-03T14:58:02.322680: step 413, loss 0.00483271, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:02.971945: step 414, loss 0.0086907, accuracy 0.996094, precision 0.9880952380952381, recall 1.0
2019-03-03T14:58:03.633177: step 415, loss 0.00310626, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:04.277455: step 416, loss 0.0048013, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:04.912756: step 417, loss 0.00740418, accuracy 0.996094, precision 1.0, recall 0.987012987012987
2019-03-03T14:58:05.556035: step 418, loss 0.00250481, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:06.203304: step 419, loss 0.0096622, accuracy 0.996094, precision 0.9882352941176471, recall 1.0
2019-03-03T14:58:06.846586: step 420, loss 0.00874259, accuracy 0.996094, precision 1.0, recall 0.9882352941176471
2019-03-03T14:58:07.496847: step 421, loss 0.0070211, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:08.154090: step 422, loss 0.00389959, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:08.813327: step 423, loss 0.00445942, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:09.458602: step 424, loss 0.00247169, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:09.781739: step 425, loss 0.00203071, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:10.428009: step 426, loss 0.00280357, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:11.076276: step 427, loss 0.00333058, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:11.719557: step 428, loss 0.00242242, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:12.351865: step 429, loss 0.00331556, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:12.984176: step 430, loss 0.00199517, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:13.625461: step 431, loss 0.00492727, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:14.276720: step 432, loss 0.00774297, accuracy 0.996094, precision 1.0, recall 0.9882352941176471
2019-03-03T14:58:14.922992: step 433, loss 0.00425325, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:15.578240: step 434, loss 0.0130845, accuracy 0.996094, precision 1.0, recall 0.9897959183673469
2019-03-03T14:58:16.218530: step 435, loss 0.00210455, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:16.858816: step 436, loss 0.00571534, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:17.515062: step 437, loss 0.00337492, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:18.153355: step 438, loss 0.00194802, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:18.807607: step 439, loss 0.0033561, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:19.468838: step 440, loss 0.00257976, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:20.125083: step 441, loss 0.00488308, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:20.454203: step 442, loss 0.00952481, accuracy 0.991453, precision 0.972972972972973, recall 1.0
2019-03-03T14:58:21.103469: step 443, loss 0.00412566, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:21.757719: step 444, loss 0.00354079, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:22.400003: step 445, loss 0.00145468, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:23.048269: step 446, loss 0.00481567, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:23.687559: step 447, loss 0.00171711, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:24.334829: step 448, loss 0.00159575, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:24.979105: step 449, loss 0.00804686, accuracy 0.996094, precision 0.9897959183673469, recall 1.0
2019-03-03T14:58:25.626376: step 450, loss 0.00177517, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:26.278631: step 451, loss 0.00495943, accuracy 0.996094, precision 1.0, recall 0.9878048780487805
2019-03-03T14:58:26.930888: step 452, loss 0.00730196, accuracy 0.996094, precision 0.9906542056074766, recall 1.0
2019-03-03T14:58:27.581148: step 453, loss 0.00935004, accuracy 0.996094, precision 1.0, recall 0.9885057471264368
2019-03-03T14:58:28.234402: step 454, loss 0.00303326, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:28.881671: step 455, loss 0.0088465, accuracy 0.996094, precision 0.9896907216494846, recall 1.0
2019-03-03T14:58:29.529939: step 456, loss 0.00230956, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:30.177208: step 457, loss 0.00255801, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:30.819490: step 458, loss 0.016781, accuracy 0.996094, precision 0.989247311827957, recall 1.0
2019-03-03T14:58:31.143624: step 459, loss 0.00153361, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:31.792888: step 460, loss 0.00165486, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:32.453123: step 461, loss 0.00438796, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:33.100393: step 462, loss 0.00190063, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:33.753647: step 463, loss 0.00151248, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:34.415875: step 464, loss 0.00228622, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:35.066138: step 465, loss 0.0073657, accuracy 0.996094, precision 1.0, recall 0.9894736842105263
2019-03-03T14:58:35.708420: step 466, loss 0.0154314, accuracy 0.992188, precision 0.9906542056074766, recall 0.9906542056074766
2019-03-03T14:58:36.350703: step 467, loss 0.00400067, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:36.990990: step 468, loss 0.00151602, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:37.634271: step 469, loss 0.0036621, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:38.279547: step 470, loss 0.00144928, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:38.933796: step 471, loss 0.00418786, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:39.601013: step 472, loss 0.00393132, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:40.243295: step 473, loss 0.00338657, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:40.889566: step 474, loss 0.00978693, accuracy 0.996094, precision 0.9883720930232558, recall 1.0
2019-03-03T14:58:41.540825: step 475, loss 0.00164499, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:41.861966: step 476, loss 0.0241644, accuracy 0.991453, precision 0.9807692307692307, recall 1.0
2019-03-03T14:58:42.499266: step 477, loss 0.00159166, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:43.127616: step 478, loss 0.00134124, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:43.762884: step 479, loss 0.00385977, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:44.398187: step 480, loss 0.00439077, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:45.023514: step 481, loss 0.00406857, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:45.662805: step 482, loss 0.00259344, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:46.330022: step 483, loss 0.00266056, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:46.986266: step 484, loss 0.00237632, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:47.730279: step 485, loss 0.00204591, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:48.430407: step 486, loss 0.00365374, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:49.109591: step 487, loss 0.00182875, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:49.881529: step 488, loss 0.00424534, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:50.580657: step 489, loss 0.00212588, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:51.300733: step 490, loss 0.00125777, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:52.146471: step 491, loss 0.0025259, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:53.002183: step 492, loss 0.0310962, accuracy 0.996094, precision 1.0, recall 0.9893617021276596
2019-03-03T14:58:53.456967: step 493, loss 0.00162952, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:54.256828: step 494, loss 0.0139611, accuracy 0.996094, precision 1.0, recall 0.9882352941176471
2019-03-03T14:58:55.255161: step 495, loss 0.00166111, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:56.151765: step 496, loss 0.00568457, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:57.008472: step 497, loss 0.00283278, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:57.769441: step 498, loss 0.00128542, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:58.590242: step 499, loss 0.00319306, accuracy 1, precision 1.0, recall 1.0
2019-03-03T14:58:59.355198: step 500, loss 0.00234053, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[152  20]
 [ 15 281]]
2019-03-03T14:58:59.775075: step 500, loss 0.360856, accuracy 0.925214, precision 0.8837209302325582, recall 0.9101796407185628

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1551646409\checkpoints\model-500


Process finished with exit code 0
