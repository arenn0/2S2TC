Pretrained Embedding: ELMo
Italian: False
Loading data...
11566
Max Document length: 36
Vocabulary Size: 1
Train/Dev split: 11335/231
ELMo module loaded from tensorflow-hub
34
33
32
Writing to /home/ubuntu/Project/runs/1551753005

2019-03-05T02:30:13.105973: step 1, loss 15.7974, accuracy 0.347656, precision 0.005952380952380952, recall 1.0
2019-03-05T02:30:15.057041: step 2, loss 4.32115, accuracy 0.589844, precision 0.8165680473372781, recall 0.6509433962264151
2019-03-05T02:30:16.984095: step 3, loss 7.63354, accuracy 0.652344, precision 0.9821428571428571, recall 0.6573705179282868
2019-03-05T02:30:18.917966: step 4, loss 6.24722, accuracy 0.726562, precision 0.9946524064171123, recall 0.7294117647058823
2019-03-05T02:30:20.849949: step 5, loss 4.89807, accuracy 0.714844, precision 0.9675675675675676, recall 0.7276422764227642
2019-03-05T02:30:22.785271: step 6, loss 3.75398, accuracy 0.65625, precision 0.9308176100628931, recall 0.6577777777777778
2019-03-05T02:30:24.705663: step 7, loss 2.77893, accuracy 0.660156, precision 0.6526946107784432, recall 0.7898550724637681
2019-03-05T02:30:26.631126: step 8, loss 3.8451, accuracy 0.566406, precision 0.42441860465116277, recall 0.8588235294117647
2019-03-05T02:30:28.575439: step 9, loss 4.57977, accuracy 0.582031, precision 0.47802197802197804, recall 0.8787878787878788
2019-03-05T02:30:30.504548: step 10, loss 2.88502, accuracy 0.695312, precision 0.6086956521739131, recall 0.8672566371681416
2019-03-05T02:30:32.433996: step 11, loss 1.90443, accuracy 0.78125, precision 0.7891566265060241, recall 0.8618421052631579
2019-03-05T02:30:34.359409: step 12, loss 2.40893, accuracy 0.746094, precision 0.8520710059171598, recall 0.782608695652174
2019-03-05T02:30:36.293817: step 13, loss 2.72341, accuracy 0.757812, precision 0.9333333333333333, recall 0.751219512195122
2019-03-05T02:30:38.217371: step 14, loss 2.74751, accuracy 0.78125, precision 0.9202127659574468, recall 0.8084112149532711
2019-03-05T02:30:40.161964: step 15, loss 3.8145, accuracy 0.691406, precision 0.9352941176470588, recall 0.7004405286343612
2019-03-05T02:30:42.088121: step 16, loss 2.65185, accuracy 0.746094, precision 0.9132947976878613, recall 0.7596153846153846
2019-03-05T02:30:44.018483: step 17, loss 2.64253, accuracy 0.738281, precision 0.879746835443038, recall 0.7433155080213903
2019-03-05T02:30:45.942653: step 18, loss 2.03276, accuracy 0.753906, precision 0.8113207547169812, recall 0.7962962962962963
2019-03-05T02:30:47.886194: step 19, loss 1.97666, accuracy 0.773438, precision 0.7272727272727273, recall 0.875
2019-03-05T02:30:49.808507: step 20, loss 2.449, accuracy 0.71875, precision 0.625, recall 0.9210526315789473
2019-03-05T02:30:51.746074: step 21, loss 2.04448, accuracy 0.746094, precision 0.6449704142011834, recall 0.956140350877193
2019-03-05T02:30:53.682459: step 22, loss 2.57906, accuracy 0.695312, precision 0.6211180124223602, recall 0.8547008547008547
2019-03-05T02:30:55.639585: step 23, loss 1.61734, accuracy 0.765625, precision 0.7616279069767442, recall 0.8733333333333333
2019-03-05T02:30:57.581338: step 24, loss 1.49461, accuracy 0.828125, precision 0.8757396449704142, recall 0.8654970760233918
2019-03-05T02:30:59.522358: step 25, loss 1.7674, accuracy 0.800781, precision 0.8956043956043956, recall 0.8358974358974359
2019-03-05T02:31:01.466131: step 26, loss 2.73493, accuracy 0.730469, precision 0.9302325581395349, recall 0.7373271889400922
2019-03-05T02:31:03.417749: step 27, loss 2.076, accuracy 0.761719, precision 0.9375, recall 0.7674418604651163
2019-03-05T02:31:05.349512: step 28, loss 2.16211, accuracy 0.757812, precision 0.9408284023668639, recall 0.7535545023696683
2019-03-05T02:31:07.281607: step 29, loss 1.83841, accuracy 0.773438, precision 0.9107142857142857, recall 0.7806122448979592
2019-03-05T02:31:09.219108: step 30, loss 1.43564, accuracy 0.792969, precision 0.8218390804597702, recall 0.8666666666666667
2019-03-05T02:31:11.179511: step 31, loss 1.66993, accuracy 0.785156, precision 0.7975460122699386, recall 0.8552631578947368
2019-03-05T02:31:13.135529: step 32, loss 1.48939, accuracy 0.769531, precision 0.7317073170731707, recall 0.8888888888888888
2019-03-05T02:31:15.064620: step 33, loss 1.94312, accuracy 0.75, precision 0.7349397590361446, recall 0.8591549295774648
2019-03-05T02:31:17.022129: step 34, loss 1.96189, accuracy 0.726562, precision 0.6608187134502924, recall 0.904
2019-03-05T02:31:18.949646: step 35, loss 1.69593, accuracy 0.757812, precision 0.754601226993865, recall 0.8482758620689655
2019-03-05T02:31:20.893740: step 36, loss 1.11074, accuracy 0.8125, precision 0.8662420382165605, recall 0.8343558282208589
2019-03-05T02:31:22.842735: step 37, loss 0.910415, accuracy 0.84375, precision 0.8901734104046243, recall 0.88
2019-03-05T02:31:24.776480: step 38, loss 0.886637, accuracy 0.84375, precision 0.9166666666666666, recall 0.868421052631579
2019-03-05T02:31:26.749114: step 39, loss 1.247, accuracy 0.824219, precision 0.9386503067484663, recall 0.8138297872340425
2019-03-05T02:31:28.686443: step 40, loss 1.63826, accuracy 0.773438, precision 0.8846153846153846, recall 0.8131313131313131
2019-03-05T02:31:30.624581: step 41, loss 1.42509, accuracy 0.785156, precision 0.8932584269662921, recall 0.8153846153846154
2019-03-05T02:31:32.564937: step 42, loss 1.23577, accuracy 0.78125, precision 0.8614457831325302, recall 0.8125
2019-03-05T02:31:34.512024: step 43, loss 1.09551, accuracy 0.8125, precision 0.845679012345679, recall 0.85625
2019-03-05T02:31:36.460965: step 44, loss 1.0661, accuracy 0.777344, precision 0.7740112994350282, recall 0.8896103896103896
2019-03-05T02:31:38.625304: step 45, loss 1.65094, accuracy 0.732394, precision 0.7804878048780488, recall 0.7619047619047619
2019-03-05T02:31:40.547189: step 46, loss 1.03228, accuracy 0.820312, precision 0.8202247191011236, recall 0.9125
2019-03-05T02:31:42.501665: step 47, loss 1.23738, accuracy 0.777344, precision 0.8181818181818182, recall 0.8333333333333334
2019-03-05T02:31:44.439247: step 48, loss 0.923077, accuracy 0.847656, precision 0.9036144578313253, recall 0.8670520231213873
2019-03-05T02:31:46.381166: step 49, loss 1.15204, accuracy 0.761719, precision 0.8342541436464088, recall 0.8296703296703297
2019-03-05T02:31:48.321686: step 50, loss 1.04103, accuracy 0.777344, precision 0.8461538461538461, recall 0.8
2019-03-05T02:31:50.272147: step 51, loss 0.808938, accuracy 0.78125, precision 0.8834355828220859, recall 0.7955801104972375
2019-03-05T02:31:52.227052: step 52, loss 0.900315, accuracy 0.800781, precision 0.8555555555555555, recall 0.8603351955307262
2019-03-05T02:31:54.158384: step 53, loss 0.898193, accuracy 0.820312, precision 0.844311377245509, recall 0.8757763975155279
2019-03-05T02:31:56.113578: step 54, loss 1.25216, accuracy 0.769531, precision 0.7662337662337663, recall 0.8368794326241135
2019-03-05T02:31:58.059139: step 55, loss 0.674123, accuracy 0.828125, precision 0.8486486486486486, recall 0.9075144508670521
2019-03-05T02:32:00.002592: step 56, loss 1.08566, accuracy 0.800781, precision 0.8414634146341463, recall 0.8466257668711656
2019-03-05T02:32:01.943085: step 57, loss 0.979367, accuracy 0.800781, precision 0.8457446808510638, recall 0.8784530386740331
2019-03-05T02:32:03.888859: step 58, loss 0.869074, accuracy 0.796875, precision 0.8650306748466258, recall 0.8245614035087719
2019-03-05T02:32:05.832233: step 59, loss 0.815948, accuracy 0.820312, precision 0.9151515151515152, recall 0.825136612021858
2019-03-05T02:32:07.779173: step 60, loss 1.11267, accuracy 0.796875, precision 0.8867924528301887, recall 0.8057142857142857
2019-03-05T02:32:09.710826: step 61, loss 0.774505, accuracy 0.824219, precision 0.8834355828220859, recall 0.8470588235294118
2019-03-05T02:32:11.659266: step 62, loss 0.754425, accuracy 0.832031, precision 0.8176470588235294, recall 0.9205298013245033
2019-03-05T02:32:13.585939: step 63, loss 0.86545, accuracy 0.773438, precision 0.8098159509202454, recall 0.8301886792452831
2019-03-05T02:32:15.536808: step 64, loss 0.874251, accuracy 0.773438, precision 0.7873563218390804, recall 0.8670886075949367
2019-03-05T02:32:17.485753: step 65, loss 0.797811, accuracy 0.785156, precision 0.8246753246753247, recall 0.8193548387096774
2019-03-05T02:32:19.454735: step 66, loss 0.717021, accuracy 0.804688, precision 0.8466257668711656, recall 0.8466257668711656
2019-03-05T02:32:21.393689: step 67, loss 0.80645, accuracy 0.808594, precision 0.875, recall 0.8609625668449198
2019-03-05T02:32:23.330574: step 68, loss 0.865172, accuracy 0.769531, precision 0.8641975308641975, recall 0.7909604519774012
2019-03-05T02:32:25.306293: step 69, loss 0.663251, accuracy 0.824219, precision 0.896774193548387, recall 0.8273809523809523
2019-03-05T02:32:27.268028: step 70, loss 0.660941, accuracy 0.835938, precision 0.8390804597701149, recall 0.9125
2019-03-05T02:32:29.222945: step 71, loss 0.808499, accuracy 0.804688, precision 0.8494623655913979, recall 0.8777777777777778
2019-03-05T02:32:31.156121: step 72, loss 0.573155, accuracy 0.847656, precision 0.8685714285714285, recall 0.9047619047619048
2019-03-05T02:32:33.081373: step 73, loss 0.784603, accuracy 0.796875, precision 0.891566265060241, recall 0.8131868131868132
2019-03-05T02:32:35.028764: step 74, loss 0.768038, accuracy 0.808594, precision 0.8852459016393442, recall 0.8526315789473684
2019-03-05T02:32:36.994686: step 75, loss 0.705656, accuracy 0.796875, precision 0.844311377245509, recall 0.844311377245509
2019-03-05T02:32:38.961821: step 76, loss 0.773976, accuracy 0.789062, precision 0.8571428571428571, recall 0.8165680473372781
2019-03-05T02:32:40.889929: step 77, loss 0.467827, accuracy 0.828125, precision 0.8597560975609756, recall 0.8703703703703703
2019-03-05T02:32:42.837875: step 78, loss 0.792299, accuracy 0.785156, precision 0.7777777777777778, recall 0.91875
2019-03-05T02:32:44.779291: step 79, loss 0.808865, accuracy 0.753906, precision 0.8352941176470589, recall 0.8022598870056498
2019-03-05T02:32:46.726415: step 80, loss 0.525517, accuracy 0.832031, precision 0.8734939759036144, recall 0.8682634730538922
2019-03-05T02:32:48.677597: step 81, loss 0.63028, accuracy 0.808594, precision 0.8770949720670391, recall 0.8532608695652174
2019-03-05T02:32:50.625278: step 82, loss 0.530525, accuracy 0.839844, precision 0.8908045977011494, recall 0.8757062146892656
2019-03-05T02:32:52.569728: step 83, loss 0.599485, accuracy 0.84375, precision 0.8926553672316384, recall 0.88268156424581
2019-03-05T02:32:54.520257: step 84, loss 0.736075, accuracy 0.769531, precision 0.8303030303030303, recall 0.8154761904761905
2019-03-05T02:32:56.463245: step 85, loss 0.693408, accuracy 0.789062, precision 0.8520710059171598, recall 0.8323699421965318
2019-03-05T02:32:58.421504: step 86, loss 0.770748, accuracy 0.808594, precision 0.863905325443787, recall 0.8488372093023255
2019-03-05T02:33:00.391883: step 87, loss 0.602477, accuracy 0.828125, precision 0.8614457831325302, recall 0.8719512195121951
2019-03-05T02:33:02.358136: step 88, loss 0.618025, accuracy 0.792969, precision 0.8154761904761905, recall 0.8616352201257862
2019-03-05T02:33:04.293697: step 89, loss 0.544168, accuracy 0.828125, precision 0.8674698795180723, recall 0.8674698795180723
2019-03-05T02:33:05.167009: step 90, loss 0.737622, accuracy 0.802817, precision 0.82, recall 0.8913043478260869
2019-03-05T02:33:07.109810: step 91, loss 0.395015, accuracy 0.863281, precision 0.872093023255814, recall 0.9202453987730062
2019-03-05T02:33:09.054170: step 92, loss 0.473288, accuracy 0.835938, precision 0.8505747126436781, recall 0.9024390243902439
2019-03-05T02:33:11.029475: step 93, loss 0.450854, accuracy 0.832031, precision 0.8980891719745223, recall 0.8392857142857143
2019-03-05T02:33:12.997977: step 94, loss 0.530906, accuracy 0.851562, precision 0.9137931034482759, recall 0.8736263736263736
2019-03-05T02:33:14.957260: step 95, loss 0.435392, accuracy 0.851562, precision 0.9139784946236559, recall 0.8854166666666666
2019-03-05T02:33:16.910058: step 96, loss 0.466185, accuracy 0.832031, precision 0.875, recall 0.88
2019-03-05T02:33:18.881578: step 97, loss 0.567229, accuracy 0.804688, precision 0.8902439024390244, recall 0.8202247191011236
2019-03-05T02:33:20.823491: step 98, loss 0.473121, accuracy 0.835938, precision 0.8793103448275862, recall 0.8793103448275862
2019-03-05T02:33:22.761597: step 99, loss 0.422246, accuracy 0.863281, precision 0.8870056497175142, recall 0.9127906976744186
2019-03-05T02:33:24.701526: step 100, loss 0.438483, accuracy 0.847656, precision 0.9096385542168675, recall 0.8628571428571429
2019-03-05T02:33:26.669975: step 101, loss 0.514604, accuracy 0.839844, precision 0.8666666666666667, recall 0.8827160493827161
2019-03-05T02:33:28.618207: step 102, loss 0.494903, accuracy 0.832031, precision 0.8625, recall 0.8679245283018868
2019-03-05T02:33:30.574248: step 103, loss 0.512383, accuracy 0.804688, precision 0.85, recall 0.8395061728395061
2019-03-05T02:33:32.533821: step 104, loss 0.534109, accuracy 0.792969, precision 0.7942857142857143, recall 0.8910256410256411
2019-03-05T02:33:34.496518: step 105, loss 0.548488, accuracy 0.824219, precision 0.8563218390804598, recall 0.8816568047337278
2019-03-05T02:33:36.453578: step 106, loss 0.489176, accuracy 0.820312, precision 0.8520710059171598, recall 0.8727272727272727
2019-03-05T02:33:38.411736: step 107, loss 0.419308, accuracy 0.820312, precision 0.8870056497175142, recall 0.8579234972677595
2019-03-05T02:33:40.371460: step 108, loss 0.634413, accuracy 0.789062, precision 0.9036144578313253, recall 0.7978723404255319
2019-03-05T02:33:42.335708: step 109, loss 0.394389, accuracy 0.851562, precision 0.9152542372881356, recall 0.8756756756756757
2019-03-05T02:33:44.273661: step 110, loss 0.408536, accuracy 0.855469, precision 0.9024390243902439, recall 0.8757396449704142
2019-03-05T02:33:46.231504: step 111, loss 0.542276, accuracy 0.792969, precision 0.8529411764705882, recall 0.838150289017341
2019-03-05T02:33:48.174900: step 112, loss 0.552279, accuracy 0.824219, precision 0.8827160493827161, recall 0.8461538461538461
2019-03-05T02:33:50.125662: step 113, loss 0.438897, accuracy 0.84375, precision 0.8352941176470589, recall 0.922077922077922
2019-03-05T02:33:52.088118: step 114, loss 0.533209, accuracy 0.820312, precision 0.8169934640522876, recall 0.8741258741258742
2019-03-05T02:33:54.033342: step 115, loss 0.476484, accuracy 0.878906, precision 0.9244186046511628, recall 0.8983050847457628
2019-03-05T02:33:55.981327: step 116, loss 0.57832, accuracy 0.789062, precision 0.8269230769230769, recall 0.8269230769230769
2019-03-05T02:33:57.941838: step 117, loss 0.424134, accuracy 0.8125, precision 0.864516129032258, recall 0.8322981366459627
2019-03-05T02:33:59.890349: step 118, loss 0.394935, accuracy 0.871094, precision 0.8914285714285715, recall 0.9176470588235294
2019-03-05T02:34:01.834334: step 119, loss 0.403765, accuracy 0.847656, precision 0.8848484848484849, recall 0.8795180722891566
2019-03-05T02:34:03.807341: step 120, loss 0.394855, accuracy 0.835938, precision 0.8690476190476191, recall 0.8795180722891566
2019-03-05T02:34:05.772634: step 121, loss 0.281505, accuracy 0.890625, precision 0.9318181818181818, recall 0.9111111111111111
2019-03-05T02:34:07.711176: step 122, loss 0.494613, accuracy 0.847656, precision 0.8862275449101796, recall 0.8809523809523809
2019-03-05T02:34:09.663016: step 123, loss 0.281803, accuracy 0.898438, precision 0.8994082840236687, recall 0.9440993788819876
2019-03-05T02:34:11.621626: step 124, loss 0.375678, accuracy 0.867188, precision 0.8977272727272727, recall 0.9080459770114943
2019-03-05T02:34:13.561563: step 125, loss 0.533688, accuracy 0.828125, precision 0.8662790697674418, recall 0.8764705882352941
2019-03-05T02:34:15.522744: step 126, loss 0.378802, accuracy 0.851562, precision 0.9079754601226994, recall 0.8654970760233918
2019-03-05T02:34:17.458842: step 127, loss 0.324381, accuracy 0.878906, precision 0.9162011173184358, recall 0.9111111111111111
2019-03-05T02:34:19.404777: step 128, loss 0.317606, accuracy 0.886719, precision 0.9655172413793104, recall 0.8795811518324608
2019-03-05T02:34:21.362570: step 129, loss 0.555406, accuracy 0.796875, precision 0.8614457831325302, recall 0.8313953488372093
2019-03-05T02:34:23.310489: step 130, loss 0.357559, accuracy 0.84375, precision 0.8850574712643678, recall 0.8850574712643678
2019-03-05T02:34:25.267111: step 131, loss 0.389288, accuracy 0.851562, precision 0.8793103448275862, recall 0.9
2019-03-05T02:34:27.205214: step 132, loss 0.425541, accuracy 0.847656, precision 0.8700564971751412, recall 0.9058823529411765
2019-03-05T02:34:29.156609: step 133, loss 0.48045, accuracy 0.820312, precision 0.8273809523809523, recall 0.8910256410256411
2019-03-05T02:34:31.105741: step 134, loss 0.346974, accuracy 0.847656, precision 0.8816568047337278, recall 0.8869047619047619
2019-03-05T02:34:31.987040: step 135, loss 0.313991, accuracy 0.859155, precision 0.8571428571428571, recall 0.9333333333333333
2019-03-05T02:34:33.930590: step 136, loss 0.392917, accuracy 0.839844, precision 0.9102564102564102, recall 0.8402366863905325
2019-03-05T02:34:35.907447: step 137, loss 0.38489, accuracy 0.859375, precision 0.9254658385093167, recall 0.861271676300578
2019-03-05T02:34:37.861306: step 138, loss 0.369371, accuracy 0.867188, precision 0.9132947976878613, recall 0.8926553672316384
2019-03-05T02:34:39.810444: step 139, loss 0.377448, accuracy 0.84375, precision 0.8727272727272727, recall 0.8834355828220859
2019-03-05T02:34:41.765197: step 140, loss 0.365915, accuracy 0.835938, precision 0.8875, recall 0.8554216867469879
2019-03-05T02:34:43.717322: step 141, loss 0.332962, accuracy 0.855469, precision 0.8596491228070176, recall 0.91875
2019-03-05T02:34:45.661983: step 142, loss 0.393667, accuracy 0.847656, precision 0.84, recall 0.930379746835443
2019-03-05T02:34:47.603924: step 143, loss 0.288199, accuracy 0.875, precision 0.8654970760233918, recall 0.9426751592356688
2019-03-05T02:34:49.549220: step 144, loss 0.360182, accuracy 0.84375, precision 0.8830409356725146, recall 0.8830409356725146
2019-03-05T02:34:51.499966: step 145, loss 0.292169, accuracy 0.890625, precision 0.9239766081871345, recall 0.9132947976878613
2019-03-05T02:34:53.449634: step 146, loss 0.298925, accuracy 0.878906, precision 0.9597701149425287, recall 0.8743455497382199
2019-03-05T02:34:55.406826: step 147, loss 0.4479, accuracy 0.84375, precision 0.8863636363636364, recall 0.8863636363636364
2019-03-05T02:34:57.374873: step 148, loss 0.445902, accuracy 0.835938, precision 0.8895348837209303, recall 0.8693181818181818
2019-03-05T02:34:59.348777: step 149, loss 0.346622, accuracy 0.855469, precision 0.9101796407185628, recall 0.8735632183908046
2019-03-05T02:35:01.298803: step 150, loss 0.360876, accuracy 0.84375, precision 0.8941176470588236, recall 0.8735632183908046
2019-03-05T02:35:03.268572: step 151, loss 0.34108, accuracy 0.878906, precision 0.9322033898305084, recall 0.8967391304347826
2019-03-05T02:35:05.212879: step 152, loss 0.330311, accuracy 0.851562, precision 0.861271676300578, recall 0.9141104294478528
2019-03-05T02:35:07.171025: step 153, loss 0.352648, accuracy 0.851562, precision 0.8604651162790697, recall 0.9135802469135802
2019-03-05T02:35:09.135617: step 154, loss 0.323353, accuracy 0.875, precision 0.8895705521472392, recall 0.9119496855345912
2019-03-05T02:35:11.066881: step 155, loss 0.430782, accuracy 0.84375, precision 0.8920454545454546, recall 0.8820224719101124
2019-03-05T02:35:12.998799: step 156, loss 0.281579, accuracy 0.882812, precision 0.9190751445086706, recall 0.9085714285714286
2019-03-05T02:35:14.949510: step 157, loss 0.322532, accuracy 0.878906, precision 0.9024390243902439, recall 0.9079754601226994
2019-03-05T02:35:16.897883: step 158, loss 0.251607, accuracy 0.90625, precision 0.9252873563218391, recall 0.936046511627907
2019-03-05T02:35:18.853468: step 159, loss 0.353674, accuracy 0.859375, precision 0.9298245614035088, recall 0.8688524590163934
2019-03-05T02:35:20.813887: step 160, loss 0.334005, accuracy 0.875, precision 0.9397590361445783, recall 0.8764044943820225
2019-03-05T02:35:22.766115: step 161, loss 0.253774, accuracy 0.898438, precision 0.9212121212121213, recall 0.9212121212121213
2019-03-05T02:35:24.738900: step 162, loss 0.355762, accuracy 0.847656, precision 0.8711656441717791, recall 0.8875
2019-03-05T02:35:26.686271: step 163, loss 0.254176, accuracy 0.902344, precision 0.9090909090909091, recall 0.9467455621301775
2019-03-05T02:35:28.626621: step 164, loss 0.285504, accuracy 0.847656, precision 0.8571428571428571, recall 0.9056603773584906
2019-03-05T02:35:30.577338: step 165, loss 0.316527, accuracy 0.84375, precision 0.8564102564102564, recall 0.9329608938547486
2019-03-05T02:35:32.551867: step 166, loss 0.346777, accuracy 0.851562, precision 0.9090909090909091, recall 0.8670520231213873
2019-03-05T02:35:34.484976: step 167, loss 0.368261, accuracy 0.855469, precision 0.95625, recall 0.8360655737704918
2019-03-05T02:35:36.443143: step 168, loss 0.354525, accuracy 0.867188, precision 0.9371069182389937, recall 0.861271676300578
2019-03-05T02:35:38.411513: step 169, loss 0.3669, accuracy 0.84375, precision 0.8877005347593583, recall 0.8972972972972973
2019-03-05T02:35:40.374198: step 170, loss 0.270163, accuracy 0.882812, precision 0.9464285714285714, recall 0.8833333333333333
2019-03-05T02:35:42.322154: step 171, loss 0.414863, accuracy 0.828125, precision 0.8988095238095238, recall 0.848314606741573
2019-03-05T02:35:44.281924: step 172, loss 0.292789, accuracy 0.890625, precision 0.9028571428571428, recall 0.9349112426035503
2019-03-05T02:35:46.226440: step 173, loss 0.342008, accuracy 0.867188, precision 0.8895348837209303, recall 0.9107142857142857
2019-03-05T02:35:48.187096: step 174, loss 0.316097, accuracy 0.863281, precision 0.9064327485380117, recall 0.8908045977011494
2019-03-05T02:35:50.139509: step 175, loss 0.326914, accuracy 0.863281, precision 0.906832298136646, recall 0.8795180722891566
2019-03-05T02:35:52.092922: step 176, loss 0.267833, accuracy 0.882812, precision 0.9085714285714286, recall 0.9190751445086706
2019-03-05T02:35:54.056406: step 177, loss 0.331711, accuracy 0.855469, precision 0.8930817610062893, recall 0.8765432098765432
2019-03-05T02:35:56.001253: step 178, loss 0.334738, accuracy 0.855469, precision 0.8950617283950617, recall 0.8787878787878788
2019-03-05T02:35:57.937074: step 179, loss 0.383456, accuracy 0.832031, precision 0.874251497005988, recall 0.8690476190476191
2019-03-05T02:35:58.792183: step 180, loss 0.42226, accuracy 0.802817, precision 0.8333333333333334, recall 0.8695652173913043
2019-03-05T02:36:00.732160: step 181, loss 0.301961, accuracy 0.886719, precision 0.9364161849710982, recall 0.9
2019-03-05T02:36:02.693475: step 182, loss 0.332484, accuracy 0.839844, precision 0.8957055214723927, recall 0.8588235294117647
2019-03-05T02:36:04.662162: step 183, loss 0.338231, accuracy 0.871094, precision 0.9622641509433962, recall 0.85
2019-03-05T02:36:06.613612: step 184, loss 0.291806, accuracy 0.847656, precision 0.8895348837209303, recall 0.884393063583815
2019-03-05T02:36:08.577747: step 185, loss 0.264522, accuracy 0.914062, precision 0.9529411764705882, recall 0.9204545454545454
2019-03-05T02:36:10.550117: step 186, loss 0.268505, accuracy 0.878906, precision 0.9166666666666666, recall 0.9005847953216374
2019-03-05T02:36:12.503802: step 187, loss 0.298865, accuracy 0.875, precision 0.88125, recall 0.9155844155844156
2019-03-05T02:36:14.445226: step 188, loss 0.326659, accuracy 0.867188, precision 0.8728323699421965, recall 0.9263803680981595
2019-03-05T02:36:16.396576: step 189, loss 0.205196, accuracy 0.910156, precision 0.8950617283950617, recall 0.9602649006622517
2019-03-05T02:36:18.342045: step 190, loss 0.405386, accuracy 0.828125, precision 0.8409090909090909, recall 0.9024390243902439
2019-03-05T02:36:20.304605: step 191, loss 0.307526, accuracy 0.863281, precision 0.8875739644970414, recall 0.9036144578313253
2019-03-05T02:36:22.279637: step 192, loss 0.35228, accuracy 0.875, precision 0.9265536723163842, recall 0.8961748633879781
2019-03-05T02:36:24.229437: step 193, loss 0.28034, accuracy 0.890625, precision 0.9444444444444444, recall 0.9042553191489362
2019-03-05T02:36:26.169232: step 194, loss 0.330159, accuracy 0.863281, precision 0.9127906976744186, recall 0.8870056497175142
2019-03-05T02:36:28.125638: step 195, loss 0.232194, accuracy 0.898438, precision 0.9371428571428572, recall 0.9162011173184358
2019-03-05T02:36:30.070563: step 196, loss 0.385323, accuracy 0.855469, precision 0.9117647058823529, recall 0.8757062146892656
2019-03-05T02:36:32.004459: step 197, loss 0.267536, accuracy 0.902344, precision 0.9375, recall 0.9090909090909091
2019-03-05T02:36:33.967690: step 198, loss 0.242355, accuracy 0.886719, precision 0.9080459770114943, recall 0.9239766081871345
2019-03-05T02:36:35.919002: step 199, loss 0.329127, accuracy 0.851562, precision 0.8727272727272727, recall 0.8944099378881988
2019-03-05T02:36:37.875590: step 200, loss 0.276283, accuracy 0.898438, precision 0.9005847953216374, recall 0.9447852760736196
2019-03-05T02:36:39.832912: step 201, loss 0.312733, accuracy 0.878906, precision 0.9036144578313253, recall 0.9090909090909091
2019-03-05T02:36:41.797265: step 202, loss 0.258635, accuracy 0.910156, precision 0.9248554913294798, recall 0.9411764705882353
2019-03-05T02:36:43.765573: step 203, loss 0.305095, accuracy 0.875, precision 0.9125, recall 0.8902439024390244
2019-03-05T02:36:45.740198: step 204, loss 0.323517, accuracy 0.878906, precision 0.89171974522293, recall 0.9090909090909091
2019-03-05T02:36:47.681683: step 205, loss 0.244599, accuracy 0.894531, precision 0.9197530864197531, recall 0.9141104294478528
2019-03-05T02:36:49.628566: step 206, loss 0.280417, accuracy 0.875, precision 0.9137931034482759, recall 0.9034090909090909
2019-03-05T02:36:51.589254: step 207, loss 0.308436, accuracy 0.847656, precision 0.8670520231213873, recall 0.9036144578313253
2019-03-05T02:36:53.541679: step 208, loss 0.255916, accuracy 0.894531, precision 0.9190751445086706, recall 0.9244186046511628
2019-03-05T02:36:55.488926: step 209, loss 0.303117, accuracy 0.871094, precision 0.888268156424581, recall 0.9244186046511628
2019-03-05T02:36:57.431457: step 210, loss 0.269956, accuracy 0.902344, precision 0.9411764705882353, recall 0.9142857142857143
2019-03-05T02:36:59.397234: step 211, loss 0.276024, accuracy 0.863281, precision 0.9329268292682927, recall 0.864406779661017
2019-03-05T02:37:01.353216: step 212, loss 0.311207, accuracy 0.878906, precision 0.9428571428571428, recall 0.8870967741935484
2019-03-05T02:37:03.299881: step 213, loss 0.314688, accuracy 0.878906, precision 0.9320987654320988, recall 0.8830409356725146
2019-03-05T02:37:05.245129: step 214, loss 0.224791, accuracy 0.898438, precision 0.9297297297297298, recall 0.9297297297297298
2019-03-05T02:37:07.194120: step 215, loss 0.222103, accuracy 0.898438, precision 0.9226190476190477, recall 0.9226190476190477
2019-03-05T02:37:09.139591: step 216, loss 0.326433, accuracy 0.851562, precision 0.9117647058823529, recall 0.8707865168539326
2019-03-05T02:37:11.121703: step 217, loss 0.261115, accuracy 0.894531, precision 0.9325842696629213, recall 0.9171270718232044
2019-03-05T02:37:13.059075: step 218, loss 0.276969, accuracy 0.859375, precision 0.8802395209580839, recall 0.901840490797546
2019-03-05T02:37:15.008605: step 219, loss 0.316818, accuracy 0.878906, precision 0.893491124260355, recall 0.9207317073170732
2019-03-05T02:37:16.958394: step 220, loss 0.279034, accuracy 0.898438, precision 0.9, recall 0.9444444444444444
2019-03-05T02:37:18.913610: step 221, loss 0.326406, accuracy 0.847656, precision 0.9041916167664671, recall 0.867816091954023
2019-03-05T02:37:20.879568: step 222, loss 0.278136, accuracy 0.886719, precision 0.8932584269662921, recall 0.9408284023668639
2019-03-05T02:37:22.831052: step 223, loss 0.262902, accuracy 0.917969, precision 0.9404761904761905, recall 0.9349112426035503
2019-03-05T02:37:24.788733: step 224, loss 0.272591, accuracy 0.898438, precision 0.9631901840490797, recall 0.8870056497175142
2019-03-05T02:37:25.660864: step 225, loss 0.417467, accuracy 0.816901, precision 0.8260869565217391, recall 0.8837209302325582
2019-03-05T02:37:27.612558: step 226, loss 0.224885, accuracy 0.910156, precision 0.9473684210526315, recall 0.9204545454545454
2019-03-05T02:37:29.572352: step 227, loss 0.229825, accuracy 0.886719, precision 0.9025974025974026, recall 0.9084967320261438
2019-03-05T02:37:31.535254: step 228, loss 0.207823, accuracy 0.910156, precision 0.9627329192546584, recall 0.9011627906976745
2019-03-05T02:37:33.479077: step 229, loss 0.254134, accuracy 0.867188, precision 0.9171597633136095, recall 0.8857142857142857
2019-03-05T02:37:35.418751: step 230, loss 0.217003, accuracy 0.894531, precision 0.8977272727272727, recall 0.9461077844311377
2019-03-05T02:37:37.349244: step 231, loss 0.282711, accuracy 0.878906, precision 0.8805031446540881, recall 0.9210526315789473
2019-03-05T02:37:39.287734: step 232, loss 0.259051, accuracy 0.894531, precision 0.9146341463414634, recall 0.9202453987730062
2019-03-05T02:37:41.237049: step 233, loss 0.220608, accuracy 0.90625, precision 0.9285714285714286, recall 0.9285714285714286
2019-03-05T02:37:43.198066: step 234, loss 0.263521, accuracy 0.882812, precision 0.9106145251396648, recall 0.9209039548022598
2019-03-05T02:37:45.143557: step 235, loss 0.221544, accuracy 0.921875, precision 0.9668508287292817, recall 0.9259259259259259
2019-03-05T02:37:47.107407: step 236, loss 0.263213, accuracy 0.90625, precision 0.9593023255813954, recall 0.9065934065934066
2019-03-05T02:37:49.054116: step 237, loss 0.296004, accuracy 0.890625, precision 0.9352941176470588, recall 0.9034090909090909
2019-03-05T02:37:51.013948: step 238, loss 0.22959, accuracy 0.90625, precision 0.9593023255813954, recall 0.9065934065934066
2019-03-05T02:37:52.959655: step 239, loss 0.217731, accuracy 0.902344, precision 0.9254658385093167, recall 0.9197530864197531
2019-03-05T02:37:54.914171: step 240, loss 0.188212, accuracy 0.914062, precision 0.9216867469879518, recall 0.9444444444444444
2019-03-05T02:37:56.875813: step 241, loss 0.200349, accuracy 0.90625, precision 0.9085714285714286, recall 0.9520958083832335
2019-03-05T02:37:58.827733: step 242, loss 0.249062, accuracy 0.890625, precision 0.8928571428571429, recall 0.9375
2019-03-05T02:38:00.773748: step 243, loss 0.242651, accuracy 0.90625, precision 0.927710843373494, recall 0.927710843373494
2019-03-05T02:38:02.734691: step 244, loss 0.261844, accuracy 0.878906, precision 0.9095744680851063, recall 0.9243243243243243
2019-03-05T02:38:04.705696: step 245, loss 0.204156, accuracy 0.910156, precision 0.9375, recall 0.9322033898305084
2019-03-05T02:38:06.667806: step 246, loss 0.202722, accuracy 0.933594, precision 0.9606741573033708, recall 0.9447513812154696
2019-03-05T02:38:08.609100: step 247, loss 0.267683, accuracy 0.882812, precision 0.9661016949152542, recall 0.8769230769230769
2019-03-05T02:38:10.560694: step 248, loss 0.252487, accuracy 0.894531, precision 0.9595375722543352, recall 0.8924731182795699
2019-03-05T02:38:12.511163: step 249, loss 0.259375, accuracy 0.886719, precision 0.9512195121951219, recall 0.8813559322033898
2019-03-05T02:38:14.476874: step 250, loss 0.3108, accuracy 0.855469, precision 0.896969696969697, recall 0.8809523809523809

Evaluation:
[[95 33]
 [12 91]]
2019-03-05T02:38:17.109230: step 250, loss 0.444874, accuracy 0.805195, precision 0.7421875, recall 0.8878504672897196

Saved model checkpoint to /home/ubuntu/Project/runs/1551753005/checkpoints/model-250

2019-03-05T02:38:19.782316: step 251, loss 0.299425, accuracy 0.871094, precision 0.8950617283950617, recall 0.9006211180124224
2019-03-05T02:38:21.718554: step 252, loss 0.285383, accuracy 0.859375, precision 0.8562874251497006, recall 0.9225806451612903
2019-03-05T02:38:23.669608: step 253, loss 0.203459, accuracy 0.917969, precision 0.8988095238095238, recall 0.9741935483870968
2019-03-05T02:38:25.630353: step 254, loss 0.193005, accuracy 0.917969, precision 0.930635838150289, recall 0.9470588235294117
2019-03-05T02:38:27.583930: step 255, loss 0.259529, accuracy 0.878906, precision 0.9117647058823529, recall 0.9064327485380117
2019-03-05T02:38:29.533326: step 256, loss 0.207429, accuracy 0.910156, precision 0.9431818181818182, recall 0.9273743016759777
2019-03-05T02:38:31.485354: step 257, loss 0.23917, accuracy 0.90625, precision 0.9226190476190477, recall 0.9337349397590361
2019-03-05T02:38:33.446714: step 258, loss 0.238535, accuracy 0.902344, precision 0.9386503067484663, recall 0.9107142857142857
2019-03-05T02:38:35.397126: step 259, loss 0.28611, accuracy 0.878906, precision 0.9520958083832335, recall 0.8736263736263736
2019-03-05T02:38:37.335115: step 260, loss 0.255622, accuracy 0.878906, precision 0.9520958083832335, recall 0.8736263736263736
2019-03-05T02:38:39.295713: step 261, loss 0.275424, accuracy 0.902344, precision 0.9411764705882353, recall 0.9142857142857143
2019-03-05T02:38:41.240915: step 262, loss 0.240478, accuracy 0.902344, precision 0.9080459770114943, recall 0.9461077844311377
2019-03-05T02:38:43.188318: step 263, loss 0.230723, accuracy 0.921875, precision 0.9142857142857143, recall 0.9696969696969697
2019-03-05T02:38:45.143680: step 264, loss 0.211157, accuracy 0.914062, precision 0.9186046511627907, recall 0.9518072289156626
2019-03-05T02:38:47.103686: step 265, loss 0.210671, accuracy 0.90625, precision 0.8888888888888888, recall 0.9681528662420382
2019-03-05T02:38:49.067373: step 266, loss 0.252284, accuracy 0.894531, precision 0.9244186046511628, recall 0.9190751445086706
2019-03-05T02:38:51.009123: step 267, loss 0.243857, accuracy 0.890625, precision 0.9090909090909091, recall 0.9202453987730062
2019-03-05T02:38:52.965174: step 268, loss 0.313637, accuracy 0.875, precision 0.9573170731707317, recall 0.8626373626373627
2019-03-05T02:38:54.929913: step 269, loss 0.297451, accuracy 0.878906, precision 0.9506172839506173, recall 0.8700564971751412
2019-03-05T02:38:55.799845: step 270, loss 0.187529, accuracy 0.901408, precision 0.9148936170212766, recall 0.9347826086956522
2019-03-05T02:38:57.752903: step 271, loss 0.203364, accuracy 0.925781, precision 0.9562841530054644, recall 0.9408602150537635
2019-03-05T02:38:59.702187: step 272, loss 0.191825, accuracy 0.925781, precision 0.9, recall 0.9870967741935484
2019-03-05T02:39:01.653545: step 273, loss 0.190991, accuracy 0.90625, precision 0.9010989010989011, recall 0.9647058823529412
2019-03-05T02:39:03.602315: step 274, loss 0.186371, accuracy 0.933594, precision 0.9411764705882353, recall 0.9580838323353293
2019-03-05T02:39:05.555960: step 275, loss 0.230481, accuracy 0.886719, precision 0.90625, recall 0.9119496855345912
2019-03-05T02:39:07.518218: step 276, loss 0.221042, accuracy 0.929688, precision 0.9479768786127167, recall 0.9479768786127167
2019-03-05T02:39:09.473106: step 277, loss 0.245869, accuracy 0.90625, precision 0.9939393939393939, recall 0.8770053475935828
2019-03-05T02:39:11.419030: step 278, loss 0.245818, accuracy 0.894531, precision 0.9041916167664671, recall 0.9320987654320988
2019-03-05T02:39:13.377664: step 279, loss 0.204892, accuracy 0.914062, precision 0.9559748427672956, recall 0.9101796407185628
2019-03-05T02:39:15.359288: step 280, loss 0.206062, accuracy 0.90625, precision 0.925, recall 0.925
2019-03-05T02:39:17.303848: step 281, loss 0.231523, accuracy 0.882812, precision 0.877906976744186, recall 0.94375
2019-03-05T02:39:19.260877: step 282, loss 0.196099, accuracy 0.914062, precision 0.9107142857142857, recall 0.95625
2019-03-05T02:39:21.214479: step 283, loss 0.233071, accuracy 0.886719, precision 0.8823529411764706, recall 0.9433962264150944
2019-03-05T02:39:23.150611: step 284, loss 0.216342, accuracy 0.914062, precision 0.9090909090909091, recall 0.963855421686747
2019-03-05T02:39:25.095313: step 285, loss 0.200416, accuracy 0.925781, precision 0.9526627218934911, recall 0.936046511627907
2019-03-05T02:39:27.039654: step 286, loss 0.211076, accuracy 0.917969, precision 0.9404761904761905, recall 0.9349112426035503
2019-03-05T02:39:29.001056: step 287, loss 0.246479, accuracy 0.890625, precision 0.9540229885057471, recall 0.8924731182795699
2019-03-05T02:39:30.962089: step 288, loss 0.17606, accuracy 0.933594, precision 0.9766081871345029, recall 0.9277777777777778
2019-03-05T02:39:32.914596: step 289, loss 0.230363, accuracy 0.910156, precision 0.9717514124293786, recall 0.9052631578947369
2019-03-05T02:39:34.853874: step 290, loss 0.213711, accuracy 0.917969, precision 0.9595375722543352, recall 0.9222222222222223
2019-03-05T02:39:36.794287: step 291, loss 0.255873, accuracy 0.890625, precision 0.9080459770114943, recall 0.9294117647058824
2019-03-05T02:39:38.745598: step 292, loss 0.176008, accuracy 0.9375, precision 0.9473684210526315, recall 0.9585798816568047
2019-03-05T02:39:40.729347: step 293, loss 0.232229, accuracy 0.902344, precision 0.893491124260355, recall 0.9556962025316456
2019-03-05T02:39:42.665227: step 294, loss 0.184742, accuracy 0.914062, precision 0.9036144578313253, recall 0.9615384615384616
2019-03-05T02:39:44.617630: step 295, loss 0.198434, accuracy 0.917969, precision 0.9289617486338798, recall 0.9550561797752809
2019-03-05T02:39:46.552978: step 296, loss 0.268973, accuracy 0.882812, precision 0.8881987577639752, recall 0.9225806451612903
2019-03-05T02:39:48.497395: step 297, loss 0.248943, accuracy 0.871094, precision 0.9337349397590361, recall 0.8757062146892656
2019-03-05T02:39:50.449247: step 298, loss 0.237853, accuracy 0.902344, precision 0.9595375722543352, recall 0.9021739130434783
2019-03-05T02:39:52.392730: step 299, loss 0.241408, accuracy 0.894531, precision 0.9709302325581395, recall 0.8835978835978836
2019-03-05T02:39:54.345742: step 300, loss 0.180524, accuracy 0.941406, precision 0.9867549668874173, recall 0.9197530864197531
2019-03-05T02:39:56.288494: step 301, loss 0.219714, accuracy 0.914062, precision 0.9447852760736196, recall 0.9221556886227545
2019-03-05T02:39:58.240822: step 302, loss 0.220434, accuracy 0.925781, precision 0.9248554913294798, recall 0.963855421686747
2019-03-05T02:40:00.177515: step 303, loss 0.231124, accuracy 0.917969, precision 0.9012345679012346, recall 0.9668874172185431
2019-03-05T02:40:02.125285: step 304, loss 0.197635, accuracy 0.933594, precision 0.9204545454545454, recall 0.9818181818181818
2019-03-05T02:40:04.070097: step 305, loss 0.238046, accuracy 0.890625, precision 0.8959537572254336, recall 0.9393939393939394
2019-03-05T02:40:06.013090: step 306, loss 0.173853, accuracy 0.925781, precision 0.9171597633136095, recall 0.96875
2019-03-05T02:40:07.956737: step 307, loss 0.207734, accuracy 0.914062, precision 0.9401197604790419, recall 0.9289940828402367
2019-03-05T02:40:09.929589: step 308, loss 0.247574, accuracy 0.898438, precision 0.9651162790697675, recall 0.8924731182795699
2019-03-05T02:40:11.879563: step 309, loss 0.170781, accuracy 0.9375, precision 0.9771428571428571, recall 0.9344262295081968
2019-03-05T02:40:13.833281: step 310, loss 0.175581, accuracy 0.914062, precision 0.9476744186046512, recall 0.9261363636363636
2019-03-05T02:40:15.777250: step 311, loss 0.26778, accuracy 0.898438, precision 0.9192546583850931, recall 0.9192546583850931
2019-03-05T02:40:17.735217: step 312, loss 0.20145, accuracy 0.90625, precision 0.9139784946236559, recall 0.9550561797752809
2019-03-05T02:40:19.688392: step 313, loss 0.23273, accuracy 0.890625, precision 0.8860759493670886, recall 0.9333333333333333
2019-03-05T02:40:21.629134: step 314, loss 0.158744, accuracy 0.941406, precision 0.949685534591195, recall 0.9556962025316456
2019-03-05T02:40:22.495008: step 315, loss 0.204971, accuracy 0.929577, precision 0.9148936170212766, recall 0.9772727272727273
2019-03-05T02:40:24.433001: step 316, loss 0.190512, accuracy 0.898438, precision 0.9181286549707602, recall 0.9289940828402367
2019-03-05T02:40:26.377103: step 317, loss 0.203031, accuracy 0.902344, precision 0.9367088607594937, recall 0.9079754601226994
2019-03-05T02:40:28.363778: step 318, loss 0.183491, accuracy 0.921875, precision 0.9512195121951219, recall 0.9285714285714286
2019-03-05T02:40:30.335545: step 319, loss 0.198563, accuracy 0.921875, precision 0.9491525423728814, recall 0.9385474860335196
2019-03-05T02:40:32.300941: step 320, loss 0.152748, accuracy 0.929688, precision 0.95, recall 0.95
2019-03-05T02:40:34.247314: step 321, loss 0.216506, accuracy 0.914062, precision 0.943502824858757, recall 0.9329608938547486
2019-03-05T02:40:36.191254: step 322, loss 0.14832, accuracy 0.941406, precision 0.9695121951219512, recall 0.9408284023668639
2019-03-05T02:40:38.148829: step 323, loss 0.246154, accuracy 0.898438, precision 0.9325153374233128, recall 0.9101796407185628
2019-03-05T02:40:40.109604: step 324, loss 0.201964, accuracy 0.90625, precision 0.9408602150537635, recall 0.9308510638297872
2019-03-05T02:40:42.056974: step 325, loss 0.151322, accuracy 0.949219, precision 0.95625, recall 0.9622641509433962
2019-03-05T02:40:44.007646: step 326, loss 0.151978, accuracy 0.945312, precision 0.9709302325581395, recall 0.9488636363636364
2019-03-05T02:40:45.965814: step 327, loss 0.170147, accuracy 0.929688, precision 0.9319371727748691, recall 0.9726775956284153
2019-03-05T02:40:47.905054: step 328, loss 0.191962, accuracy 0.917969, precision 0.9470588235294117, recall 0.930635838150289
2019-03-05T02:40:49.841430: step 329, loss 0.187816, accuracy 0.910156, precision 0.9195402298850575, recall 0.9467455621301775
2019-03-05T02:40:51.784874: step 330, loss 0.203082, accuracy 0.921875, precision 0.9573170731707317, recall 0.9235294117647059
2019-03-05T02:40:53.740244: step 331, loss 0.206779, accuracy 0.929688, precision 0.9583333333333334, recall 0.936046511627907
2019-03-05T02:40:55.703233: step 332, loss 0.209769, accuracy 0.910156, precision 0.9503105590062112, recall 0.9107142857142857
2019-03-05T02:40:57.647951: step 333, loss 0.193148, accuracy 0.925781, precision 0.9700598802395209, recall 0.9204545454545454
2019-03-05T02:40:59.601678: step 334, loss 0.230202, accuracy 0.917969, precision 0.9209039548022598, recall 0.9588235294117647
2019-03-05T02:41:01.550238: step 335, loss 0.179023, accuracy 0.957031, precision 0.9681528662420382, recall 0.9620253164556962
2019-03-05T02:41:03.480576: step 336, loss 0.158093, accuracy 0.929688, precision 0.9401197604790419, recall 0.9515151515151515
2019-03-05T02:41:05.443012: step 337, loss 0.17913, accuracy 0.941406, precision 0.936046511627907, recall 0.9757575757575757
2019-03-05T02:41:07.402161: step 338, loss 0.2062, accuracy 0.917969, precision 0.937888198757764, recall 0.9320987654320988
2019-03-05T02:41:09.356527: step 339, loss 0.213049, accuracy 0.917969, precision 0.9142857142857143, recall 0.963855421686747
2019-03-05T02:41:11.329954: step 340, loss 0.21748, accuracy 0.894531, precision 0.9390243902439024, recall 0.9005847953216374
2019-03-05T02:41:13.289862: step 341, loss 0.179633, accuracy 0.917969, precision 0.9411764705882353, recall 0.935672514619883
2019-03-05T02:41:15.231491: step 342, loss 0.153756, accuracy 0.941406, precision 0.9476744186046512, recall 0.9644970414201184
2019-03-05T02:41:17.187677: step 343, loss 0.206621, accuracy 0.925781, precision 0.9515151515151515, recall 0.9345238095238095
2019-03-05T02:41:19.159800: step 344, loss 0.248112, accuracy 0.898438, precision 0.92, recall 0.930635838150289
2019-03-05T02:41:21.112551: step 345, loss 0.164287, accuracy 0.941406, precision 0.9454545454545454, recall 0.9629629629629629
2019-03-05T02:41:23.045583: step 346, loss 0.183733, accuracy 0.90625, precision 0.9294117647058824, recall 0.9294117647058824
2019-03-05T02:41:24.996238: step 347, loss 0.192143, accuracy 0.917969, precision 0.9464285714285714, recall 0.9298245614035088
2019-03-05T02:41:26.940820: step 348, loss 0.159749, accuracy 0.941406, precision 0.9644970414201184, recall 0.9476744186046512
2019-03-05T02:41:28.896897: step 349, loss 0.185918, accuracy 0.914062, precision 0.9415204678362573, recall 0.930635838150289
2019-03-05T02:41:30.867550: step 350, loss 0.179872, accuracy 0.933594, precision 0.9431818181818182, recall 0.9595375722543352
2019-03-05T02:41:32.808758: step 351, loss 0.210575, accuracy 0.898438, precision 0.9325153374233128, recall 0.9101796407185628
2019-03-05T02:41:34.793314: step 352, loss 0.205593, accuracy 0.929688, precision 0.9428571428571428, recall 0.953757225433526
2019-03-05T02:41:36.733083: step 353, loss 0.229003, accuracy 0.914062, precision 0.9273743016759777, recall 0.9485714285714286
2019-03-05T02:41:38.691116: step 354, loss 0.20916, accuracy 0.902344, precision 0.9186046511627907, recall 0.9349112426035503
2019-03-05T02:41:40.672655: step 355, loss 0.201134, accuracy 0.929688, precision 0.9583333333333334, recall 0.936046511627907
2019-03-05T02:41:42.616782: step 356, loss 0.228374, accuracy 0.910156, precision 0.9195402298850575, recall 0.9467455621301775
2019-03-05T02:41:44.564310: step 357, loss 0.186387, accuracy 0.910156, precision 0.961038961038961, recall 0.896969696969697
2019-03-05T02:41:46.518222: step 358, loss 0.217406, accuracy 0.898438, precision 0.9685534591194969, recall 0.88
2019-03-05T02:41:48.478789: step 359, loss 0.201722, accuracy 0.925781, precision 0.9710982658959537, recall 0.9230769230769231
2019-03-05T02:41:49.343571: step 360, loss 0.148998, accuracy 0.929577, precision 0.9375, recall 0.9574468085106383
2019-03-05T02:41:51.285610: step 361, loss 0.132432, accuracy 0.957031, precision 0.9371069182389937, recall 0.9933333333333333
2019-03-05T02:41:53.238411: step 362, loss 0.169083, accuracy 0.929688, precision 0.9171270718232044, recall 0.9822485207100592
2019-03-05T02:41:55.184800: step 363, loss 0.175042, accuracy 0.921875, precision 0.93125, recall 0.9430379746835443
2019-03-05T02:41:57.132803: step 364, loss 0.273042, accuracy 0.886719, precision 0.8816568047337278, recall 0.9430379746835443
2019-03-05T02:41:59.086562: step 365, loss 0.150104, accuracy 0.957031, precision 0.9595375722543352, recall 0.9764705882352941
2019-03-05T02:42:01.064930: step 366, loss 0.187828, accuracy 0.921875, precision 0.9386503067484663, recall 0.9386503067484663
2019-03-05T02:42:03.028357: step 367, loss 0.168062, accuracy 0.9375, precision 0.9647058823529412, recall 0.9425287356321839
2019-03-05T02:42:04.966134: step 368, loss 0.178118, accuracy 0.929688, precision 0.9580838323353293, recall 0.935672514619883
2019-03-05T02:42:06.910457: step 369, loss 0.199539, accuracy 0.902344, precision 0.9122807017543859, recall 0.9397590361445783
2019-03-05T02:42:08.863439: step 370, loss 0.191923, accuracy 0.914062, precision 0.9588235294117647, recall 0.9157303370786517
2019-03-05T02:42:10.828146: step 371, loss 0.160456, accuracy 0.933594, precision 0.9190751445086706, recall 0.9814814814814815
2019-03-05T02:42:12.764780: step 372, loss 0.20832, accuracy 0.910156, precision 0.9142857142857143, recall 0.9523809523809523
2019-03-05T02:42:14.709351: step 373, loss 0.14886, accuracy 0.933594, precision 0.9578313253012049, recall 0.9408284023668639
2019-03-05T02:42:16.686353: step 374, loss 0.159645, accuracy 0.9375, precision 0.9627329192546584, recall 0.9393939393939394
2019-03-05T02:42:18.611769: step 375, loss 0.126377, accuracy 0.929688, precision 0.9704142011834319, recall 0.9265536723163842
2019-03-05T02:42:20.566194: step 376, loss 0.175091, accuracy 0.917969, precision 0.9433962264150944, recall 0.9259259259259259
2019-03-05T02:42:22.541619: step 377, loss 0.186772, accuracy 0.921875, precision 0.9431818181818182, recall 0.9431818181818182
2019-03-05T02:42:24.497326: step 378, loss 0.21021, accuracy 0.917969, precision 0.9503105590062112, recall 0.9216867469879518
2019-03-05T02:42:26.440753: step 379, loss 0.125679, accuracy 0.953125, precision 0.9655172413793104, recall 0.9655172413793104
2019-03-05T02:42:28.396533: step 380, loss 0.136137, accuracy 0.945312, precision 0.9700598802395209, recall 0.9473684210526315
2019-03-05T02:42:30.342689: step 381, loss 0.17842, accuracy 0.929688, precision 0.925, recall 0.961038961038961
2019-03-05T02:42:32.288674: step 382, loss 0.21989, accuracy 0.902344, precision 0.8795180722891566, recall 0.9668874172185431
2019-03-05T02:42:34.232720: step 383, loss 0.215565, accuracy 0.894531, precision 0.8938547486033519, recall 0.9523809523809523
2019-03-05T02:42:36.181183: step 384, loss 0.137624, accuracy 0.945312, precision 0.9661016949152542, recall 0.9553072625698324
2019-03-05T02:42:38.122702: step 385, loss 0.173415, accuracy 0.933594, precision 0.9578313253012049, recall 0.9408284023668639
2019-03-05T02:42:40.056807: step 386, loss 0.174797, accuracy 0.945312, precision 0.9745222929936306, recall 0.9386503067484663
2019-03-05T02:42:42.004222: step 387, loss 0.147187, accuracy 0.9375, precision 0.9883720930232558, recall 0.9239130434782609
2019-03-05T02:42:43.967918: step 388, loss 0.176714, accuracy 0.929688, precision 0.9518072289156626, recall 0.9404761904761905
2019-03-05T02:42:45.913958: step 389, loss 0.157662, accuracy 0.945312, precision 0.96, recall 0.96
2019-03-05T02:42:47.888482: step 390, loss 0.158767, accuracy 0.945312, precision 0.9693251533742331, recall 0.9461077844311377
2019-03-05T02:42:49.833991: step 391, loss 0.167748, accuracy 0.925781, precision 0.9202127659574468, recall 0.9774011299435028
2019-03-05T02:42:51.794098: step 392, loss 0.170757, accuracy 0.917969, precision 0.8977272727272727, recall 0.9813664596273292
2019-03-05T02:42:53.746466: step 393, loss 0.160105, accuracy 0.929688, precision 0.9299363057324841, recall 0.954248366013072
2019-03-05T02:42:55.712911: step 394, loss 0.130881, accuracy 0.949219, precision 0.9717514124293786, recall 0.9555555555555556
2019-03-05T02:42:57.665288: step 395, loss 0.19931, accuracy 0.90625, precision 0.953757225433526, recall 0.9116022099447514
2019-03-05T02:42:59.597239: step 396, loss 0.120804, accuracy 0.945312, precision 0.9611111111111111, recall 0.9611111111111111
2019-03-05T02:43:01.558467: step 397, loss 0.123853, accuracy 0.949219, precision 0.9940828402366864, recall 0.9333333333333333
2019-03-05T02:43:03.521042: step 398, loss 0.134003, accuracy 0.945312, precision 0.9777777777777777, recall 0.946236559139785
2019-03-05T02:43:05.445190: step 399, loss 0.160622, accuracy 0.945312, precision 0.9575757575757575, recall 0.9575757575757575
2019-03-05T02:43:07.385467: step 400, loss 0.143357, accuracy 0.925781, precision 0.9314285714285714, recall 0.9588235294117647
2019-03-05T02:43:09.335202: step 401, loss 0.209012, accuracy 0.898438, precision 0.8908045977011494, recall 0.9567901234567902
2019-03-05T02:43:11.287559: step 402, loss 0.140377, accuracy 0.941406, precision 0.9477124183006536, recall 0.9539473684210527
2019-03-05T02:43:13.243453: step 403, loss 0.19937, accuracy 0.921875, precision 0.9441340782122905, recall 0.9441340782122905
2019-03-05T02:43:15.220382: step 404, loss 0.153849, accuracy 0.941406, precision 0.9693251533742331, recall 0.9404761904761905
2019-03-05T02:43:16.082042: step 405, loss 0.148101, accuracy 0.915493, precision 0.9615384615384616, recall 0.9259259259259259
2019-03-05T02:43:18.030790: step 406, loss 0.152706, accuracy 0.941406, precision 0.9717514124293786, recall 0.945054945054945
2019-03-05T02:43:19.975334: step 407, loss 0.131258, accuracy 0.957031, precision 0.9520958083832335, recall 0.9814814814814815
2019-03-05T02:43:21.912627: step 408, loss 0.14698, accuracy 0.941406, precision 0.9617486338797814, recall 0.9565217391304348
2019-03-05T02:43:23.877319: step 409, loss 0.143498, accuracy 0.949219, precision 0.9874213836477987, recall 0.9345238095238095
2019-03-05T02:43:25.811384: step 410, loss 0.122935, accuracy 0.960938, precision 0.9700598802395209, recall 0.9700598802395209
2019-03-05T02:43:27.772937: step 411, loss 0.147841, accuracy 0.953125, precision 0.9588235294117647, recall 0.9702380952380952
2019-03-05T02:43:29.720393: step 412, loss 0.118698, accuracy 0.953125, precision 0.9661016949152542, recall 0.9661016949152542
2019-03-05T02:43:31.670965: step 413, loss 0.148026, accuracy 0.933594, precision 0.925, recall 0.9673202614379085
2019-03-05T02:43:33.622944: step 414, loss 0.15628, accuracy 0.933594, precision 0.9408284023668639, recall 0.9578313253012049
2019-03-05T02:43:35.596636: step 415, loss 0.174991, accuracy 0.921875, precision 0.9444444444444444, recall 0.9444444444444444
2019-03-05T02:43:37.553931: step 416, loss 0.171877, accuracy 0.925781, precision 0.9418604651162791, recall 0.9473684210526315
2019-03-05T02:43:39.505351: step 417, loss 0.159425, accuracy 0.941406, precision 0.9707602339181286, recall 0.9431818181818182
2019-03-05T02:43:41.448901: step 418, loss 0.16084, accuracy 0.929688, precision 0.968944099378882, recall 0.9230769230769231
2019-03-05T02:43:43.406285: step 419, loss 0.14463, accuracy 0.960938, precision 0.9941860465116279, recall 0.95
2019-03-05T02:43:45.355616: step 420, loss 0.105992, accuracy 0.972656, precision 0.9878048780487805, recall 0.9700598802395209
2019-03-05T02:43:47.319397: step 421, loss 0.143207, accuracy 0.945312, precision 0.9503105590062112, recall 0.9622641509433962
2019-03-05T02:43:49.285128: step 422, loss 0.18549, accuracy 0.9375, precision 0.9302325581395349, recall 0.975609756097561
2019-03-05T02:43:51.228806: step 423, loss 0.151143, accuracy 0.941406, precision 0.9259259259259259, recall 0.9803921568627451
2019-03-05T02:43:53.190770: step 424, loss 0.130321, accuracy 0.949219, precision 0.9485714285714286, recall 0.9764705882352941
2019-03-05T02:43:55.145740: step 425, loss 0.172541, accuracy 0.921875, precision 0.9467455621301775, recall 0.935672514619883
2019-03-05T02:43:57.114183: step 426, loss 0.17936, accuracy 0.921875, precision 0.9583333333333334, recall 0.9252873563218391
2019-03-05T02:43:59.067997: step 427, loss 0.139763, accuracy 0.941406, precision 0.9746835443037974, recall 0.9333333333333333
2019-03-05T02:44:01.012922: step 428, loss 0.150845, accuracy 0.933594, precision 0.9829545454545454, recall 0.9251336898395722
2019-03-05T02:44:02.975010: step 429, loss 0.146693, accuracy 0.9375, precision 0.9518072289156626, recall 0.9518072289156626
2019-03-05T02:44:04.927216: step 430, loss 0.128401, accuracy 0.9375, precision 0.9647058823529412, recall 0.9425287356321839
2019-03-05T02:44:06.910596: step 431, loss 0.101956, accuracy 0.980469, precision 0.9834254143646409, recall 0.9888888888888889
2019-03-05T02:44:08.839673: step 432, loss 0.1925, accuracy 0.910156, precision 0.9111111111111111, recall 0.9590643274853801
2019-03-05T02:44:10.789187: step 433, loss 0.137068, accuracy 0.953125, precision 0.9573170731707317, recall 0.9691358024691358
2019-03-05T02:44:12.730578: step 434, loss 0.16274, accuracy 0.941406, precision 0.9421965317919075, recall 0.9702380952380952
2019-03-05T02:44:14.702520: step 435, loss 0.154935, accuracy 0.933594, precision 0.9382022471910112, recall 0.9653179190751445
2019-03-05T02:44:16.640877: step 436, loss 0.139811, accuracy 0.949219, precision 0.9485714285714286, recall 0.9764705882352941
2019-03-05T02:44:18.579754: step 437, loss 0.181225, accuracy 0.929688, precision 0.9433962264150944, recall 0.9433962264150944
2019-03-05T02:44:20.528314: step 438, loss 0.184876, accuracy 0.917969, precision 0.9655172413793104, recall 0.9180327868852459
2019-03-05T02:44:22.478386: step 439, loss 0.134061, accuracy 0.941406, precision 0.9941860465116279, recall 0.9243243243243243
2019-03-05T02:44:24.414732: step 440, loss 0.16214, accuracy 0.949219, precision 1.0, recall 0.9257142857142857
2019-03-05T02:44:26.380145: step 441, loss 0.112385, accuracy 0.960938, precision 0.9709302325581395, recall 0.9709302325581395
2019-03-05T02:44:28.322442: step 442, loss 0.22685, accuracy 0.894531, precision 0.8902439024390244, recall 0.9419354838709677
2019-03-05T02:44:30.282344: step 443, loss 0.143607, accuracy 0.933594, precision 0.9375, recall 0.9649122807017544
2019-03-05T02:44:32.212746: step 444, loss 0.157662, accuracy 0.9375, precision 0.9431818181818182, recall 0.9651162790697675
2019-03-05T02:44:34.154854: step 445, loss 0.151533, accuracy 0.9375, precision 0.9382716049382716, recall 0.9620253164556962
2019-03-05T02:44:36.109967: step 446, loss 0.189522, accuracy 0.921875, precision 0.9333333333333333, recall 0.9447852760736196
2019-03-05T02:44:38.047978: step 447, loss 0.147657, accuracy 0.9375, precision 0.9425287356321839, recall 0.9647058823529412
2019-03-05T02:44:39.990965: step 448, loss 0.146253, accuracy 0.953125, precision 0.9529411764705882, recall 0.9759036144578314
2019-03-05T02:44:41.945419: step 449, loss 0.138292, accuracy 0.953125, precision 0.9625, recall 0.9625
2019-03-05T02:44:42.797994: step 450, loss 0.158738, accuracy 0.901408, precision 0.9767441860465116, recall 0.875
2019-03-05T02:44:44.740947: step 451, loss 0.113245, accuracy 0.976562, precision 0.9943181818181818, recall 0.9722222222222222
2019-03-05T02:44:46.700653: step 452, loss 0.111918, accuracy 0.964844, precision 0.9837837837837838, recall 0.9680851063829787
2019-03-05T02:44:48.661802: step 453, loss 0.1344, accuracy 0.9375, precision 0.9367088607594937, recall 0.961038961038961
2019-03-05T02:44:50.606843: step 454, loss 0.0966723, accuracy 0.960938, precision 0.9505494505494505, recall 0.9942528735632183
2019-03-05T02:44:52.561798: step 455, loss 0.136266, accuracy 0.941406, precision 0.9259259259259259, recall 0.9803921568627451
2019-03-05T02:44:54.516619: step 456, loss 0.162129, accuracy 0.929688, precision 0.9457831325301205, recall 0.9457831325301205
2019-03-05T02:44:56.484355: step 457, loss 0.117269, accuracy 0.949219, precision 0.9712643678160919, recall 0.9548022598870056
2019-03-05T02:44:58.426048: step 458, loss 0.125389, accuracy 0.945312, precision 0.953757225433526, recall 0.9649122807017544
2019-03-05T02:45:00.350884: step 459, loss 0.136774, accuracy 0.945312, precision 0.9456521739130435, recall 0.9775280898876404
2019-03-05T02:45:02.293160: step 460, loss 0.107195, accuracy 0.964844, precision 0.9828571428571429, recall 0.9662921348314607
2019-03-05T02:45:04.248618: step 461, loss 0.147488, accuracy 0.945312, precision 0.9745222929936306, recall 0.9386503067484663
2019-03-05T02:45:06.197040: step 462, loss 0.141701, accuracy 0.9375, precision 0.9709302325581395, recall 0.9382022471910112
2019-03-05T02:45:08.154683: step 463, loss 0.174624, accuracy 0.921875, precision 0.9397590361445783, recall 0.9397590361445783
2019-03-05T02:45:10.110082: step 464, loss 0.118373, accuracy 0.96875, precision 0.9707602339181286, recall 0.9822485207100592
2019-03-05T02:45:12.067051: step 465, loss 0.15196, accuracy 0.953125, precision 0.9675324675324676, recall 0.9551282051282052
2019-03-05T02:45:14.012950: step 466, loss 0.133733, accuracy 0.953125, precision 0.9548022598870056, recall 0.976878612716763
2019-03-05T02:45:15.945344: step 467, loss 0.142541, accuracy 0.960938, precision 0.9523809523809523, recall 0.9876543209876543
2019-03-05T02:45:17.885183: step 468, loss 0.128883, accuracy 0.941406, precision 0.96, recall 0.9545454545454546
2019-03-05T02:45:19.849585: step 469, loss 0.139701, accuracy 0.945312, precision 0.9644970414201184, recall 0.9532163742690059
2019-03-05T02:45:21.784817: step 470, loss 0.111509, accuracy 0.945312, precision 0.9588235294117647, recall 0.9588235294117647
2019-03-05T02:45:23.735573: step 471, loss 0.103951, accuracy 0.964844, precision 0.9760479041916168, recall 0.9702380952380952
2019-03-05T02:45:25.680149: step 472, loss 0.10517, accuracy 0.976562, precision 0.9879518072289156, recall 0.9761904761904762
2019-03-05T02:45:27.623485: step 473, loss 0.154454, accuracy 0.941406, precision 0.9556962025316456, recall 0.949685534591195
2019-03-05T02:45:29.576801: step 474, loss 0.167807, accuracy 0.933594, precision 0.9583333333333334, recall 0.9415204678362573
2019-03-05T02:45:31.507074: step 475, loss 0.163028, accuracy 0.945312, precision 0.9602272727272727, recall 0.9602272727272727
2019-03-05T02:45:33.463174: step 476, loss 0.112853, accuracy 0.96875, precision 0.9822485207100592, recall 0.9707602339181286
2019-03-05T02:45:35.416858: step 477, loss 0.173999, accuracy 0.929688, precision 0.9397590361445783, recall 0.9512195121951219
2019-03-05T02:45:37.375846: step 478, loss 0.150376, accuracy 0.949219, precision 0.9659090909090909, recall 0.96045197740113
2019-03-05T02:45:39.327998: step 479, loss 0.151429, accuracy 0.90625, precision 0.91005291005291, recall 0.9608938547486033
2019-03-05T02:45:41.277002: step 480, loss 0.147058, accuracy 0.945312, precision 0.959731543624161, recall 0.9470198675496688
2019-03-05T02:45:43.246441: step 481, loss 0.120651, accuracy 0.949219, precision 0.9644970414201184, recall 0.9588235294117647
2019-03-05T02:45:45.202398: step 482, loss 0.157105, accuracy 0.9375, precision 0.9731182795698925, recall 0.9427083333333334
2019-03-05T02:45:47.133597: step 483, loss 0.172988, accuracy 0.933594, precision 0.9467455621301775, recall 0.9523809523809523
2019-03-05T02:45:49.080265: step 484, loss 0.10606, accuracy 0.96875, precision 0.9825581395348837, recall 0.9712643678160919
2019-03-05T02:45:51.026879: step 485, loss 0.139343, accuracy 0.957031, precision 0.9878048780487805, recall 0.9473684210526315
2019-03-05T02:45:52.970051: step 486, loss 0.128605, accuracy 0.957031, precision 0.9882352941176471, recall 0.9491525423728814
2019-03-05T02:45:54.937008: step 487, loss 0.124852, accuracy 0.941406, precision 0.9647058823529412, recall 0.9479768786127167
2019-03-05T02:45:56.894297: step 488, loss 0.125528, accuracy 0.949219, precision 0.9448275862068966, recall 0.9647887323943662
2019-03-05T02:45:58.816641: step 489, loss 0.17303, accuracy 0.925781, precision 0.9234972677595629, recall 0.9712643678160919
2019-03-05T02:46:00.755045: step 490, loss 0.176482, accuracy 0.910156, precision 0.9012345679012346, recall 0.954248366013072
2019-03-05T02:46:02.718147: step 491, loss 0.12497, accuracy 0.949219, precision 0.9482758620689655, recall 0.9763313609467456
2019-03-05T02:46:04.668959: step 492, loss 0.109674, accuracy 0.980469, precision 0.9935483870967742, recall 0.9746835443037974
2019-03-05T02:46:06.629386: step 493, loss 0.15016, accuracy 0.949219, precision 0.9887640449438202, recall 0.9411764705882353
2019-03-05T02:46:08.569597: step 494, loss 0.16039, accuracy 0.929688, precision 0.9644970414201184, recall 0.9314285714285714
2019-03-05T02:46:09.427711: step 495, loss 0.239708, accuracy 0.915493, precision 0.9523809523809523, recall 0.9090909090909091
2019-03-05T02:46:11.369541: step 496, loss 0.133378, accuracy 0.953125, precision 0.9700598802395209, recall 0.9585798816568047
2019-03-05T02:46:13.323154: step 497, loss 0.147098, accuracy 0.949219, precision 0.9397590361445783, recall 0.9811320754716981
2019-03-05T02:46:15.272525: step 498, loss 0.146185, accuracy 0.9375, precision 0.9425287356321839, recall 0.9647058823529412
2019-03-05T02:46:17.220088: step 499, loss 0.157481, accuracy 0.921875, precision 0.8888888888888888, recall 1.0
2019-03-05T02:46:19.179314: step 500, loss 0.123799, accuracy 0.953125, precision 0.9375, recall 0.9868421052631579

Evaluation:
[[111  17]
 [ 18  85]]
2019-03-05T02:46:20.833846: step 500, loss 0.410033, accuracy 0.848485, precision 0.8671875, recall 0.8604651162790697

Saved model checkpoint to /home/ubuntu/Project/runs/1551753005/checkpoints/model-500

