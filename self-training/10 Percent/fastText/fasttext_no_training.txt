fastText model loaded
Pretrained Embedding: fastText
Italian: False
Loading data...
11566
Max Document length: 36
Dimension of the training set:  300
Vocabulary Size: 1
Train/Unlabeled/Dev split: 3000/7500/750
Writing to /home/ubuntu/Project/runs/1554179751

2019-04-02T04:35:54.533676: step 1, loss 0.721844, accuracy 0.42, precision 0.27053140096618356, recall 0.7088607594936709
2019-04-02T04:35:54.718810: step 2, loss 0.698631, accuracy 0.47, precision 0.4782608695652174, recall 0.66
2019-04-02T04:35:54.905082: step 3, loss 0.641342, accuracy 0.69, precision 0.8647342995169082, recall 0.7336065573770492
2019-04-02T04:35:55.088699: step 4, loss 0.587063, accuracy 0.753333, precision 0.9855072463768116, recall 0.7418181818181818
2019-04-02T04:35:55.270792: step 5, loss 0.55507, accuracy 0.743333, precision 0.9951690821256038, recall 0.7304964539007093
2019-04-02T04:35:55.458409: step 6, loss 0.517077, accuracy 0.723333, precision 1.0, recall 0.7137931034482758
2019-04-02T04:35:55.642329: step 7, loss 0.487124, accuracy 0.716667, precision 1.0, recall 0.708904109589041
2019-04-02T04:35:55.825404: step 8, loss 0.460974, accuracy 0.713333, precision 1.0, recall 0.7064846416382252
2019-04-02T04:35:56.012888: step 9, loss 0.429418, accuracy 0.713333, precision 1.0, recall 0.7064846416382252
2019-04-02T04:35:56.197079: step 10, loss 0.40199, accuracy 0.733333, precision 1.0, recall 0.7212543554006968
2019-04-02T04:35:56.382528: step 11, loss 0.371158, accuracy 0.77, precision 1.0, recall 0.75
2019-04-02T04:35:56.565798: step 12, loss 0.347109, accuracy 0.84, precision 1.0, recall 0.8117647058823529
2019-04-02T04:35:56.750283: step 13, loss 0.30749, accuracy 0.883333, precision 1.0, recall 0.8553719008264463
2019-04-02T04:35:56.935757: step 14, loss 0.278196, accuracy 0.906667, precision 1.0, recall 0.8808510638297873
2019-04-02T04:35:57.122046: step 15, loss 0.251446, accuracy 0.933333, precision 1.0, recall 0.9118942731277533
2019-04-02T04:35:57.320883: step 16, loss 0.223466, accuracy 0.946667, precision 1.0, recall 0.9282511210762332
2019-04-02T04:35:57.517236: step 17, loss 0.186129, accuracy 0.976667, precision 1.0, recall 0.9672897196261683
2019-04-02T04:35:57.715969: step 18, loss 0.15896, accuracy 0.983333, precision 0.9951690821256038, recall 0.9809523809523809
2019-04-02T04:35:57.913564: step 19, loss 0.147108, accuracy 0.976667, precision 0.9951690821256038, recall 0.9716981132075472
2019-04-02T04:35:58.110976: step 20, loss 0.12327, accuracy 0.99, precision 0.9951690821256038, recall 0.9903846153846154
2019-04-02T04:35:58.307264: step 21, loss 0.107788, accuracy 0.983333, precision 0.9951690821256038, recall 0.9809523809523809
2019-04-02T04:35:58.503923: step 22, loss 0.0969702, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:35:58.702998: step 23, loss 0.0756217, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:35:58.900987: step 24, loss 0.0681078, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:35:59.099241: step 25, loss 0.0577064, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038

Evaluation:
[[442  37]
 [154 117]]
2019-04-02T04:35:59.447824: step 25, loss 0.520632, accuracy 0.745333, precision 0.9227557411273486, recall 0.7416107382550335

2019-04-02T04:35:59.639239: step 26, loss 0.0489059, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:35:59.828943: step 27, loss 0.0390227, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:00.016888: step 28, loss 0.031157, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:00.207254: step 29, loss 0.0298207, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:00.395523: step 30, loss 0.0278202, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:00.586096: step 31, loss 0.0228243, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:00.777441: step 32, loss 0.0170297, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:00.966589: step 33, loss 0.0182054, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:01.154235: step 34, loss 0.0161647, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:01.342158: step 35, loss 0.0143519, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:01.531197: step 36, loss 0.0140293, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:01.721200: step 37, loss 0.0114803, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:01.909475: step 38, loss 0.0126159, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:02.099946: step 39, loss 0.010109, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:02.289083: step 40, loss 0.0099072, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:02.477665: step 41, loss 0.0101104, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:02.665793: step 42, loss 0.00991424, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:02.854974: step 43, loss 0.00903519, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:03.045288: step 44, loss 0.0107429, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:03.236803: step 45, loss 0.00778235, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:03.429134: step 46, loss 0.00831292, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:03.616537: step 47, loss 0.00817771, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:03.807682: step 48, loss 0.00807949, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:03.996300: step 49, loss 0.00649511, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:04.185162: step 50, loss 0.00694916, accuracy 0.996667, precision 0.9951690821256038, recall 1.0

Evaluation:
[[432  47]
 [154 117]]
2019-04-02T04:36:04.216034: step 50, loss 0.735607, accuracy 0.732, precision 0.9018789144050104, recall 0.7372013651877133

2019-04-02T04:36:04.406301: step 51, loss 0.00880282, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:04.594703: step 52, loss 0.00700875, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:04.782981: step 53, loss 0.00696752, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:04.974289: step 54, loss 0.00707007, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:05.163170: step 55, loss 0.00643378, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:05.352640: step 56, loss 0.00708662, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:05.545326: step 57, loss 0.00755569, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:05.733465: step 58, loss 0.00633872, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:05.925572: step 59, loss 0.00640209, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:06.114767: step 60, loss 0.0064518, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:06.305391: step 61, loss 0.00646523, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:06.496759: step 62, loss 0.00634102, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:06.685038: step 63, loss 0.00660739, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:06.875282: step 64, loss 0.00728428, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:07.066704: step 65, loss 0.00641456, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:07.256311: step 66, loss 0.00709985, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:07.448055: step 67, loss 0.00701384, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:07.638574: step 68, loss 0.00747458, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:07.825472: step 69, loss 0.00657185, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:08.013555: step 70, loss 0.00856838, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:08.204269: step 71, loss 0.00599716, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:08.391707: step 72, loss 0.00798208, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:08.579427: step 73, loss 0.00663967, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:08.767999: step 74, loss 0.00589447, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:08.955681: step 75, loss 0.00558025, accuracy 0.996667, precision 0.9951690821256038, recall 1.0

Evaluation:
[[431  48]
 [154 117]]
2019-04-02T04:36:08.986687: step 75, loss 0.789319, accuracy 0.730667, precision 0.8997912317327766, recall 0.7367521367521368

2019-04-02T04:36:09.174390: step 76, loss 0.00557836, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:09.366050: step 77, loss 0.00562342, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:09.556986: step 78, loss 0.00544194, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:09.747494: step 79, loss 0.00758213, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:09.936545: step 80, loss 0.0058002, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:10.125876: step 81, loss 0.00520614, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:10.318200: step 82, loss 0.00628257, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:10.508507: step 83, loss 0.00799763, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:10.696489: step 84, loss 0.00530227, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:10.883831: step 85, loss 0.0069565, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:11.074536: step 86, loss 0.00512029, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:11.262140: step 87, loss 0.00585856, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:11.453111: step 88, loss 0.00609943, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:11.642464: step 89, loss 0.00870257, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:11.833352: step 90, loss 0.00530839, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:12.022763: step 91, loss 0.00738647, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:12.210612: step 92, loss 0.0070354, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:12.400515: step 93, loss 0.00450283, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:12.589664: step 94, loss 0.00652522, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:12.778394: step 95, loss 0.00626056, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:12.967374: step 96, loss 0.00613293, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:13.157706: step 97, loss 0.00712868, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:13.348058: step 98, loss 0.00521808, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:13.538693: step 99, loss 0.00802718, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:13.727709: step 100, loss 0.00715453, accuracy 0.996667, precision 0.9951690821256038, recall 1.0

Evaluation:
[[431  48]
 [151 120]]
2019-04-02T04:36:13.758416: step 100, loss 0.78204, accuracy 0.734667, precision 0.8997912317327766, recall 0.7405498281786942

2019-04-02T04:36:13.948454: step 101, loss 0.00661191, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:14.137363: step 102, loss 0.00662896, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:14.326234: step 103, loss 0.00817786, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:14.518469: step 104, loss 0.00632496, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:14.705377: step 105, loss 0.00787123, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:14.897809: step 106, loss 0.00747464, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:15.086364: step 107, loss 0.00398037, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:15.275861: step 108, loss 0.00629121, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:15.466193: step 109, loss 0.00484426, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:15.654737: step 110, loss 0.00564886, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:15.844979: step 111, loss 0.0061636, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:16.033048: step 112, loss 0.00836448, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:16.224941: step 113, loss 0.00443294, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:16.413574: step 114, loss 0.00553413, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:16.601998: step 115, loss 0.00566153, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:16.789010: step 116, loss 0.00523887, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:16.978550: step 117, loss 0.00591401, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:17.167951: step 118, loss 0.00596381, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:17.356543: step 119, loss 0.0060789, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:17.547108: step 120, loss 0.00445294, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:17.735812: step 121, loss 0.00501744, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:17.925383: step 122, loss 0.00588658, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:18.116913: step 123, loss 0.00583503, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:18.308987: step 124, loss 0.00567675, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:18.498555: step 125, loss 0.00535607, accuracy 0.996667, precision 0.9951690821256038, recall 1.0

Evaluation:
[[435  44]
 [154 117]]
2019-04-02T04:36:18.529625: step 125, loss 0.810302, accuracy 0.736, precision 0.9081419624217119, recall 0.7385398981324278

2019-04-02T04:36:18.717702: step 126, loss 0.00514792, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:18.908184: step 127, loss 0.00617191, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:19.096910: step 128, loss 0.00692719, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:19.286532: step 129, loss 0.00616281, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:19.475127: step 130, loss 0.0033364, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:19.668477: step 131, loss 0.00466598, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:19.858057: step 132, loss 0.00478166, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:20.046533: step 133, loss 0.00691786, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:20.235258: step 134, loss 0.00544935, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:20.427610: step 135, loss 0.00552599, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:20.616133: step 136, loss 0.00545897, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:20.804533: step 137, loss 0.00647051, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:20.992900: step 138, loss 0.00505695, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:21.181213: step 139, loss 0.00467557, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:21.373208: step 140, loss 0.0063922, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:21.561853: step 141, loss 0.00433888, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:21.752528: step 142, loss 0.00384464, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:21.940738: step 143, loss 0.00814539, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:22.128454: step 144, loss 0.00462284, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:22.316829: step 145, loss 0.00663737, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:22.505756: step 146, loss 0.00443355, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:22.693731: step 147, loss 0.00712941, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:22.884473: step 148, loss 0.00840253, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:23.075933: step 149, loss 0.00495201, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:23.265138: step 150, loss 0.00502558, accuracy 0.996667, precision 1.0, recall 0.9951923076923077

Evaluation:
[[437  42]
 [152 119]]
2019-04-02T04:36:23.295662: step 150, loss 0.796814, accuracy 0.741333, precision 0.9123173277661796, recall 0.7419354838709677

2019-04-02T04:36:23.483545: step 151, loss 0.00723128, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:23.671280: step 152, loss 0.00773375, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:23.859566: step 153, loss 0.00464111, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:24.048281: step 154, loss 0.00427385, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:24.240650: step 155, loss 0.00722838, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:24.431096: step 156, loss 0.00464389, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:24.618264: step 157, loss 0.0038596, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:24.808388: step 158, loss 0.00395633, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:24.997832: step 159, loss 0.00570775, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:25.187324: step 160, loss 0.00391992, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:25.376313: step 161, loss 0.00676506, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:25.564800: step 162, loss 0.00584555, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:25.753087: step 163, loss 0.00386096, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:25.941201: step 164, loss 0.00455763, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:26.130087: step 165, loss 0.00714607, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:26.322126: step 166, loss 0.00947574, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:26.508774: step 167, loss 0.00783859, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:26.700894: step 168, loss 0.00513047, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:26.889319: step 169, loss 0.00380396, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:27.077883: step 170, loss 0.00550176, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:27.266107: step 171, loss 0.00760493, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:27.458567: step 172, loss 0.00409107, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:27.646581: step 173, loss 0.00719949, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:27.837144: step 174, loss 0.00432555, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:28.029847: step 175, loss 0.00332768, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[435  44]
 [152 119]]
2019-04-02T04:36:28.060791: step 175, loss 0.819926, accuracy 0.738667, precision 0.9081419624217119, recall 0.7410562180579217

2019-04-02T04:36:28.250078: step 176, loss 0.00541135, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:28.439121: step 177, loss 0.00580205, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:28.629152: step 178, loss 0.00516738, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:28.820857: step 179, loss 0.00788291, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:29.009299: step 180, loss 0.00828972, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:29.197977: step 181, loss 0.00610934, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:29.387159: step 182, loss 0.00627941, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:29.576129: step 183, loss 0.00479276, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:29.762721: step 184, loss 0.00666425, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:29.954079: step 185, loss 0.00745725, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:30.142671: step 186, loss 0.00427202, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:30.333526: step 187, loss 0.00573473, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:30.522488: step 188, loss 0.00523718, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:30.711036: step 189, loss 0.00444706, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:30.901672: step 190, loss 0.00524424, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:31.093141: step 191, loss 0.00437187, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:31.281367: step 192, loss 0.00526141, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:31.470851: step 193, loss 0.00405636, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:31.660609: step 194, loss 0.0062421, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:31.848086: step 195, loss 0.00401074, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:32.039601: step 196, loss 0.00662049, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:32.229862: step 197, loss 0.00798591, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:32.421232: step 198, loss 0.00687292, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:32.614598: step 199, loss 0.00407027, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:32.803505: step 200, loss 0.00390153, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[436  43]
 [152 119]]
2019-04-02T04:36:32.834516: step 200, loss 0.822397, accuracy 0.74, precision 0.9102296450939458, recall 0.7414965986394558

2019-04-02T04:36:33.021293: step 201, loss 0.00558121, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:33.212617: step 202, loss 0.00415995, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:33.402588: step 203, loss 0.0040689, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:33.594270: step 204, loss 0.00503868, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:33.781969: step 205, loss 0.00432134, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:33.970399: step 206, loss 0.00658688, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:34.169616: step 207, loss 0.007658, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:34.360456: step 208, loss 0.0053436, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:34.549827: step 209, loss 0.00654549, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:34.739307: step 210, loss 0.00417925, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:34.926058: step 211, loss 0.00710475, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:35.116730: step 212, loss 0.0060165, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:35.305824: step 213, loss 0.0049838, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:35.494629: step 214, loss 0.00426605, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:35.686529: step 215, loss 0.0049904, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:35.876178: step 216, loss 0.00379256, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:36.063200: step 217, loss 0.00853617, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:36.252628: step 218, loss 0.00595808, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:36.440268: step 219, loss 0.00441067, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:36.631205: step 220, loss 0.00577762, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:36.823153: step 221, loss 0.0073751, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:37.014786: step 222, loss 0.00684171, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:37.205525: step 223, loss 0.00670095, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:37.394978: step 224, loss 0.00345819, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:37.583604: step 225, loss 0.00420345, accuracy 0.996667, precision 1.0, recall 0.9951923076923077

Evaluation:
[[440  39]
 [154 117]]
2019-04-02T04:36:37.614737: step 225, loss 0.866478, accuracy 0.742667, precision 0.918580375782881, recall 0.7407407407407407

2019-04-02T04:36:37.801928: step 226, loss 0.00406573, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:37.990487: step 227, loss 0.00652525, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:38.178643: step 228, loss 0.00398205, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:38.369163: step 229, loss 0.00829249, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:38.557644: step 230, loss 0.00656553, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:38.750092: step 231, loss 0.00502089, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:38.937670: step 232, loss 0.00742264, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:39.125970: step 233, loss 0.00475793, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:39.317212: step 234, loss 0.00557981, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:39.506335: step 235, loss 0.00790216, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:39.695197: step 236, loss 0.00781911, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:39.892309: step 237, loss 0.00699069, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:40.081071: step 238, loss 0.00600709, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:40.269931: step 239, loss 0.00554468, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:40.459068: step 240, loss 0.0043619, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:40.647772: step 241, loss 0.00561959, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:40.838635: step 242, loss 0.00644435, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:41.038336: step 243, loss 0.00596371, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:41.227024: step 244, loss 0.00352667, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:41.415924: step 245, loss 0.00784899, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:41.606827: step 246, loss 0.00546852, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:41.795588: step 247, loss 0.00330932, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:41.984377: step 248, loss 0.00323806, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:42.174662: step 249, loss 0.00745724, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:42.360935: step 250, loss 0.00562904, accuracy 0.996667, precision 1.0, recall 0.9951923076923077

Evaluation:
[[433  46]
 [149 122]]
2019-04-02T04:36:42.391929: step 250, loss 0.853235, accuracy 0.74, precision 0.9039665970772442, recall 0.7439862542955327

Saved model checkpoint to /home/ubuntu/Project/runs/1554179751/checkpoints/model-250

2019-04-02T04:36:42.788603: step 251, loss 0.00508242, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:42.988324: step 252, loss 0.00516998, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:43.184300: step 253, loss 0.00317792, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:43.379940: step 254, loss 0.00578271, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:43.578542: step 255, loss 0.00424121, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:43.776327: step 256, loss 0.00379209, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:43.976775: step 257, loss 0.00571899, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:44.172577: step 258, loss 0.00474506, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:44.378420: step 259, loss 0.00673001, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:44.575691: step 260, loss 0.00591779, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:44.771871: step 261, loss 0.00566362, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:44.966807: step 262, loss 0.00697869, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:45.166828: step 263, loss 0.00456426, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:45.366279: step 264, loss 0.00480472, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:45.563950: step 265, loss 0.00658949, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:45.763670: step 266, loss 0.00604514, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:45.960231: step 267, loss 0.00498463, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:46.157230: step 268, loss 0.00581882, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:46.353538: step 269, loss 0.00441541, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:46.549977: step 270, loss 0.00699704, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:46.747024: step 271, loss 0.0061434, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:46.944769: step 272, loss 0.00576485, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:47.142117: step 273, loss 0.00630762, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:47.339346: step 274, loss 0.00457026, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:47.544807: step 275, loss 0.00587134, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038

Evaluation:
[[434  45]
 [150 121]]
2019-04-02T04:36:47.579490: step 275, loss 0.878587, accuracy 0.74, precision 0.906054279749478, recall 0.7431506849315068

2019-04-02T04:36:47.775682: step 276, loss 0.00369556, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:47.969377: step 277, loss 0.00319593, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:48.166923: step 278, loss 0.00657499, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:48.364151: step 279, loss 0.005766, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:48.565402: step 280, loss 0.00560885, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:48.763012: step 281, loss 0.00400765, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:48.962320: step 282, loss 0.00347301, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:49.159631: step 283, loss 0.00556229, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:49.355873: step 284, loss 0.00661127, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:49.552895: step 285, loss 0.00395201, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:49.750361: step 286, loss 0.003176, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:49.945592: step 287, loss 0.00471915, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:50.149003: step 288, loss 0.00449018, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:50.347834: step 289, loss 0.00390679, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:50.544490: step 290, loss 0.0049579, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:50.744416: step 291, loss 0.00703357, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:50.941871: step 292, loss 0.00725269, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:51.139147: step 293, loss 0.00505026, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:51.336333: step 294, loss 0.00264443, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:51.534335: step 295, loss 0.00488046, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:51.730298: step 296, loss 0.00586634, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:51.927694: step 297, loss 0.00727334, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:52.123434: step 298, loss 0.00274893, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:52.318241: step 299, loss 0.00790671, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:52.516503: step 300, loss 0.00322485, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[436  43]
 [149 122]]
2019-04-02T04:36:52.551541: step 300, loss 0.873437, accuracy 0.744, precision 0.9102296450939458, recall 0.7452991452991453

2019-04-02T04:36:52.748180: step 301, loss 0.00408832, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:52.947581: step 302, loss 0.00443297, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:53.144323: step 303, loss 0.00283646, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:53.344208: step 304, loss 0.00524118, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:53.543708: step 305, loss 0.00459426, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:53.741122: step 306, loss 0.00679316, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:53.938707: step 307, loss 0.00447742, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:54.134972: step 308, loss 0.00334034, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:54.331013: step 309, loss 0.00473937, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:54.527605: step 310, loss 0.00493684, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:54.725822: step 311, loss 0.00531734, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:54.922157: step 312, loss 0.00743064, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:55.120045: step 313, loss 0.00697555, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:55.317704: step 314, loss 0.00776131, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:55.513882: step 315, loss 0.00757663, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:55.710655: step 316, loss 0.00633776, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:55.906581: step 317, loss 0.00559045, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:56.102097: step 318, loss 0.00654256, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:56.298827: step 319, loss 0.00607421, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:56.496484: step 320, loss 0.00410995, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:56.701404: step 321, loss 0.00394852, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:56.896779: step 322, loss 0.00454176, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:57.096508: step 323, loss 0.00557312, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:57.294686: step 324, loss 0.00278623, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:57.491724: step 325, loss 0.00409545, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[434  45]
 [147 124]]
2019-04-02T04:36:57.526578: step 325, loss 0.875218, accuracy 0.744, precision 0.906054279749478, recall 0.7469879518072289

2019-04-02T04:36:57.721403: step 326, loss 0.00325019, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:36:57.919385: step 327, loss 0.00901281, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:58.125779: step 328, loss 0.00657409, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:36:58.321334: step 329, loss 0.00450643, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:58.523795: step 330, loss 0.00405418, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:58.719986: step 331, loss 0.00529355, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:58.916969: step 332, loss 0.00458492, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:59.117231: step 333, loss 0.00426796, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:59.315159: step 334, loss 0.00638091, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:59.522797: step 335, loss 0.00515404, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:36:59.719604: step 336, loss 0.00494342, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:36:59.914504: step 337, loss 0.00360752, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:00.113469: step 338, loss 0.00374201, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:00.313518: step 339, loss 0.00654632, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:00.510420: step 340, loss 0.00558305, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:00.712142: step 341, loss 0.00688623, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:00.909289: step 342, loss 0.00643004, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:01.107602: step 343, loss 0.00411186, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:01.303410: step 344, loss 0.00709912, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:01.501506: step 345, loss 0.00376869, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:01.699294: step 346, loss 0.00805872, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:01.906500: step 347, loss 0.00498671, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:02.105273: step 348, loss 0.00482732, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:02.304194: step 349, loss 0.00706219, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:02.503888: step 350, loss 0.00522275, accuracy 0.996667, precision 1.0, recall 0.9951923076923077

Evaluation:
[[433  46]
 [147 124]]
2019-04-02T04:37:02.538701: step 350, loss 0.890921, accuracy 0.742667, precision 0.9039665970772442, recall 0.746551724137931

2019-04-02T04:37:02.733332: step 351, loss 0.00304599, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:02.929208: step 352, loss 0.00424939, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:03.126577: step 353, loss 0.0061164, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:03.324102: step 354, loss 0.00498981, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:03.521058: step 355, loss 0.0048233, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:03.718154: step 356, loss 0.00329508, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:03.916374: step 357, loss 0.00558978, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:04.113473: step 358, loss 0.00386262, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:04.310189: step 359, loss 0.00418462, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:04.508405: step 360, loss 0.00309901, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:04.706095: step 361, loss 0.004188, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:04.903313: step 362, loss 0.00488199, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:05.107848: step 363, loss 0.00685996, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:05.304056: step 364, loss 0.00361219, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:05.501833: step 365, loss 0.00621993, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:05.697604: step 366, loss 0.00427614, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:05.892969: step 367, loss 0.00559517, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:06.090523: step 368, loss 0.00254715, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:06.288852: step 369, loss 0.00767925, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:06.486165: step 370, loss 0.00574617, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:06.684490: step 371, loss 0.0026553, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:06.882141: step 372, loss 0.00738941, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:07.080137: step 373, loss 0.00415495, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:07.275958: step 374, loss 0.00366669, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:07.474049: step 375, loss 0.00468246, accuracy 0.996667, precision 0.9951690821256038, recall 1.0

Evaluation:
[[432  47]
 [143 128]]
2019-04-02T04:37:07.509517: step 375, loss 0.894145, accuracy 0.746667, precision 0.9018789144050104, recall 0.7513043478260869

2019-04-02T04:37:07.705983: step 376, loss 0.00437061, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:07.902828: step 377, loss 0.00489994, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:08.099895: step 378, loss 0.00652391, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:08.295924: step 379, loss 0.00343171, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:08.494579: step 380, loss 0.00567171, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:08.692612: step 381, loss 0.00927014, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:08.889445: step 382, loss 0.00527876, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:09.085407: step 383, loss 0.00302237, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:09.281959: step 384, loss 0.00478011, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:09.481054: step 385, loss 0.00560455, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:09.679086: step 386, loss 0.00545759, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:09.879017: step 387, loss 0.00439264, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:10.076804: step 388, loss 0.00664752, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:10.276829: step 389, loss 0.00508844, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:10.473002: step 390, loss 0.00252873, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:10.671008: step 391, loss 0.00383211, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:10.865260: step 392, loss 0.00426634, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:11.061242: step 393, loss 0.00539201, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:11.257847: step 394, loss 0.00626414, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:11.454097: step 395, loss 0.00592702, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:11.658886: step 396, loss 0.0063425, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:11.853639: step 397, loss 0.00656539, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:12.049127: step 398, loss 0.00462832, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:12.245379: step 399, loss 0.00963675, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:12.444428: step 400, loss 0.00887305, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038

Evaluation:
[[434  45]
 [146 125]]
2019-04-02T04:37:12.479528: step 400, loss 0.913727, accuracy 0.745333, precision 0.906054279749478, recall 0.7482758620689656

2019-04-02T04:37:12.676557: step 401, loss 0.00426842, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:12.876647: step 402, loss 0.00670352, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:13.075015: step 403, loss 0.00469536, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:13.285987: step 404, loss 0.00399142, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:13.483935: step 405, loss 0.00483714, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:13.680864: step 406, loss 0.00297044, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:13.877652: step 407, loss 0.00757344, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:14.082726: step 408, loss 0.00782749, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:14.278999: step 409, loss 0.00943839, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:14.473326: step 410, loss 0.0062621, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:14.668976: step 411, loss 0.00509957, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:14.865955: step 412, loss 0.0063813, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:15.064361: step 413, loss 0.00614718, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:15.262477: step 414, loss 0.0049809, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:15.460662: step 415, loss 0.00486591, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:15.658556: step 416, loss 0.00407358, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:15.855417: step 417, loss 0.00349562, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:16.052961: step 418, loss 0.003679, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:16.248654: step 419, loss 0.00805096, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:16.446686: step 420, loss 0.00577671, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:16.647171: step 421, loss 0.00328137, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:16.844224: step 422, loss 0.00437253, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:17.043680: step 423, loss 0.00643703, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:17.240709: step 424, loss 0.00429089, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:17.440961: step 425, loss 0.00646765, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038

Evaluation:
[[426  53]
 [139 132]]
2019-04-02T04:37:17.475788: step 425, loss 0.907697, accuracy 0.744, precision 0.8893528183716075, recall 0.7539823008849558

2019-04-02T04:37:17.672780: step 426, loss 0.0045473, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:17.872082: step 427, loss 0.00360914, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:18.068812: step 428, loss 0.00506223, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:18.274419: step 429, loss 0.00696106, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:18.472528: step 430, loss 0.00348116, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:18.670009: step 431, loss 0.00378936, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:18.866883: step 432, loss 0.00727561, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:19.066553: step 433, loss 0.00668, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:19.263669: step 434, loss 0.00339264, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:19.462793: step 435, loss 0.00409745, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:19.667080: step 436, loss 0.0070781, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:19.863607: step 437, loss 0.00635655, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:20.062921: step 438, loss 0.00549173, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:20.260838: step 439, loss 0.00443892, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:20.461531: step 440, loss 0.00717118, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:20.658812: step 441, loss 0.00656284, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:20.855170: step 442, loss 0.00550341, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:21.055547: step 443, loss 0.00567912, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:21.252571: step 444, loss 0.00552934, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:21.452468: step 445, loss 0.00565902, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:21.650943: step 446, loss 0.00453322, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:21.845230: step 447, loss 0.00553823, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:22.041458: step 448, loss 0.00265251, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:22.238902: step 449, loss 0.00620354, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:22.435221: step 450, loss 0.00562469, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038

Evaluation:
[[421  58]
 [132 139]]
2019-04-02T04:37:22.469955: step 450, loss 0.888283, accuracy 0.746667, precision 0.8789144050104384, recall 0.7613019891500904

2019-04-02T04:37:22.664684: step 451, loss 0.00670977, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:22.862847: step 452, loss 0.00264805, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:23.059072: step 453, loss 0.00700224, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:23.259137: step 454, loss 0.00906875, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:23.457921: step 455, loss 0.00277577, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:23.656180: step 456, loss 0.00566966, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:23.853472: step 457, loss 0.00497686, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:24.050930: step 458, loss 0.00354038, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:24.249960: step 459, loss 0.00428581, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:24.447438: step 460, loss 0.0039116, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:24.645112: step 461, loss 0.0052926, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:24.842423: step 462, loss 0.00885365, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:25.038788: step 463, loss 0.00573329, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:25.235463: step 464, loss 0.00652893, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:25.432027: step 465, loss 0.00488414, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:25.632977: step 466, loss 0.00573977, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:25.830297: step 467, loss 0.00423845, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:26.028926: step 468, loss 0.00411072, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:26.227460: step 469, loss 0.00330519, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:26.425419: step 470, loss 0.00692002, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:26.624652: step 471, loss 0.00873138, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:26.821236: step 472, loss 0.00546294, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:27.016831: step 473, loss 0.00385376, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:27.212573: step 474, loss 0.00390271, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:27.412255: step 475, loss 0.00725513, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038

Evaluation:
[[434  45]
 [149 122]]
2019-04-02T04:37:27.446956: step 475, loss 0.952144, accuracy 0.741333, precision 0.906054279749478, recall 0.7444253859348199

2019-04-02T04:37:27.644936: step 476, loss 0.00464576, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:27.841658: step 477, loss 0.00349894, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:28.039654: step 478, loss 0.00356381, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:28.238369: step 479, loss 0.00690386, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:28.437389: step 480, loss 0.00475687, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:28.633423: step 481, loss 0.00420048, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:28.828249: step 482, loss 0.00456208, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:29.028039: step 483, loss 0.00511702, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:29.223514: step 484, loss 0.00824825, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:29.420494: step 485, loss 0.00386332, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:29.620257: step 486, loss 0.00545583, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:29.816464: step 487, loss 0.00603512, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:30.012919: step 488, loss 0.00461807, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:30.211005: step 489, loss 0.00601619, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:30.408106: step 490, loss 0.0085539, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:30.607811: step 491, loss 0.00775713, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:30.805897: step 492, loss 0.00522086, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:31.010270: step 493, loss 0.0102979, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:31.209379: step 494, loss 0.00421929, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:31.406983: step 495, loss 0.00697201, accuracy 0.993333, precision 0.9951690821256038, recall 0.9951690821256038
2019-04-02T04:37:31.604011: step 496, loss 0.00638508, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:31.801575: step 497, loss 0.00363841, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:37:31.999054: step 498, loss 0.00544351, accuracy 0.996667, precision 1.0, recall 0.9951923076923077
2019-04-02T04:37:32.196609: step 499, loss 0.00437265, accuracy 0.996667, precision 0.9951690821256038, recall 1.0
2019-04-02T04:37:32.396292: step 500, loss 0.00312272, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[432  47]
 [149 122]]
2019-04-02T04:37:32.430995: step 500, loss 0.962067, accuracy 0.738667, precision 0.9018789144050104, recall 0.7435456110154905

Saved model checkpoint to /home/ubuntu/Project/runs/1554179751/checkpoints/model-500

