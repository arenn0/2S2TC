GloVe model loaded
Pretrained Embedding: GloVe
Italian: False
Loading data...
11566
Max Document length: 36
Dimension of the training set:  300
Vocabulary Size: 1
Train/Unlabeled/Dev split: 3000/7500/750
Writing to /home/ubuntu/Project/runs/1554180548

2019-04-02T04:49:12.251864: step 1, loss 0.729997, accuracy 0.41, precision 0.09392265193370165, recall 0.5666666666666667
2019-04-02T04:49:12.430818: step 2, loss 0.714923, accuracy 0.466667, precision 0.2596685082872928, recall 0.6438356164383562
2019-04-02T04:49:12.613235: step 3, loss 0.661173, accuracy 0.61, precision 0.6519337016574586, recall 0.686046511627907
2019-04-02T04:49:12.793005: step 4, loss 0.604135, accuracy 0.766667, precision 0.8839779005524862, recall 0.7655502392344498
2019-04-02T04:49:12.971277: step 5, loss 0.575899, accuracy 0.773333, precision 0.9779005524861878, recall 0.7344398340248963
2019-04-02T04:49:13.148712: step 6, loss 0.540007, accuracy 0.776667, precision 1.0, recall 0.7298387096774194
2019-04-02T04:49:13.325416: step 7, loss 0.513543, accuracy 0.763333, precision 1.0, recall 0.7182539682539683
2019-04-02T04:49:13.507214: step 8, loss 0.464233, accuracy 0.78, precision 1.0, recall 0.7327935222672065
2019-04-02T04:49:13.689890: step 9, loss 0.453874, accuracy 0.776667, precision 1.0, recall 0.7298387096774194
2019-04-02T04:49:13.868573: step 10, loss 0.400941, accuracy 0.843333, precision 1.0, recall 0.793859649122807
2019-04-02T04:49:14.048750: step 11, loss 0.366636, accuracy 0.893333, precision 1.0, recall 0.8497652582159625
2019-04-02T04:49:14.226390: step 12, loss 0.318301, accuracy 0.92, precision 1.0, recall 0.8829268292682927
2019-04-02T04:49:14.405591: step 13, loss 0.293296, accuracy 0.933333, precision 1.0, recall 0.900497512437811
2019-04-02T04:49:14.588912: step 14, loss 0.260698, accuracy 0.953333, precision 0.994475138121547, recall 0.9326424870466321
2019-04-02T04:49:14.770489: step 15, loss 0.217174, accuracy 0.956667, precision 1.0, recall 0.9329896907216495
2019-04-02T04:49:14.950639: step 16, loss 0.200427, accuracy 0.963333, precision 1.0, recall 0.9427083333333334
2019-04-02T04:49:15.148108: step 17, loss 0.174906, accuracy 0.98, precision 1.0, recall 0.9679144385026738
2019-04-02T04:49:15.343787: step 18, loss 0.147314, accuracy 0.986667, precision 0.994475138121547, recall 0.9836065573770492
2019-04-02T04:49:15.539509: step 19, loss 0.128302, accuracy 0.986667, precision 0.994475138121547, recall 0.9836065573770492
2019-04-02T04:49:15.734284: step 20, loss 0.113917, accuracy 0.983333, precision 1.0, recall 0.9731182795698925
2019-04-02T04:49:15.934766: step 21, loss 0.0948411, accuracy 0.993333, precision 1.0, recall 0.9890710382513661
2019-04-02T04:49:16.132571: step 22, loss 0.0754767, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:16.327813: step 23, loss 0.0637612, accuracy 0.993333, precision 1.0, recall 0.9890710382513661
2019-04-02T04:49:16.525131: step 24, loss 0.0589963, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:16.722395: step 25, loss 0.0516843, accuracy 0.993333, precision 1.0, recall 0.9890710382513661

Evaluation:
[[396  85]
 [ 95 174]]
2019-04-02T04:49:17.077235: step 25, loss 0.493021, accuracy 0.76, precision 0.8232848232848233, recall 0.8065173116089613

2019-04-02T04:49:17.269123: step 26, loss 0.0385417, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:17.458150: step 27, loss 0.0371044, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:17.649657: step 28, loss 0.0299911, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:17.840987: step 29, loss 0.0274046, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:18.033633: step 30, loss 0.0245679, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:18.225233: step 31, loss 0.0203631, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:18.413862: step 32, loss 0.0194492, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:18.602246: step 33, loss 0.015809, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:18.790884: step 34, loss 0.0158876, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:18.981208: step 35, loss 0.0166257, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:19.169231: step 36, loss 0.0138715, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:19.357292: step 37, loss 0.0165878, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:19.548084: step 38, loss 0.0138547, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:19.737118: step 39, loss 0.0122981, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:19.928113: step 40, loss 0.0138867, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:20.115727: step 41, loss 0.0123227, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:20.304452: step 42, loss 0.0100414, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:20.494619: step 43, loss 0.010846, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:20.683750: step 44, loss 0.00953253, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:20.874130: step 45, loss 0.00929328, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:21.063772: step 46, loss 0.00774201, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:21.253310: step 47, loss 0.00972012, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:21.441739: step 48, loss 0.00956038, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:21.632113: step 49, loss 0.00914393, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:21.820747: step 50, loss 0.00984642, accuracy 0.996667, precision 1.0, recall 0.9945054945054945

Evaluation:
[[396  85]
 [101 168]]
2019-04-02T04:49:21.851966: step 50, loss 0.611395, accuracy 0.752, precision 0.8232848232848233, recall 0.7967806841046278

2019-04-02T04:49:22.039277: step 51, loss 0.00945017, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:22.231699: step 52, loss 0.0112116, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:22.423640: step 53, loss 0.0110991, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:22.613071: step 54, loss 0.0101019, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:22.803109: step 55, loss 0.00871729, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:22.993091: step 56, loss 0.00990038, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:23.180528: step 57, loss 0.0105281, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:23.372046: step 58, loss 0.00725305, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:23.562113: step 59, loss 0.0087933, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:23.754688: step 60, loss 0.00991219, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:23.946110: step 61, loss 0.00892119, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:24.136586: step 62, loss 0.00940936, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:24.325298: step 63, loss 0.00761373, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:24.517290: step 64, loss 0.00937548, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:24.709043: step 65, loss 0.00965212, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:24.899022: step 66, loss 0.00776131, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:25.091213: step 67, loss 0.00613378, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:25.282768: step 68, loss 0.00797831, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:25.474683: step 69, loss 0.00886316, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:25.663501: step 70, loss 0.00730285, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:25.857012: step 71, loss 0.00634216, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:26.060556: step 72, loss 0.00706552, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:26.252355: step 73, loss 0.00596522, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:26.443912: step 74, loss 0.00505505, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:26.637458: step 75, loss 0.00858059, accuracy 0.996667, precision 1.0, recall 0.9945054945054945

Evaluation:
[[396  85]
 [101 168]]
2019-04-02T04:49:26.669124: step 75, loss 0.638754, accuracy 0.752, precision 0.8232848232848233, recall 0.7967806841046278

2019-04-02T04:49:26.858169: step 76, loss 0.00966755, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:27.046125: step 77, loss 0.00604577, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:27.236772: step 78, loss 0.00625181, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:27.423371: step 79, loss 0.00624721, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:27.612041: step 80, loss 0.0108617, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:27.804205: step 81, loss 0.00736469, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:27.994596: step 82, loss 0.00776311, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:28.184089: step 83, loss 0.00926398, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:28.375450: step 84, loss 0.0062244, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:28.566800: step 85, loss 0.00792863, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:28.756832: step 86, loss 0.00533555, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:28.945262: step 87, loss 0.00593749, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:29.133490: step 88, loss 0.00999555, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:29.325069: step 89, loss 0.00715892, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:29.513489: step 90, loss 0.00519132, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:29.704902: step 91, loss 0.0092948, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:29.895849: step 92, loss 0.00474493, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:30.082114: step 93, loss 0.00757159, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:30.270361: step 94, loss 0.00715527, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:30.458747: step 95, loss 0.00760522, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:30.650165: step 96, loss 0.00884154, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:30.839244: step 97, loss 0.00597805, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:31.029273: step 98, loss 0.00748839, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:31.219895: step 99, loss 0.00529559, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:31.408221: step 100, loss 0.00712855, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547

Evaluation:
[[392  89]
 [ 95 174]]
2019-04-02T04:49:31.439859: step 100, loss 0.652038, accuracy 0.754667, precision 0.814968814968815, recall 0.8049281314168378

2019-04-02T04:49:31.630405: step 101, loss 0.0067189, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:31.819330: step 102, loss 0.00522408, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:32.008093: step 103, loss 0.00715366, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:32.198305: step 104, loss 0.00615839, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:32.387057: step 105, loss 0.00666931, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:32.576413: step 106, loss 0.00715898, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:32.764936: step 107, loss 0.00667864, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:32.953347: step 108, loss 0.00767286, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:33.141483: step 109, loss 0.00918954, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:33.332440: step 110, loss 0.00544814, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:33.522871: step 111, loss 0.00755691, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:33.715690: step 112, loss 0.0106779, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:33.903610: step 113, loss 0.00549204, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:34.093720: step 114, loss 0.00805682, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:34.281348: step 115, loss 0.00686446, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:34.473089: step 116, loss 0.00649182, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:34.663342: step 117, loss 0.00491104, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:34.852656: step 118, loss 0.00810301, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:35.042656: step 119, loss 0.006143, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:35.230772: step 120, loss 0.0080237, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:35.419352: step 121, loss 0.00494672, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:35.610112: step 122, loss 0.00678401, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:35.798388: step 123, loss 0.00657082, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:35.987319: step 124, loss 0.00752028, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:36.177051: step 125, loss 0.00515435, accuracy 0.996667, precision 0.994475138121547, recall 1.0

Evaluation:
[[395  86]
 [ 98 171]]
2019-04-02T04:49:36.208639: step 125, loss 0.677696, accuracy 0.754667, precision 0.8212058212058212, recall 0.8012170385395537

2019-04-02T04:49:36.395889: step 126, loss 0.00777159, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:36.587901: step 127, loss 0.00489692, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:36.777590: step 128, loss 0.0053911, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:36.967244: step 129, loss 0.00759108, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:37.156075: step 130, loss 0.00744318, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:37.347472: step 131, loss 0.00635237, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:37.538794: step 132, loss 0.00323812, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:37.728443: step 133, loss 0.00859557, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:37.916336: step 134, loss 0.00847259, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:38.105376: step 135, loss 0.00647567, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:38.294866: step 136, loss 0.0067119, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:38.486659: step 137, loss 0.00565597, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:38.676921: step 138, loss 0.00821101, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:38.867655: step 139, loss 0.00532986, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:39.058736: step 140, loss 0.00892548, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:39.246829: step 141, loss 0.00506526, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:39.437857: step 142, loss 0.00674049, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:39.626815: step 143, loss 0.00397216, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:39.815580: step 144, loss 0.0102543, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:40.003335: step 145, loss 0.00414537, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:40.193398: step 146, loss 0.00309203, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:40.382178: step 147, loss 0.00960488, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:40.570675: step 148, loss 0.00812712, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:40.759169: step 149, loss 0.00570138, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:40.948575: step 150, loss 0.00668802, accuracy 0.996667, precision 0.994475138121547, recall 1.0

Evaluation:
[[389  92]
 [ 94 175]]
2019-04-02T04:49:40.980175: step 150, loss 0.695191, accuracy 0.752, precision 0.8087318087318087, recall 0.8053830227743272

2019-04-02T04:49:41.168513: step 151, loss 0.0068233, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:41.356493: step 152, loss 0.00561323, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:41.546484: step 153, loss 0.0065488, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:41.739031: step 154, loss 0.00331012, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:41.926433: step 155, loss 0.00471716, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:42.114347: step 156, loss 0.00510296, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:42.302599: step 157, loss 0.00659259, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:42.490501: step 158, loss 0.006437, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:42.679778: step 159, loss 0.00439685, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:42.867849: step 160, loss 0.0073245, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:43.057784: step 161, loss 0.00466009, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:43.252588: step 162, loss 0.00651164, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:43.440570: step 163, loss 0.00428641, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:43.631711: step 164, loss 0.0108382, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:43.820827: step 165, loss 0.0072944, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:44.012621: step 166, loss 0.00390522, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:44.201228: step 167, loss 0.00567709, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:44.390049: step 168, loss 0.00891964, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:44.578722: step 169, loss 0.00851939, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:44.769067: step 170, loss 0.00646884, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:44.960157: step 171, loss 0.00879102, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:45.149675: step 172, loss 0.00668273, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:45.339010: step 173, loss 0.00736722, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:45.530185: step 174, loss 0.00856458, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:45.721176: step 175, loss 0.00819124, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547

Evaluation:
[[397  84]
 [ 99 170]]
2019-04-02T04:49:45.752743: step 175, loss 0.724125, accuracy 0.756, precision 0.8253638253638254, recall 0.8004032258064516

2019-04-02T04:49:45.940371: step 176, loss 0.00823193, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:46.129679: step 177, loss 0.00345654, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:46.318192: step 178, loss 0.00685411, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:46.507509: step 179, loss 0.0104056, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:46.697444: step 180, loss 0.00505292, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:46.889904: step 181, loss 0.00801375, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:47.079404: step 182, loss 0.00602811, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:47.269439: step 183, loss 0.00483097, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:47.460355: step 184, loss 0.00589786, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:47.648362: step 185, loss 0.00573264, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:47.839341: step 186, loss 0.0122018, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:48.028879: step 187, loss 0.00771206, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:48.218823: step 188, loss 0.00739936, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:48.408344: step 189, loss 0.00978317, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:48.598321: step 190, loss 0.00442922, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:48.787909: step 191, loss 0.00770475, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:48.980079: step 192, loss 0.00920372, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:49.173030: step 193, loss 0.00548676, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:49.362730: step 194, loss 0.00854693, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:49.553672: step 195, loss 0.00410507, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:49.745086: step 196, loss 0.00519378, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:49.933504: step 197, loss 0.00576783, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:50.122446: step 198, loss 0.00640145, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:50.312130: step 199, loss 0.00303474, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:50.500677: step 200, loss 0.00320172, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[401  80]
 [101 168]]
2019-04-02T04:49:50.532173: step 200, loss 0.741576, accuracy 0.758667, precision 0.8336798336798337, recall 0.798804780876494

2019-04-02T04:49:50.724514: step 201, loss 0.0064728, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:50.916310: step 202, loss 0.00586164, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:51.104788: step 203, loss 0.00908129, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:51.294929: step 204, loss 0.0094641, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:51.483944: step 205, loss 0.00905698, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:51.676135: step 206, loss 0.0066712, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:51.864344: step 207, loss 0.00551869, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:52.053004: step 208, loss 0.00380087, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:52.241670: step 209, loss 0.0051254, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:52.433113: step 210, loss 0.00816443, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:52.622259: step 211, loss 0.0119604, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:52.812189: step 212, loss 0.00940063, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:53.002257: step 213, loss 0.00702452, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:53.201218: step 214, loss 0.00620473, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:53.388562: step 215, loss 0.00350194, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:53.577807: step 216, loss 0.0087142, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:53.767438: step 217, loss 0.00964511, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:53.958207: step 218, loss 0.00576382, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:54.146490: step 219, loss 0.0112415, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:54.335984: step 220, loss 0.00350466, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:54.526540: step 221, loss 0.0104285, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:54.716814: step 222, loss 0.00801409, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:54.906084: step 223, loss 0.00683896, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:55.097018: step 224, loss 0.00851155, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:55.285912: step 225, loss 0.00807697, accuracy 0.996667, precision 0.994475138121547, recall 1.0

Evaluation:
[[387  94]
 [ 94 175]]
2019-04-02T04:49:55.317202: step 225, loss 0.738508, accuracy 0.749333, precision 0.8045738045738046, recall 0.8045738045738046

2019-04-02T04:49:55.506301: step 226, loss 0.00678332, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:55.699226: step 227, loss 0.00975914, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:55.888271: step 228, loss 0.00264586, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:56.075829: step 229, loss 0.00232596, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:56.264685: step 230, loss 0.00324399, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:56.453853: step 231, loss 0.00694119, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:56.643458: step 232, loss 0.00480155, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:56.836230: step 233, loss 0.00701152, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:57.024247: step 234, loss 0.00350807, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:57.213008: step 235, loss 0.00562843, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:57.406542: step 236, loss 0.00802213, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:57.595333: step 237, loss 0.00717889, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:57.783524: step 238, loss 0.00821572, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:57.973784: step 239, loss 0.00400943, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:58.161792: step 240, loss 0.00522902, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:49:58.350911: step 241, loss 0.00868624, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:58.540369: step 242, loss 0.00642406, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:58.731905: step 243, loss 0.00861849, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:49:58.924567: step 244, loss 0.00467726, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:59.115587: step 245, loss 0.00714029, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:59.306856: step 246, loss 0.00264747, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:59.498825: step 247, loss 0.00366477, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:49:59.689458: step 248, loss 0.00980162, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:49:59.878377: step 249, loss 0.00755752, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:00.067048: step 250, loss 0.00420676, accuracy 0.996667, precision 1.0, recall 0.9945054945054945

Evaluation:
[[398  83]
 [ 98 171]]
2019-04-02T04:50:00.098814: step 250, loss 0.757932, accuracy 0.758667, precision 0.8274428274428275, recall 0.8024193548387096

Saved model checkpoint to /home/ubuntu/Project/runs/1554180548/checkpoints/model-250

2019-04-02T04:50:00.488809: step 251, loss 0.00838666, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:00.679192: step 252, loss 0.00589423, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:00.869136: step 253, loss 0.00346893, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:01.066554: step 254, loss 0.0061398, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:01.263815: step 255, loss 0.00694415, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:01.462565: step 256, loss 0.00695741, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:01.657178: step 257, loss 0.00588614, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:01.852541: step 258, loss 0.0108811, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:02.050401: step 259, loss 0.00323449, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:02.246423: step 260, loss 0.00184968, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:02.442567: step 261, loss 0.00633574, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:02.640106: step 262, loss 0.00465913, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:02.836380: step 263, loss 0.00733898, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:03.032401: step 264, loss 0.00311232, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:03.231651: step 265, loss 0.00796236, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:03.429067: step 266, loss 0.00189287, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:03.626714: step 267, loss 0.00489847, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:03.823444: step 268, loss 0.00451221, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:04.019335: step 269, loss 0.00614621, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:04.216824: step 270, loss 0.00463188, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:04.415992: step 271, loss 0.00558603, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:04.612358: step 272, loss 0.00229376, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:04.808062: step 273, loss 0.00701678, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:05.006736: step 274, loss 0.00553388, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:05.204623: step 275, loss 0.00206583, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[397  84]
 [ 99 170]]
2019-04-02T04:50:05.239847: step 275, loss 0.767422, accuracy 0.756, precision 0.8253638253638254, recall 0.8004032258064516

2019-04-02T04:50:05.436974: step 276, loss 0.00760589, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:05.634259: step 277, loss 0.0101166, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:05.830462: step 278, loss 0.00387862, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:06.026160: step 279, loss 0.00369088, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:06.222535: step 280, loss 0.00674032, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:06.421047: step 281, loss 0.00292692, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:06.617483: step 282, loss 0.00497228, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:06.814162: step 283, loss 0.00571143, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:07.009470: step 284, loss 0.00352749, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:07.201413: step 285, loss 0.00664919, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:07.396339: step 286, loss 0.00623342, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:07.591478: step 287, loss 0.00342136, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:07.789113: step 288, loss 0.0070664, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:07.984223: step 289, loss 0.00575673, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:08.181451: step 290, loss 0.00879051, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:08.377760: step 291, loss 0.00378459, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:08.574693: step 292, loss 0.00578232, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:08.772370: step 293, loss 0.0100208, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:08.968962: step 294, loss 0.00952358, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:09.162985: step 295, loss 0.00492239, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:09.360944: step 296, loss 0.00831384, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:09.556121: step 297, loss 0.0111895, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:09.750877: step 298, loss 0.00631082, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:09.945566: step 299, loss 0.0099467, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:10.140903: step 300, loss 0.00591797, accuracy 0.996667, precision 1.0, recall 0.9945054945054945

Evaluation:
[[404  77]
 [101 168]]
2019-04-02T04:50:10.175651: step 300, loss 0.785119, accuracy 0.762667, precision 0.83991683991684, recall 0.8

2019-04-02T04:50:10.370930: step 301, loss 0.00760113, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:10.566126: step 302, loss 0.00783447, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:10.764453: step 303, loss 0.00402823, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:10.959877: step 304, loss 0.00311861, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:11.155096: step 305, loss 0.00498223, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:11.352147: step 306, loss 0.0029592, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:11.547810: step 307, loss 0.0123016, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:11.742252: step 308, loss 0.00540859, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:11.937652: step 309, loss 0.00493866, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:12.133480: step 310, loss 0.00755472, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:12.329200: step 311, loss 0.00715498, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:12.525269: step 312, loss 0.00523417, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:12.724459: step 313, loss 0.00825432, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:12.923543: step 314, loss 0.00594367, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:13.118669: step 315, loss 0.00995758, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:13.317620: step 316, loss 0.00801917, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:13.515206: step 317, loss 0.00497217, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:13.709756: step 318, loss 0.00311792, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:13.905786: step 319, loss 0.00662312, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:14.101939: step 320, loss 0.0102748, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:14.299922: step 321, loss 0.00822345, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:14.494786: step 322, loss 0.00540233, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:14.691924: step 323, loss 0.0038914, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:14.889730: step 324, loss 0.00423094, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:15.083518: step 325, loss 0.00351128, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[392  89]
 [ 94 175]]
2019-04-02T04:50:15.118434: step 325, loss 0.785994, accuracy 0.756, precision 0.814968814968815, recall 0.8065843621399177

2019-04-02T04:50:15.315856: step 326, loss 0.00562464, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:15.513118: step 327, loss 0.00756573, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:15.712105: step 328, loss 0.0072683, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:15.907358: step 329, loss 0.00594841, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:16.103417: step 330, loss 0.0110796, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:16.299904: step 331, loss 0.00280786, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:16.496555: step 332, loss 0.00604456, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:16.692519: step 333, loss 0.00583986, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:16.887642: step 334, loss 0.0055333, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:17.084737: step 335, loss 0.00420004, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:17.282707: step 336, loss 0.00746383, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:17.478830: step 337, loss 0.00983999, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:17.674297: step 338, loss 0.00306103, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:17.870155: step 339, loss 0.010427, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:18.069025: step 340, loss 0.00780163, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:18.267825: step 341, loss 0.00543066, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:18.462439: step 342, loss 0.00461218, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:18.659367: step 343, loss 0.00548713, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:18.856381: step 344, loss 0.00572562, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:19.055752: step 345, loss 0.00529888, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:19.251676: step 346, loss 0.00544516, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:19.447637: step 347, loss 0.00243476, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:19.645009: step 348, loss 0.00189055, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:19.843195: step 349, loss 0.00511084, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:20.038335: step 350, loss 0.00270445, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[397  84]
 [100 169]]
2019-04-02T04:50:20.073310: step 350, loss 0.804747, accuracy 0.754667, precision 0.8253638253638254, recall 0.7987927565392354

2019-04-02T04:50:20.267430: step 351, loss 0.00253937, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:20.464519: step 352, loss 0.00601505, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:20.660093: step 353, loss 0.00738367, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:20.855769: step 354, loss 0.00387295, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:21.054195: step 355, loss 0.00649689, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:21.251286: step 356, loss 0.00650926, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:21.448868: step 357, loss 0.00643692, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:21.646406: step 358, loss 0.00498109, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:21.842695: step 359, loss 0.00578681, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:22.038809: step 360, loss 0.00606884, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:22.234467: step 361, loss 0.00792337, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:22.430716: step 362, loss 0.00550707, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:22.626849: step 363, loss 0.00334036, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:22.822015: step 364, loss 0.0043849, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:23.020590: step 365, loss 0.0066166, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:23.215392: step 366, loss 0.0097817, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:23.411864: step 367, loss 0.00410888, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:23.607622: step 368, loss 0.00477067, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:23.803779: step 369, loss 0.0114657, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:24.004350: step 370, loss 0.0100929, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:24.199509: step 371, loss 0.00227242, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:24.397088: step 372, loss 0.004977, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:24.595002: step 373, loss 0.00440361, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:24.792275: step 374, loss 0.00918628, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:24.989066: step 375, loss 0.00492412, accuracy 0.996667, precision 1.0, recall 0.9945054945054945

Evaluation:
[[396  85]
 [ 98 171]]
2019-04-02T04:50:25.023719: step 375, loss 0.809653, accuracy 0.756, precision 0.8232848232848233, recall 0.8016194331983806

2019-04-02T04:50:25.221249: step 376, loss 0.00321336, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:25.416023: step 377, loss 0.00483204, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:25.612042: step 378, loss 0.00939439, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:25.811445: step 379, loss 0.0093547, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:26.006097: step 380, loss 0.00526102, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:26.202832: step 381, loss 0.00619134, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:26.397662: step 382, loss 0.00585163, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:26.593515: step 383, loss 0.00693563, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:26.791561: step 384, loss 0.00604343, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:26.986962: step 385, loss 0.00468847, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:27.184515: step 386, loss 0.00675221, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:27.383325: step 387, loss 0.00658227, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:27.579505: step 388, loss 0.00750161, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:27.776685: step 389, loss 0.00921804, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:27.971978: step 390, loss 0.00634878, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:28.168220: step 391, loss 0.00712862, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:28.364692: step 392, loss 0.00461507, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:28.562260: step 393, loss 0.00419267, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:28.757617: step 394, loss 0.00325649, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:28.954981: step 395, loss 0.00450776, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:29.150610: step 396, loss 0.00235068, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:29.346437: step 397, loss 0.00211739, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:29.542811: step 398, loss 0.00789309, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:29.738198: step 399, loss 0.00327294, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:29.933961: step 400, loss 0.00256447, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[402  79]
 [100 169]]
2019-04-02T04:50:29.968872: step 400, loss 0.827357, accuracy 0.761333, precision 0.8357588357588358, recall 0.8007968127490039

2019-04-02T04:50:30.163059: step 401, loss 0.00962029, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:30.361620: step 402, loss 0.00444483, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:30.557128: step 403, loss 0.00676295, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:30.755538: step 404, loss 0.0057415, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:30.950913: step 405, loss 0.00504767, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:31.147119: step 406, loss 0.00465179, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:31.345651: step 407, loss 0.00930012, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:31.542185: step 408, loss 0.00383344, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:31.740283: step 409, loss 0.00414456, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:31.937266: step 410, loss 0.00554898, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:32.134032: step 411, loss 0.0079511, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:32.328777: step 412, loss 0.00593444, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:32.527354: step 413, loss 0.00477147, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:32.722940: step 414, loss 0.00293814, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:32.920122: step 415, loss 0.00814922, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:33.118517: step 416, loss 0.00518379, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:33.317023: step 417, loss 0.00599254, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:33.513169: step 418, loss 0.00632172, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:33.708293: step 419, loss 0.00473179, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:33.903968: step 420, loss 0.00713391, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:34.097997: step 421, loss 0.00621156, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:34.292471: step 422, loss 0.00743956, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:34.488744: step 423, loss 0.00400806, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:34.685891: step 424, loss 0.0136056, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:34.881140: step 425, loss 0.00518178, accuracy 0.996667, precision 0.994475138121547, recall 1.0

Evaluation:
[[393  88]
 [ 95 174]]
2019-04-02T04:50:34.916260: step 425, loss 0.823699, accuracy 0.756, precision 0.817047817047817, recall 0.805327868852459

2019-04-02T04:50:35.116028: step 426, loss 0.00363299, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:35.311382: step 427, loss 0.00597969, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:35.506389: step 428, loss 0.00895202, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:35.702842: step 429, loss 0.00493901, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:35.898795: step 430, loss 0.00803737, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:36.097658: step 431, loss 0.0031952, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:36.294692: step 432, loss 0.00561473, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:36.491649: step 433, loss 0.00492515, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:36.691322: step 434, loss 0.00961345, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:36.888183: step 435, loss 0.00275311, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:37.087057: step 436, loss 0.00702286, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:37.283392: step 437, loss 0.00628742, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:37.478267: step 438, loss 0.0115828, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:37.675341: step 439, loss 0.0072987, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:37.872235: step 440, loss 0.00710525, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:38.067113: step 441, loss 0.00718638, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:38.263211: step 442, loss 0.0116816, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:38.458431: step 443, loss 0.00601354, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:38.654255: step 444, loss 0.00276608, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:38.849471: step 445, loss 0.00203175, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:39.047518: step 446, loss 0.00929877, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:39.242870: step 447, loss 0.00541079, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:39.440558: step 448, loss 0.00701772, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:39.636512: step 449, loss 0.00293892, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:39.832993: step 450, loss 0.00585148, accuracy 0.996667, precision 0.994475138121547, recall 1.0

Evaluation:
[[395  86]
 [ 99 170]]
2019-04-02T04:50:39.867964: step 450, loss 0.836133, accuracy 0.753333, precision 0.8212058212058212, recall 0.7995951417004049

2019-04-02T04:50:40.063837: step 451, loss 0.0045884, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:40.260346: step 452, loss 0.00381304, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:40.456190: step 453, loss 0.00496865, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:40.655372: step 454, loss 0.0111311, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:40.853174: step 455, loss 0.00322069, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:41.052705: step 456, loss 0.00383831, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:41.249616: step 457, loss 0.0108983, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:41.448363: step 458, loss 0.00243201, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:41.645431: step 459, loss 0.00613157, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:41.841620: step 460, loss 0.00553437, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:42.036835: step 461, loss 0.010502, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:42.233325: step 462, loss 0.00437099, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:42.429360: step 463, loss 0.00345936, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:42.625653: step 464, loss 0.0076078, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:42.820637: step 465, loss 0.0045213, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:43.018417: step 466, loss 0.00488661, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:43.215626: step 467, loss 0.0129813, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:43.413946: step 468, loss 0.00629484, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:43.610639: step 469, loss 0.00274757, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:43.805360: step 470, loss 0.00587202, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:44.004927: step 471, loss 0.00691635, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:44.201030: step 472, loss 0.00275767, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:44.396509: step 473, loss 0.00869788, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:44.594032: step 474, loss 0.0051102, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:44.791741: step 475, loss 0.00439035, accuracy 0.996667, precision 1.0, recall 0.9945054945054945

Evaluation:
[[395  86]
 [ 99 170]]
2019-04-02T04:50:44.826556: step 475, loss 0.839624, accuracy 0.753333, precision 0.8212058212058212, recall 0.7995951417004049

2019-04-02T04:50:45.023372: step 476, loss 0.00832378, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:45.219824: step 477, loss 0.00418303, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:45.418602: step 478, loss 0.00435805, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:45.616738: step 479, loss 0.00556975, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:45.812812: step 480, loss 0.00417555, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:46.009407: step 481, loss 0.00554773, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:46.204718: step 482, loss 0.00512438, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:46.403294: step 483, loss 0.0102883, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:46.599036: step 484, loss 0.00734465, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:46.795356: step 485, loss 0.00552227, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:46.992368: step 486, loss 0.00811269, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:47.188318: step 487, loss 0.0067793, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:47.384257: step 488, loss 0.00664897, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:47.579424: step 489, loss 0.00728323, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:47.778409: step 490, loss 0.0045333, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:47.972782: step 491, loss 0.00534124, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:48.172351: step 492, loss 0.00217038, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:48.368805: step 493, loss 0.00536779, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:48.566237: step 494, loss 0.0100738, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:48.762920: step 495, loss 0.0089313, accuracy 0.993333, precision 0.994475138121547, recall 0.994475138121547
2019-04-02T04:50:48.958410: step 496, loss 0.00350704, accuracy 1, precision 1.0, recall 1.0
2019-04-02T04:50:49.154135: step 497, loss 0.00471217, accuracy 0.996667, precision 0.994475138121547, recall 1.0
2019-04-02T04:50:49.349700: step 498, loss 0.00536667, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:49.545638: step 499, loss 0.00509717, accuracy 0.996667, precision 1.0, recall 0.9945054945054945
2019-04-02T04:50:49.743670: step 500, loss 0.00287745, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[396  85]
 [ 98 171]]
2019-04-02T04:50:49.779023: step 500, loss 0.847054, accuracy 0.756, precision 0.8232848232848233, recall 0.8016194331983806

Saved model checkpoint to /home/ubuntu/Project/runs/1554180548/checkpoints/model-500

