"C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\python.exe" "C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py"
Using TensorFlow backend.
GloVe model loaded
Pretrained Embedding: GloVe
Italian: False
Loading data...
11566
Max Document length: 36
WARNING:tensorflow:From C:/Users/aless/Documents/University of Illinois at Chicago/Spring 2019/Project/train.py:88: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\tensorflow\contrib\learn\python\learn\preprocessing\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
Dimension of the training set:  150
Vocabulary Size: 1
Train/Unlabeled/Dev split: 3000/7500/750
2019-04-03 10:19:51.890603: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
WARNING:tensorflow:From C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\main_pre_trained_embeddings.py:511: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See `tf.nn.softmax_cross_entropy_with_logits_v2`.

Writing to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1554304796

2019-04-03T10:20:02.388553: step 1, loss 2.70248, accuracy 0.52, precision 0.5272727272727272, recall 0.7435897435897436
2019-04-03T10:20:03.194873: step 2, loss 3.53515, accuracy 0.733333, precision 0.990909090909091, recall 0.7364864864864865
2019-04-03T10:20:04.060558: step 3, loss 3.29215, accuracy 0.726667, precision 0.9727272727272728, recall 0.7379310344827587
2019-04-03T10:20:04.932258: step 4, loss 1.9833, accuracy 0.673333, precision 0.8454545454545455, recall 0.744
2019-04-03T10:20:05.836839: step 5, loss 2.3232, accuracy 0.493333, precision 0.43636363636363634, recall 0.7741935483870968
2019-04-03T10:20:06.654651: step 6, loss 1.9502, accuracy 0.586667, precision 0.5181818181818182, recall 0.8636363636363636
2019-04-03T10:20:07.428582: step 7, loss 1.58505, accuracy 0.666667, precision 0.7, recall 0.8191489361702128
2019-04-03T10:20:08.313216: step 8, loss 1.53967, accuracy 0.74, precision 0.9272727272727272, recall 0.7669172932330827
2019-04-03T10:20:08.887680: step 9, loss 1.60403, accuracy 0.74, precision 0.9090909090909091, recall 0.7751937984496124
2019-04-03T10:20:09.605760: step 10, loss 1.58322, accuracy 0.76, precision 0.9545454545454546, recall 0.7720588235294118
2019-04-03T10:20:10.316857: step 11, loss 1.44559, accuracy 0.78, precision 0.9545454545454546, recall 0.7894736842105263
2019-04-03T10:20:11.038927: step 12, loss 0.971219, accuracy 0.813333, precision 0.9363636363636364, recall 0.8306451612903226
2019-04-03T10:20:11.755043: step 13, loss 1.155, accuracy 0.726667, precision 0.7818181818181819, recall 0.8349514563106796
2019-04-03T10:20:12.495574: step 14, loss 1.16162, accuracy 0.74, precision 0.7363636363636363, recall 0.8901098901098901
2019-04-03T10:20:13.205673: step 15, loss 0.841755, accuracy 0.76, precision 0.7454545454545455, recall 0.9111111111111111
2019-04-03T10:20:13.949683: step 16, loss 0.637428, accuracy 0.8, precision 0.8181818181818182, recall 0.9
2019-04-03T10:20:14.512179: step 17, loss 0.725842, accuracy 0.76, precision 0.7727272727272727, recall 0.8854166666666666
2019-04-03T10:20:15.208317: step 18, loss 0.531639, accuracy 0.86, precision 0.9272727272727272, recall 0.8869565217391304
2019-04-03T10:20:15.890492: step 19, loss 0.771383, accuracy 0.833333, precision 0.9454545454545454, recall 0.8455284552845529
2019-04-03T10:20:16.601592: step 20, loss 0.840595, accuracy 0.84, precision 0.9363636363636364, recall 0.8583333333333333
2019-04-03T10:20:17.308701: step 21, loss 0.686979, accuracy 0.846667, precision 0.9454545454545454, recall 0.859504132231405
2019-04-03T10:20:18.113548: step 22, loss 0.438068, accuracy 0.873333, precision 0.9181818181818182, recall 0.9099099099099099
2019-04-03T10:20:18.882998: step 23, loss 0.29572, accuracy 0.9, precision 0.9636363636363636, recall 0.905982905982906
2019-04-03T10:20:19.393023: step 24, loss 0.302762, accuracy 0.893333, precision 0.9545454545454546, recall 0.9051724137931034
2019-04-03T10:20:20.014362: step 25, loss 0.478853, accuracy 0.886667, precision 0.8909090909090909, recall 0.9514563106796117

Evaluation:
[[428  42]
 [177 103]]
2019-04-03T10:20:20.884036: step 25, loss 0.703636, accuracy 0.708, precision 0.9106382978723404, recall 0.7074380165289256

2019-04-03T10:20:21.569204: step 26, loss 0.211693, accuracy 0.94, precision 0.9545454545454546, recall 0.963302752293578
2019-04-03T10:20:22.276313: step 27, loss 0.385204, accuracy 0.9, precision 0.9181818181818182, recall 0.9439252336448598
2019-04-03T10:20:22.951507: step 28, loss 0.293079, accuracy 0.906667, precision 0.9181818181818182, recall 0.9528301886792453
2019-04-03T10:20:23.647645: step 29, loss 0.263068, accuracy 0.926667, precision 0.9272727272727272, recall 0.9714285714285714
2019-04-03T10:20:24.162650: step 30, loss 0.183252, accuracy 0.953333, precision 0.9636363636363636, recall 0.9724770642201835
2019-04-03T10:20:24.817897: step 31, loss 0.178511, accuracy 0.94, precision 0.9818181818181818, recall 0.9391304347826087
2019-04-03T10:20:25.400340: step 32, loss 0.130067, accuracy 0.946667, precision 0.990909090909091, recall 0.9396551724137931
2019-04-03T10:20:26.025692: step 33, loss 0.187583, accuracy 0.926667, precision 0.9454545454545454, recall 0.9541284403669725
2019-04-03T10:20:26.633067: step 34, loss 0.118442, accuracy 0.966667, precision 0.9727272727272728, recall 0.981651376146789
2019-04-03T10:20:27.289312: step 35, loss 0.0817742, accuracy 0.96, precision 0.990909090909091, recall 0.956140350877193
2019-04-03T10:20:27.950544: step 36, loss 0.0904337, accuracy 0.96, precision 0.9818181818181818, recall 0.9642857142857143
2019-04-03T10:20:28.661607: step 37, loss 0.0734025, accuracy 0.973333, precision 0.990909090909091, recall 0.9732142857142857
2019-04-03T10:20:29.115580: step 38, loss 0.08323, accuracy 0.973333, precision 0.9818181818181818, recall 0.9818181818181818
2019-04-03T10:20:29.780800: step 39, loss 0.0671421, accuracy 0.966667, precision 0.9818181818181818, recall 0.972972972972973
2019-04-03T10:20:30.432059: step 40, loss 0.0703452, accuracy 0.966667, precision 0.9818181818181818, recall 0.972972972972973
2019-04-03T10:20:31.091322: step 41, loss 0.0334507, accuracy 0.986667, precision 0.990909090909091, recall 0.990909090909091
2019-04-03T10:20:31.709701: step 42, loss 0.104572, accuracy 0.953333, precision 0.9727272727272728, recall 0.963963963963964
2019-04-03T10:20:32.400851: step 43, loss 0.0607588, accuracy 0.98, precision 0.9818181818181818, recall 0.9908256880733946
2019-04-03T10:20:33.114943: step 44, loss 0.114047, accuracy 0.966667, precision 0.9818181818181818, recall 0.972972972972973
2019-04-03T10:20:33.778170: step 45, loss 0.120954, accuracy 0.966667, precision 0.990909090909091, recall 0.9646017699115044
2019-04-03T10:20:34.441606: step 46, loss 0.0997494, accuracy 0.966667, precision 0.990909090909091, recall 0.9646017699115044
2019-04-03T10:20:35.113808: step 47, loss 0.0741159, accuracy 0.96, precision 0.9545454545454546, recall 0.9905660377358491
2019-04-03T10:20:35.814933: step 48, loss 0.0330443, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:20:36.497109: step 49, loss 0.0764577, accuracy 0.966667, precision 0.9727272727272728, recall 0.981651376146789
2019-04-03T10:20:37.148368: step 50, loss 0.135649, accuracy 0.94, precision 0.9636363636363636, recall 0.954954954954955

Evaluation:
[[453  17]
 [223  57]]
2019-04-03T10:20:37.643044: step 50, loss 0.990979, accuracy 0.68, precision 0.9638297872340426, recall 0.6701183431952663

2019-04-03T10:20:38.339183: step 51, loss 0.0291522, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:20:39.056181: step 52, loss 0.0296124, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:20:39.688001: step 53, loss 0.0734393, accuracy 0.966667, precision 0.990909090909091, recall 0.9646017699115044
2019-04-03T10:20:40.322367: step 54, loss 0.0996078, accuracy 0.973333, precision 0.9818181818181818, recall 0.9818181818181818
2019-04-03T10:20:41.008532: step 55, loss 0.0529137, accuracy 0.98, precision 0.9818181818181818, recall 0.9908256880733946
2019-04-03T10:20:41.671758: step 56, loss 0.0353437, accuracy 0.98, precision 0.9727272727272728, recall 1.0
2019-04-03T10:20:42.370890: step 57, loss 0.0208014, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:20:43.085491: step 58, loss 0.0226355, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:20:43.837479: step 59, loss 0.0576942, accuracy 0.98, precision 0.9818181818181818, recall 0.9908256880733946
2019-04-03T10:20:44.390001: step 60, loss 0.0223161, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:20:45.042257: step 61, loss 0.0176426, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:20:45.702490: step 62, loss 0.0223347, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:20:46.378683: step 63, loss 0.0149932, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:20:47.127679: step 64, loss 0.0294497, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:20:47.795892: step 65, loss 0.0155961, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:20:48.481061: step 66, loss 0.0370833, accuracy 0.973333, precision 0.990909090909091, recall 0.9732142857142857
2019-04-03T10:20:49.175588: step 67, loss 0.0539821, accuracy 0.98, precision 0.990909090909091, recall 0.9819819819819819
2019-04-03T10:20:49.826177: step 68, loss 0.0828759, accuracy 0.98, precision 0.9818181818181818, recall 0.9908256880733946
2019-04-03T10:20:50.774640: step 69, loss 0.0080249, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:20:51.705155: step 70, loss 0.00456672, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:20:52.671080: step 71, loss 0.0487738, accuracy 0.98, precision 1.0, recall 0.9734513274336283
2019-04-03T10:20:53.554717: step 72, loss 0.00529637, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:20:54.577535: step 73, loss 0.0261867, accuracy 0.98, precision 0.9727272727272728, recall 1.0
2019-04-03T10:20:55.539961: step 74, loss 0.0116656, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:20:56.495406: step 75, loss 0.0307993, accuracy 0.993333, precision 1.0, recall 0.990990990990991

Evaluation:
[[443  27]
 [202  78]]
2019-04-03T10:20:56.893341: step 75, loss 0.95185, accuracy 0.694667, precision 0.9425531914893617, recall 0.6868217054263566

2019-04-03T10:20:57.785953: step 76, loss 0.0226154, accuracy 0.986667, precision 0.990909090909091, recall 0.990909090909091
2019-04-03T10:20:58.634684: step 77, loss 0.0128179, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:20:59.492899: step 78, loss 0.0133599, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:00.404971: step 79, loss 0.0202568, accuracy 0.986667, precision 0.9818181818181818, recall 1.0
2019-04-03T10:21:01.246720: step 80, loss 0.019785, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:02.141838: step 81, loss 0.00716108, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:03.112245: step 82, loss 0.028224, accuracy 0.986667, precision 1.0, recall 0.9821428571428571
2019-04-03T10:21:03.676731: step 83, loss 0.00718572, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:04.481579: step 84, loss 0.00641284, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:05.306373: step 85, loss 0.0256554, accuracy 0.986667, precision 0.990909090909091, recall 0.990909090909091
2019-04-03T10:21:06.070330: step 86, loss 0.0145619, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:06.881162: step 87, loss 0.0216146, accuracy 0.986667, precision 0.990909090909091, recall 0.990909090909091
2019-04-03T10:21:07.666065: step 88, loss 0.0172593, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:08.446975: step 89, loss 0.00742678, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:09.165056: step 90, loss 0.00550794, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:09.799358: step 91, loss 0.0261426, accuracy 0.986667, precision 0.9818181818181818, recall 1.0
2019-04-03T10:21:10.557330: step 92, loss 0.0131211, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:11.298348: step 93, loss 0.00642121, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:11.977532: step 94, loss 0.0770423, accuracy 0.98, precision 0.990909090909091, recall 0.9819819819819819
2019-04-03T10:21:12.647740: step 95, loss 0.0133551, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:13.421672: step 96, loss 0.00662054, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:14.097862: step 97, loss 0.0314447, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:14.778971: step 98, loss 0.00912279, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:15.486079: step 99, loss 0.0115949, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:16.200170: step 100, loss 0.0125901, accuracy 0.993333, precision 1.0, recall 0.990990990990991

Evaluation:
[[449  21]
 [221  59]]
2019-04-03T10:21:16.682389: step 100, loss 1.16441, accuracy 0.677333, precision 0.9553191489361702, recall 0.6701492537313433

2019-04-03T10:21:17.374538: step 101, loss 0.00931133, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:18.067685: step 102, loss 0.00530519, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:18.785764: step 103, loss 0.00605409, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:19.498856: step 104, loss 0.00951996, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:20.202974: step 105, loss 0.00663935, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:20.725269: step 106, loss 0.00769284, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:21.352592: step 107, loss 0.00550715, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:21.991393: step 108, loss 0.0212872, accuracy 0.986667, precision 0.9818181818181818, recall 1.0
2019-04-03T10:21:22.642650: step 109, loss 0.00691261, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:23.323829: step 110, loss 0.00648614, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:24.038916: step 111, loss 0.00472945, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:24.667237: step 112, loss 0.0197438, accuracy 0.986667, precision 0.9818181818181818, recall 1.0
2019-04-03T10:21:25.319492: step 113, loss 0.00212975, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:25.796398: step 114, loss 0.0125813, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:26.414744: step 115, loss 0.00492969, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:27.049047: step 116, loss 0.0173556, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:27.628498: step 117, loss 0.0122413, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:28.255820: step 118, loss 0.00430424, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:28.890125: step 119, loss 0.00492524, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:29.529415: step 120, loss 0.00312565, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:30.163717: step 121, loss 0.0232692, accuracy 0.986667, precision 0.990909090909091, recall 0.990909090909091
2019-04-03T10:21:30.902741: step 122, loss 0.00401079, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:31.391280: step 123, loss 0.00328509, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:32.014614: step 124, loss 0.0224222, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:32.618508: step 125, loss 0.0142741, accuracy 0.993333, precision 1.0, recall 0.990990990990991

Evaluation:
[[452  18]
 [230  50]]
2019-04-03T10:21:33.086257: step 125, loss 1.219, accuracy 0.669333, precision 0.9617021276595744, recall 0.6627565982404692

2019-04-03T10:21:33.712581: step 126, loss 0.0213628, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:34.368827: step 127, loss 0.047467, accuracy 0.986667, precision 0.990909090909091, recall 0.990909090909091
2019-04-03T10:21:35.013104: step 128, loss 0.00224226, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:35.646410: step 129, loss 0.00865651, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:36.283706: step 130, loss 0.00671379, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:36.786571: step 131, loss 0.0133384, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:37.420374: step 132, loss 0.00918308, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:38.050688: step 133, loss 0.00667098, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:38.684992: step 134, loss 0.00802541, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:39.382128: step 135, loss 0.00678655, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:40.017429: step 136, loss 0.00406952, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:40.695616: step 137, loss 0.00917019, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:41.401727: step 138, loss 0.0118611, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:42.095497: step 139, loss 0.0120528, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:42.622088: step 140, loss 0.011854, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:43.235954: step 141, loss 0.00974885, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:43.903170: step 142, loss 0.00225338, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:44.675614: step 143, loss 0.00250452, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:45.320888: step 144, loss 0.0078178, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:45.868424: step 145, loss 0.00409264, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:46.485773: step 146, loss 0.00845804, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:47.052258: step 147, loss 0.0137949, accuracy 0.986667, precision 1.0, recall 0.9821428571428571
2019-04-03T10:21:47.745405: step 148, loss 0.00298149, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:48.277314: step 149, loss 0.00541908, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:48.962481: step 150, loss 0.00612286, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[455  15]
 [229  51]]
2019-04-03T10:21:49.417265: step 150, loss 1.28394, accuracy 0.674667, precision 0.9680851063829787, recall 0.6652046783625731

2019-04-03T10:21:50.007686: step 151, loss 0.0097137, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:21:50.616059: step 152, loss 0.00375453, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:51.323167: step 153, loss 0.00249383, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:52.009844: step 154, loss 0.0031345, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:52.638164: step 155, loss 0.00717596, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:21:53.400636: step 156, loss 0.00268689, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:53.953667: step 157, loss 0.0137715, accuracy 0.986667, precision 0.990909090909091, recall 0.990909090909091
2019-04-03T10:21:54.622877: step 158, loss 0.00208881, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:55.283111: step 159, loss 0.00103281, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:55.956310: step 160, loss 0.00169835, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:56.618539: step 161, loss 0.00114684, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:57.287262: step 162, loss 0.00497588, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:57.995368: step 163, loss 0.0081135, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:58.624685: step 164, loss 0.000778978, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:59.355729: step 165, loss 0.00247769, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:21:59.843425: step 166, loss 0.00937912, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:00.501665: step 167, loss 0.0325752, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:22:01.136965: step 168, loss 0.00312302, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:01.819142: step 169, loss 0.00578381, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:02.560160: step 170, loss 0.00582541, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:03.301179: step 171, loss 0.00081838, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:04.035216: step 172, loss 0.00245197, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:04.711408: step 173, loss 0.00146486, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:05.395118: step 174, loss 0.0107503, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:22:05.895785: step 175, loss 0.0041533, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[444  26]
 [200  80]]
2019-04-03T10:22:06.312669: step 175, loss 1.0275, accuracy 0.698667, precision 0.9446808510638298, recall 0.6894409937888198

2019-04-03T10:22:06.947970: step 176, loss 0.00315281, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:07.618178: step 177, loss 0.00606423, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:08.277414: step 178, loss 0.00774759, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:22:08.915708: step 179, loss 0.00285026, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:09.580931: step 180, loss 0.00837334, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:22:10.261620: step 181, loss 0.00365199, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:10.914872: step 182, loss 0.00228275, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:11.572041: step 183, loss 0.00173439, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:12.265188: step 184, loss 0.00153833, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:12.990249: step 185, loss 0.00135624, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:13.719299: step 186, loss 0.00223185, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:14.434390: step 187, loss 0.00147012, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:15.172413: step 188, loss 0.00906767, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:22:15.912944: step 189, loss 0.00261479, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:16.608085: step 190, loss 0.00238253, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:17.387998: step 191, loss 0.00168376, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:17.908272: step 192, loss 0.00244263, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:18.534597: step 193, loss 0.0111708, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:22:19.231733: step 194, loss 0.00661355, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:19.874015: step 195, loss 0.00267954, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:20.568159: step 196, loss 0.00611961, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:22:21.268287: step 197, loss 0.00375296, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:21.956448: step 198, loss 0.00247185, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:22.660592: step 199, loss 0.00977258, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:22:23.281929: step 200, loss 0.0014182, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[454  16]
 [225  55]]
2019-04-03T10:22:23.598084: step 200, loss 1.24798, accuracy 0.678667, precision 0.9659574468085106, recall 0.6686303387334315

2019-04-03T10:22:24.215459: step 201, loss 0.00247193, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:24.869710: step 202, loss 0.0030521, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:25.519972: step 203, loss 0.0185292, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:22:26.167239: step 204, loss 0.00189998, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:26.849943: step 205, loss 0.00142475, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:27.582965: step 206, loss 0.00243188, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:28.232227: step 207, loss 0.00369728, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:28.897448: step 208, loss 0.00205405, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:29.411286: step 209, loss 0.00320161, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:30.079500: step 210, loss 0.0010753, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:30.739734: step 211, loss 0.0106159, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:31.366058: step 212, loss 0.000526206, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:32.005349: step 213, loss 0.00258745, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:32.640650: step 214, loss 0.00284964, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:33.276950: step 215, loss 0.00265351, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:33.943168: step 216, loss 0.002356, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:34.665122: step 217, loss 0.00364728, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:35.197849: step 218, loss 0.00115368, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:35.819187: step 219, loss 0.00267981, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:36.464461: step 220, loss 0.0077479, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:37.070839: step 221, loss 0.00347942, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:37.748537: step 222, loss 0.00298003, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:38.381846: step 223, loss 0.00132858, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:39.048121: step 224, loss 0.00067871, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:39.752239: step 225, loss 0.000816691, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[448  22]
 [209  71]]
2019-04-03T10:22:40.216994: step 225, loss 1.14118, accuracy 0.692, precision 0.9531914893617022, recall 0.6818873668188736

2019-04-03T10:22:40.759543: step 226, loss 0.0016533, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:41.374898: step 227, loss 0.00498823, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:42.000225: step 228, loss 0.00105824, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:42.635551: step 229, loss 0.00040165, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:43.347675: step 230, loss 0.000652623, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:44.003920: step 231, loss 0.000630294, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:44.690087: step 232, loss 0.000968176, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:45.359295: step 233, loss 0.00178523, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:46.108292: step 234, loss 0.00778809, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:22:46.634884: step 235, loss 0.011468, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:22:47.245251: step 236, loss 0.00110651, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:47.867586: step 237, loss 0.0112044, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:22:48.478952: step 238, loss 0.00228087, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:49.103282: step 239, loss 0.00384279, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:49.726616: step 240, loss 0.00206922, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:50.348461: step 241, loss 0.00340661, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:51.052578: step 242, loss 0.00127847, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:51.808392: step 243, loss 0.00351424, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:52.286114: step 244, loss 0.00079623, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:52.973277: step 245, loss 0.004524, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:53.594614: step 246, loss 0.0033418, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:54.353586: step 247, loss 0.00110943, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:55.045735: step 248, loss 0.00211798, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:55.658097: step 249, loss 0.00127189, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:56.335793: step 250, loss 0.000437746, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[453  17]
 [225  55]]
2019-04-03T10:22:56.820497: step 250, loss 1.3616, accuracy 0.677333, precision 0.9638297872340426, recall 0.668141592920354

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1554304796\checkpoints\model-250

2019-04-03T10:22:58.133934: step 251, loss 0.00136869, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:58.771230: step 252, loss 0.00224256, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:22:59.462382: step 253, loss 0.00123045, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:00.135611: step 254, loss 0.000740875, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:00.818784: step 255, loss 0.000662468, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:01.526892: step 256, loss 0.00359773, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:02.489318: step 257, loss 0.025012, accuracy 0.986667, precision 0.990909090909091, recall 0.990909090909091
2019-04-03T10:23:03.518081: step 258, loss 0.00105695, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:04.323924: step 259, loss 0.00112557, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:05.290341: step 260, loss 0.000927174, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:06.216863: step 261, loss 0.00732242, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:23:07.206217: step 262, loss 0.000757613, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:08.108803: step 263, loss 0.000464293, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:09.105148: step 264, loss 0.00130403, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:10.048616: step 265, loss 0.00124045, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:10.867426: step 266, loss 0.0076031, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:23:11.943548: step 267, loss 0.000731439, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:12.589819: step 268, loss 0.00573919, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:13.393729: step 269, loss 0.0019488, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:14.213536: step 270, loss 0.00250873, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:15.024370: step 271, loss 0.000520734, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:15.794310: step 272, loss 0.000587983, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:16.556272: step 273, loss 0.00042482, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:17.335699: step 274, loss 0.000765958, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:18.061757: step 275, loss 0.00114782, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[448  22]
 [208  72]]
2019-04-03T10:23:18.615277: step 275, loss 1.14939, accuracy 0.693333, precision 0.9531914893617022, recall 0.6829268292682927

2019-04-03T10:23:19.225644: step 276, loss 0.00233956, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:19.935745: step 277, loss 0.00337521, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:20.707680: step 278, loss 0.00763241, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:23:21.471637: step 279, loss 0.00168078, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:22.194704: step 280, loss 0.00115149, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:22.955671: step 281, loss 0.00150264, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:23.693696: step 282, loss 0.00112039, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:24.412798: step 283, loss 0.0019738, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:25.022168: step 284, loss 0.000921007, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:25.642509: step 285, loss 0.00244733, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:26.319698: step 286, loss 0.00186572, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:27.006889: step 287, loss 0.00110809, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:27.656152: step 288, loss 0.000535012, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:28.293475: step 289, loss 0.000681181, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:28.930771: step 290, loss 0.00176687, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:29.595991: step 291, loss 0.00255177, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:30.290136: step 292, loss 0.00210405, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:30.865704: step 293, loss 0.00287263, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:31.466098: step 294, loss 0.00362406, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:32.043554: step 295, loss 0.00162222, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:32.688828: step 296, loss 0.00445827, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:33.319649: step 297, loss 0.00223283, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:33.947969: step 298, loss 0.00160228, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:34.575323: step 299, loss 0.00273178, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:35.231569: step 300, loss 0.00141216, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[450  20]
 [221  59]]
2019-04-03T10:23:35.697323: step 300, loss 1.30624, accuracy 0.678667, precision 0.9574468085106383, recall 0.6706408345752608

2019-04-03T10:23:36.457797: step 301, loss 0.00129181, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:36.943465: step 302, loss 0.00146187, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:37.562809: step 303, loss 0.001447, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:38.183150: step 304, loss 0.000707839, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:38.865325: step 305, loss 0.00137407, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:39.544509: step 306, loss 0.00186208, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:40.280563: step 307, loss 0.000855384, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:40.902900: step 308, loss 0.00112715, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:41.521273: step 309, loss 0.00116162, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:42.172851: step 310, loss 0.00240715, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:42.707422: step 311, loss 0.00173194, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:43.320292: step 312, loss 0.000846335, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:43.950606: step 313, loss 0.000391183, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:44.614830: step 314, loss 0.00629267, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:23:45.335902: step 315, loss 0.000816092, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:45.983172: step 316, loss 0.00060525, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:46.572596: step 317, loss 0.000310872, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:47.187950: step 318, loss 0.000637291, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:47.826590: step 319, loss 0.000754187, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:48.271908: step 320, loss 0.00744016, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:23:48.957075: step 321, loss 0.00389357, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:49.611324: step 322, loss 0.000304665, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:50.246625: step 323, loss 0.00127132, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:50.944759: step 324, loss 0.000626654, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:51.611486: step 325, loss 0.00058409, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[450  20]
 [218  62]]
2019-04-03T10:23:52.074759: step 325, loss 1.27725, accuracy 0.682667, precision 0.9574468085106383, recall 0.6736526946107785

2019-04-03T10:23:52.745964: step 326, loss 0.000334233, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:53.550812: step 327, loss 0.000700025, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:54.054469: step 328, loss 0.00139674, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:54.710716: step 329, loss 0.000376784, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:55.372984: step 330, loss 0.00092689, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:56.049176: step 331, loss 0.000441135, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:56.689492: step 332, loss 0.000849327, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:57.348729: step 333, loss 0.00116437, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:58.015945: step 334, loss 0.00122418, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:58.677177: step 335, loss 0.00103308, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:59.425749: step 336, loss 0.000801079, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:23:59.920426: step 337, loss 0.000580538, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:00.849941: step 338, loss 0.000392164, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:01.813874: step 339, loss 0.00229201, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:02.795249: step 340, loss 0.000254485, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:03.721771: step 341, loss 0.00057818, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:04.635328: step 342, loss 0.00186172, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:05.532521: step 343, loss 0.00142578, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:06.409177: step 344, loss 0.00231913, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:07.233322: step 345, loss 0.0039465, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:07.893568: step 346, loss 0.00121148, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:08.695423: step 347, loss 0.000592295, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:09.484314: step 348, loss 0.00426123, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:10.305119: step 349, loss 0.00198079, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:11.095006: step 350, loss 0.000625528, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[447  23]
 [210  70]]
2019-04-03T10:24:11.707883: step 350, loss 1.1892, accuracy 0.689333, precision 0.951063829787234, recall 0.680365296803653

2019-04-03T10:24:12.549141: step 351, loss 0.000925113, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:13.315125: step 352, loss 0.000704343, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:14.114016: step 353, loss 0.0011762, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:14.641605: step 354, loss 0.000156686, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:15.335748: step 355, loss 0.000346055, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:16.045849: step 356, loss 0.00137559, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:16.735007: step 357, loss 0.00148935, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:17.477022: step 358, loss 0.00090317, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:18.189117: step 359, loss 0.00179776, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:18.941618: step 360, loss 0.00148158, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:19.603847: step 361, loss 0.000831674, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:20.341873: step 362, loss 0.00286713, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:20.842533: step 363, loss 0.0015713, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:21.532688: step 364, loss 0.000713532, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:22.208879: step 365, loss 0.00039695, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:22.876604: step 366, loss 0.00389147, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:23.611638: step 367, loss 0.000577869, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:24.576569: step 368, loss 0.00522026, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:25.454220: step 369, loss 0.00144561, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:26.357804: step 370, loss 0.00183951, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:27.339181: step 371, loss 0.000675017, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:28.063243: step 372, loss 0.000393575, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:28.983293: step 373, loss 0.000499872, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:29.876902: step 374, loss 0.0216795, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:24:30.678267: step 375, loss 0.000524227, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[451  19]
 [219  61]]
2019-04-03T10:24:31.268689: step 375, loss 1.33555, accuracy 0.682667, precision 0.9595744680851064, recall 0.673134328358209

2019-04-03T10:24:32.097475: step 376, loss 0.000368317, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:32.950196: step 377, loss 0.000248631, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:33.747062: step 378, loss 0.000478717, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:34.589808: step 379, loss 0.00138489, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:35.115435: step 380, loss 0.00129692, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:35.839499: step 381, loss 0.000827835, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:36.504721: step 382, loss 0.0019181, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:37.215818: step 383, loss 0.00263835, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:37.924922: step 384, loss 0.00041902, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:38.680900: step 385, loss 0.00337392, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:39.400975: step 386, loss 0.000384607, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:40.226766: step 387, loss 0.000748354, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:41.096441: step 388, loss 0.00107611, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:41.612061: step 389, loss 0.000818714, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:42.276285: step 390, loss 0.00285735, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:42.974417: step 391, loss 0.00070115, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:43.674545: step 392, loss 0.000907263, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:44.398120: step 393, loss 0.00040654, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:45.110217: step 394, loss 0.00193481, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:45.863203: step 395, loss 0.00361269, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:46.713927: step 396, loss 0.00138993, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:47.409068: step 397, loss 0.00282292, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:48.293218: step 398, loss 0.000254616, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:48.819418: step 399, loss 0.000883076, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:49.519058: step 400, loss 0.000224885, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[448  22]
 [205  75]]
2019-04-03T10:24:50.027698: step 400, loss 1.19011, accuracy 0.697333, precision 0.9531914893617022, recall 0.6860643185298622

2019-04-03T10:24:50.932279: step 401, loss 0.000205221, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:52.043309: step 402, loss 0.00107914, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:53.260568: step 403, loss 0.000427515, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:54.372206: step 404, loss 0.00023313, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:55.319673: step 405, loss 0.000520752, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:56.488573: step 406, loss 0.000706886, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:57.349272: step 407, loss 0.000805528, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:58.493213: step 408, loss 0.0011401, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:24:59.542406: step 409, loss 0.00267924, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:00.460950: step 410, loss 0.00630241, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:25:01.321161: step 411, loss 0.000345134, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:02.200807: step 412, loss 0.00140818, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:03.188724: step 413, loss 0.000458049, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:04.124730: step 414, loss 0.000678043, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:04.966479: step 415, loss 0.0140954, accuracy 0.993333, precision 0.990909090909091, recall 1.0
2019-04-03T10:25:05.803242: step 416, loss 0.000644134, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:06.633022: step 417, loss 0.00341013, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:07.459810: step 418, loss 0.000299568, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:08.274632: step 419, loss 0.000590346, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:09.041581: step 420, loss 0.000305758, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:09.901281: step 421, loss 0.0016979, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:10.673217: step 422, loss 0.000228671, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:11.779259: step 423, loss 0.00227728, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:12.783573: step 424, loss 0.00310185, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:13.599681: step 425, loss 0.000411843, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[449  21]
 [211  69]]
2019-04-03T10:25:14.099345: step 425, loss 1.26007, accuracy 0.690667, precision 0.9553191489361702, recall 0.6803030303030303

2019-04-03T10:25:14.941098: step 426, loss 0.00991266, accuracy 0.993333, precision 1.0, recall 0.990990990990991
2019-04-03T10:25:15.704052: step 427, loss 0.000209726, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:16.484964: step 428, loss 0.00168768, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:17.279839: step 429, loss 0.000156482, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:18.032825: step 430, loss 0.000494393, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:18.848166: step 431, loss 0.000479454, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:19.574224: step 432, loss 0.000177531, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:20.450879: step 433, loss 0.00364395, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:21.053268: step 434, loss 0.000201542, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:21.793290: step 435, loss 0.000633232, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:22.586169: step 436, loss 0.000939305, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:23.327693: step 437, loss 0.000368623, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:24.039790: step 438, loss 0.000534308, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:24.824691: step 439, loss 0.00282052, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:25.654011: step 440, loss 0.000350416, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:26.448886: step 441, loss 0.000221206, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:27.258721: step 442, loss 0.000638406, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:28.004235: step 443, loss 0.00223652, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:28.876901: step 444, loss 0.000436939, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:29.647839: step 445, loss 0.00227049, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:30.321039: step 446, loss 0.00116411, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:31.143839: step 447, loss 0.000316761, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:31.826014: step 448, loss 0.000261199, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:32.711646: step 449, loss 0.000388586, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:33.460643: step 450, loss 0.000302353, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[443  27]
 [196  84]]
2019-04-03T10:25:33.916423: step 450, loss 1.11262, accuracy 0.702667, precision 0.9425531914893617, recall 0.6932707355242567

2019-04-03T10:25:34.656446: step 451, loss 0.000257899, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:35.579975: step 452, loss 0.00426405, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:36.250182: step 453, loss 0.00150546, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:36.942337: step 454, loss 0.000256978, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:37.706288: step 455, loss 0.000530974, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:38.406416: step 456, loss 0.000960177, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:39.129482: step 457, loss 0.0006342, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:39.853547: step 458, loss 0.000459492, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:40.558175: step 459, loss 0.000531101, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:41.310162: step 460, loss 0.000676241, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:41.945463: step 461, loss 0.000270577, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:42.795190: step 462, loss 0.00127082, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:43.608526: step 463, loss 0.000226705, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:44.381460: step 464, loss 0.00042806, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:45.213235: step 465, loss 0.00110789, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:45.926328: step 466, loss 0.00037631, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:46.729209: step 467, loss 0.00162362, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:47.625812: step 468, loss 0.000769564, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:48.220220: step 469, loss 0.000259381, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:48.837420: step 470, loss 0.000808582, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:49.541536: step 471, loss 0.00119091, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:50.197781: step 472, loss 0.000336711, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:50.909387: step 473, loss 0.000347889, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:51.567628: step 474, loss 0.000254111, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:52.262767: step 475, loss 0.000726976, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[450  20]
 [214  66]]
2019-04-03T10:25:52.766421: step 475, loss 1.29497, accuracy 0.688, precision 0.9574468085106383, recall 0.677710843373494

2019-04-03T10:25:53.423663: step 476, loss 0.000907143, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:54.238483: step 477, loss 0.00035264, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:54.758094: step 478, loss 0.000294441, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:55.433288: step 479, loss 0.000237281, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:56.115464: step 480, loss 0.000618454, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:56.868450: step 481, loss 0.000646619, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:57.569575: step 482, loss 0.00110734, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:58.286656: step 483, loss 0.000263646, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:58.989794: step 484, loss 0.000339996, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:25:59.667979: step 485, loss 0.00378879, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:00.490778: step 486, loss 0.00069797, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:01.067430: step 487, loss 0.000607301, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:01.738635: step 488, loss 0.00184697, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:02.396875: step 489, loss 0.000962365, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:03.189755: step 490, loss 8.29881e-05, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:03.969670: step 491, loss 0.000333854, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:04.718666: step 492, loss 0.000373936, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:05.456694: step 493, loss 0.000172191, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:06.151347: step 494, loss 0.000431075, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:06.969159: step 495, loss 0.000324092, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:07.623409: step 496, loss 0.000785858, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:08.329521: step 497, loss 0.000256757, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:08.961339: step 498, loss 0.000322565, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:09.733275: step 499, loss 0.000176661, accuracy 1, precision 1.0, recall 1.0
2019-04-03T10:26:10.478283: step 500, loss 0.000220519, accuracy 1, precision 1.0, recall 1.0

Evaluation:
[[448  22]
 [206  74]]
2019-04-03T10:26:11.015845: step 500, loss 1.2346, accuracy 0.696, precision 0.9531914893617022, recall 0.6850152905198776

Saved model checkpoint to C:\Users\aless\Documents\University of Illinois at Chicago\Spring 2019\Project\runs\1554304796\checkpoints\model-500


Process finished with exit code 0
