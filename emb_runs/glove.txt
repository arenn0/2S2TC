Loading data...
{'sympathy_and_emotional_support': 0, 'not_related_or_irrelevant': 1, 'infrastructure_and_utilities_damage': 2, 'other_useful_information': 3, 'injured_or_dead_people': 4, 'caution_and_advice': 5, 'displaced_people_and_evacuations': 6, 'donation_needs_or_offers_or_volunteering_services': 7, 'missing_trapped_or_found_people': 8}
Max Document length: 2407
Vocabulary Size: 1
Train/Dev split: 9226/1025
Writing to /home/ubuntu/Project/runs/1550605378

2019-02-19T19:43:00.992332: step 1, loss 11.988, accuracy 0.0625, precision [0.0, 0.25, nan, 0.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.125, nan, 0.0, 0.0, nan, 0.0, nan, nan]
2019-02-19T19:43:01.171719: step 2, loss 6.70296, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.4166666666666667, 0.0, nan, 0.0, nan, nan]
2019-02-19T19:43:01.326731: step 3, loss 10.8083, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.1, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:43:01.481228: step 4, loss 9.19841, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, nan, 1.0, nan], recall [0.0, nan, 0.0, 0.0, 0.25, nan, nan, 0.16666666666666666, nan]
2019-02-19T19:43:01.635361: step 5, loss 7.40704, accuracy 0.125, precision [0.5, 0.0, 0.0, 0.25, 0.0, nan, nan, 0.0, nan], recall [0.5, nan, 0.0, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:01.791018: step 6, loss 6.71634, accuracy 0.125, precision [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, nan], recall [1.0, nan, 0.0, 0.0, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:43:01.940837: step 7, loss 5.83113, accuracy 0, precision [0.0, 0.0, 0.0, 0.0, nan, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, nan, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:43:02.096262: step 8, loss 4.35533, accuracy 0.25, precision [0.0, nan, 0.75, 0.2, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.6, 0.3333333333333333, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:43:02.251892: step 9, loss 8.92563, accuracy 0.0625, precision [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 0.1111111111111111, 0.0, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:43:02.405805: step 10, loss 4.67967, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.5, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5714285714285714, 1.0, 0.0, nan, nan, nan]
2019-02-19T19:43:02.559276: step 11, loss 5.69805, accuracy 0.25, precision [0.5, 1.0, nan, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.4, 0.0, 0.16666666666666666, nan, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:02.711855: step 12, loss 4.54375, accuracy 0.5, precision [0.0, 0.6666666666666666, nan, 0.8571428571428571, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.5, nan, 0.75, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:43:02.865779: step 13, loss 6.79332, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:03.021499: step 14, loss 4.80106, accuracy 0.25, precision [0.0, 0.0, nan, 0.5714285714285714, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:03.176746: step 15, loss 6.75275, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.3333333333333333, 0.25, nan, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:03.331066: step 16, loss 4.29153, accuracy 0.375, precision [0.0, 1.0, nan, 0.3333333333333333, 0.4444444444444444, nan, nan, 0.0, nan], recall [0.0, 0.5, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:03.490126: step 17, loss 6.07959, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.4, nan, nan, 0.0, 0.2, nan], recall [nan, nan, nan, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, nan]
2019-02-19T19:43:03.643469: step 18, loss 6.1883, accuracy 0.25, precision [0.0, nan, 0.0, 0.16666666666666666, 0.75, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.2, 0.6, nan, 0.0, 0.0, nan]
2019-02-19T19:43:03.799957: step 19, loss 7.3637, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.2857142857142857, 0.3333333333333333, nan, nan, nan, 0.0], recall [0.0, nan, nan, 0.4, 0.25, nan, 0.0, 0.0, nan]
2019-02-19T19:43:03.952230: step 20, loss 8.00662, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.2857142857142857, nan, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:43:04.106409: step 21, loss 4.43756, accuracy 0.25, precision [nan, 0.0, 0.0, 0.5, 0.5, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.5, 0.16666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:43:04.263841: step 22, loss 7.0485, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.6666666666666666, nan, 0.0, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.2, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:04.419549: step 23, loss 6.69932, accuracy 0.25, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, 0.0], recall [0.0, nan, 0.0, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:04.573973: step 24, loss 7.5443, accuracy 0.0625, precision [nan, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.1111111111111111, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:04.732446: step 25, loss 5.95731, accuracy 0.25, precision [nan, 0.5, 0.25, 0.4, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.2, 0.25, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:04.887572: step 26, loss 6.92521, accuracy 0, precision [0.0, 0.0, nan, 0.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.0, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:05.040829: step 27, loss 7.14045, accuracy 0.1875, precision [nan, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.16666666666666666, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:05.198519: step 28, loss 4.93523, accuracy 0.25, precision [0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.2, 0.4, 0.0, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:05.353400: step 29, loss 5.17176, accuracy 0.3125, precision [0.0, 1.0, 0.3333333333333333, 0.5, 0.2, nan, 0.0, 0.0, nan], recall [nan, 0.5, 0.2, 1.0, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:43:05.506116: step 30, loss 4.75092, accuracy 0.1875, precision [1.0, 0.0, 0.0, 0.14285714285714285, 0.3333333333333333, nan, nan, nan, 0.0], recall [1.0, 0.0, 0.0, 0.5, 0.14285714285714285, 0.0, nan, 0.0, nan]
2019-02-19T19:43:05.658121: step 31, loss 7.32689, accuracy 0.1875, precision [nan, nan, 0.3333333333333333, 0.0, 0.25, 0.0, nan, 0.3333333333333333, 0.0], recall [nan, 0.0, 0.25, nan, 0.14285714285714285, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:43:05.809104: step 32, loss 4.3758, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.5, 0.4, 0.5, nan, 0.0, 0.0, nan], recall [0.0, 0.5, 0.5, 1.0, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:05.967502: step 33, loss 5.59442, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, nan, nan, nan], recall [0.0, nan, nan, 0.16666666666666666, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:43:06.121274: step 34, loss 4.54339, accuracy 0.125, precision [0.16666666666666666, 0.0, nan, 0.3333333333333333, nan, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.14285714285714285, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:43:06.275355: step 35, loss 3.30921, accuracy 0.25, precision [nan, 0.0, 0.0, 0.375, 0.0, 1.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.5, 0.0, 0.25, 0.0, nan, nan]
2019-02-19T19:43:06.427383: step 36, loss 3.07722, accuracy 0.3125, precision [0.0, 0.25, 0.0, 0.8, 0.0, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:43:06.578626: step 37, loss 5.81302, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, nan, 0.25, nan], recall [0.0, 1.0, nan, 0.1111111111111111, 0.0, 0.0, 0.0, 0.5, nan]
2019-02-19T19:43:06.730514: step 38, loss 3.95517, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, 0.5, nan, 0.0, 0.5, 0.0], recall [0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0]
2019-02-19T19:43:06.879595: step 39, loss 4.9398, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.14285714285714285, nan, 0.0, nan, nan, 0.0], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0]
2019-02-19T19:43:07.028889: step 40, loss 3.94293, accuracy 0.1875, precision [0.5, 0.3333333333333333, 0.0, 0.0, 0.5, 0.0, nan, 0.0, nan], recall [0.3333333333333333, 0.16666666666666666, 0.0, 0.0, 1.0, nan, 0.0, 0.0, nan]
2019-02-19T19:43:07.184200: step 41, loss 2.99759, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.3333333333333333, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, nan, 0.5, nan]
2019-02-19T19:43:07.335585: step 42, loss 3.10569, accuracy 0.25, precision [nan, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, nan, nan, 0.5, nan], recall [0.0, 0.0, 0.0, 0.5, 0.25, 0.0, nan, 1.0, nan]
2019-02-19T19:43:07.487230: step 43, loss 4.48698, accuracy 0.0625, precision [0.0, nan, 0.0, 0.14285714285714285, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.2, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:07.642756: step 44, loss 4.62675, accuracy 0.3125, precision [0.0, 0.3333333333333333, 1.0, 0.4, 0.0, nan, nan, 0.3333333333333333, 0.0], recall [nan, 0.5, 0.25, 1.0, 0.0, nan, nan, 0.25, 0.0]
2019-02-19T19:43:07.794308: step 45, loss 2.95385, accuracy 0.25, precision [nan, 0.0, nan, 0.42857142857142855, 0.2, 0.0, nan, nan, 0.0], recall [0.0, 0.0, 0.0, 0.75, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:07.946613: step 46, loss 4.45115, accuracy 0.125, precision [nan, 0.25, nan, 0.2, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, 0.0, 0.14285714285714285, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:08.099047: step 47, loss 3.99096, accuracy 0.125, precision [0.0, nan, 0.0, 0.25, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:08.252411: step 48, loss 3.54209, accuracy 0.375, precision [0.0, 0.5, nan, 0.6666666666666666, 0.3333333333333333, nan, 0.0, nan, nan], recall [nan, 0.2, nan, 0.4444444444444444, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:08.400926: step 49, loss 3.94675, accuracy 0.1875, precision [0.0, 0.0, nan, 0.4, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, nan, nan, 0.0, 0.0]
2019-02-19T19:43:08.553193: step 50, loss 3.43681, accuracy 0.375, precision [0.3333333333333333, 0.0, nan, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.5, 0.0, nan, nan, nan, 0.0]
2019-02-19T19:43:08.700186: step 51, loss 3.99099, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, nan, nan, nan, nan], recall [nan, nan, nan, 0.3076923076923077, 0.0, nan, nan, 0.0, 0.0]
2019-02-19T19:43:08.852342: step 52, loss 3.46614, accuracy 0.3125, precision [nan, 0.25, 0.0, 0.75, 0.16666666666666666, nan, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.3, 0.25, nan, nan, nan, nan]
2019-02-19T19:43:09.007997: step 53, loss 3.02497, accuracy 0.5, precision [0.0, nan, nan, 0.5, 0.8333333333333334, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.75, 0.625, nan, 0.0, nan, nan]
2019-02-19T19:43:09.162909: step 54, loss 3.94156, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.125, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.25, nan, 1.0, 0.125, 0.0, nan, nan, nan]
2019-02-19T19:43:09.315357: step 55, loss 4.74921, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.5, 1.0, 0.0, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.4, 0.14285714285714285, nan, 0.0, nan, 0.0]
2019-02-19T19:43:09.471477: step 56, loss 3.25906, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, 0.4, nan, 0.0, nan, 0.0], recall [0.0, nan, 0.0, 0.4, 0.25, nan, nan, nan, 0.0]
2019-02-19T19:43:09.621906: step 57, loss 2.5513, accuracy 0.5, precision [0.0, 1.0, nan, 0.6666666666666666, 0.75, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.6666666666666666, 1.0, nan, nan, 0.0, 0.0]
2019-02-19T19:43:09.778337: step 58, loss 2.35659, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.3333333333333333, 0.5, nan, nan, nan, nan], recall [0.0, 1.0, 0.0, 0.75, 0.16666666666666666, nan, 0.0, nan, nan]
2019-02-19T19:43:09.926456: step 59, loss 2.69318, accuracy 0.3125, precision [0.0, 0.5, nan, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, 0.0, 0.25, 0.6666666666666666, nan, nan, nan, 0.0]
2019-02-19T19:43:10.077636: step 60, loss 2.92832, accuracy 0.25, precision [nan, 0.0, nan, 0.2857142857142857, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.2857142857142857, 0.6666666666666666, nan, 0.0, nan, nan]
2019-02-19T19:43:10.228194: step 61, loss 3.05262, accuracy 0.25, precision [nan, nan, 0.0, 0.6666666666666666, 0.0, nan, 0.0, nan, nan], recall [0.0, nan, 0.0, 0.36363636363636365, nan, 0.0, nan, nan, nan]
2019-02-19T19:43:10.379588: step 62, loss 2.90863, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.42857142857142855, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:10.531567: step 63, loss 2.98138, accuracy 0.0625, precision [0.0, nan, 0.0, 0.16666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:10.687752: step 64, loss 2.32138, accuracy 0.25, precision [0.0, 0.0, nan, 0.5, 1.0, 0.0, nan, 1.0, nan], recall [nan, nan, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.3333333333333333, nan]
2019-02-19T19:43:10.837187: step 65, loss 2.7365, accuracy 0.125, precision [nan, 0.0, 0.0, 0.2, 0.0, 0.5, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.14285714285714285, 0.0, 0.3333333333333333, 0.0, 0.0, nan]
2019-02-19T19:43:10.989415: step 66, loss 2.54982, accuracy 0.1875, precision [0.5, 0.0, 0.0, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.4, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:43:11.138955: step 67, loss 3.4169, accuracy 0.125, precision [0.0, 0.2, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 1.0, 0.0, 0.2, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:43:11.289474: step 68, loss 2.58558, accuracy 0.25, precision [0.0, 0.25, 0.0, 0.5, 0.5, 0.0, nan, 0.5, nan], recall [0.0, 0.25, 0.0, 0.5, 0.25, 0.0, 0.0, 1.0, nan]
2019-02-19T19:43:11.442314: step 69, loss 2.73185, accuracy 0.1875, precision [0.0, 0.0, nan, 0.2, 0.6666666666666666, 0.0, nan, nan, 0.0], recall [nan, 0.0, 0.0, 0.2, 0.5, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:11.595659: step 70, loss 4.36611, accuracy 0.1875, precision [nan, 1.0, 0.0, 0.14285714285714285, 1.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.5, nan, 0.5, 0.125, 0.0, nan, nan, nan]
2019-02-19T19:43:11.750338: step 71, loss 3.01472, accuracy 0.125, precision [nan, 0.0, nan, 0.2857142857142857, nan, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:43:11.901553: step 72, loss 2.84009, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.5, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [nan, 0.2, nan, 0.6666666666666666, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:43:12.052726: step 73, loss 2.65538, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.25, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.25, nan, 0.0, nan, nan]
2019-02-19T19:43:12.204520: step 74, loss 2.62874, accuracy 0.125, precision [0.0, 1.0, 0.0, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.25, nan, 0.1, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:12.357548: step 75, loss 3.11018, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.5, 0.3333333333333333, nan, 0.0, 0.5, 0.0], recall [0.0, 0.0, nan, 0.1, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:43:12.514504: step 76, loss 2.26362, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.6666666666666666, 0.0, 0.375, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:43:12.665812: step 77, loss 2.82463, accuracy 0.25, precision [nan, 0.0, 0.0, 0.75, 0.0, nan, 0.0, 0.5, 0.0], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, 0.0, nan, 0.3333333333333333, nan]
2019-02-19T19:43:12.823075: step 78, loss 2.55395, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.2857142857142857, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:43:12.971775: step 79, loss 2.62198, accuracy 0.5, precision [0.0, nan, 0.5, 1.0, 0.4, nan, nan, 1.0, 0.0], recall [nan, nan, 1.0, 1.0, 1.0, 0.0, nan, 0.14285714285714285, 0.0]
2019-02-19T19:43:13.126133: step 80, loss 2.53892, accuracy 0.25, precision [nan, 0.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0], recall [nan, nan, 0.4, 0.25, 0.5, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:13.279854: step 81, loss 1.95778, accuracy 0.25, precision [0.0, 0.6, 1.0, 0.0, 0.0, 0.0, nan, nan, nan], recall [nan, 1.0, 0.125, nan, 0.0, nan, nan, 0.0, 0.0]
2019-02-19T19:43:13.433357: step 82, loss 2.37635, accuracy 0.1875, precision [0.0, 0.0, 0.5, 0.2857142857142857, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.0, 0.16666666666666666, 1.0, 0.0, nan, nan, 0.0, 0.0]
2019-02-19T19:43:13.586146: step 83, loss 2.81221, accuracy 0.1875, precision [0.0, 0.0, 0.3333333333333333, 0.0, 0.4, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.3333333333333333, nan, 0.3333333333333333, 0.0, nan, 0.0, 0.0]
2019-02-19T19:43:13.740607: step 84, loss 2.00394, accuracy 0.3125, precision [0.25, 0.0, 1.0, 0.0, 0.75, nan, 0.0, nan, nan], recall [1.0, 0.0, 0.2, 0.0, 0.6, nan, nan, nan, nan]
2019-02-19T19:43:13.892524: step 85, loss 2.77567, accuracy 0.125, precision [0.6666666666666666, 0.0, 0.0, 0.0, nan, nan, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.0, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:14.043075: step 86, loss 2.53654, accuracy 0.25, precision [nan, 1.0, 0.0, 0.0, 0.6, nan, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.0, 0.3, nan, nan, nan, nan]
2019-02-19T19:43:14.196191: step 87, loss 1.91762, accuracy 0.25, precision [0.3333333333333333, 0.0, 0.25, nan, 0.5, nan, nan, nan, 0.0], recall [0.5, 0.0, 0.3333333333333333, 0.0, 0.2222222222222222, nan, nan, nan, nan]
2019-02-19T19:43:14.354433: step 88, loss 2.12461, accuracy 0.3125, precision [0.3333333333333333, 0.3333333333333333, 0.25, 0.0, 1.0, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.5, 0.5, 0.0, 0.2857142857142857, nan, nan, nan, nan]
2019-02-19T19:43:14.509997: step 89, loss 2.51898, accuracy 0.25, precision [0.0, 1.0, 0.25, 0.0, 1.0, nan, nan, 0.0, nan], recall [0.0, 0.2, 0.5, nan, 0.4, nan, nan, nan, 0.0]
2019-02-19T19:43:14.659196: step 90, loss 1.71984, accuracy 0.3125, precision [0.0, 0.0, 0.5, 0.0, 0.8, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.3333333333333333, 0.0, 0.5, nan, 0.0, 0.0, nan]
2019-02-19T19:43:14.813809: step 91, loss 2.16918, accuracy 0.3125, precision [0.0, 0.25, 1.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, 0.16666666666666666, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:43:14.968792: step 92, loss 2.05076, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.2857142857142857, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.4, 0.0, nan, 0.0, nan, nan]
2019-02-19T19:43:15.119604: step 93, loss 1.9399, accuracy 0.125, precision [nan, 1.0, nan, 0.0, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [nan, 0.14285714285714285, 0.0, 0.0, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:43:15.269724: step 94, loss 1.79085, accuracy 0.125, precision [0.0, 0.2, 0.0, 0.25, 0.0, nan, nan, 0.0, nan], recall [nan, 0.25, 0.0, 0.16666666666666666, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:15.421976: step 95, loss 2.93722, accuracy 0.25, precision [nan, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.25, 0.0, 0.4, nan, nan, 0.0, nan, 0.0]
2019-02-19T19:43:15.573295: step 96, loss 2.50653, accuracy 0.1875, precision [0.5, 0.0, 0.0, 0.4, 0.0, nan, 0.0, 0.0, 0.0], recall [1.0, 0.0, 0.0, 0.2857142857142857, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:15.726172: step 97, loss 1.78947, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.6, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.25, nan, 0.3333333333333333, nan, nan, 0.0, 0.0, nan]
2019-02-19T19:43:15.874810: step 98, loss 2.60662, accuracy 0.25, precision [0.0, 0.0, nan, 0.5714285714285714, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.5714285714285714, nan, nan, 0.0, 0.0, nan]
2019-02-19T19:43:16.027172: step 99, loss 2.05096, accuracy 0.1875, precision [0.0, 0.0, nan, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.23076923076923078, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:16.180058: step 100, loss 2.0487, accuracy 0.375, precision [0.0, nan, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.4444444444444444, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:16.334357: step 101, loss 2.18878, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:43:16.485107: step 102, loss 2.42021, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.5, 0.0], recall [0.0, nan, 0.0, 0.0, 0.3333333333333333, nan, nan, 0.16666666666666666, nan]
2019-02-19T19:43:16.634600: step 103, loss 1.92126, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.45454545454545453, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:43:16.792715: step 104, loss 1.97382, accuracy 0.1875, precision [nan, 0.0, nan, 0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0], recall [0.0, nan, nan, 0.4, 0.0, 0.0, nan, 0.25, nan]
2019-02-19T19:43:16.942932: step 105, loss 1.67588, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, nan], recall [0.3333333333333333, nan, nan, 0.5, 0.4, nan, nan, 0.25, nan]
2019-02-19T19:43:17.096648: step 106, loss 2.68273, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, nan], recall [1.0, nan, nan, 0.125, 0.0, 0.0, nan, 0.5, nan]
2019-02-19T19:43:17.247686: step 107, loss 2.82882, accuracy 0.0625, precision [nan, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.2, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:17.399986: step 108, loss 2.73566, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, 0.16666666666666666, nan, 1.0, nan, nan]
2019-02-19T19:43:17.552683: step 109, loss 2.49313, accuracy 0.25, precision [0.0, nan, 0.0, 0.2, 0.6666666666666666, nan, 1.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.5, 0.25, nan, 0.5, 0.0, nan]
2019-02-19T19:43:17.708004: step 110, loss 2.40434, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:43:17.863411: step 111, loss 1.76851, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.4444444444444444, nan, nan, nan, 0.5, nan], recall [0.0, 0.2, nan, 0.8, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:43:18.017658: step 112, loss 1.75941, accuracy 0.4375, precision [nan, 1.0, nan, 0.2857142857142857, 0.5, 0.0, nan, 0.3333333333333333, nan], recall [0.0, 0.5, nan, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, nan]
2019-02-19T19:43:18.172597: step 113, loss 1.57113, accuracy 0.375, precision [0.0, 0.4, nan, 0.75, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.4, 0.0, 0.42857142857142855, nan, nan, 0.0, 1.0, nan]
2019-02-19T19:43:18.321312: step 114, loss 2.35836, accuracy 0.1875, precision [nan, 0.0, nan, 0.5, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:43:18.477666: step 115, loss 1.67913, accuracy 0.5625, precision [0.5, nan, 0.0, 0.8888888888888888, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.6666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:18.627287: step 116, loss 1.75766, accuracy 0.3125, precision [nan, 0.16666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, 0.5, 0.25, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:18.780529: step 117, loss 1.93114, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.625, 1.0, 0.0, nan, 1.0, nan], recall [nan, 0.0, nan, 0.5, 1.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:43:18.931187: step 118, loss 2.06266, accuracy 0.3125, precision [0.0, 0.25, nan, 0.6666666666666666, 0.25, 0.0, nan, 0.5, 0.0], recall [nan, 0.25, nan, 0.2, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:43:19.087074: step 119, loss 2.45837, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.6666666666666666, 0.25, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.36363636363636365, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:19.238274: step 120, loss 1.89298, accuracy 0.5, precision [nan, nan, 0.0, 0.8888888888888888, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:43:19.390179: step 121, loss 1.47503, accuracy 0.5, precision [nan, 0.5, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:19.536356: step 122, loss 2.24171, accuracy 0.3125, precision [0.0, 1.0, nan, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:43:19.687898: step 123, loss 1.91912, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, nan, nan, nan], recall [nan, 0.5, 0.0, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:43:19.838386: step 124, loss 2.33231, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.375, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:43:19.987108: step 125, loss 1.91473, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.8, 0.0, 0.0, nan, nan, 0.0], recall [0.3333333333333333, nan, 0.0, 0.5, 0.0, nan, nan, 0.0, 0.0]
2019-02-19T19:43:20.138118: step 126, loss 2.29153, accuracy 0.375, precision [0.25, 0.0, 0.0, 0.6666666666666666, 1.0, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.25, 0.6, nan, nan, 0.0, nan]
2019-02-19T19:43:20.287254: step 127, loss 1.61603, accuracy 0.5625, precision [0.0, 1.0, nan, 0.42857142857142855, 0.8, nan, nan, 0.0, nan], recall [0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:20.443215: step 128, loss 2.07446, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.3333333333333333, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:43:20.594860: step 129, loss 2.0939, accuracy 0.25, precision [0.3333333333333333, nan, 0.0, 0.0, 0.4, 0.0, nan, 1.0, nan], recall [0.5, 0.0, 0.0, 0.0, 0.5, nan, nan, 1.0, 0.0]
2019-02-19T19:43:20.749379: step 130, loss 1.89322, accuracy 0.4375, precision [nan, nan, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [0.0, nan, 1.0, 0.0, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:20.897042: step 131, loss 1.86235, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.2, 0.8, nan, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.5, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:43:21.050035: step 132, loss 2.2576, accuracy 0.25, precision [0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan], recall [0.5, nan, 0.2, 0.0, 0.16666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:21.199978: step 133, loss 2.04295, accuracy 0.1875, precision [0.0, 0.0, 1.0, 0.0, 0.5, 0.0, nan, nan, nan], recall [0.0, nan, 0.5, 0.0, 0.2222222222222222, nan, nan, nan, 0.0]
2019-02-19T19:43:21.355148: step 134, loss 2.08019, accuracy 0.3125, precision [0.5, 0.0, 0.5, 0.25, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [0.5, nan, 1.0, 0.25, 0.2857142857142857, 0.0, nan, 0.0, nan]
2019-02-19T19:43:21.504825: step 135, loss 2.00779, accuracy 0.375, precision [0.5, 0.0, 0.5, 0.6, 0.5, nan, nan, 0.0, nan], recall [0.5, nan, 1.0, 0.375, 0.25, nan, nan, nan, 0.0]
2019-02-19T19:43:21.655074: step 136, loss 2.15416, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, nan, nan], recall [0.0, nan, 0.0, 0.16666666666666666, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:43:21.810649: step 137, loss 1.53307, accuracy 0.5, precision [0.5, 0.0, 0.0, 0.5714285714285714, 1.0, 0.0, nan, nan, nan], recall [0.3333333333333333, nan, 0.0, 0.8, 0.5, 0.0, nan, nan, nan]
2019-02-19T19:43:21.963564: step 138, loss 1.61942, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.6, 0.6, nan, nan, 0.0, nan], recall [0.5, nan, 0.0, 0.3333333333333333, 0.75, nan, nan, nan, nan]
2019-02-19T19:43:22.117385: step 139, loss 2.18934, accuracy 0.25, precision [nan, 0.2, nan, 0.4, 0.25, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.2857142857142857, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:22.268668: step 140, loss 1.91932, accuracy 0.375, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, nan], recall [nan, 0.0, 0.0, 0.36363636363636365, 1.0, 0.0, nan, 1.0, nan]
2019-02-19T19:43:22.421694: step 141, loss 1.83018, accuracy 0.5, precision [nan, 0.5, nan, 0.75, 0.3333333333333333, 0.0, nan, 1.0, nan], recall [nan, 0.5, nan, 0.375, 1.0, 0.0, nan, 0.6666666666666666, nan]
2019-02-19T19:43:22.570282: step 142, loss 1.70479, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.4444444444444444, nan, 0.0, nan, nan, nan]
2019-02-19T19:43:22.721950: step 143, loss 1.93701, accuracy 0.3125, precision [0.0, 0.0, nan, 0.7142857142857143, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.45454545454545453, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:43:22.874131: step 144, loss 1.92905, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.75, 0.4, nan, nan, 0.5, nan], recall [nan, nan, nan, 0.3, 1.0, 0.0, nan, 0.3333333333333333, nan]
2019-02-19T19:43:23.023764: step 145, loss 1.81013, accuracy 0.375, precision [0.3333333333333333, 0.0, nan, 0.6666666666666666, 0.5, 0.0, 0.0, nan, 0.0], recall [0.5, 0.0, nan, 0.5714285714285714, 0.3333333333333333, nan, 0.0, 0.0, nan]
2019-02-19T19:43:23.175533: step 146, loss 1.40147, accuracy 0.5, precision [0.5, 0.5, 0.0, 0.6666666666666666, 0.3333333333333333, nan, nan, 0.5, nan], recall [1.0, 1.0, 0.0, 0.4, 1.0, 0.0, nan, 1.0, nan]
2019-02-19T19:43:23.322018: step 147, loss 2.20886, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:23.473427: step 148, loss 1.65175, accuracy 0.5625, precision [0.0, 1.0, nan, 1.0, 0.6666666666666666, nan, 0.0, 0.5, nan], recall [nan, 0.3333333333333333, nan, 0.625, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:43:23.631357: step 149, loss 2.04989, accuracy 0.3125, precision [0.3333333333333333, 0.0, nan, 0.6666666666666666, 0.5, nan, 0.0, 0.2, nan], recall [1.0, 0.0, nan, 0.2857142857142857, 0.3333333333333333, 0.0, nan, 0.5, nan]
2019-02-19T19:43:23.784883: step 150, loss 2.38876, accuracy 0.25, precision [0.0, 0.0, nan, 0.75, 0.3333333333333333, 0.0, nan, nan, nan], recall [0.0, nan, nan, 0.3, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:43:23.934522: step 151, loss 2.21071, accuracy 0.25, precision [nan, 0.0, 0.0, 0.5, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:43:24.087355: step 152, loss 1.67847, accuracy 0.375, precision [0.0, nan, 0.0, 0.4, 0.6, 0.0, 0.0, 0.5, nan], recall [0.0, nan, 0.0, 0.4, 0.42857142857142855, nan, nan, 0.5, nan]
2019-02-19T19:43:24.235912: step 153, loss 1.51941, accuracy 0.5, precision [nan, 0.5, 0.0, 0.5, 0.6, nan, nan, 0.5, nan], recall [nan, 0.25, nan, 0.5, 0.75, nan, nan, 0.5, nan]
2019-02-19T19:43:24.389916: step 154, loss 2.04042, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:24.538460: step 155, loss 2.06739, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, 0.3333333333333333, nan, nan, nan, nan], recall [0.0, 0.0, nan, 0.42857142857142855, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:43:24.688635: step 156, loss 2.40499, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.3333333333333333, nan, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:24.840952: step 157, loss 2.53042, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, nan, 0.0], recall [0.0, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:24.996991: step 158, loss 1.92917, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.75, 0.0, nan, 0.0, nan], recall [0.0, 0.5, 0.0, 0.5, 0.6, nan, nan, nan, nan]
2019-02-19T19:43:25.149402: step 159, loss 1.87962, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [0.3333333333333333, nan, 0.0, 0.4, 0.4, nan, nan, 0.0, nan]
2019-02-19T19:43:25.301543: step 160, loss 1.9711, accuracy 0.5, precision [0.0, 0.5, 0.0, 0.8, 0.75, 0.0, 0.0, nan, nan], recall [0.0, 0.5, 0.0, 0.5, 0.75, nan, nan, nan, nan]
2019-02-19T19:43:25.452862: step 161, loss 2.14917, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.375, 0.6, nan, nan, nan, nan], recall [0.3333333333333333, 0.0, 0.0, 0.5, 0.6, nan, nan, nan, nan]
2019-02-19T19:43:25.604644: step 162, loss 2.51811, accuracy 0.1875, precision [0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0], recall [0.0, 0.3333333333333333, nan, 0.16666666666666666, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:43:25.759061: step 163, loss 1.52242, accuracy 0.5, precision [0.0, 0.25, 0.0, 0.8333333333333334, 1.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.625, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:43:25.911304: step 164, loss 2.00646, accuracy 0.4375, precision [nan, 0.25, nan, 1.0, 0.75, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.375, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:26.068744: step 165, loss 2.38967, accuracy 0.125, precision [nan, 0.0, 0.0, 1.0, 0.5, nan, 0.0, 0.0, 0.0], recall [0.0, nan, 0.0, 0.16666666666666666, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:43:26.219059: step 166, loss 1.64527, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.0, 0.8333333333333334, nan, nan, 0.5, 0.5, nan], recall [nan, 0.5, nan, 0.5, 0.0, nan, 1.0, 1.0, nan]
2019-02-19T19:43:26.376192: step 167, loss 2.03454, accuracy 0.3125, precision [0.5, 1.0, 0.0, 0.4, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.25, 0.3333333333333333, 0.0, nan, nan, nan]
2019-02-19T19:43:26.533074: step 168, loss 2.20891, accuracy 0.375, precision [0.0, nan, 0.0, 0.625, 0.5, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.625, 0.3333333333333333, nan, 0.0, nan, nan]
2019-02-19T19:43:26.683329: step 169, loss 1.70856, accuracy 0.5, precision [0.5, 0.0, 0.3333333333333333, 0.8333333333333334, 1.0, nan, nan, 0.0, nan], recall [0.3333333333333333, nan, 0.5, 0.5555555555555556, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:26.837540: step 170, loss 1.77814, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.75, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 0.25, 0.0, 0.42857142857142855, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:26.987931: step 171, loss 2.24095, accuracy 0.3125, precision [nan, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, 1.0, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:27.141412: step 172, loss 1.80301, accuracy 0.25, precision [nan, 0.5, 0.0, 1.0, 0.2, 0.0, nan, 0.3333333333333333, nan], recall [nan, 0.5, nan, 0.08333333333333333, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:43:27.295151: step 173, loss 2.12844, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.4, 0.5, nan, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.2, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:27.443839: step 174, loss 2.31869, accuracy 0.3125, precision [nan, 0.0, 1.0, 0.75, 0.0, 0.0, 0.0, 1.0, 0.0], recall [0.0, nan, 1.0, 0.3, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:43:27.596394: step 175, loss 2.14125, accuracy 0.125, precision [0.0, 0.0, nan, 0.6666666666666666, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.2857142857142857, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:43:27.747153: step 176, loss 2.37941, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [0.5, 0.0, 0.0, 0.5, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:27.895483: step 177, loss 2.73904, accuracy 0.25, precision [nan, 0.0, 0.3333333333333333, 0.75, nan, 0.0, nan, 0.0, 0.0], recall [0.0, nan, 0.5, 0.2727272727272727, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:28.051022: step 178, loss 2.07894, accuracy 0.3125, precision [0.0, 0.0, 0.5, 0.6, 0.5, 0.0, 0.0, nan, nan], recall [nan, nan, 0.25, 0.375, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:43:28.206009: step 179, loss 1.92722, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:28.360467: step 180, loss 2.1638, accuracy 0.25, precision [0.0, 0.0, 0.5, 0.42857142857142855, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:28.505062: step 181, loss 1.96143, accuracy 0.4375, precision [0.0, 0.0, 0.5, 0.5714285714285714, 0.6666666666666666, nan, 0.0, nan, nan], recall [nan, nan, 0.2, 0.5714285714285714, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:28.658983: step 182, loss 1.6107, accuracy 0.4375, precision [0.0, 1.0, 0.3333333333333333, 0.7142857142857143, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.25, 0.3333333333333333, 0.5555555555555556, nan, nan, nan, nan, nan]
2019-02-19T19:43:28.810208: step 183, loss 1.90886, accuracy 0.25, precision [0.0, 0.0, 0.3333333333333333, 0.75, nan, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.5, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:43:28.964006: step 184, loss 1.91218, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, nan, nan, nan], recall [0.0, nan, 0.0, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:29.110749: step 185, loss 1.26553, accuracy 0.5625, precision [nan, 0.5, 0.0, 0.8333333333333334, 0.3333333333333333, nan, nan, nan, nan], recall [nan, 1.0, 0.0, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:29.265014: step 186, loss 1.64974, accuracy 0.5, precision [0.0, 0.5, nan, 0.7777777777777778, nan, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:29.417987: step 187, loss 2.61358, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, nan, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:43:29.567459: step 188, loss 2.07607, accuracy 0.375, precision [0.0, 0.0, nan, 0.8571428571428571, 0.0, 0.0, nan, nan, nan], recall [nan, 0.0, 0.0, 0.5454545454545454, nan, nan, nan, nan, nan]
2019-02-19T19:43:29.716093: step 189, loss 2.04809, accuracy 0.25, precision [0.3333333333333333, nan, 0.0, 0.6666666666666666, 0.16666666666666666, 0.0, nan, nan, nan], recall [1.0, 0.0, nan, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:29.869129: step 190, loss 2.33524, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:30.015153: step 191, loss 1.87006, accuracy 0.5625, precision [0.0, 1.0, nan, 0.8888888888888888, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.6153846153846154, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:30.166971: step 192, loss 2.35453, accuracy 0.1875, precision [0.0, 0.0, nan, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:30.315500: step 193, loss 2.31957, accuracy 0.1875, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, nan, nan, 0.14285714285714285, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:30.467901: step 194, loss 2.15824, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, nan, nan], recall [0.0, nan, nan, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:30.616822: step 195, loss 1.9628, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.4, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:43:30.767465: step 196, loss 1.86801, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.4, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, 0.2222222222222222, nan, nan, nan, nan]
2019-02-19T19:43:30.914618: step 197, loss 2.00049, accuracy 0.5, precision [1.0, 0.3333333333333333, 0.0, 0.8, 0.5, nan, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.4444444444444444, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:31.061573: step 198, loss 1.94416, accuracy 0.5, precision [0.6666666666666666, 0.0, 0.0, 0.7142857142857143, 0.3333333333333333, nan, 0.0, nan, nan], recall [0.5, 0.0, nan, 0.5555555555555556, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:31.212852: step 199, loss 1.8104, accuracy 0.375, precision [0.5, 0.0, 0.0, 0.6, 0.25, nan, nan, 0.3333333333333333, nan], recall [0.16666666666666666, nan, nan, 0.5, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:43:31.363596: step 200, loss 1.72645, accuracy 0.4375, precision [0.0, 0.0, nan, 0.8333333333333334, 0.6666666666666666, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.5555555555555556, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:31.516889: step 201, loss 2.1686, accuracy 0.1875, precision [0.5, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0, nan], recall [0.25, 1.0, nan, 0.125, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:31.669778: step 202, loss 1.77394, accuracy 0.5, precision [0.5, 0.0, 0.0, 1.0, 0.75, nan, 0.0, nan, nan], recall [0.3333333333333333, nan, nan, 0.4444444444444444, 0.75, nan, nan, nan, nan]
2019-02-19T19:43:31.825709: step 203, loss 1.35728, accuracy 0.5, precision [0.3333333333333333, 0.5, 0.0, 0.6, 1.0, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.5, nan, 0.5, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:31.977341: step 204, loss 1.43154, accuracy 0.5, precision [0.3333333333333333, 1.0, nan, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [1.0, 1.0, 0.0, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:32.131296: step 205, loss 1.69579, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.42857142857142855, 0.5, nan, nan, 0.0, 0.0], recall [0.3333333333333333, 0.0, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:43:32.283404: step 206, loss 1.76501, accuracy 0.375, precision [0.0, 0.0, nan, 0.8, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.4, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:32.437351: step 207, loss 2.42599, accuracy 0.4375, precision [0.0, 0.0, nan, 0.8333333333333334, 0.3333333333333333, nan, 0.0, 0.5, 0.0], recall [0.0, 0.0, nan, 0.5555555555555556, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:43:32.592361: step 208, loss 2.09945, accuracy 0.1875, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.3333333333333333, 0.0, nan, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:32.745027: step 209, loss 2.16747, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [0.0, 0.25, nan, 0.375, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:43:32.891031: step 210, loss 1.74072, accuracy 0.3125, precision [nan, nan, 0.0, 0.5, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.36363636363636365, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:33.047651: step 211, loss 2.26693, accuracy 0.1875, precision [nan, 1.0, 0.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, nan], recall [nan, 0.2, nan, 0.125, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:43:33.201490: step 212, loss 1.49959, accuracy 0.5625, precision [0.0, 0.6666666666666666, nan, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [nan, 0.8, nan, 0.45454545454545453, nan, nan, nan, nan, nan]
2019-02-19T19:43:33.348126: step 213, loss 2.15096, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:33.499500: step 214, loss 1.92404, accuracy 0.375, precision [nan, 0.5, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:33.646949: step 215, loss 1.90149, accuracy 0.375, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, 0.5, 0.0], recall [nan, 0.5, nan, 0.2, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:43:33.799524: step 216, loss 1.70513, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8, 0.5, nan, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.4444444444444444, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:43:33.950019: step 217, loss 1.60445, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.5, 0.75, nan, 0.0, nan, nan], recall [nan, 0.5, 0.0, 0.42857142857142855, 0.6, nan, nan, 0.0, nan]
2019-02-19T19:43:34.108225: step 218, loss 2.25661, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, nan, nan, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:34.255813: step 219, loss 2.1348, accuracy 0.3125, precision [0.0, 0.2, 0.0, 1.0, 0.5, nan, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.2222222222222222, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:34.405119: step 220, loss 2.1343, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:43:34.556762: step 221, loss 1.60557, accuracy 0.5625, precision [0.0, 1.0, 0.0, 0.75, 0.5, 0.0, nan, nan, nan], recall [nan, 0.6666666666666666, nan, 0.6, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:43:34.705657: step 222, loss 2.3515, accuracy 0.125, precision [0.0, 0.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 1.0, 0.125, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:34.857339: step 223, loss 2.12195, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.8, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:35.005774: step 224, loss 1.91855, accuracy 0.25, precision [0.3333333333333333, 0.0, 0.0, 0.25, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.5, 0.0, 0.0, 0.25, 0.6666666666666666, nan, 0.0, 0.0, nan]
2019-02-19T19:43:35.159797: step 225, loss 2.45695, accuracy 0.125, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.5, 0.0], recall [0.0, nan, nan, 0.1, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:43:35.311840: step 226, loss 1.97841, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.3, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:35.456047: step 227, loss 1.91215, accuracy 0.375, precision [0.0, 0.0, 0.6666666666666666, 0.75, 0.25, nan, nan, 0.0, nan], recall [0.0, 0.0, 1.0, 0.375, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:35.609207: step 228, loss 1.74161, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.5, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.3333333333333333, nan, 0.0, 0.2, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:35.760851: step 229, loss 1.42817, accuracy 0.625, precision [1.0, 0.0, 1.0, 0.8333333333333334, 0.5, nan, nan, 0.5, nan], recall [0.6666666666666666, 0.0, 0.5, 0.625, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:43:35.911432: step 230, loss 1.74245, accuracy 0.375, precision [nan, 0.3333333333333333, 0.3333333333333333, 0.5, 1.0, nan, nan, 0.0, 0.0], recall [0.0, 0.5, 0.3333333333333333, 0.375, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:36.061713: step 231, loss 1.61286, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.5, nan], recall [nan, 0.0, 0.0, 0.25, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:43:36.213483: step 232, loss 2.19612, accuracy 0.375, precision [1.0, 0.3333333333333333, 0.0, 0.75, 0.5, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.3333333333333333, 0.0, 0.5, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:43:36.365685: step 233, loss 1.22884, accuracy 0.5625, precision [0.0, 0.8, 0.0, 0.7142857142857143, 0.0, nan, nan, nan, nan], recall [nan, 0.8, 0.0, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:43:36.517002: step 234, loss 1.94261, accuracy 0.4375, precision [0.5, 1.0, 0.0, 0.5, nan, nan, nan, 0.0, 0.0], recall [0.5, 0.25, 0.0, 0.625, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:36.669483: step 235, loss 2.22571, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, nan, 0.0, 0.18181818181818182, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:36.819734: step 236, loss 1.95706, accuracy 0.375, precision [1.0, 0.0, 0.0, 1.0, 0.16666666666666666, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:36.971991: step 237, loss 2.19778, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.5, 0.0], recall [nan, 0.0, nan, 0.3076923076923077, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:43:37.122688: step 238, loss 2.13063, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.25, nan], recall [1.0, 0.0, 0.0, 0.16666666666666666, nan, nan, nan, 1.0, nan]
2019-02-19T19:43:37.277981: step 239, loss 2.09477, accuracy 0.3125, precision [0.0, nan, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.38461538461538464, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:37.429819: step 240, loss 2.31566, accuracy 0.25, precision [0.0, 0.5, 0.0, 1.0, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, 0.0, 0.09090909090909091, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:37.581414: step 241, loss 1.93979, accuracy 0.375, precision [0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.6666666666666666, 0.3333333333333333, nan, 0.42857142857142855, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:37.733076: step 242, loss 1.66058, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.8, 0.5, nan, 0.0, 0.0, nan], recall [0.0, 0.5, 0.0, 0.5714285714285714, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:37.888449: step 243, loss 1.54983, accuracy 0.4375, precision [nan, 0.25, 0.0, 0.6, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:38.039870: step 244, loss 2.16894, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.75, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.42857142857142855, 0.25, nan, nan, nan, nan]
2019-02-19T19:43:38.190648: step 245, loss 1.91073, accuracy 0.25, precision [0.0, nan, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.25, nan], recall [nan, 0.0, nan, 0.1111111111111111, 0.6666666666666666, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:43:38.345802: step 246, loss 2.17532, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.5, nan, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.25, 0.14285714285714285, nan, nan, 0.0, nan]
2019-02-19T19:43:38.496115: step 247, loss 2.1283, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.25, nan, 0.0, 0.0, 0.6666666666666666, 0.0], recall [0.3333333333333333, nan, nan, 0.25, 0.0, nan, nan, 0.4, nan]
2019-02-19T19:43:38.642904: step 248, loss 2.18713, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, nan, 0.0, 0.0, 0.5, 0.0], recall [0.0, 0.0, nan, 0.375, 0.0, 0.0, nan, 0.3333333333333333, nan]
2019-02-19T19:43:38.791695: step 249, loss 1.72664, accuracy 0.4375, precision [0.6666666666666666, 0.0, 0.0, 0.5714285714285714, 0.3333333333333333, nan, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.6666666666666666, 0.2, nan, nan, 0.0, nan]
2019-02-19T19:43:38.942751: step 250, loss 1.84197, accuracy 0.4375, precision [0.5, 0.5, 0.0, 0.25, 1.0, nan, nan, 0.0, 0.0], recall [0.25, 1.0, nan, 0.25, 0.6666666666666666, nan, nan, nan, 0.0]

Evaluation:
[[ 45   8   0  28   1   0   0   4   0]
 [ 45  41   0  68   4   0   0   6   0]
 [  9   1   0  68  11   0   0   2   0]
 [ 38   9   0 234  15   0   0   8   0]
 [  6   1   0  92  71   0   0   1   0]
 [  2   1   0  39   6   0   0   3   0]
 [  1   0   0  18   6   0   0   1   0]
 [ 18   1   0  51   8   0   0  29   0]
 [  5   1   0  17   2   0   0   0   0]]
2019-02-19T19:43:44.754202: step 250, loss 1.74809, accuracy 0.409756, precision [0.5232558139534884, 0.25, 0.0, 0.7697368421052632, 0.4152046783625731, 0.0, 0.0, 0.27102803738317754, 0.0], recall [0.26627218934911245, 0.6507936507936508, nan, 0.3804878048780488, 0.5725806451612904, nan, nan, 0.5370370370370371, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550605378/checkpoints/model-250

2019-02-19T19:43:44.997130: step 251, loss 1.65701, accuracy 0.5625, precision [nan, 0.0, 0.0, 0.75, 0.75, 0.0, nan, nan, 0.0], recall [nan, 0.0, nan, 0.6666666666666666, 1.0, nan, nan, 0.0, 0.0]
2019-02-19T19:43:45.144554: step 252, loss 1.53194, accuracy 0.625, precision [1.0, nan, 0.0, 0.8888888888888888, 0.5, nan, 0.0, 0.0, 0.0], recall [1.0, 0.0, nan, 0.7272727272727273, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:45.291826: step 253, loss 1.81491, accuracy 0.375, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, nan, 1.0, nan], recall [nan, 0.0, nan, 0.5, 0.2, nan, nan, 0.5, nan]
2019-02-19T19:43:45.440078: step 254, loss 2.12084, accuracy 0.3125, precision [nan, 0.0, nan, 0.8, 0.0, 0.0, 0.0, 0.5, nan], recall [0.0, nan, nan, 0.3076923076923077, nan, nan, nan, 1.0, nan]
2019-02-19T19:43:45.589634: step 255, loss 1.59705, accuracy 0.5625, precision [nan, 0.3333333333333333, 0.25, 1.0, 1.0, 0.0, 0.0, nan, nan], recall [0.0, 1.0, 1.0, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:45.743407: step 256, loss 1.45319, accuracy 0.625, precision [0.3333333333333333, 1.0, 0.0, 1.0, 0.0, nan, 0.0, nan, nan], recall [1.0, 1.0, nan, 0.5384615384615384, nan, nan, nan, nan, nan]
2019-02-19T19:43:45.895795: step 257, loss 1.68791, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, nan, nan, nan], recall [nan, nan, nan, 0.3333333333333333, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:43:46.042818: step 258, loss 1.87438, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.38461538461538464, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:46.188686: step 259, loss 1.87658, accuracy 0.4375, precision [nan, 0.25, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.35714285714285715, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:46.336760: step 260, loss 2.01931, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.25, nan], recall [nan, 0.0, nan, 0.21428571428571427, nan, nan, nan, 1.0, nan]
2019-02-19T19:43:46.485619: step 261, loss 1.97458, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.6, 0.5, nan, nan, nan, nan], recall [nan, 0.5, 0.0, 0.3, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:46.639853: step 262, loss 1.95307, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:43:46.792342: step 263, loss 2.21933, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [nan, 0.5, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:43:46.946980: step 264, loss 1.71553, accuracy 0.5625, precision [0.0, 1.0, 0.5, 0.875, nan, 0.0, nan, 0.0, nan], recall [nan, 0.5, 1.0, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:47.096974: step 265, loss 1.44447, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.4166666666666667, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:47.244898: step 266, loss 1.70736, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:47.396188: step 267, loss 1.81198, accuracy 0.25, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.6666666666666666, 0.0, 0.2222222222222222, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:47.543542: step 268, loss 2.01811, accuracy 0.375, precision [0.0, nan, 0.0, 0.7142857142857143, 0.5, 0.0, nan, nan, 0.0], recall [nan, nan, nan, 0.4166666666666667, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:47.692639: step 269, loss 1.82298, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.5, 0.6666666666666666, nan, nan, 0.25, nan], recall [0.0, 0.0, 0.0, 0.5, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:43:47.840829: step 270, loss 1.81074, accuracy 0.4375, precision [0.5, 1.0, 0.0, 0.5, 0.3333333333333333, nan, 0.0, nan, nan], recall [1.0, 0.3333333333333333, 0.0, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:47.989708: step 271, loss 1.76149, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.5, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.6, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:48.148661: step 272, loss 1.96921, accuracy 0.375, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [0.6666666666666666, nan, nan, 0.3, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:48.296818: step 273, loss 2.11558, accuracy 0.25, precision [0.0, 0.25, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0], recall [0.0, 0.5, nan, 0.1111111111111111, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:43:48.450696: step 274, loss 1.64249, accuracy 0.375, precision [0.5, 0.3333333333333333, 0.0, 0.5, 0.75, nan, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.125, 0.75, nan, nan, 0.0, nan]
2019-02-19T19:43:48.598683: step 275, loss 1.81458, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 0.25, 0.0, nan, 1.0, 0.0], recall [nan, nan, nan, 0.3076923076923077, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:43:48.752506: step 276, loss 2.54868, accuracy 0.3125, precision [0.0, 1.0, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:48.899332: step 277, loss 1.62457, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.8, 0.5, nan, nan, 0.0, nan], recall [0.3333333333333333, nan, 0.0, 0.5, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:49.050669: step 278, loss 1.76559, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, nan, 0.0, 0.0, nan], recall [0.6666666666666666, 0.0, nan, 0.2222222222222222, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:43:49.200941: step 279, loss 1.50591, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, nan, nan, nan, nan], recall [nan, 1.0, 0.0, 0.42857142857142855, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:43:49.351411: step 280, loss 1.98613, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0], recall [0.0, 0.3333333333333333, nan, 0.16666666666666666, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:43:49.504178: step 281, loss 1.97137, accuracy 0.25, precision [0.0, 0.25, 1.0, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, 1.0, 0.2, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:49.655646: step 282, loss 2.0054, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.5714285714285714, nan, 0.0, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.6666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:49.806292: step 283, loss 1.87178, accuracy 0.3125, precision [0.6666666666666666, 0.0, 0.0, 0.5, nan, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.0, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:49.952171: step 284, loss 1.97539, accuracy 0.4375, precision [0.5, 0.0, 0.0, 0.75, 0.75, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.3, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:50.099488: step 285, loss 1.73468, accuracy 0.25, precision [1.0, nan, 0.0, 0.2857142857142857, 0.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.0, 0.0, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:50.252253: step 286, loss 1.82115, accuracy 0.375, precision [0.0, 0.0, 0.5, 0.4444444444444444, 1.0, nan, 0.0, nan, nan], recall [0.0, nan, 1.0, 0.5714285714285714, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:43:50.398581: step 287, loss 1.67093, accuracy 0.5, precision [nan, nan, 0.0, 0.75, 0.25, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.5454545454545454, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:43:50.553454: step 288, loss 1.80439, accuracy 0.375, precision [1.0, 0.4, nan, 0.25, 0.3333333333333333, nan, nan, 0.3333333333333333, nan], recall [0.16666666666666666, 1.0, 0.0, 0.2, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:43:50.701638: step 289, loss 1.86152, accuracy 0.5, precision [nan, 0.0, nan, 0.7777777777777778, 0.0, 0.0, nan, 1.0, nan], recall [0.0, nan, 0.0, 0.5833333333333334, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:43:50.851860: step 290, loss 2.26969, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:43:51.001362: step 291, loss 1.46356, accuracy 0.5625, precision [0.5, 0.0, nan, 1.0, 0.2, nan, nan, nan, 0.0], recall [1.0, 0.0, nan, 0.5384615384615384, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:51.152670: step 292, loss 1.46476, accuracy 0.5625, precision [0.0, 0.0, 0.0, 0.8888888888888888, 0.0, 0.0, nan, 1.0, nan], recall [nan, 0.0, nan, 0.6153846153846154, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:43:51.302477: step 293, loss 2.51639, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:43:51.452950: step 294, loss 2.29761, accuracy 0.25, precision [0.0, nan, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.3076923076923077, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:51.601415: step 295, loss 2.11349, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, 1.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:51.750750: step 296, loss 1.70152, accuracy 0.5, precision [0.5, nan, nan, 0.8333333333333334, 0.4, nan, 0.0, 0.0, 0.0], recall [1.0, 0.0, nan, 0.4166666666666667, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:51.895405: step 297, loss 2.10029, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan, 0.0], recall [0.0, nan, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:52.043823: step 298, loss 1.83291, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.5, nan], recall [nan, nan, 0.0, 0.4166666666666667, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:43:52.203015: step 299, loss 1.51022, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8, 0.75, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.36363636363636365, 0.75, nan, nan, nan, nan]
2019-02-19T19:43:52.352804: step 300, loss 1.95624, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, nan, nan, nan], recall [0.0, nan, 0.0, 0.3333333333333333, nan, nan, nan, 0.0, 0.0]
2019-02-19T19:43:52.500447: step 301, loss 1.94564, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, nan, 0.0, 0.0, 0.0, 0.0], recall [nan, nan, nan, 0.35714285714285715, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:52.656713: step 302, loss 1.69004, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 0.75, 0.0, nan, 0.0, nan, nan], recall [nan, 1.0, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:52.807075: step 303, loss 1.84255, accuracy 0.5, precision [0.0, 0.6666666666666666, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:52.960628: step 304, loss 2.09449, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, nan, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:53.112975: step 305, loss 1.7057, accuracy 0.5, precision [0.0, 0.5, 0.0, 0.75, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.25, nan, 0.6666666666666666, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:43:53.263211: step 306, loss 1.71525, accuracy 0.5, precision [0.0, 0.5, 0.0, 0.8, 1.0, nan, nan, 0.3333333333333333, 0.0], recall [nan, 1.0, 0.0, 0.5714285714285714, 0.2, nan, nan, 1.0, nan]
2019-02-19T19:43:53.412751: step 307, loss 2.06726, accuracy 0.25, precision [0.0, 0.0, nan, 0.5714285714285714, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.4, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:53.562015: step 308, loss 1.7475, accuracy 0.375, precision [nan, 0.4, 0.0, 0.75, 0.5, nan, nan, 0.0, nan], recall [nan, 0.6666666666666666, 0.0, 0.3, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:53.710871: step 309, loss 2.29443, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.5, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.25, nan, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:53.861462: step 310, loss 2.01833, accuracy 0.4375, precision [0.0, 0.0, nan, 0.7142857142857143, 0.5, nan, 0.0, nan, 0.0], recall [nan, 0.0, nan, 0.5, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:43:54.013184: step 311, loss 1.91663, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.45454545454545453, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:54.165610: step 312, loss 2.30312, accuracy 0.1875, precision [nan, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, 0.0, 0.2, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:54.317057: step 313, loss 1.86641, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.75, 0.0, 0.0, nan, nan, 0.0], recall [nan, 0.4, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:54.463169: step 314, loss 1.89238, accuracy 0.4375, precision [0.5, 0.16666666666666666, nan, 1.0, 0.0, 0.0, nan, nan, nan], recall [1.0, 1.0, 0.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:43:54.614870: step 315, loss 1.93947, accuracy 0.5, precision [0.0, 1.0, nan, 0.7142857142857143, 1.0, 0.0, nan, 1.0, 0.0], recall [nan, 0.2, 0.0, 0.7142857142857143, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:43:54.770742: step 316, loss 1.9163, accuracy 0.4375, precision [1.0, 0.5, 0.0, 0.75, 1.0, nan, nan, 0.0, 0.0], recall [1.0, 0.4, nan, 0.42857142857142855, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:54.924168: step 317, loss 1.67314, accuracy 0.5, precision [nan, 0.6, nan, 0.75, 0.25, nan, 0.0, 0.5, nan], recall [nan, 1.0, 0.0, 0.375, 1.0, nan, nan, 0.5, 0.0]
2019-02-19T19:43:55.070880: step 318, loss 1.89842, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.5555555555555556, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:55.222243: step 319, loss 1.61792, accuracy 0.375, precision [0.0, 0.6, 0.0, 0.6666666666666666, 0.5, nan, 0.0, 0.0, nan], recall [nan, 0.6, 0.0, 0.25, 1.0, nan, nan, nan, 0.0]
2019-02-19T19:43:55.373419: step 320, loss 1.87904, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, nan, nan, 0.0], recall [nan, 0.2, nan, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:55.519809: step 321, loss 2.14043, accuracy 0.25, precision [nan, 0.0, 0.0, 0.4, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.4444444444444444, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:55.668508: step 322, loss 2.11728, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3076923076923077, nan, nan, nan, 0.0, nan]
2019-02-19T19:43:55.819320: step 323, loss 2.00013, accuracy 0.375, precision [1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:55.967497: step 324, loss 1.64937, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.2, nan, 0.0, 1.0, nan], recall [nan, nan, nan, 0.38461538461538464, 1.0, nan, nan, 1.0, 0.0]
2019-02-19T19:43:56.122138: step 325, loss 2.18224, accuracy 0.1875, precision [0.0, 0.0, nan, 0.75, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:43:56.271939: step 326, loss 2.24489, accuracy 0.125, precision [0.0, 0.0, 0.0, 1.0, nan, 0.0, 0.0, 0.0, 0.0], recall [nan, nan, 0.0, 0.13333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:43:56.428600: step 327, loss 2.12641, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, 0.0, nan, nan], recall [nan, 0.0, 0.0, 0.5454545454545454, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:56.575123: step 328, loss 1.87534, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:43:56.726065: step 329, loss 1.93617, accuracy 0.375, precision [0.0, 0.2, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:43:56.875060: step 330, loss 1.95746, accuracy 0.3125, precision [nan, 0.25, 0.0, 0.75, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.25, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:43:57.028901: step 331, loss 1.91198, accuracy 0.5, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.6, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.45454545454545453, 0.75, nan, nan, 0.0, nan]
2019-02-19T19:43:57.173709: step 332, loss 1.79327, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.5555555555555556, 1.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.5, 0.5, nan, 0.0, nan, 0.0]
2019-02-19T19:43:57.329140: step 333, loss 1.77136, accuracy 0.375, precision [0.0, nan, nan, 0.5, 0.6, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.75, 0.6, nan, 0.0, 0.0, nan]
2019-02-19T19:43:57.478392: step 334, loss 2.02914, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.42857142857142855, 1.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.5, 0.0, 0.5, 0.2, nan, nan, nan, nan]
2019-02-19T19:43:57.628175: step 335, loss 1.84359, accuracy 0.25, precision [nan, 0.2, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:43:57.778886: step 336, loss 1.84769, accuracy 0.4375, precision [nan, 0.0, nan, 0.8571428571428571, 0.5, 0.0, nan, nan, nan], recall [nan, 0.0, 0.0, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:57.930804: step 337, loss 2.14633, accuracy 0.5, precision [0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, nan], recall [nan, nan, 1.0, 0.3333333333333333, 0.6, nan, nan, 1.0, nan]
2019-02-19T19:43:58.082354: step 338, loss 2.00834, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.2857142857142857, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:58.235479: step 339, loss 2.40416, accuracy 0.25, precision [0.0, nan, 0.0, 0.8, nan, 0.0, 0.0, 0.0, 0.0], recall [0.0, nan, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:58.381607: step 340, loss 1.19897, accuracy 0.6875, precision [0.0, 0.75, nan, 1.0, 1.0, 0.0, nan, 1.0, nan], recall [nan, 0.75, nan, 0.5555555555555556, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:43:58.530702: step 341, loss 2.08762, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:43:58.679175: step 342, loss 1.90445, accuracy 0.375, precision [nan, 0.25, 0.0, 0.8, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.36363636363636365, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:58.833648: step 343, loss 2.11082, accuracy 0.5625, precision [0.0, 1.0, nan, 0.875, 0.0, 0.0, nan, 1.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.7, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:43:58.982477: step 344, loss 1.45029, accuracy 0.5625, precision [1.0, 0.75, nan, 0.75, 0.25, 0.0, 0.0, 1.0, nan], recall [1.0, 0.75, nan, 0.375, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:43:59.132857: step 345, loss 1.64596, accuracy 0.4375, precision [0.5, 0.6666666666666666, 0.0, 0.5, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.375, 0.2, nan, nan, nan, nan]
2019-02-19T19:43:59.291038: step 346, loss 1.92502, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.2, 0.0, 0.0, 0.0, nan], recall [0.0, 0.25, nan, 0.2, 1.0, nan, nan, nan, nan]
2019-02-19T19:43:59.445589: step 347, loss 1.6616, accuracy 0.5625, precision [0.0, 0.5, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.5833333333333334, 0.5, nan, nan, nan, nan]
2019-02-19T19:43:59.591061: step 348, loss 1.77514, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.5, nan], recall [0.0, 0.0, nan, 0.4166666666666667, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:43:59.741730: step 349, loss 1.8507, accuracy 0.3125, precision [0.3333333333333333, 0.25, 0.0, 1.0, 1.0, nan, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.2, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:43:59.894558: step 350, loss 1.91392, accuracy 0.5, precision [1.0, 0.3333333333333333, nan, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 0.5, nan, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:44:00.048844: step 351, loss 2.32257, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:00.200668: step 352, loss 2.12815, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.25, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.15384615384615385, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:00.350645: step 353, loss 1.46747, accuracy 0.5625, precision [nan, 0.6666666666666666, 0.0, 0.7, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.6666666666666666, nan, 0.7, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:00.500408: step 354, loss 1.839, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.5, nan], recall [nan, 1.0, nan, 0.4166666666666667, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:44:00.650766: step 355, loss 1.9471, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0], recall [nan, nan, nan, 0.45454545454545453, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:00.798783: step 356, loss 1.90626, accuracy 0.5, precision [nan, 0.5, 0.0, 1.0, 0.5, nan, nan, 0.3333333333333333, 0.0], recall [0.0, 1.0, nan, 0.4444444444444444, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:44:00.948397: step 357, loss 1.64744, accuracy 0.4375, precision [0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.0, 0.5714285714285714, 0.0, nan, 0.0, nan]
2019-02-19T19:44:01.102484: step 358, loss 1.9315, accuracy 0.3125, precision [0.0, 0.25, 0.0, 1.0, 0.6666666666666666, nan, 0.0, 0.5, nan], recall [nan, 0.3333333333333333, nan, 0.14285714285714285, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:44:01.252360: step 359, loss 1.67272, accuracy 0.5, precision [1.0, 1.0, 0.25, 0.6, nan, 0.0, 0.0, 1.0, 0.0], recall [1.0, 0.5, 1.0, 0.42857142857142855, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:44:01.404036: step 360, loss 1.93619, accuracy 0.375, precision [0.0, 0.6, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.6, nan, 0.375, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:01.552584: step 361, loss 1.96437, accuracy 0.4375, precision [0.5, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.5, nan], recall [0.3333333333333333, 0.6666666666666666, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:44:01.701609: step 362, loss 1.77808, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.5, 0.4, nan, nan, 0.0, 0.0], recall [0.0, 0.25, 0.0, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:01.851249: step 363, loss 2.13309, accuracy 0.25, precision [0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, nan, 0.3333333333333333, 0.0], recall [nan, 0.5, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, nan]
2019-02-19T19:44:02.001301: step 364, loss 1.68375, accuracy 0.375, precision [nan, 1.0, 0.0, 0.5555555555555556, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:02.151719: step 365, loss 2.05249, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.7142857142857143, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.25, nan, 0.5555555555555556, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:02.296372: step 366, loss 1.66827, accuracy 0.375, precision [0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, nan], recall [nan, 0.3333333333333333, 0.5, 0.0, 1.0, nan, 0.0, 1.0, nan]
2019-02-19T19:44:02.444542: step 367, loss 1.7513, accuracy 0.5625, precision [0.3333333333333333, 1.0, 0.0, 0.8571428571428571, 0.0, nan, nan, 0.0, 0.0], recall [1.0, 0.5, nan, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:02.597730: step 368, loss 2.06148, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.25, 0.0, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:02.751438: step 369, loss 2.06112, accuracy 0.3125, precision [0.0, 0.0, nan, 0.5555555555555556, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 0.0, 0.0, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:02.900641: step 370, loss 1.48805, accuracy 0.625, precision [1.0, 0.6, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.625, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:03.049031: step 371, loss 1.85896, accuracy 0.375, precision [0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 0.2, nan, 0.0, nan, nan], recall [0.5, 1.0, nan, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:03.200681: step 372, loss 1.48485, accuracy 0.5625, precision [nan, 0.75, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:03.349632: step 373, loss 1.78523, accuracy 0.4375, precision [nan, 1.0, 0.0, 1.0, 0.2, 0.0, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:03.502300: step 374, loss 1.51317, accuracy 0.4375, precision [nan, 0.0, 0.3333333333333333, 0.7142857142857143, 0.5, 0.0, 0.0, nan, nan], recall [0.0, nan, 1.0, 0.4166666666666667, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:03.651046: step 375, loss 1.51454, accuracy 0.5625, precision [0.3333333333333333, nan, 0.0, 0.8, 0.75, nan, 0.0, 1.0, nan], recall [1.0, 0.0, nan, 0.4444444444444444, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:44:03.800270: step 376, loss 1.64725, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.5, 0.0, nan, nan, 1.0, nan], recall [0.0, 0.5, nan, 0.18181818181818182, nan, nan, nan, 1.0, nan]
2019-02-19T19:44:03.948284: step 377, loss 1.73806, accuracy 0.4375, precision [nan, 0.2, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.42857142857142855, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:04.096250: step 378, loss 2.52607, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.5, 0.0, 0.16666666666666666, nan, nan, nan, 0.0, nan]
2019-02-19T19:44:04.247767: step 379, loss 1.98605, accuracy 0.375, precision [0.4, 0.0, nan, 1.0, 0.25, 0.0, nan, 0.0, nan], recall [1.0, 0.0, 0.0, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:04.401646: step 380, loss 2.04592, accuracy 0.3125, precision [0.0, 0.5, nan, 0.5, 0.5, 0.0, 0.0, 1.0, nan], recall [nan, 0.6666666666666666, nan, 0.14285714285714285, 0.2, nan, nan, 1.0, nan]
2019-02-19T19:44:04.552270: step 381, loss 1.57298, accuracy 0.5, precision [0.6666666666666666, nan, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.5, nan, nan, nan, 0.0, nan]
2019-02-19T19:44:04.701938: step 382, loss 1.44112, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.5714285714285714, 0.3333333333333333, nan, nan, nan, nan], recall [nan, 0.3333333333333333, nan, 0.5, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:44:04.849717: step 383, loss 1.75839, accuracy 0.375, precision [nan, 0.0, 0.0, 0.625, 0.25, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5555555555555556, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:05.000869: step 384, loss 1.67597, accuracy 0.5, precision [0.5, 0.6666666666666666, nan, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [0.5, 0.6666666666666666, nan, 0.45454545454545453, nan, nan, nan, nan, nan]
2019-02-19T19:44:05.149273: step 385, loss 2.10211, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.5, 0.6666666666666666, 0.0, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.2, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:05.299844: step 386, loss 2.03151, accuracy 0.375, precision [0.0, 0.25, 1.0, 0.75, 0.0, 0.0, nan, 0.5, 0.0], recall [0.0, 0.3333333333333333, 1.0, 0.5, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:44:05.452397: step 387, loss 1.76926, accuracy 0.4375, precision [nan, 0.5, nan, 0.75, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, 0.6, nan, 0.42857142857142855, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:05.606788: step 388, loss 2.23168, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, nan], recall [0.0, 0.3333333333333333, nan, 0.1111111111111111, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:05.753894: step 389, loss 1.73414, accuracy 0.4375, precision [0.3333333333333333, 0.5, 0.0, 0.8, 0.25, nan, nan, 0.0, nan], recall [0.5, 0.3333333333333333, nan, 0.5, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:05.901825: step 390, loss 2.06909, accuracy 0.1875, precision [0.25, nan, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:06.055308: step 391, loss 1.62956, accuracy 0.4375, precision [1.0, nan, 1.0, 0.5714285714285714, 0.0, nan, 0.0, 0.0, 0.0], recall [0.6666666666666666, 0.0, 1.0, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:06.206365: step 392, loss 2.11584, accuracy 0.3125, precision [0.3333333333333333, 0.0, 0.0, 0.8, 0.0, 0.0, nan, 0.0, 0.0], recall [0.5, 0.0, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:06.356961: step 393, loss 2.12225, accuracy 0.3125, precision [nan, 0.5, nan, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0], recall [0.0, 1.0, 0.0, 0.4, 0.16666666666666666, 0.0, nan, nan, nan]
2019-02-19T19:44:06.506545: step 394, loss 1.89337, accuracy 0.125, precision [0.0, nan, 0.0, 0.0, 0.3333333333333333, nan, 0.0, 0.0, 0.0], recall [0.0, nan, 0.0, 0.0, 0.4, 0.0, nan, 0.0, nan]
2019-02-19T19:44:06.656381: step 395, loss 1.76002, accuracy 0.5, precision [0.6, nan, 0.0, 0.8, 1.0, nan, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.6666666666666666, 0.25, nan, nan, 0.0, 0.0]
2019-02-19T19:44:06.809430: step 396, loss 1.68081, accuracy 0.375, precision [0.0, 0.2, nan, 0.7142857142857143, 0.0, nan, 0.0, nan, nan], recall [0.0, 0.5, 0.0, 0.625, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:06.956696: step 397, loss 1.6468, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.7777777777777778, nan, nan, 0.0, 1.0, nan], recall [0.0, nan, nan, 0.5833333333333334, 0.0, nan, nan, 1.0, 0.0]
2019-02-19T19:44:07.109398: step 398, loss 1.478, accuracy 0.5, precision [nan, nan, 0.0, 0.75, 0.3333333333333333, nan, nan, 1.0, 0.0], recall [0.0, nan, nan, 0.6, 0.5, nan, nan, 0.5, 0.0]
2019-02-19T19:44:07.257581: step 399, loss 1.74684, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.6666666666666666, 0.5, nan, nan, 0.0, nan], recall [0.5, nan, nan, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:07.404442: step 400, loss 1.41618, accuracy 0.5625, precision [1.0, 0.6666666666666666, 0.0, 0.8, 0.5, nan, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.4444444444444444, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:07.556492: step 401, loss 1.61407, accuracy 0.5, precision [0.5, 0.0, nan, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, nan, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:07.706756: step 402, loss 2.11087, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.0, nan, 0.0, nan, nan], recall [0.0, nan, nan, 0.1, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:07.860430: step 403, loss 1.8027, accuracy 0.3125, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:08.007007: step 404, loss 1.78759, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, nan], recall [0.0, 1.0, nan, 0.4444444444444444, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:44:08.155183: step 405, loss 2.2657, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.2727272727272727, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:08.309839: step 406, loss 1.53019, accuracy 0.6875, precision [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, nan, 0.5, 0.0], recall [1.0, 1.0, nan, 0.6363636363636364, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:08.456388: step 407, loss 1.43902, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.7777777777777778, nan, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, 0.0, 0.6363636363636364, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:08.607539: step 408, loss 1.60676, accuracy 0.5625, precision [nan, 0.5, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:08.758905: step 409, loss 1.76504, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.75, 0.3333333333333333, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:44:08.907392: step 410, loss 2.69024, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.6, nan, 0.0, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.42857142857142855, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:09.054570: step 411, loss 2.39626, accuracy 0.375, precision [0.75, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, nan, 0.0, 0.0], recall [0.5, 0.0, 1.0, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:44:09.210325: step 412, loss 1.34, accuracy 0.625, precision [nan, 0.3333333333333333, 0.0, 0.875, 0.5, nan, nan, 1.0, nan], recall [nan, 1.0, nan, 0.5833333333333334, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:44:09.358064: step 413, loss 2.11113, accuracy 0.125, precision [1.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.16666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:09.511380: step 414, loss 1.25707, accuracy 0.5625, precision [0.0, 0.5, nan, 0.5, 0.5, nan, nan, 1.0, nan], recall [0.0, 0.6666666666666666, nan, 1.0, 0.5, nan, nan, 0.6, nan]
2019-02-19T19:44:09.659606: step 415, loss 1.52372, accuracy 0.5625, precision [nan, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, nan, 0.0, nan], recall [0.0, nan, 1.0, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:09.805278: step 416, loss 1.66311, accuracy 0.4375, precision [0.0, 0.0, nan, 0.8, 0.0, nan, nan, 0.6, nan], recall [0.0, 0.0, nan, 0.5, nan, nan, nan, 0.6, nan]
2019-02-19T19:44:09.952027: step 417, loss 1.85424, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.3333333333333333, nan, 0.0, 0.5, nan], recall [nan, 0.5, 0.0, 0.16666666666666666, 0.5, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:44:10.097759: step 418, loss 1.83151, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.75, 0.6666666666666666, 0.0, 0.0, 1.0, nan], recall [nan, 0.0, nan, 0.3, 1.0, nan, 0.0, 1.0, nan]
2019-02-19T19:44:10.246601: step 419, loss 2.32686, accuracy 0.3125, precision [0.3333333333333333, 1.0, 0.0, 0.5, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.4, nan, 0.14285714285714285, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:10.394769: step 420, loss 1.56564, accuracy 0.5625, precision [0.0, 0.3333333333333333, nan, 0.8333333333333334, 0.3333333333333333, nan, nan, 1.0, nan], recall [nan, 0.3333333333333333, nan, 0.5555555555555556, 1.0, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:44:10.541636: step 421, loss 2.06166, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.3333333333333333, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:10.692394: step 422, loss 1.86585, accuracy 0.4375, precision [0.25, 0.3333333333333333, nan, 0.6, 1.0, nan, nan, nan, 0.0], recall [1.0, 0.25, nan, 0.42857142857142855, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:10.847201: step 423, loss 2.25321, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6, 0.5, 0.0, nan, 0.5, 0.0], recall [0.0, 0.0, nan, 0.42857142857142855, 0.25, nan, nan, 1.0, nan]
2019-02-19T19:44:11.003764: step 424, loss 1.54339, accuracy 0.5, precision [0.5, 0.5, 0.0, 0.7142857142857143, 0.0, nan, nan, 1.0, nan], recall [1.0, 0.3333333333333333, nan, 0.5555555555555556, nan, 0.0, nan, 0.5, nan]
2019-02-19T19:44:11.158072: step 425, loss 1.38173, accuracy 0.5, precision [0.0, 1.0, nan, 0.5714285714285714, 0.3333333333333333, nan, nan, 1.0, nan], recall [nan, 0.5, nan, 0.5, 0.3333333333333333, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:44:11.304466: step 426, loss 2.2562, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.5, nan, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:11.451314: step 427, loss 1.98759, accuracy 0.375, precision [0.5, 0.5, 0.0, 0.75, 0.25, nan, nan, 0.0, 0.0], recall [1.0, 1.0, nan, 0.2727272727272727, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:11.605415: step 428, loss 2.15024, accuracy 0.25, precision [0.0, nan, 0.0, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:11.754875: step 429, loss 1.91873, accuracy 0.5, precision [0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, nan, nan, nan], recall [nan, 1.0, nan, 0.36363636363636365, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:11.907452: step 430, loss 1.49523, accuracy 0.5625, precision [1.0, 0.3333333333333333, nan, 1.0, 0.0, 0.0, nan, 0.5, nan], recall [0.6666666666666666, 0.5, nan, 0.4444444444444444, nan, nan, nan, 1.0, nan]
2019-02-19T19:44:12.057722: step 431, loss 1.68021, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.5, 1.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, nan, nan, 0.5, 0.16666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:12.214222: step 432, loss 1.73178, accuracy 0.4375, precision [0.0, 0.3333333333333333, nan, 0.8333333333333334, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [0.0, 1.0, nan, 0.5, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:12.362526: step 433, loss 1.58996, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.8571428571428571, 0.25, nan, nan, 0.0, nan], recall [0.3333333333333333, nan, nan, 0.5454545454545454, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:12.511525: step 434, loss 1.51432, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.6, 0.5, 0.0, nan, 1.0, nan], recall [1.0, nan, 0.0, 0.3333333333333333, 0.3333333333333333, nan, nan, 0.5, nan]
2019-02-19T19:44:12.658895: step 435, loss 1.81773, accuracy 0.375, precision [nan, 0.3333333333333333, nan, 0.75, 0.2, 0.0, nan, nan, nan], recall [0.0, 0.6666666666666666, nan, 0.3, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:12.811346: step 436, loss 1.99831, accuracy 0.4375, precision [0.5, 0.5, 0.0, 0.7142857142857143, nan, 0.0, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.45454545454545453, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:12.963193: step 437, loss 2.24706, accuracy 0.1875, precision [1.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, 0.0], recall [0.6666666666666666, 0.0, 0.0, 0.1111111111111111, nan, nan, nan, nan, nan]
2019-02-19T19:44:13.115832: step 438, loss 1.79853, accuracy 0.25, precision [0.0, 0.25, 0.0, 0.6666666666666666, 1.0, nan, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.2, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:13.269561: step 439, loss 1.76841, accuracy 0.3125, precision [1.0, 1.0, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:13.421538: step 440, loss 1.86939, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.36363636363636365, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:13.571113: step 441, loss 1.62205, accuracy 0.3125, precision [nan, nan, 0.3333333333333333, 0.5, 0.5, 0.0, nan, 0.0, nan], recall [0.0, nan, 0.5, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:13.723600: step 442, loss 1.57913, accuracy 0.4375, precision [0.0, 0.2, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.45454545454545453, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:13.873145: step 443, loss 1.85955, accuracy 0.3125, precision [0.3333333333333333, 0.5, 0.0, 1.0, 0.5, nan, nan, 0.0, 0.0], recall [1.0, 0.3333333333333333, nan, 0.1111111111111111, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:14.029313: step 444, loss 1.55927, accuracy 0.5, precision [0.3333333333333333, 0.0, nan, 0.6666666666666666, 0.5, nan, nan, 0.5, nan], recall [1.0, 0.0, nan, 0.5, 0.6666666666666666, nan, nan, 0.5, nan]
2019-02-19T19:44:14.179084: step 445, loss 2.20956, accuracy 0.375, precision [nan, 1.0, 0.0, 0.75, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.3, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:14.329525: step 446, loss 1.67856, accuracy 0.5, precision [0.0, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.5, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:14.484033: step 447, loss 1.5152, accuracy 0.5, precision [1.0, 1.0, 0.0, 0.6, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.5, 0.6666666666666666, nan, 0.42857142857142855, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:14.637343: step 448, loss 1.78491, accuracy 0.4375, precision [nan, 0.25, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, 0.0, 0.6, 0.0, nan, 0.0, nan, nan]
2019-02-19T19:44:14.789074: step 449, loss 1.72473, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 1.0, nan, 0.0, 0.0, nan], recall [nan, nan, 0.0, 0.5, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:14.936766: step 450, loss 1.49304, accuracy 0.5625, precision [nan, 0.0, 0.3333333333333333, 0.75, 1.0, nan, nan, 0.5, nan], recall [nan, nan, 1.0, 0.5454545454545454, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:44:15.091030: step 451, loss 1.42925, accuracy 0.5, precision [0.0, 0.0, 0.5, 0.8571428571428571, 0.25, 0.0, nan, nan, nan], recall [nan, 0.0, 1.0, 0.6, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:15.239672: step 452, loss 1.77794, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.4, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:15.388512: step 453, loss 1.92496, accuracy 0.375, precision [nan, 0.0, 0.3333333333333333, 0.8, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 0.0, 1.0, 0.36363636363636365, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:15.537094: step 454, loss 1.85933, accuracy 0.5625, precision [0.0, 0.0, nan, 0.8888888888888888, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.7272727272727273, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:15.687616: step 455, loss 1.92449, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 1.0, nan, nan, 0.0, 0.0], recall [0.0, 1.0, nan, 0.18181818181818182, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:15.836689: step 456, loss 2.3266, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:15.986403: step 457, loss 1.76038, accuracy 0.3125, precision [0.5, nan, 0.0, 0.42857142857142855, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [1.0, 0.0, 0.0, 0.375, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:16.140275: step 458, loss 2.16888, accuracy 0.375, precision [0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.25, 0.5, 0.2857142857142857, nan, nan, nan, 0.0, nan]
2019-02-19T19:44:16.289335: step 459, loss 1.52272, accuracy 0.4375, precision [0.0, 0.6666666666666666, 0.0, 0.75, 0.0, nan, nan, 1.0, 0.0], recall [0.0, 1.0, nan, 0.3, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:44:16.441931: step 460, loss 1.8248, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:16.590635: step 461, loss 2.07442, accuracy 0.25, precision [0.5, 0.2, 0.0, 1.0, nan, 0.0, 0.0, 0.0, 0.0], recall [0.5, 1.0, 0.0, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:16.738843: step 462, loss 1.77189, accuracy 0.25, precision [0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.2, nan, nan, 0.0, nan], recall [0.0, 0.0, 1.0, 0.18181818181818182, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:16.884997: step 463, loss 1.73387, accuracy 0.4375, precision [nan, 0.25, 1.0, 1.0, 0.25, 0.0, nan, 0.3333333333333333, nan], recall [nan, 1.0, 1.0, 0.2727272727272727, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:17.034767: step 464, loss 1.47681, accuracy 0.375, precision [0.3333333333333333, 1.0, 0.0, 0.42857142857142855, 0.5, 0.0, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.42857142857142855, 0.25, nan, nan, 0.0, 0.0]
2019-02-19T19:44:17.183262: step 465, loss 1.68229, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.6, 1.0, 0.0, nan, nan, nan], recall [0.5, 0.0, nan, 0.6, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:17.332564: step 466, loss 1.62375, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 0.75, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.5714285714285714, 0.6, nan, nan, 0.5, nan]
2019-02-19T19:44:17.483756: step 467, loss 1.78697, accuracy 0.375, precision [0.0, nan, 0.0, 0.6, 0.6, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.3, 0.75, nan, nan, nan, nan]
2019-02-19T19:44:17.633483: step 468, loss 1.48361, accuracy 0.5625, precision [0.5, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.6666666666666666, nan, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:17.784542: step 469, loss 2.36006, accuracy 0.25, precision [nan, 0.0, 0.0, 0.75, 0.5, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:17.931980: step 470, loss 1.77427, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.5, nan], recall [nan, nan, nan, 0.18181818181818182, 0.3333333333333333, nan, nan, 0.5, nan]
2019-02-19T19:44:18.085962: step 471, loss 1.98066, accuracy 0.125, precision [0.5, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, 0.0], recall [0.5, 0.0, 0.0, 0.125, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:18.233856: step 472, loss 1.45509, accuracy 0.5625, precision [nan, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.6, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:44:18.387542: step 473, loss 1.58079, accuracy 0.4375, precision [0.0, 0.0, nan, 0.8333333333333334, 1.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.5, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:18.541301: step 474, loss 1.97189, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, 0.0, 0.42857142857142855, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:18.688476: step 475, loss 1.48478, accuracy 0.5625, precision [0.0, 1.0, 0.0, 1.0, 0.4, nan, nan, nan, nan], recall [nan, 1.0, nan, 0.75, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:18.837119: step 476, loss 2.18554, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.16666666666666666, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:18.990676: step 477, loss 2.07182, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, nan, nan, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.2, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:19.138750: step 478, loss 1.99776, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.75, 0.75, nan, nan, 0.2, 0.0], recall [nan, 0.0, nan, 0.375, 0.75, nan, nan, 0.5, nan]
2019-02-19T19:44:19.287863: step 479, loss 1.81317, accuracy 0.3125, precision [0.5, 0.0, 0.3333333333333333, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, nan, 0.5, 0.3, nan, nan, 0.0, 0.0, nan]
2019-02-19T19:44:19.436916: step 480, loss 1.94281, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, nan, 0.0], recall [nan, nan, 0.0, 0.3333333333333333, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:19.586055: step 481, loss 2.18891, accuracy 0.25, precision [0.0, 0.25, nan, 0.6666666666666666, 0.2, 0.0, nan, nan, 0.0], recall [0.0, 1.0, 0.0, 0.18181818181818182, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:19.735671: step 482, loss 1.85075, accuracy 0.375, precision [0.5, nan, 0.0, 0.8, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [1.0, nan, 0.0, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:19.885770: step 483, loss 1.6973, accuracy 0.4375, precision [nan, 0.75, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.75, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:20.031921: step 484, loss 2.03265, accuracy 0.25, precision [0.0, nan, 0.0, 0.6666666666666666, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:20.184036: step 485, loss 2.19879, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:44:20.332278: step 486, loss 2.17108, accuracy 0.1875, precision [nan, 1.0, 0.0, 0.5, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.5, 0.0, 0.2857142857142857, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:20.481459: step 487, loss 1.72474, accuracy 0.5625, precision [0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, nan], recall [nan, 1.0, nan, 0.25, 0.8, nan, nan, 1.0, nan]
2019-02-19T19:44:20.628380: step 488, loss 1.95478, accuracy 0.375, precision [nan, 0.0, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.5, nan], recall [nan, nan, nan, 0.4444444444444444, 0.3333333333333333, nan, 0.0, 0.3333333333333333, nan]
2019-02-19T19:44:20.778432: step 489, loss 1.6182, accuracy 0.4375, precision [1.0, 0.25, 0.0, 0.5714285714285714, 0.0, nan, nan, 1.0, nan], recall [1.0, 1.0, nan, 0.5, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:44:20.926503: step 490, loss 2.04914, accuracy 0.25, precision [0.0, 0.0, nan, 0.42857142857142855, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:21.075662: step 491, loss 1.83538, accuracy 0.375, precision [1.0, nan, 0.0, 0.4444444444444444, 0.5, 0.0, nan, 0.0, 0.0], recall [0.5, nan, nan, 0.5714285714285714, 0.14285714285714285, nan, nan, nan, nan]
2019-02-19T19:44:21.225117: step 492, loss 1.89137, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, nan, nan, nan], recall [0.0, 0.3333333333333333, nan, 0.25, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:21.376807: step 493, loss 1.67012, accuracy 0.5, precision [0.5, 0.0, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [1.0, nan, nan, 0.5555555555555556, 0.4, nan, nan, 0.0, nan]
2019-02-19T19:44:21.528028: step 494, loss 1.54802, accuracy 0.5, precision [0.0, nan, 0.25, 1.0, 0.5, nan, nan, 1.0, 0.0], recall [nan, nan, 1.0, 0.3, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:44:21.676839: step 495, loss 1.51313, accuracy 0.4375, precision [nan, nan, 0.0, 0.8571428571428571, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.5, 0.5, nan, 0.0, nan, nan]
2019-02-19T19:44:21.829100: step 496, loss 1.68676, accuracy 0.375, precision [nan, 0.25, 0.0, 0.75, 0.0, 0.0, nan, 0.6666666666666666, 0.0], recall [0.0, 1.0, nan, 0.375, 0.0, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:44:21.983422: step 497, loss 1.50538, accuracy 0.5, precision [0.5, 0.6666666666666666, 0.0, 1.0, 1.0, nan, 0.0, 0.0, 0.0], recall [1.0, 0.6666666666666666, 0.0, 0.4444444444444444, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:22.132683: step 498, loss 1.63592, accuracy 0.5625, precision [0.0, 0.5, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.5833333333333334, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:22.284158: step 499, loss 1.8519, accuracy 0.5, precision [0.5, nan, nan, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0], recall [1.0, nan, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:22.435920: step 500, loss 1.81902, accuracy 0.5, precision [nan, 0.5, 0.0, 1.0, 0.5, 0.0, nan, nan, 0.0], recall [nan, 1.0, nan, 0.36363636363636365, 1.0, nan, nan, 0.0, nan]

Evaluation:
[[ 17  20   0  49   0   0   0   0   0]
 [  2  63   0  97   1   0   0   1   0]
 [  0   2   0  88   1   0   0   0   0]
 [  0  13   0 288   3   0   0   0   0]
 [  1   1   0 115  54   0   0   0   0]
 [  1   2   0  46   1   0   0   1   0]
 [  1   0   0  24   0   0   0   1   0]
 [  3   5   0  76   1   0   0  22   0]
 [  1   2   0  21   1   0   0   0   0]]
2019-02-19T19:44:24.868905: step 500, loss 1.66425, accuracy 0.433171, precision [0.19767441860465115, 0.38414634146341464, 0.0, 0.9473684210526315, 0.3157894736842105, 0.0, 0.0, 0.205607476635514, 0.0], recall [0.6538461538461539, 0.5833333333333334, nan, 0.3582089552238806, 0.8709677419354839, nan, nan, 0.88, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550605378/checkpoints/model-500

2019-02-19T19:44:25.101805: step 501, loss 2.15684, accuracy 0.375, precision [nan, 0.25, 0.0, 1.0, 0.25, 0.0, 0.0, nan, nan], recall [nan, 0.3333333333333333, 0.0, 0.36363636363636365, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:25.252080: step 502, loss 1.8079, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, 0.5, nan, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:25.402056: step 503, loss 1.89067, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.8, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:44:25.552191: step 504, loss 1.58053, accuracy 0.5625, precision [1.0, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, nan, nan, nan, 0.0], recall [1.0, 1.0, nan, 0.5454545454545454, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:25.700252: step 505, loss 1.90723, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:25.847234: step 506, loss 1.47547, accuracy 0.3125, precision [0.0, 0.75, 0.0, 0.3333333333333333, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.75, 0.0, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:25.996589: step 507, loss 2.42077, accuracy 0.25, precision [0.0, nan, 0.0, 1.0, nan, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.26666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:26.148843: step 508, loss 1.39178, accuracy 0.5625, precision [0.5, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.5, 1.0, 0.0, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:26.307166: step 509, loss 2.23311, accuracy 0.25, precision [0.4, 0.5, 0.0, 0.3333333333333333, nan, 0.0, 0.0, nan, nan], recall [1.0, 0.3333333333333333, 0.0, 0.125, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:26.456631: step 510, loss 1.97114, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.25, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.23076923076923078, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:26.602009: step 511, loss 1.95056, accuracy 0.3125, precision [0.0, 0.3333333333333333, nan, 0.6666666666666666, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.4, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:26.750309: step 512, loss 2.25676, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0], recall [0.0, nan, nan, 0.14285714285714285, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:26.905214: step 513, loss 2.20684, accuracy 0.125, precision [0.0, nan, 0.0, 0.0, 0.3333333333333333, 0.0, nan, 0.3333333333333333, nan], recall [nan, 0.0, nan, 0.0, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:27.052675: step 514, loss 1.83414, accuracy 0.4375, precision [0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 0.3333333333333333, 0.0, 0.5, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:27.200682: step 515, loss 2.11359, accuracy 0.375, precision [nan, 0.5, nan, 0.5714285714285714, 1.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.25, 0.0, 0.5714285714285714, 0.3333333333333333, 0.0, nan, nan, nan]
2019-02-19T19:44:27.356371: step 516, loss 1.60804, accuracy 0.4375, precision [1.0, 0.5, 0.0, 0.5, 1.0, 0.0, nan, 0.3333333333333333, nan], recall [0.25, 1.0, nan, 0.3333333333333333, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:27.510884: step 517, loss 1.6786, accuracy 0.4375, precision [0.0, 0.25, 0.0, 0.6666666666666666, 1.0, nan, nan, 0.0, nan], recall [0.0, 0.5, 0.0, 0.5714285714285714, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:27.665661: step 518, loss 1.67726, accuracy 0.4375, precision [1.0, 0.0, nan, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [0.3333333333333333, nan, nan, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:27.810325: step 519, loss 1.84803, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:27.962178: step 520, loss 1.53472, accuracy 0.5625, precision [0.3333333333333333, 0.6666666666666666, 1.0, 0.75, 1.0, nan, nan, 0.0, nan], recall [1.0, 0.5, 1.0, 0.5, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:28.114267: step 521, loss 2.22964, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.5, 0.0, nan, 0.0, 0.2, 0.0], recall [0.0, 0.5, nan, 0.18181818181818182, nan, nan, nan, 0.5, nan]
2019-02-19T19:44:28.264211: step 522, loss 1.73191, accuracy 0.4375, precision [0.5, 0.25, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0], recall [1.0, 0.5, 1.0, 0.2, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:28.413395: step 523, loss 1.92516, accuracy 0.3125, precision [0.0, 0.25, 0.0, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.5, 0.0, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:28.562840: step 524, loss 1.86472, accuracy 0.5, precision [nan, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.3333333333333333, nan], recall [0.0, nan, nan, 0.46153846153846156, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:28.711894: step 525, loss 2.05336, accuracy 0.4375, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, nan, nan], recall [1.0, 0.0, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:28.861901: step 526, loss 1.76571, accuracy 0.4375, precision [nan, nan, 0.0, 0.8333333333333334, 0.6666666666666666, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.4166666666666667, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:29.010477: step 527, loss 1.98853, accuracy 0.3125, precision [1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.6666666666666666, 0.0, 0.2222222222222222, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:29.164068: step 528, loss 2.13699, accuracy 0.25, precision [0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.0, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:44:29.318369: step 529, loss 1.73488, accuracy 0.5, precision [1.0, 0.5, 0.0, 0.8, 0.5, 0.0, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:29.464964: step 530, loss 1.73327, accuracy 0.5625, precision [nan, 0.0, nan, 1.0, 0.5, 0.0, nan, nan, 0.0], recall [nan, nan, nan, 0.5384615384615384, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:29.611427: step 531, loss 1.84291, accuracy 0.5, precision [0.0, 0.0, nan, 1.0, 0.75, 0.0, nan, 0.5, 0.0], recall [0.0, nan, nan, 0.36363636363636365, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:29.762599: step 532, loss 1.81145, accuracy 0.4375, precision [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.0, nan, 0.4444444444444444, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:44:29.911779: step 533, loss 1.93627, accuracy 0.25, precision [nan, 0.2, 0.0, 0.6666666666666666, 0.16666666666666666, nan, nan, nan, nan], recall [nan, 1.0, nan, 0.16666666666666666, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:30.059725: step 534, loss 1.85945, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.6, 1.0, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:30.211627: step 535, loss 1.65417, accuracy 0.5, precision [nan, 0.0, 0.5, 0.7142857142857143, 1.0, 0.0, nan, 1.0, 0.0], recall [0.0, 0.0, 1.0, 0.5, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:30.358222: step 536, loss 1.72069, accuracy 0.4375, precision [nan, 0.0, nan, 1.0, 0.6, nan, 0.0, 0.75, nan], recall [nan, nan, nan, 0.2, 0.5, nan, nan, 0.6, nan]
2019-02-19T19:44:30.509680: step 537, loss 2.22166, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:30.658224: step 538, loss 1.35383, accuracy 0.625, precision [1.0, 0.5, nan, 0.7142857142857143, 0.5, nan, nan, 0.5, nan], recall [1.0, 0.5, nan, 0.5555555555555556, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:44:30.804914: step 539, loss 1.95233, accuracy 0.4375, precision [0.0, nan, 0.0, 0.7142857142857143, 1.0, 0.0, 0.0, 0.5, nan], recall [nan, nan, nan, 0.5555555555555556, 0.16666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:44:30.953761: step 540, loss 2.25136, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:31.105441: step 541, loss 1.6674, accuracy 0.4375, precision [0.5, 0.0, 0.0, 0.75, 0.5, 0.0, nan, 0.3333333333333333, nan], recall [1.0, nan, nan, 0.42857142857142855, 0.3333333333333333, nan, nan, 1.0, 0.0]
2019-02-19T19:44:31.254524: step 542, loss 1.68129, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, 0.75, nan, 0.0, 0.0, 0.0], recall [0.5, nan, nan, 0.2222222222222222, 0.75, nan, nan, 0.0, nan]
2019-02-19T19:44:31.404584: step 543, loss 1.72262, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [nan, 0.0, nan, 0.5833333333333334, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:31.553070: step 544, loss 1.65967, accuracy 0.375, precision [nan, 0.0, 0.0, 0.75, 0.42857142857142855, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.3333333333333333, 0.6, nan, nan, 0.0, nan]
2019-02-19T19:44:31.704457: step 545, loss 1.66073, accuracy 0.4375, precision [1.0, 0.2857142857142857, nan, 0.8, 0.0, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:31.854217: step 546, loss 1.4487, accuracy 0.5625, precision [0.3333333333333333, nan, 0.0, 0.8571428571428571, 0.4, nan, nan, nan, nan], recall [0.5, nan, nan, 0.5454545454545454, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:32.006081: step 547, loss 1.67571, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, nan], recall [nan, 0.5, nan, 0.5, 0.2, nan, nan, 1.0, nan]
2019-02-19T19:44:32.155271: step 548, loss 2.2887, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.6666666666666666, nan, nan, 0.0, 0.0, 0.0], recall [0.0, 0.5, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:32.309483: step 549, loss 1.9654, accuracy 0.1875, precision [0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.0, nan, nan, nan], recall [nan, 0.4, nan, 0.14285714285714285, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:44:32.460304: step 550, loss 1.74933, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.4, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:44:32.610838: step 551, loss 1.82202, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.5, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:32.761535: step 552, loss 2.26521, accuracy 0.125, precision [0.25, 0.0, 0.0, 0.3333333333333333, nan, 0.0, 0.0, 0.0, nan], recall [0.5, 0.0, 0.0, 0.14285714285714285, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:32.913152: step 553, loss 1.87851, accuracy 0.3125, precision [0.0, 0.0, 0.5, 0.3333333333333333, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, 1.0, 0.14285714285714285, 0.42857142857142855, nan, nan, nan, nan]
2019-02-19T19:44:33.067967: step 554, loss 1.42356, accuracy 0.5625, precision [0.0, 0.5, nan, 0.6666666666666666, 0.75, nan, nan, 0.5, nan], recall [0.0, 1.0, nan, 0.3333333333333333, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:44:33.216052: step 555, loss 1.86185, accuracy 0.5, precision [0.0, 0.0, nan, 0.875, nan, 0.0, 0.0, 0.5, 0.0], recall [0.0, 0.0, 0.0, 0.5833333333333334, nan, nan, nan, 1.0, nan]
2019-02-19T19:44:33.367868: step 556, loss 1.3448, accuracy 0.625, precision [0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.5, nan, nan, 0.3333333333333333, nan], recall [1.0, 1.0, nan, 0.4, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:33.514438: step 557, loss 1.39615, accuracy 0.5625, precision [1.0, 0.6666666666666666, 0.0, 0.8333333333333334, 1.0, nan, nan, 0.0, nan], recall [0.5, 0.5, nan, 0.5555555555555556, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:33.667001: step 558, loss 2.09959, accuracy 0.25, precision [0.5, nan, 0.0, 0.4, 0.25, 0.0, nan, 0.0, 0.0], recall [0.5, 0.0, 0.0, 0.2857142857142857, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:33.821564: step 559, loss 1.62297, accuracy 0.375, precision [0.5, 0.5, 0.0, 0.75, 0.2, nan, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:33.967314: step 560, loss 2.21896, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, nan, 0.0], recall [0.0, 0.5, nan, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:34.116719: step 561, loss 1.60029, accuracy 0.5625, precision [1.0, 0.6666666666666666, nan, 1.0, 0.6, nan, 0.0, 0.0, 0.0], recall [0.3333333333333333, 1.0, nan, 0.42857142857142855, 0.75, nan, nan, nan, nan]
2019-02-19T19:44:34.270485: step 562, loss 1.90442, accuracy 0.5, precision [0.25, nan, 0.0, 1.0, 1.0, nan, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:34.421500: step 563, loss 1.34093, accuracy 0.625, precision [1.0, 1.0, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.0, 0.0], recall [1.0, 1.0, 0.0, 0.7142857142857143, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:34.571985: step 564, loss 1.48481, accuracy 0.5, precision [0.0, 1.0, 0.2, 0.8, nan, 0.0, 0.0, 1.0, nan], recall [0.0, 1.0, 0.5, 0.4444444444444444, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:44:34.722848: step 565, loss 1.60219, accuracy 0.375, precision [1.0, 0.25, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, nan, 0.5, nan], recall [0.3333333333333333, 1.0, nan, 0.2, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:34.876721: step 566, loss 1.70276, accuracy 0.4375, precision [0.0, 0.3333333333333333, nan, 0.75, 0.6666666666666666, 0.0, nan, 0.5, nan], recall [0.0, 1.0, 0.0, 0.375, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:44:35.026356: step 567, loss 1.28044, accuracy 0.5625, precision [0.5, 0.5, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, 0.0, 0.4, 0.5714285714285714, nan, nan, nan, nan]
2019-02-19T19:44:35.177906: step 568, loss 2.01764, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.5, 0.6, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.5, 0.3333333333333333, 0.42857142857142855, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:35.324138: step 569, loss 1.97176, accuracy 0.375, precision [0.0, 0.5, nan, 0.6666666666666666, 0.75, nan, nan, 0.0, 0.0], recall [0.0, 1.0, 0.0, 0.2857142857142857, 0.75, nan, nan, nan, nan]
2019-02-19T19:44:35.478522: step 570, loss 1.55353, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.0, nan, 0.0, 0.6, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:35.626903: step 571, loss 1.67273, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, 0.0, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:35.778760: step 572, loss 1.65644, accuracy 0.375, precision [1.0, 0.16666666666666666, 0.0, 1.0, 0.0, nan, nan, 0.4, nan], recall [1.0, 1.0, nan, 0.2, 0.0, 0.0, nan, 1.0, nan]
2019-02-19T19:44:35.931645: step 573, loss 1.89367, accuracy 0.375, precision [0.0, 1.0, nan, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, 0.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:44:36.080052: step 574, loss 1.6096, accuracy 0.375, precision [1.0, 0.0, nan, 0.6, 1.0, 0.0, nan, 0.0, nan], recall [0.3333333333333333, 0.0, nan, 0.42857142857142855, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:36.229303: step 575, loss 1.62217, accuracy 0.4375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.75, nan, 0.0, nan, nan], recall [0.5, 0.0, nan, 0.375, 0.75, nan, nan, nan, nan]
2019-02-19T19:44:36.381721: step 576, loss 1.82221, accuracy 0.3125, precision [0.0, 0.25, 0.0, 0.75, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:36.621248: step 577, loss 1.89436, accuracy 0.1, precision [nan, 0.0, nan, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan], recall [nan, 0.0, nan, 0.0, 0.0, 1.0, nan, 0.0, nan]
2019-02-19T19:44:36.767086: step 578, loss 1.55646, accuracy 0.1875, precision [0.5, 0.0, 0.0, 0.3333333333333333, 0.0, nan, 0.0, nan, nan], recall [1.0, 0.0, 0.0, 0.2857142857142857, nan, nan, nan, 0.0, nan]
2019-02-19T19:44:36.917114: step 579, loss 2.08973, accuracy 0.5, precision [0.0, 0.5, nan, 0.6363636363636364, nan, nan, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.875, nan, nan, nan, 0.0, nan]
2019-02-19T19:44:37.064609: step 580, loss 1.63567, accuracy 0.5, precision [0.0, 1.0, 0.0, 0.8, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.6, 0.0, 0.4444444444444444, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:37.216711: step 581, loss 2.11684, accuracy 0.1875, precision [0.5, 0.0, 0.0, 0.25, 0.0, nan, 0.0, 0.25, nan], recall [0.5, 0.0, nan, 0.16666666666666666, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:44:37.369712: step 582, loss 1.64269, accuracy 0.5, precision [0.2, 0.6666666666666666, 0.0, 0.75, 1.0, nan, nan, 0.5, nan], recall [0.5, 0.6666666666666666, nan, 0.375, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:44:37.520269: step 583, loss 1.5528, accuracy 0.625, precision [nan, 0.8, 0.0, 0.75, 0.0, nan, 0.0, 1.0, 0.0], recall [nan, 0.6666666666666666, nan, 0.42857142857142855, nan, nan, nan, 1.0, nan]
2019-02-19T19:44:37.669497: step 584, loss 1.81634, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 1.0, nan, nan, nan, nan], recall [nan, 0.0, nan, 0.6363636363636364, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:37.822228: step 585, loss 1.96496, accuracy 0.375, precision [nan, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.6666666666666666, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:37.968605: step 586, loss 1.57499, accuracy 0.375, precision [nan, 0.2, 0.0, 1.0, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.3333333333333333, 0.4, nan, nan, nan, nan]
2019-02-19T19:44:38.117865: step 587, loss 2.1827, accuracy 0.3125, precision [0.5, 0.5, 0.0, 0.75, 0.0, nan, 0.0, nan, 0.0], recall [1.0, 0.3333333333333333, nan, 0.2727272727272727, nan, nan, nan, 0.0, nan]
2019-02-19T19:44:38.265980: step 588, loss 1.63956, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.14285714285714285, nan, nan, 1.0, nan], recall [nan, 0.3333333333333333, nan, 0.18181818181818182, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:38.415940: step 589, loss 1.72866, accuracy 0.375, precision [0.5, 0.5, 0.0, 0.75, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:38.564542: step 590, loss 1.55991, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.6, 0.75, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.375, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:38.714408: step 591, loss 1.73725, accuracy 0.375, precision [1.0, 0.25, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [0.5, 0.5, nan, 0.2222222222222222, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:38.865010: step 592, loss 1.83399, accuracy 0.4375, precision [nan, 0.0, 0.5, 0.7142857142857143, 0.5, nan, nan, 0.0, 0.0], recall [nan, 0.0, 1.0, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:39.010070: step 593, loss 1.67545, accuracy 0.5, precision [0.5, 0.0, nan, 0.6666666666666666, 1.0, 0.0, nan, 0.5, nan], recall [1.0, 0.0, nan, 0.4444444444444444, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:39.154610: step 594, loss 1.54548, accuracy 0.375, precision [nan, 0.0, 0.0, 0.75, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, nan], recall [nan, nan, 0.0, 0.375, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:44:39.307297: step 595, loss 2.01437, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.16666666666666666, 0.375, nan, nan, nan, nan]
2019-02-19T19:44:39.456025: step 596, loss 1.82202, accuracy 0.4375, precision [0.0, 1.0, 0.25, 0.5, 0.5, nan, nan, nan, nan], recall [0.0, 0.6666666666666666, 1.0, 0.6, 0.16666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:39.607687: step 597, loss 1.29763, accuracy 0.5625, precision [1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 0.75, nan, nan, 0.0, nan], recall [0.5, 1.0, 1.0, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:39.762745: step 598, loss 2.14241, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, nan], recall [0.0, 0.5, nan, 0.2857142857142857, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:44:39.909839: step 599, loss 1.48057, accuracy 0.5, precision [0.3333333333333333, nan, 0.0, 1.0, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.625, 0.4, nan, nan, 0.0, nan]
2019-02-19T19:44:40.064138: step 600, loss 1.90094, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.5, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.16666666666666666, 0.5714285714285714, nan, nan, nan, nan]
2019-02-19T19:44:40.212140: step 601, loss 1.58525, accuracy 0.5625, precision [0.0, 0.6666666666666666, 0.5, 0.7142857142857143, 0.5, nan, nan, nan, nan], recall [nan, 1.0, 0.3333333333333333, 0.7142857142857143, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:40.360886: step 602, loss 1.59534, accuracy 0.5625, precision [0.5, nan, nan, 0.6, 0.6666666666666666, 0.0, nan, 0.5, nan], recall [0.5, nan, 0.0, 0.42857142857142855, 0.8, nan, nan, 1.0, nan]
2019-02-19T19:44:40.507825: step 603, loss 2.07625, accuracy 0.3125, precision [0.0, 0.0, nan, 0.6666666666666666, 0.25, nan, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.36363636363636365, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:40.655745: step 604, loss 1.70894, accuracy 0.25, precision [0.0, 0.0, 0.3333333333333333, 0.6, 0.0, nan, nan, 0.0, nan], recall [nan, nan, 0.5, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:40.802713: step 605, loss 1.67905, accuracy 0.4375, precision [nan, 0.0, 0.25, 0.8333333333333334, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [0.0, nan, 1.0, 0.4166666666666667, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:40.951139: step 606, loss 1.73274, accuracy 0.3125, precision [nan, 0.3333333333333333, nan, 0.4, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:41.097544: step 607, loss 1.60011, accuracy 0.5625, precision [0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, 0.3333333333333333, 0.6666666666666666, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:41.245690: step 608, loss 1.8199, accuracy 0.4375, precision [0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.25, nan], recall [nan, 1.0, 1.0, 0.2727272727272727, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:41.397664: step 609, loss 1.40954, accuracy 0.625, precision [1.0, 0.6666666666666666, 0.0, 0.875, nan, nan, nan, 0.0, nan], recall [0.3333333333333333, 1.0, nan, 0.7, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:41.543793: step 610, loss 1.72185, accuracy 0.3125, precision [0.0, 0.0, 1.0, 0.75, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.5, 0.3333333333333333, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:41.692784: step 611, loss 1.47629, accuracy 0.5625, precision [nan, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.3333333333333333, nan], recall [0.0, nan, nan, 0.5384615384615384, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:41.844633: step 612, loss 1.44319, accuracy 0.5625, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.25, 0.0, nan, nan, nan], recall [nan, 1.0, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:41.998655: step 613, loss 2.16634, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:42.145828: step 614, loss 2.20905, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.4, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:42.293276: step 615, loss 1.50428, accuracy 0.4375, precision [nan, 0.3333333333333333, nan, 0.5, 0.75, nan, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.5, 0.6, nan, nan, 0.0, nan]
2019-02-19T19:44:42.441615: step 616, loss 1.60117, accuracy 0.5625, precision [nan, 1.0, 0.0, 1.0, 0.25, nan, 0.0, 0.0, 0.0], recall [0.0, 1.0, 0.0, 0.6363636363636364, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:42.588237: step 617, loss 1.78715, accuracy 0.375, precision [0.6666666666666666, 0.5, 0.0, 0.5, 0.3333333333333333, 0.0, nan, nan, nan], recall [1.0, 0.5, nan, 0.2, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:42.737827: step 618, loss 2.0586, accuracy 0.375, precision [nan, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:42.886965: step 619, loss 1.68831, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, 0.3333333333333333, nan], recall [nan, nan, nan, 0.5, 0.5, nan, nan, 0.25, nan]
2019-02-19T19:44:43.038718: step 620, loss 1.62565, accuracy 0.5, precision [0.0, nan, 0.3333333333333333, 0.8333333333333334, 0.5, nan, 0.0, 0.0, nan], recall [nan, nan, 1.0, 0.45454545454545453, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:43.190206: step 621, loss 1.55584, accuracy 0.625, precision [0.5, 0.0, nan, 1.0, 0.6666666666666666, 1.0, nan, nan, nan], recall [0.5, nan, nan, 0.5454545454545454, 1.0, 1.0, nan, nan, nan]
2019-02-19T19:44:43.340166: step 622, loss 1.53204, accuracy 0.5, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.6666666666666666, nan], recall [nan, 1.0, nan, 0.45454545454545453, 0.0, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:44:43.493276: step 623, loss 1.1426, accuracy 0.5625, precision [0.0, 0.5, 0.4, 1.0, 0.75, nan, nan, nan, nan], recall [nan, 0.6666666666666666, 1.0, 0.2857142857142857, 0.75, nan, nan, nan, nan]
2019-02-19T19:44:43.647298: step 624, loss 2.02401, accuracy 0.5, precision [0.0, nan, 0.0, 1.0, 1.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.0, nan, 0.6, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:43.801772: step 625, loss 2.06509, accuracy 0.25, precision [0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.5, 0.0, nan, 0.5, nan], recall [0.5, 0.3333333333333333, 0.0, 0.0, 0.25, nan, nan, 0.25, nan]
2019-02-19T19:44:43.952315: step 626, loss 1.71772, accuracy 0.375, precision [nan, 0.0, 0.0, 0.75, 1.0, nan, 0.0, 1.0, nan], recall [nan, nan, nan, 0.3, 0.25, nan, nan, 1.0, nan]
2019-02-19T19:44:44.098843: step 627, loss 1.8719, accuracy 0.3125, precision [0.3333333333333333, 0.0, nan, 0.42857142857142855, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, nan, 0.0, 0.42857142857142855, 0.16666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:44.249668: step 628, loss 1.58435, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.6, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, 0.0, 0.375, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:44.398106: step 629, loss 1.74863, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.6, 1.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.375, 0.42857142857142855, nan, nan, nan, nan]
2019-02-19T19:44:44.548957: step 630, loss 2.04631, accuracy 0.375, precision [0.5, nan, 0.0, 1.0, 0.5, 0.0, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.4444444444444444, 0.2, nan, nan, nan, nan]
2019-02-19T19:44:44.696211: step 631, loss 1.59436, accuracy 0.3125, precision [0.0, 0.2, 0.0, 0.6666666666666666, nan, nan, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, nan, nan]
2019-02-19T19:44:44.844365: step 632, loss 1.50332, accuracy 0.5625, precision [1.0, 1.0, 0.5, 0.6666666666666666, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 0.5, 1.0, 0.3333333333333333, 0.75, nan, nan, nan, nan]
2019-02-19T19:44:44.991096: step 633, loss 1.8388, accuracy 0.25, precision [0.0, 0.0, nan, 0.6666666666666666, 0.4, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.25, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:45.140833: step 634, loss 1.75647, accuracy 0.4375, precision [0.0, 0.4, 0.0, 1.0, 1.0, nan, nan, 0.0, 0.0], recall [nan, 1.0, 0.0, 0.2222222222222222, 0.75, nan, nan, nan, nan]
2019-02-19T19:44:45.297686: step 635, loss 1.59423, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.6, 0.5, nan, nan, 1.0, nan], recall [nan, 0.25, 0.0, 0.42857142857142855, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:44:45.446035: step 636, loss 2.11695, accuracy 0.5, precision [0.5, 0.0, nan, 0.8333333333333334, 1.0, 0.0, 0.0, nan, nan], recall [0.3333333333333333, 0.0, nan, 0.5555555555555556, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:45.600245: step 637, loss 1.72984, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:45.745775: step 638, loss 1.47582, accuracy 0.5625, precision [0.6666666666666666, nan, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.45454545454545453, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:45.894268: step 639, loss 1.74887, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, nan], recall [nan, 0.5, 0.0, 0.2222222222222222, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:44:46.045587: step 640, loss 1.58319, accuracy 0.5625, precision [1.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, nan, nan, nan], recall [1.0, nan, nan, 0.5384615384615384, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:46.198393: step 641, loss 1.56471, accuracy 0.4375, precision [1.0, nan, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.5, nan], recall [0.5, nan, 0.0, 0.45454545454545453, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:44:46.347462: step 642, loss 1.87457, accuracy 0.375, precision [0.5, 0.25, nan, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, 1.0, 0.0, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:46.497792: step 643, loss 1.78257, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.6, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.2727272727272727, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:46.647560: step 644, loss 1.66473, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.8333333333333334, 0.5, 0.0, nan, nan, 0.0], recall [0.4, nan, nan, 0.5555555555555556, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:46.802008: step 645, loss 2.25381, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, 1.0, 0.2857142857142857, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:46.947601: step 646, loss 1.72252, accuracy 0.25, precision [0.0, 0.25, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.42857142857142855, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:47.100699: step 647, loss 2.00528, accuracy 0.4375, precision [0.0, 0.5, nan, 1.0, 0.25, nan, 0.0, 0.0, nan], recall [nan, 1.0, 0.0, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:47.247451: step 648, loss 1.69839, accuracy 0.5, precision [nan, 0.5, 0.0, 0.5, 1.0, nan, nan, nan, 0.0], recall [0.0, 1.0, 0.0, 0.5714285714285714, 0.6, nan, nan, 0.0, nan]
2019-02-19T19:44:47.398034: step 649, loss 1.35845, accuracy 0.625, precision [0.0, 0.5, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.5555555555555556, 0.8, nan, nan, nan, nan]
2019-02-19T19:44:47.545684: step 650, loss 1.67438, accuracy 0.4375, precision [nan, 1.0, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.36363636363636365, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:47.696218: step 651, loss 1.62736, accuracy 0.375, precision [0.4, 1.0, 0.0, 1.0, nan, 0.0, 0.0, 0.0, nan], recall [1.0, 0.6666666666666666, 0.0, 0.2222222222222222, nan, nan, nan, nan, nan]
2019-02-19T19:44:47.843361: step 652, loss 1.71817, accuracy 0.5, precision [0.3333333333333333, 0.5, 0.5, 0.6, 1.0, 0.0, nan, 0.5, nan], recall [1.0, 0.5, 0.5, 0.375, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:47.988431: step 653, loss 2.35516, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.5, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.25, 0.0, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:48.141027: step 654, loss 1.60624, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.6666666666666666, nan, 0.0, nan, 0.0, nan], recall [nan, 0.75, nan, 0.4444444444444444, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:48.288819: step 655, loss 1.80375, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, 0.0, nan, nan], recall [0.0, nan, nan, 0.4166666666666667, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:48.436676: step 656, loss 1.6239, accuracy 0.5, precision [0.0, 0.5, 0.0, 1.0, 0.6666666666666666, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:48.586045: step 657, loss 1.68038, accuracy 0.5, precision [0.0, nan, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 0.3333333333333333, nan], recall [nan, 0.0, nan, 0.5, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:48.735079: step 658, loss 1.65366, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.4, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:48.882331: step 659, loss 1.7409, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 1.0, nan], recall [nan, nan, nan, 0.3333333333333333, nan, nan, nan, 1.0, nan]
2019-02-19T19:44:49.035988: step 660, loss 1.27461, accuracy 0.4375, precision [1.0, 1.0, 0.0, 0.6666666666666666, 0.6, 0.0, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.2222222222222222, 0.75, nan, nan, nan, nan]
2019-02-19T19:44:49.186044: step 661, loss 1.86568, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.75, 1.0, 0.0, nan, 0.5, nan], recall [nan, 0.25, nan, 0.375, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:49.334032: step 662, loss 1.63318, accuracy 0.5625, precision [nan, 0.6666666666666666, nan, 0.8571428571428571, 0.25, 0.0, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.5454545454545454, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:49.481213: step 663, loss 1.82147, accuracy 0.375, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [1.0, nan, nan, 0.25, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:49.629441: step 664, loss 1.86104, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.2, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:49.783414: step 665, loss 1.50252, accuracy 0.4375, precision [1.0, nan, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.25, nan], recall [1.0, nan, nan, 0.36363636363636365, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:44:49.927082: step 666, loss 1.74684, accuracy 0.25, precision [0.0, 0.0, 0.0, nan, 0.8, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.0, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:50.075426: step 667, loss 2.02901, accuracy 0.1875, precision [0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 1.0, 0.2222222222222222, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:50.222777: step 668, loss 1.79408, accuracy 0.25, precision [0.6666666666666666, 1.0, 0.0, 0.16666666666666666, 0.0, nan, nan, 0.0, 0.0], recall [1.0, 0.3333333333333333, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:50.376460: step 669, loss 2.2487, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.3333333333333333, 0.5, 0.0, nan, nan, nan], recall [0.0, 0.25, nan, 0.3333333333333333, 0.16666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:50.527040: step 670, loss 1.60675, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.6, 1.0, nan, nan, 0.5, nan], recall [0.0, 0.5, 0.0, 0.6, 0.6666666666666666, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:44:50.679778: step 671, loss 1.56196, accuracy 0.375, precision [1.0, 1.0, 0.3333333333333333, 0.0, 0.3333333333333333, nan, nan, 0.5, nan], recall [0.5, 0.5, 1.0, 0.0, 0.2, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:44:50.830019: step 672, loss 1.89511, accuracy 0.3125, precision [nan, 0.0, 0.4, 0.25, 0.5, 1.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, 0.5, 0.3333333333333333, 0.3333333333333333, 1.0, nan, 0.0, nan]
2019-02-19T19:44:50.976791: step 673, loss 1.53576, accuracy 0.4375, precision [0.0, 0.6666666666666666, 1.0, 0.5, 0.5, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.3333333333333333, 0.25, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:51.124575: step 674, loss 1.73763, accuracy 0.375, precision [1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.5, 0.3333333333333333, 0.0, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:44:51.269819: step 675, loss 2.07984, accuracy 0.1875, precision [0.0, 0.16666666666666666, 0.0, 0.5, 0.0, nan, nan, 0.0, 0.0], recall [nan, 1.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:51.419628: step 676, loss 1.93295, accuracy 0.25, precision [0.5, 0.3333333333333333, 1.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0], recall [0.5, 0.5, 0.16666666666666666, 0.0, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:51.566234: step 677, loss 2.00017, accuracy 0.1875, precision [nan, 0.0, 1.0, 0.16666666666666666, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [0.0, nan, 0.1111111111111111, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:51.715293: step 678, loss 1.35822, accuracy 0.5, precision [nan, 0.0, 0.0, 0.7142857142857143, 0.6666666666666666, nan, nan, 0.5, nan], recall [0.0, 0.0, 0.0, 0.5555555555555556, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:51.866348: step 679, loss 1.51814, accuracy 0.5625, precision [0.0, 1.0, 1.0, 0.5, 0.6, 0.0, 0.0, 1.0, nan], recall [nan, 0.5, 0.5, 0.3333333333333333, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:52.016297: step 680, loss 1.60461, accuracy 0.375, precision [0.0, 0.2, 0.0, 0.6666666666666666, 1.0, nan, 0.0, nan, nan], recall [nan, 0.5, 0.0, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:52.161786: step 681, loss 2.11229, accuracy 0.375, precision [1.0, 1.0, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.5, 0.0, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:52.310329: step 682, loss 2.2363, accuracy 0.375, precision [nan, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, nan, 0.0, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:52.459612: step 683, loss 1.47607, accuracy 0.4375, precision [1.0, 0.25, nan, 0.6666666666666666, 0.5, nan, nan, 0.0, 0.0], recall [0.4, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:52.606139: step 684, loss 1.52527, accuracy 0.5625, precision [nan, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, 1.0, 0.36363636363636365, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:52.758428: step 685, loss 1.60608, accuracy 0.4375, precision [0.25, 0.0, 0.0, 1.0, 0.5, 0.0, nan, nan, nan], recall [0.5, 0.0, nan, 0.4, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:52.909973: step 686, loss 1.61347, accuracy 0.4375, precision [0.0, nan, 0.5, 0.625, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, 1.0, 0.5555555555555556, 0.25, nan, nan, nan, nan]
2019-02-19T19:44:53.061729: step 687, loss 1.4714, accuracy 0.6875, precision [1.0, 1.0, 0.0, 0.7, 1.0, nan, nan, 0.0, nan], recall [0.6666666666666666, 0.25, nan, 1.0, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:53.209203: step 688, loss 1.7121, accuracy 0.5, precision [nan, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.45454545454545453, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:53.357439: step 689, loss 1.92536, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.75, 0.0, 0.1, nan, nan, nan, nan, nan]
2019-02-19T19:44:53.509623: step 690, loss 1.27253, accuracy 0.5625, precision [0.0, 0.5, nan, 1.0, 1.0, 0.0, 0.0, nan, nan], recall [nan, 0.25, nan, 0.7142857142857143, 0.6, nan, nan, nan, nan]
2019-02-19T19:44:53.662766: step 691, loss 2.06969, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, nan], recall [nan, 1.0, nan, 0.2, 0.25, nan, nan, 1.0, nan]
2019-02-19T19:44:53.812803: step 692, loss 1.96184, accuracy 0.1875, precision [nan, 0.0, nan, 0.75, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, nan, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:53.967972: step 693, loss 1.55927, accuracy 0.375, precision [0.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.16666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:54.121033: step 694, loss 1.82958, accuracy 0.5625, precision [0.0, 1.0, 0.0, 1.0, 0.5, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, nan, 0.4, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:44:54.271294: step 695, loss 1.76478, accuracy 0.5, precision [nan, 1.0, 0.0, 0.6, 0.8, 0.0, nan, nan, nan], recall [0.0, 1.0, nan, 0.375, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:54.427055: step 696, loss 1.39632, accuracy 0.5, precision [nan, nan, 0.0, 0.5714285714285714, 0.8, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5714285714285714, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:54.578526: step 697, loss 2.04548, accuracy 0.25, precision [0.0, 0.0, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:54.730262: step 698, loss 1.68165, accuracy 0.5625, precision [0.3333333333333333, 0.3333333333333333, nan, 1.0, 0.5, nan, nan, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.6, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:44:54.880107: step 699, loss 1.68671, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 0.6, nan, 0.0, 0.25, nan], recall [0.0, nan, nan, 0.2, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:44:55.030131: step 700, loss 1.76706, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.6, 0.5, nan, 0.0, 0.5, nan], recall [1.0, nan, nan, 0.3333333333333333, 0.5, 0.0, nan, 1.0, nan]
2019-02-19T19:44:55.181015: step 701, loss 1.55829, accuracy 0.375, precision [nan, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.4, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:55.328851: step 702, loss 1.89363, accuracy 0.4375, precision [0.0, nan, 0.0, 0.8571428571428571, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.5454545454545454, 0.2, nan, nan, nan, nan]
2019-02-19T19:44:55.477981: step 703, loss 1.45297, accuracy 0.5, precision [1.0, 0.0, nan, 0.7142857142857143, 0.5, nan, 0.0, 0.3333333333333333, nan], recall [1.0, 0.0, nan, 0.5, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:44:55.631540: step 704, loss 2.40214, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6666666666666666, nan, nan, nan, nan, 0.0], recall [nan, 0.25, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:55.779378: step 705, loss 1.77072, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.3333333333333333, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:55.934645: step 706, loss 1.41161, accuracy 0.625, precision [1.0, 1.0, nan, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.6666666666666666, 0.6666666666666666, nan, 0.0, 0.0, nan]
2019-02-19T19:44:56.084689: step 707, loss 1.52318, accuracy 0.4375, precision [1.0, nan, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.6666666666666666, nan], recall [0.5, 0.0, nan, 0.4444444444444444, 0.0, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:44:56.235664: step 708, loss 1.36547, accuracy 0.5625, precision [1.0, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:44:56.384568: step 709, loss 1.61958, accuracy 0.5, precision [nan, 0.0, nan, 0.8, 0.5, 0.0, 0.0, 1.0, 0.0], recall [nan, 0.0, nan, 0.4444444444444444, 0.6, nan, nan, 1.0, nan]
2019-02-19T19:44:56.538988: step 710, loss 1.70555, accuracy 0.6875, precision [1.0, 1.0, 0.0, 0.8333333333333334, 0.75, nan, nan, 1.0, 0.0], recall [0.3333333333333333, 1.0, nan, 0.625, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:44:56.689106: step 711, loss 1.91257, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.75, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:56.839470: step 712, loss 1.63261, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 0.6666666666666666, 0.6, nan, 0.0, nan, nan], recall [nan, 1.0, nan, 0.25, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:44:56.991346: step 713, loss 2.12777, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 1.0, 0.0], recall [nan, 1.0, nan, 0.25, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:44:57.141583: step 714, loss 2.05904, accuracy 0.3125, precision [0.0, 0.25, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:57.285538: step 715, loss 1.16092, accuracy 0.625, precision [0.0, 0.0, nan, 0.75, 0.75, nan, 0.0, 1.0, nan], recall [nan, nan, nan, 0.6, 0.6, nan, nan, 1.0, nan]
2019-02-19T19:44:57.438107: step 716, loss 2.09572, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 0.5, 0.0, nan, 0.0, 0.3333333333333333, 0.0], recall [nan, 0.5, nan, 0.18181818181818182, nan, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:44:57.590346: step 717, loss 1.75197, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, 0.0, 0.36363636363636365, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:44:57.742409: step 718, loss 1.65704, accuracy 0.5, precision [nan, 0.5, nan, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0], recall [0.0, 1.0, nan, 0.5555555555555556, 0.4, nan, nan, nan, nan]
2019-02-19T19:44:57.891259: step 719, loss 1.65027, accuracy 0.375, precision [1.0, 0.2, 0.5, 0.6666666666666666, 0.5, nan, 0.0, 0.0, nan], recall [0.5, 0.5, 0.5, 0.3333333333333333, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:44:58.043545: step 720, loss 1.67524, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.5, 0.6666666666666666, nan, nan, nan, nan], recall [0.0, 0.0, nan, 0.375, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:58.192850: step 721, loss 1.80048, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.25, 0.0, 0.4444444444444444, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:58.345988: step 722, loss 1.86607, accuracy 0.4375, precision [0.5, 0.0, nan, 0.75, 0.6666666666666666, nan, 0.0, 0.3333333333333333, 0.0], recall [0.5, nan, nan, 0.5, 0.3333333333333333, nan, nan, 0.5, nan]
2019-02-19T19:44:58.494400: step 723, loss 1.98891, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:44:58.643386: step 724, loss 1.59085, accuracy 0.5625, precision [0.0, 1.0, nan, 0.5714285714285714, 1.0, 0.0, nan, 0.3333333333333333, nan], recall [nan, 0.5, 0.0, 0.8, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:44:58.796050: step 725, loss 1.66274, accuracy 0.375, precision [0.0, 0.25, nan, 0.5, 0.6, 0.0, 0.0, nan, nan], recall [nan, 0.3333333333333333, nan, 0.25, 0.6, nan, nan, nan, nan]
2019-02-19T19:44:58.951602: step 726, loss 1.43825, accuracy 0.5, precision [1.0, 0.75, 0.3333333333333333, 0.6, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 0.75, 1.0, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:59.103578: step 727, loss 2.12328, accuracy 0.3125, precision [0.0, 0.25, nan, 1.0, 0.5, nan, 0.0, 0.0, 0.3333333333333333], recall [nan, 0.3333333333333333, nan, 0.125, 0.6666666666666666, nan, nan, 0.0, 1.0]
2019-02-19T19:44:59.253874: step 728, loss 1.89631, accuracy 0.3125, precision [nan, 1.0, 0.0, 0.5, nan, nan, 0.0, 0.0, 0.0], recall [0.0, 0.75, nan, 0.4, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:44:59.402588: step 729, loss 1.85503, accuracy 0.25, precision [0.0, 0.6666666666666666, nan, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.6666666666666666, nan, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:44:59.553447: step 730, loss 1.41393, accuracy 0.5, precision [0.0, 0.6, 0.0, 0.6, 1.0, nan, nan, 0.0, nan], recall [0.0, 0.6, nan, 0.5, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:44:59.708864: step 731, loss 1.9935, accuracy 0.25, precision [nan, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.25, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:44:59.857652: step 732, loss 1.85339, accuracy 0.4375, precision [0.0, nan, 1.0, 0.6666666666666666, 0.3333333333333333, nan, nan, nan, 0.0], recall [nan, 0.0, 1.0, 0.4444444444444444, 0.6666666666666666, nan, nan, nan, 0.0]
2019-02-19T19:45:00.006216: step 733, loss 1.65619, accuracy 0.375, precision [nan, 0.5, 0.0, 0.5, 0.0, 0.0, nan, 0.5, nan], recall [nan, 0.3333333333333333, nan, 0.3333333333333333, nan, nan, nan, 0.5, nan]
2019-02-19T19:45:00.153810: step 734, loss 2.01957, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.625, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.625, 0.5, nan, nan, 0.0, 0.0]
2019-02-19T19:45:00.302703: step 735, loss 1.53927, accuracy 0.5625, precision [0.3333333333333333, 0.3333333333333333, 0.5, 0.8333333333333334, 1.0, nan, nan, nan, 0.0], recall [1.0, 1.0, 1.0, 0.5555555555555556, 1.0, nan, nan, 0.0, 0.0]
2019-02-19T19:45:00.452632: step 736, loss 1.86974, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.75, 1.0, nan, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, 0.0, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:00.605876: step 737, loss 1.9741, accuracy 0.4375, precision [0.0, 1.0, 0.0, 1.0, 0.0, nan, 0.0, nan, 0.0], recall [nan, 1.0, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:45:00.762027: step 738, loss 1.49295, accuracy 0.5, precision [0.0, 0.2, 0.0, 0.8333333333333334, 1.0, nan, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:00.907882: step 739, loss 1.12763, accuracy 0.8125, precision [nan, 0.5, 1.0, 0.875, 0.6666666666666666, nan, nan, 1.0, nan], recall [nan, 1.0, 1.0, 0.875, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:01.054203: step 740, loss 1.71648, accuracy 0.4375, precision [0.0, 0.25, 0.0, 0.75, nan, nan, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.5454545454545454, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:01.203447: step 741, loss 2.15529, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, nan, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:01.350970: step 742, loss 1.68244, accuracy 0.4375, precision [0.5, 1.0, nan, 0.8333333333333334, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, 0.5, 0.0, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:45:01.499703: step 743, loss 2.06578, accuracy 0.4375, precision [nan, 0.5, 0.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:01.647092: step 744, loss 1.46298, accuracy 0.5625, precision [0.0, 1.0, 0.0, 0.8571428571428571, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:01.795317: step 745, loss 2.03121, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.5, 0.0], recall [0.0, nan, nan, 0.38461538461538464, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:01.943077: step 746, loss 2.27136, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 1.0, 0.0, 0.15384615384615385, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:02.097124: step 747, loss 2.14573, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.3333333333333333, nan, 0.0, 0.0, 0.0], recall [nan, nan, 0.0, 0.38461538461538464, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:02.251710: step 748, loss 1.92534, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.5, nan], recall [0.0, nan, 0.0, 0.4444444444444444, 0.6666666666666666, nan, nan, 0.5, nan]
2019-02-19T19:45:02.404744: step 749, loss 1.45342, accuracy 0.5, precision [1.0, 0.5, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [1.0, 0.5, 0.3333333333333333, 0.2, 0.8, nan, nan, nan, nan]
2019-02-19T19:45:02.556210: step 750, loss 1.63698, accuracy 0.5, precision [0.5, 0.0, 0.2, 0.6, 1.0, nan, nan, nan, nan], recall [1.0, 0.0, 1.0, 0.5, 0.6, nan, nan, 0.0, nan]

Evaluation:
[[ 22  29   0  31   2   0   0   2   0]
 [  2  78   1  75   5   0   0   3   0]
 [  1   3  10  64  13   0   0   0   0]
 [  0  28  14 241  18   0   0   3   0]
 [  1   3   1  87  79   0   0   0   0]
 [  0   3   2  38   6   0   0   2   0]
 [  1   0   0  17   7   0   0   1   0]
 [  4   6   1  53   9   0   0  34   0]
 [  1   1   0  21   2   0   0   0   0]]
2019-02-19T19:45:04.980591: step 750, loss 1.62286, accuracy 0.452683, precision [0.2558139534883721, 0.47560975609756095, 0.10989010989010989, 0.7927631578947368, 0.4619883040935672, 0.0, 0.0, 0.3177570093457944, 0.0], recall [0.6875, 0.5165562913907285, 0.3448275862068966, 0.3843700159489633, 0.5602836879432624, nan, nan, 0.7555555555555555, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550605378/checkpoints/model-750

2019-02-19T19:45:05.211833: step 751, loss 1.8784, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.7142857142857143, nan, nan, nan, 0.5, nan], recall [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:45:05.358630: step 752, loss 2.01256, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, nan, 0.0, 0.0, nan], recall [1.0, 0.0, 0.0, 0.3, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:05.509385: step 753, loss 1.75906, accuracy 0.375, precision [0.3333333333333333, 0.2, 0.0, 0.5, 0.6666666666666666, nan, nan, 0.5, nan], recall [0.5, 0.5, 0.0, 0.2, 0.6666666666666666, nan, nan, 0.5, nan]
2019-02-19T19:45:05.659766: step 754, loss 2.2584, accuracy 0.25, precision [0.3333333333333333, 0.4, nan, 0.16666666666666666, nan, 0.0, nan, nan, 0.0], recall [0.5, 0.6666666666666666, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:05.813528: step 755, loss 1.4926, accuracy 0.375, precision [0.0, 1.0, 0.5, 0.2, 1.0, nan, 0.0, 0.5, nan], recall [0.0, 1.0, 0.3333333333333333, 0.25, 0.3333333333333333, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:45:05.970971: step 756, loss 1.53583, accuracy 0.5, precision [1.0, 0.0, nan, 0.4, 0.6666666666666666, nan, 0.0, 0.5, nan], recall [0.3333333333333333, nan, 0.0, 0.5, 0.5714285714285714, nan, nan, 1.0, nan]
2019-02-19T19:45:06.115618: step 757, loss 1.81102, accuracy 0.5, precision [nan, 0.3333333333333333, 1.0, 0.6666666666666666, 0.4, nan, 0.0, nan, nan], recall [0.0, 1.0, 0.5, 0.4444444444444444, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:06.265312: step 758, loss 2.10627, accuracy 0.25, precision [0.0, 0.2, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:45:06.416639: step 759, loss 1.40984, accuracy 0.6875, precision [0.0, 0.6666666666666666, nan, 0.875, 1.0, 0.0, nan, nan, nan], recall [nan, 0.5, nan, 0.7, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:06.565390: step 760, loss 1.47971, accuracy 0.5625, precision [0.6666666666666666, 0.2, nan, 0.8333333333333334, 1.0, nan, 0.0, nan, nan], recall [0.6666666666666666, 1.0, 0.0, 0.5555555555555556, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:06.717973: step 761, loss 1.40897, accuracy 0.625, precision [0.0, 0.8, nan, 0.8, 0.5, nan, 0.0, nan, nan], recall [nan, 0.8, 0.0, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:06.866805: step 762, loss 1.67792, accuracy 0.5, precision [0.0, 0.5, 0.0, 0.7777777777777778, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 0.3333333333333333, 0.0, 0.6363636363636364, nan, nan, nan, nan, nan]
2019-02-19T19:45:07.016049: step 763, loss 1.80926, accuracy 0.375, precision [nan, 0.4, 0.0, 0.4, 1.0, nan, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:07.167732: step 764, loss 1.19436, accuracy 0.625, precision [1.0, 1.0, nan, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, nan], recall [1.0, 0.75, nan, 0.2857142857142857, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:07.321407: step 765, loss 1.84856, accuracy 0.375, precision [1.0, nan, 0.0, 0.75, 0.3333333333333333, 0.0, nan, 0.5, nan], recall [0.3333333333333333, 0.0, nan, 0.42857142857142855, 0.3333333333333333, nan, nan, 0.5, nan]
2019-02-19T19:45:07.471253: step 766, loss 1.86171, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 1.0, nan, 0.25, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:07.621150: step 767, loss 1.60374, accuracy 0.5, precision [nan, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, nan, 1.0, nan], recall [0.0, nan, 0.5, 0.4, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:07.769192: step 768, loss 1.65563, accuracy 0.5, precision [0.0, 1.0, 0.0, 0.8333333333333334, 0.4, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:07.918346: step 769, loss 1.85958, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.75, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.2727272727272727, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:45:08.064752: step 770, loss 2.06243, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.75, 1.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.25, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:08.215908: step 771, loss 1.73145, accuracy 0.375, precision [0.0, 0.0, nan, 0.75, 0.6, 0.0, nan, nan, 0.0], recall [nan, 0.0, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:08.364251: step 772, loss 1.63725, accuracy 0.375, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:08.511010: step 773, loss 2.57367, accuracy 0.3125, precision [0.0, nan, 0.0, 0.8, 1.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:45:08.659829: step 774, loss 1.86424, accuracy 0.375, precision [0.0, 0.5, nan, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, 0.0, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:45:08.807961: step 775, loss 1.67998, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.16666666666666666, nan, nan, nan, nan], recall [nan, 0.0, nan, 0.46153846153846156, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:08.958899: step 776, loss 1.57341, accuracy 0.375, precision [nan, 0.0, 0.3333333333333333, 0.5, 0.5, nan, nan, 0.0, nan], recall [nan, 0.0, 0.5, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:09.107002: step 777, loss 1.63945, accuracy 0.4375, precision [0.0, 0.5, nan, 0.8, 0.5, nan, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.4, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:45:09.253538: step 778, loss 1.89856, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:09.405686: step 779, loss 1.55529, accuracy 0.5625, precision [0.0, 0.3333333333333333, 0.0, 0.8333333333333334, 1.0, nan, nan, 0.6666666666666666, nan], recall [nan, 0.5, nan, 0.5555555555555556, 0.5, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:45:09.553303: step 780, loss 1.39292, accuracy 0.625, precision [nan, 1.0, 0.0, 1.0, 0.75, 0.0, 0.0, nan, nan], recall [nan, 1.0, nan, 0.75, 0.6, nan, nan, 0.0, nan]
2019-02-19T19:45:09.701637: step 781, loss 1.49227, accuracy 0.5, precision [0.0, 1.0, 0.0, 0.75, 1.0, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.375, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:09.852057: step 782, loss 1.84002, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.2857142857142857, nan, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.1111111111111111, 0.4, nan, nan, nan, nan]
2019-02-19T19:45:10.001868: step 783, loss 1.5044, accuracy 0.6875, precision [1.0, 0.5, nan, 1.0, 1.0, 0.0, 0.0, nan, nan], recall [1.0, 1.0, nan, 0.8571428571428571, 0.2, nan, nan, nan, nan]
2019-02-19T19:45:10.155017: step 784, loss 1.56147, accuracy 0.4375, precision [0.5, 0.0, 0.25, 0.3333333333333333, 1.0, nan, nan, 0.0, nan], recall [1.0, 0.0, 1.0, 0.25, 0.4444444444444444, nan, nan, nan, nan]
2019-02-19T19:45:10.303448: step 785, loss 1.66796, accuracy 0.5, precision [0.6666666666666666, 0.5, nan, 0.75, 1.0, 0.0, nan, 0.2, nan], recall [0.6666666666666666, 1.0, nan, 0.5, 0.2, nan, nan, 1.0, nan]
2019-02-19T19:45:10.452690: step 786, loss 1.33218, accuracy 0.625, precision [0.5, 1.0, 1.0, 1.0, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 1.0, 1.0, 0.375, 0.75, nan, nan, nan, nan]
2019-02-19T19:45:10.604166: step 787, loss 1.83215, accuracy 0.4375, precision [nan, 0.4, 0.0, 0.6666666666666666, 1.0, nan, nan, 0.0, nan], recall [0.0, 0.5, 0.0, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:10.757443: step 788, loss 1.39933, accuracy 0.5625, precision [0.5, 0.5, 1.0, 0.6666666666666666, nan, nan, nan, 0.0, nan], recall [1.0, 0.75, 1.0, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:10.909727: step 789, loss 2.29284, accuracy 0.375, precision [0.0, 0.5, nan, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.375, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:11.057832: step 790, loss 1.6522, accuracy 0.5, precision [nan, 0.0, 0.16666666666666666, 1.0, 1.0, nan, 0.0, nan, 0.0], recall [0.0, nan, 1.0, 0.42857142857142855, 0.5714285714285714, nan, nan, nan, nan]
2019-02-19T19:45:11.209965: step 791, loss 2.25571, accuracy 0.375, precision [nan, 0.0, nan, 0.5555555555555556, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 1.0, 0.14285714285714285, nan, nan, 0.0, nan]
2019-02-19T19:45:11.356784: step 792, loss 1.82836, accuracy 0.3125, precision [0.5, nan, 0.5, 0.375, nan, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, 1.0, 0.42857142857142855, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:11.508407: step 793, loss 1.80288, accuracy 0.375, precision [0.0, 0.6666666666666666, nan, 0.5, 0.6666666666666666, nan, 0.0, 0.0, 0.0], recall [0.0, 0.6666666666666666, nan, 0.2857142857142857, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:11.658416: step 794, loss 1.89108, accuracy 0.3125, precision [0.0, 0.25, 1.0, 0.5, 0.5, nan, nan, 0.25, 0.0], recall [0.0, 0.25, 1.0, 0.2, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:45:11.807533: step 795, loss 1.89613, accuracy 0.625, precision [0.0, 1.0, nan, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.4, nan, 0.6, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:11.955186: step 796, loss 1.81712, accuracy 0.3125, precision [0.5, 0.25, 0.0, 0.75, 0.0, 0.0, nan, 0.0, 0.0], recall [0.3333333333333333, 0.3333333333333333, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:12.102797: step 797, loss 2.04275, accuracy 0.4375, precision [nan, 1.0, 0.0, 0.7142857142857143, 1.0, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.45454545454545453, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:12.256025: step 798, loss 1.69106, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.5, 0.3333333333333333, 0.0, nan, 0.25, nan], recall [nan, 0.25, 0.0, 0.2857142857142857, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:12.404542: step 799, loss 1.22658, accuracy 0.75, precision [nan, 0.3333333333333333, 1.0, 1.0, 1.0, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, 1.0, 0.5555555555555556, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:12.553009: step 800, loss 1.71251, accuracy 0.5, precision [0.0, 0.0, nan, 0.75, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.75, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:12.708728: step 801, loss 1.71688, accuracy 0.3125, precision [0.0, 0.3333333333333333, nan, 0.6, 0.0, nan, nan, 0.0, nan], recall [nan, 0.4, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:12.858759: step 802, loss 1.28448, accuracy 0.625, precision [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, nan, nan, 1.0, nan], recall [1.0, nan, nan, 0.6, 1.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:45:13.017863: step 803, loss 1.32853, accuracy 0.5625, precision [0.0, 1.0, nan, 1.0, 0.3333333333333333, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, nan, 0.36363636363636365, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:13.163877: step 804, loss 1.47876, accuracy 0.625, precision [0.0, 0.5, 0.0, 0.8571428571428571, 1.0, nan, 0.0, 1.0, nan], recall [nan, 0.5, nan, 0.8571428571428571, 0.25, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:45:13.308239: step 805, loss 2.14626, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.75, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:13.459456: step 806, loss 1.50979, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:13.608452: step 807, loss 1.42017, accuracy 0.5625, precision [nan, 0.0, 0.6666666666666666, 1.0, 0.5714285714285714, 0.0, 0.0, 1.0, nan], recall [nan, 0.0, 1.0, 0.2857142857142857, 0.8, nan, nan, 1.0, nan]
2019-02-19T19:45:13.757674: step 808, loss 1.59971, accuracy 0.5, precision [nan, 0.0, 0.0, 0.3333333333333333, 0.8333333333333334, 0.0, nan, 0.6666666666666666, nan], recall [nan, 0.0, nan, 0.16666666666666666, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:45:13.908954: step 809, loss 2.03491, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.2, nan, nan, 0.0, 0.0], recall [0.0, 1.0, nan, 0.16666666666666666, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:14.055325: step 810, loss 2.22762, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.6, 0.0, 0.0, 0.0, 0.5, nan], recall [nan, 0.3333333333333333, nan, 0.2727272727272727, nan, nan, nan, 0.5, nan]
2019-02-19T19:45:14.213728: step 811, loss 1.6614, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.25, 1.0, nan, nan, 0.5, 0.0], recall [0.0, 0.25, nan, 0.3333333333333333, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:45:14.364703: step 812, loss 1.4544, accuracy 0.625, precision [0.3333333333333333, nan, 0.0, 1.0, 0.6666666666666666, nan, 0.0, nan, nan], recall [1.0, nan, 0.0, 0.5833333333333334, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:14.513311: step 813, loss 1.53034, accuracy 0.5, precision [0.0, nan, nan, 0.7142857142857143, 0.6, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.5555555555555556, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:14.663600: step 814, loss 2.06005, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, nan, nan, nan, 0.0], recall [nan, 0.6666666666666666, 0.0, 0.2857142857142857, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:14.813303: step 815, loss 2.05546, accuracy 0.375, precision [0.3333333333333333, 0.2, 0.0, 0.75, 1.0, nan, nan, 0.0, nan], recall [1.0, 0.5, 0.0, 0.375, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:14.962703: step 816, loss 2.29731, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.1111111111111111, 0.25, nan, nan, nan, nan]
2019-02-19T19:45:15.116046: step 817, loss 1.80936, accuracy 0.375, precision [nan, 0.5, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, nan, 0.0, 0.6666666666666666, nan], recall [0.0, 1.0, 1.0, 0.14285714285714285, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:15.267579: step 818, loss 2.07296, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.5, 0.0, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:15.422800: step 819, loss 1.88889, accuracy 0.3125, precision [nan, 0.5, nan, 0.42857142857142855, 0.25, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5, 0.14285714285714285, nan, nan, nan, nan]
2019-02-19T19:45:15.578709: step 820, loss 1.6907, accuracy 0.25, precision [nan, 0.3333333333333333, nan, 0.2, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, 0.0, 0.16666666666666666, 0.4, nan, nan, 0.0, nan]
2019-02-19T19:45:15.726032: step 821, loss 1.6953, accuracy 0.5625, precision [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, nan, nan, 0.0, nan], recall [0.5, 0.6666666666666666, nan, 0.8, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:15.877826: step 822, loss 1.75087, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.4444444444444444, 0.0, 0.0, nan, 1.0, nan], recall [nan, 1.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:45:16.030957: step 823, loss 1.37658, accuracy 0.4375, precision [0.0, 0.25, 1.0, 0.5, 0.6666666666666666, nan, nan, nan, nan], recall [nan, 1.0, 0.5, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:16.176152: step 824, loss 1.98498, accuracy 0.4375, precision [0.0, 0.5, nan, 0.6666666666666666, 0.6, nan, 0.0, 0.5, 0.0], recall [nan, 0.5, nan, 0.4, 0.5, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:45:16.328186: step 825, loss 2.02294, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.4, 0.25, nan, 0.0, 0.0, nan], recall [0.0, 1.0, 0.0, 0.25, 0.25, nan, nan, nan, nan]
2019-02-19T19:45:16.474221: step 826, loss 1.83428, accuracy 0.5625, precision [1.0, 0.0, 0.0, 0.6, 1.0, 0.0, nan, 1.0, 0.0], recall [1.0, 0.0, nan, 0.5, 0.6, nan, nan, 0.5, nan]
2019-02-19T19:45:16.625102: step 827, loss 1.42127, accuracy 0.5, precision [0.0, 0.5, nan, 0.625, 1.0, 0.0, nan, 0.3333333333333333, nan], recall [0.0, 0.3333333333333333, 0.0, 0.7142857142857143, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:16.773876: step 828, loss 1.78369, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:16.921163: step 829, loss 2.01154, accuracy 0.4375, precision [0.5, 1.0, 0.0, 0.8, nan, 0.0, 0.0, 0.2, nan], recall [1.0, 0.5, 0.0, 0.5714285714285714, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:17.076549: step 830, loss 1.45459, accuracy 0.5625, precision [0.25, 0.0, 0.0, 0.875, 1.0, nan, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.7, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:17.227068: step 831, loss 1.56577, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.4, nan], recall [nan, 0.0, nan, 0.2727272727272727, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:17.375295: step 832, loss 1.78128, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.3333333333333333, 0.0], recall [0.0, 0.0, nan, 0.5, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:45:17.527390: step 833, loss 2.35546, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [nan, 0.0, nan, 0.2727272727272727, 0.25, nan, nan, nan, nan]
2019-02-19T19:45:17.676048: step 834, loss 1.86925, accuracy 0.5, precision [0.0, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.3333333333333333, nan], recall [nan, nan, nan, 0.5384615384615384, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:45:17.830536: step 835, loss 1.32812, accuracy 0.625, precision [nan, 0.8, 0.0, 1.0, 1.0, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, nan, 0.3333333333333333, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:17.980952: step 836, loss 1.82068, accuracy 0.5, precision [0.0, nan, 0.0, 0.875, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5384615384615384, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:18.130815: step 837, loss 1.874, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [nan, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:45:18.276640: step 838, loss 1.1896, accuracy 0.75, precision [nan, 0.25, nan, 1.0, 1.0, nan, 0.0, 1.0, nan], recall [0.0, 1.0, nan, 0.75, 1.0, nan, nan, 0.75, nan]
2019-02-19T19:45:18.429022: step 839, loss 1.99485, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, nan, nan], recall [0.0, nan, nan, 0.4166666666666667, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:18.576932: step 840, loss 1.87056, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, 0.0, nan, 0.0], recall [nan, nan, nan, 0.3333333333333333, 0.4, 0.0, nan, 0.0, nan]
2019-02-19T19:45:18.728211: step 841, loss 1.62424, accuracy 0.5, precision [1.0, 0.5, 0.0, 1.0, 0.5, nan, nan, 0.5, 0.0], recall [1.0, 1.0, nan, 0.2222222222222222, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:18.879640: step 842, loss 1.62542, accuracy 0.4375, precision [0.0, 0.0, nan, 1.0, 0.2, nan, 0.0, 0.3333333333333333, nan], recall [nan, nan, nan, 0.4166666666666667, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:45:19.034243: step 843, loss 1.60751, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, nan, 1.0, 0.0], recall [0.0, 0.0, nan, 0.5, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:45:19.184596: step 844, loss 1.68999, accuracy 0.3125, precision [nan, 0.2, 0.0, 0.6666666666666666, 0.5, 0.0, nan, 0.5, nan], recall [0.0, 1.0, nan, 0.2222222222222222, 0.3333333333333333, nan, nan, 0.5, nan]
2019-02-19T19:45:19.330648: step 845, loss 1.74389, accuracy 0.5625, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.5, nan, 0.0, nan, nan], recall [nan, 1.0, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:19.477663: step 846, loss 1.66379, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 1.0, nan, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:19.630747: step 847, loss 1.47851, accuracy 0.4375, precision [nan, 0.25, nan, 0.7142857142857143, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.4166666666666667, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:19.782411: step 848, loss 1.47107, accuracy 0.5, precision [0.5, 0.75, 0.0, 1.0, 1.0, 0.0, nan, nan, nan], recall [0.5, 1.0, nan, 0.125, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:19.934174: step 849, loss 1.66408, accuracy 0.4375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, nan, nan, nan], recall [1.0, 0.0, nan, 0.46153846153846156, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:20.081037: step 850, loss 1.656, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.75, 0.6666666666666666, 0.0, nan, 0.5, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:45:20.235865: step 851, loss 1.51039, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.7142857142857143, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.5, 0.0, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:45:20.381858: step 852, loss 1.6733, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.5714285714285714, 0.5, 0.0, nan, nan, nan], recall [nan, 0.0, nan, 0.36363636363636365, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:20.532400: step 853, loss 1.99758, accuracy 0.1875, precision [nan, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.13333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:45:20.682701: step 854, loss 1.63083, accuracy 0.5625, precision [0.5, 0.0, nan, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:45:20.832747: step 855, loss 1.85383, accuracy 0.4375, precision [1.0, 0.5, 0.0, 0.75, 1.0, nan, 0.0, 0.5, nan], recall [1.0, 0.5, 0.0, 0.375, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:45:20.981116: step 856, loss 1.69576, accuracy 0.4375, precision [0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.5, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:21.127634: step 857, loss 1.95336, accuracy 0.5, precision [0.5, 1.0, 0.0, 0.75, 1.0, 0.0, 0.0, nan, 0.0], recall [1.0, 0.6, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:21.283597: step 858, loss 1.56783, accuracy 0.5, precision [0.0, 1.0, 0.5, 0.8, 0.3333333333333333, 0.0, 0.0, 0.5, nan], recall [nan, 0.5, 0.5, 0.4, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:21.431892: step 859, loss 1.45916, accuracy 0.625, precision [0.5, 0.0, nan, 1.0, 1.0, 0.0, nan, nan, nan], recall [1.0, nan, nan, 0.5, 0.75, nan, nan, nan, nan]
2019-02-19T19:45:21.582783: step 860, loss 2.33669, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, nan, 0.0, 0.3076923076923077, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:21.732495: step 861, loss 1.42587, accuracy 0.4375, precision [0.0, 0.25, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:21.879856: step 862, loss 2.12614, accuracy 0.375, precision [1.0, 0.2, nan, 1.0, 0.0, nan, 0.0, 0.3333333333333333, 0.0], recall [0.25, 1.0, nan, 0.3, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:22.029653: step 863, loss 1.7702, accuracy 0.4375, precision [0.0, 0.4, 0.0, 1.0, 0.25, nan, nan, nan, nan], recall [nan, 0.5, 0.0, 0.4444444444444444, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:22.179879: step 864, loss 1.55648, accuracy 0.5625, precision [nan, 0.25, 0.0, 1.0, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.5454545454545454, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:22.325960: step 865, loss 1.38803, accuracy 0.5, precision [0.5, 0.25, 0.5, 0.6666666666666666, 1.0, nan, nan, nan, 0.0], recall [1.0, 1.0, 0.5, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:22.477237: step 866, loss 1.43524, accuracy 0.5, precision [0.0, 0.5, 0.0, 0.7142857142857143, 0.6666666666666666, 0.0, nan, nan, nan], recall [nan, 0.25, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:22.625529: step 867, loss 2.03293, accuracy 0.5, precision [1.0, nan, nan, 1.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0], recall [1.0, 0.0, 0.0, 0.5454545454545454, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:22.775585: step 868, loss 1.80308, accuracy 0.4375, precision [0.5, 0.0, nan, 0.75, 0.5, 0.5, nan, 0.0, nan], recall [0.3333333333333333, 0.0, nan, 0.42857142857142855, 1.0, 1.0, nan, nan, nan]
2019-02-19T19:45:22.925303: step 869, loss 1.73742, accuracy 0.625, precision [0.5, 0.5, 0.0, 0.8888888888888888, nan, nan, nan, nan, 0.0], recall [1.0, 0.3333333333333333, nan, 0.8, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:23.080652: step 870, loss 1.88096, accuracy 0.5, precision [0.0, 0.75, 0.0, 0.8, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.6, nan, 0.4444444444444444, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:23.228578: step 871, loss 2.01143, accuracy 0.5, precision [0.3333333333333333, 0.0, nan, 0.8571428571428571, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.6666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:23.377733: step 872, loss 1.57518, accuracy 0.5625, precision [0.0, 0.5, 0.0, 1.0, 0.5, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.5384615384615384, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:23.535623: step 873, loss 1.28894, accuracy 0.625, precision [1.0, 1.0, nan, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0], recall [0.5, 1.0, nan, 0.5454545454545454, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:23.687125: step 874, loss 1.68497, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.75, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:23.839254: step 875, loss 1.6617, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.25, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.36363636363636365, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:23.986842: step 876, loss 1.80081, accuracy 0.3125, precision [0.0, 0.3333333333333333, nan, 1.0, 0.4, nan, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.1, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:24.136253: step 877, loss 1.6104, accuracy 0.375, precision [nan, 0.0, 0.0, 0.75, 0.4, nan, nan, 0.5, 0.0], recall [0.0, 0.0, nan, 0.2727272727272727, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:24.285955: step 878, loss 1.56235, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.4, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:24.435726: step 879, loss 1.21567, accuracy 0.8125, precision [0.5, 1.0, 0.0, 1.0, 1.0, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.7142857142857143, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:45:24.592744: step 880, loss 1.89715, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:24.743152: step 881, loss 1.48056, accuracy 0.4375, precision [1.0, nan, 0.0, 0.8333333333333334, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [1.0, nan, nan, 0.45454545454545453, 0.3333333333333333, nan, 0.0, nan, nan]
2019-02-19T19:45:24.894423: step 882, loss 1.86133, accuracy 0.3125, precision [0.5, 0.2, nan, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:25.040564: step 883, loss 1.87224, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:45:25.191587: step 884, loss 1.86301, accuracy 0.5, precision [1.0, 0.5, 0.0, 0.8333333333333334, 1.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, 1.0, nan, 0.4166666666666667, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:25.343370: step 885, loss 1.4083, accuracy 0.5625, precision [1.0, 0.0, nan, 0.8571428571428571, 0.25, nan, 0.0, 0.5, nan], recall [0.5, nan, nan, 0.5454545454545454, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:25.494613: step 886, loss 1.73495, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:25.638939: step 887, loss 1.56477, accuracy 0.3125, precision [0.5, nan, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [0.5, nan, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:45:25.793287: step 888, loss 1.98081, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.5714285714285714, 0.2, nan, nan, nan, nan]
2019-02-19T19:45:25.941393: step 889, loss 1.50729, accuracy 0.5, precision [1.0, 0.5, 0.0, 0.625, 0.5, nan, 0.0, 0.0, nan], recall [0.5, 1.0, nan, 0.5555555555555556, 0.25, nan, nan, nan, nan]
2019-02-19T19:45:26.088441: step 890, loss 1.89751, accuracy 0.4375, precision [0.0, 1.0, nan, 0.5714285714285714, 0.6666666666666666, nan, 0.0, nan, 0.0], recall [0.0, 0.5, 0.0, 0.5714285714285714, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:26.237553: step 891, loss 1.84203, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.625, 0.0, 0.0, nan, 0.0, 0.0], recall [0.5, 0.0, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:26.389206: step 892, loss 1.56473, accuracy 0.5625, precision [1.0, 0.0, 0.0, 0.8, 1.0, 0.0, nan, 0.5, 0.0], recall [0.3333333333333333, nan, nan, 0.5, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:45:26.539505: step 893, loss 1.46428, accuracy 0.4375, precision [0.0, nan, 0.0, 0.8, 0.75, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4444444444444444, 0.6, nan, nan, nan, nan]
2019-02-19T19:45:26.688421: step 894, loss 2.25514, accuracy 0.25, precision [0.25, 0.2, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.16666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:26.838664: step 895, loss 1.86694, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.16666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:26.986457: step 896, loss 1.47998, accuracy 0.5, precision [nan, 0.5, nan, 0.6666666666666666, 0.5, nan, 0.0, 0.3333333333333333, nan], recall [nan, 0.5, nan, 0.4444444444444444, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:27.134661: step 897, loss 1.78932, accuracy 0.5, precision [0.0, nan, 0.0, 0.8333333333333334, 0.75, nan, 0.0, nan, nan], recall [0.0, nan, nan, 0.5, 0.75, nan, nan, nan, nan]
2019-02-19T19:45:27.278753: step 898, loss 1.62975, accuracy 0.5, precision [0.5, 0.3333333333333333, 0.0, 0.8, 1.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:27.435170: step 899, loss 1.48007, accuracy 0.625, precision [1.0, 0.0, 0.0, 1.0, 1.0, nan, 0.0, 1.0, 0.0], recall [1.0, nan, nan, 0.4444444444444444, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:45:27.587210: step 900, loss 2.14436, accuracy 0.125, precision [0.0, 0.0, nan, 0.2, 0.5, 0.0, nan, nan, nan], recall [0.0, 0.0, nan, 0.1111111111111111, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:27.736504: step 901, loss 1.85749, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.75, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.25, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:27.887070: step 902, loss 1.62727, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:45:28.034318: step 903, loss 1.88092, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.7, nan, 0.0, nan, nan, nan], recall [0.5, 0.0, nan, 0.6363636363636364, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:28.182325: step 904, loss 1.66515, accuracy 0.5, precision [0.0, 0.3333333333333333, nan, 0.8, 0.5, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.3333333333333333, nan, 0.5, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:45:28.330988: step 905, loss 1.84714, accuracy 0.3125, precision [nan, 0.25, 0.0, 0.8, 0.0, 0.0, 0.0, nan, nan], recall [nan, 1.0, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:28.478597: step 906, loss 1.94652, accuracy 0.5, precision [nan, 0.0, nan, 0.8571428571428571, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.5454545454545454, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:45:28.628126: step 907, loss 2.29683, accuracy 0.3125, precision [nan, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0], recall [nan, 0.5, nan, 0.2727272727272727, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:28.776085: step 908, loss 1.94129, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:45:28.921920: step 909, loss 1.68516, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.38461538461538464, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:29.071973: step 910, loss 1.55634, accuracy 0.5, precision [1.0, 0.5, nan, 0.8, 0.3333333333333333, 0.0, nan, 0.5, nan], recall [1.0, 1.0, nan, 0.36363636363636365, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:29.224126: step 911, loss 1.72076, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, nan, 0.0], recall [nan, 1.0, nan, 0.35714285714285715, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:29.378510: step 912, loss 1.47177, accuracy 0.4375, precision [nan, 0.5, 0.0, 0.625, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.25, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:29.527903: step 913, loss 1.60347, accuracy 0.375, precision [1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, nan, nan], recall [1.0, 1.0, nan, 0.2, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:29.679171: step 914, loss 2.34213, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8, 1.0, nan, nan, nan, 0.0], recall [nan, 0.0, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:45:29.830679: step 915, loss 1.83464, accuracy 0.4375, precision [0.0, nan, 0.0, 0.8333333333333334, 0.5, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.4166666666666667, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:45:29.987529: step 916, loss 1.99542, accuracy 0.3125, precision [0.3333333333333333, 1.0, 0.0, 0.75, 0.0, 0.0, 0.0, nan, 0.0], recall [1.0, 0.3333333333333333, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:45:30.136878: step 917, loss 1.51015, accuracy 0.625, precision [0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, nan, 0.5, nan], recall [nan, 1.0, 1.0, 0.375, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:30.290508: step 918, loss 1.85719, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.6, 0.5, 0.0, 0.0, 0.0, nan], recall [0.5, 0.0, 0.0, 0.42857142857142855, 0.5, 0.0, nan, nan, nan]
2019-02-19T19:45:30.438053: step 919, loss 1.69773, accuracy 0.5625, precision [0.0, 0.0, 0.5, 0.875, 1.0, nan, nan, nan, 0.0], recall [nan, nan, 0.5, 0.6363636363636364, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:30.587923: step 920, loss 1.73873, accuracy 0.4375, precision [nan, 0.25, nan, 1.0, 1.0, 0.0, 0.0, 0.5, nan], recall [0.0, 0.5, nan, 0.3, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:30.734514: step 921, loss 1.60778, accuracy 0.5625, precision [1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.3333333333333333, nan], recall [1.0, 1.0, 0.0, 0.5454545454545454, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:30.886243: step 922, loss 1.9261, accuracy 0.25, precision [0.0, 0.16666666666666666, 0.0, 0.6, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.3333333333333333, nan, 0.375, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:31.034108: step 923, loss 1.55875, accuracy 0.5, precision [0.0, 0.0, 1.0, 0.8333333333333334, 0.5, 0.0, nan, 1.0, nan], recall [0.0, 0.0, 1.0, 0.5, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:31.185525: step 924, loss 1.50926, accuracy 0.4375, precision [0.0, 0.3333333333333333, nan, 0.75, 0.75, nan, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.42857142857142855, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:31.337064: step 925, loss 1.84074, accuracy 0.5, precision [nan, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, nan], recall [0.0, 1.0, nan, 0.45454545454545453, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:31.484153: step 926, loss 1.41944, accuracy 0.625, precision [0.0, 0.0, 1.0, 1.0, 0.6, nan, nan, 0.0, nan], recall [0.0, nan, 1.0, 0.5454545454545454, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:31.628776: step 927, loss 1.78243, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:31.780827: step 928, loss 1.43985, accuracy 0.4375, precision [0.3333333333333333, 1.0, nan, 0.6666666666666666, 0.0, 0.0, nan, 0.5, nan], recall [1.0, 1.0, 0.0, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:31.933965: step 929, loss 1.27635, accuracy 0.5, precision [0.5, 0.6, 0.2, 1.0, 0.5, nan, nan, nan, nan], recall [1.0, 0.75, 1.0, 0.2222222222222222, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:32.081698: step 930, loss 1.90244, accuracy 0.3125, precision [0.0, 0.25, 0.0, 1.0, 0.5, 0.0, nan, nan, nan], recall [0.0, 0.25, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:32.232773: step 931, loss 1.67052, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.42857142857142855, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.375, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:32.385894: step 932, loss 1.64552, accuracy 0.5, precision [nan, 0.0, nan, 0.8, 0.8, 0.0, nan, 0.0, nan], recall [nan, nan, 0.0, 0.4, 0.8, nan, nan, nan, nan]
2019-02-19T19:45:32.531998: step 933, loss 1.54138, accuracy 0.375, precision [1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.2, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:32.682621: step 934, loss 1.84218, accuracy 0.4375, precision [0.0, 0.5, 0.25, 0.7142857142857143, 0.0, nan, nan, nan, nan], recall [nan, 0.3333333333333333, 1.0, 0.625, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:45:32.829960: step 935, loss 1.95768, accuracy 0.4375, precision [0.0, 0.0, 0.5, 1.0, 0.5, 0.0, nan, 0.25, nan], recall [nan, 0.0, 1.0, 0.4444444444444444, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:32.980504: step 936, loss 1.22798, accuracy 0.6875, precision [1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333, nan, nan, 1.0, nan], recall [1.0, 0.3333333333333333, 1.0, 0.5714285714285714, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:33.131625: step 937, loss 1.89866, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.5, 0.75, 0.0, nan, 0.0, nan], recall [0.0, 0.25, 0.0, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:33.278889: step 938, loss 1.96523, accuracy 0.3125, precision [0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.2, 0.0, 0.0, nan, nan], recall [0.0, 0.5, 1.0, 0.2, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:33.425616: step 939, loss 1.5748, accuracy 0.625, precision [0.0, 0.6, 0.0, 1.0, nan, nan, 0.0, nan, nan], recall [nan, 1.0, nan, 0.5833333333333334, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:33.572332: step 940, loss 1.42355, accuracy 0.375, precision [nan, nan, 0.0, 0.42857142857142855, 0.75, 0.0, nan, nan, 0.0], recall [0.0, 0.0, nan, 0.375, 0.6, nan, nan, nan, nan]
2019-02-19T19:45:33.720944: step 941, loss 1.94984, accuracy 0.5, precision [0.0, 0.3333333333333333, nan, 0.75, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:33.868444: step 942, loss 1.64755, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 0.25, 0.0, nan, 1.0, nan], recall [nan, nan, nan, 0.5, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:45:34.016739: step 943, loss 1.54895, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.5, 0.7142857142857143, 0.3333333333333333, nan, nan, nan, nan], recall [nan, 0.3333333333333333, 1.0, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:34.164530: step 944, loss 2.01183, accuracy 0.375, precision [0.0, nan, nan, 0.5714285714285714, 0.25, nan, 0.0, 0.5, nan], recall [0.0, 0.0, nan, 0.36363636363636365, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:34.311815: step 945, loss 1.70711, accuracy 0.4375, precision [1.0, 0.3333333333333333, 0.0, 0.6, 1.0, nan, nan, 0.0, nan], recall [1.0, 1.0, 0.0, 0.3, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:34.463804: step 946, loss 1.49124, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.75, 0.5, 0.0, nan, 0.5, nan], recall [nan, 0.25, 0.0, 0.375, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:34.612435: step 947, loss 1.95369, accuracy 0.25, precision [0.3333333333333333, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.1, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:34.760043: step 948, loss 2.09785, accuracy 0.25, precision [0.0, 0.25, nan, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.21428571428571427, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:34.909999: step 949, loss 2.01057, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, nan, nan, nan, 0.5, 0.0], recall [0.0, nan, nan, 0.3076923076923077, nan, nan, nan, 0.5, nan]
2019-02-19T19:45:35.059464: step 950, loss 1.41033, accuracy 0.5625, precision [nan, 0.6666666666666666, 1.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.5, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:35.210391: step 951, loss 1.26347, accuracy 0.6875, precision [nan, 1.0, 0.0, 1.0, 0.5, 0.0, nan, 1.0, nan], recall [nan, 1.0, nan, 0.6363636363636364, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:35.363757: step 952, loss 1.73795, accuracy 0.3125, precision [0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.3333333333333333, nan], recall [1.0, 0.0, 0.0, 0.14285714285714285, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:35.513078: step 953, loss 2.09526, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.6, nan, 0.0, 0.5, 0.0], recall [nan, 0.0, 0.0, 0.2222222222222222, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:45:35.660253: step 954, loss 1.61938, accuracy 0.5625, precision [0.5, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, nan, nan, 0.0], recall [1.0, 1.0, 1.0, 0.5454545454545454, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:45:35.808785: step 955, loss 1.55727, accuracy 0.375, precision [0.0, 0.2, 0.0, 0.4, 0.75, nan, nan, nan, nan], recall [0.0, 1.0, 0.0, 0.25, 0.75, 0.0, nan, nan, nan]
2019-02-19T19:45:35.960151: step 956, loss 1.68726, accuracy 0.3125, precision [0.0, 0.0, 0.25, 0.5, 0.25, nan, nan, 1.0, nan], recall [nan, 0.0, 0.5, 0.2857142857142857, 0.2, nan, nan, 1.0, nan]
2019-02-19T19:45:36.110099: step 957, loss 1.22818, accuracy 0.625, precision [1.0, 1.0, 0.0, 0.25, 1.0, nan, nan, 0.5, nan], recall [1.0, 1.0, nan, 0.3333333333333333, 0.4, nan, nan, 0.5, nan]
2019-02-19T19:45:36.260314: step 958, loss 1.94406, accuracy 0.3125, precision [0.0, 0.16666666666666666, nan, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3, 0.2, nan, nan, nan, nan]
2019-02-19T19:45:36.411067: step 959, loss 1.4371, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.8, 0.25, nan, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.4, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:45:36.555515: step 960, loss 1.70868, accuracy 0.4375, precision [0.0, 0.5, nan, 0.75, 0.75, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.5, 0.42857142857142855, nan, nan, nan, nan]
2019-02-19T19:45:36.703277: step 961, loss 1.76767, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.5, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:36.852249: step 962, loss 1.15729, accuracy 0.75, precision [nan, 0.5, 1.0, 1.0, 0.8333333333333334, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, 1.0, 0.5714285714285714, 0.8333333333333334, nan, nan, 1.0, nan]
2019-02-19T19:45:37.001529: step 963, loss 1.87572, accuracy 0.3125, precision [0.25, 0.5, 0.0, 0.5, 1.0, 0.0, nan, nan, nan], recall [1.0, 0.5, nan, 0.2857142857142857, 0.2, nan, nan, 0.0, nan]
2019-02-19T19:45:37.149275: step 964, loss 1.7383, accuracy 0.4375, precision [nan, 0.75, 0.0, 0.5, 0.5, 0.0, 0.0, 0.6666666666666666, nan], recall [0.0, 0.75, nan, 0.16666666666666666, 0.5, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:45:37.298668: step 965, loss 1.98759, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, nan, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:37.452163: step 966, loss 2.33726, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.75, 0.5, 0.0, 0.0, 0.25, 0.0], recall [nan, 0.0, 0.0, 0.375, 0.25, nan, nan, 0.5, nan]
2019-02-19T19:45:37.596962: step 967, loss 1.72531, accuracy 0.5, precision [1.0, 0.5, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 1.0, 0.0, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:37.747075: step 968, loss 1.97163, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5, nan, nan, 0.0, 0.0], recall [0.5, 0.0, nan, 0.5, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:37.898300: step 969, loss 1.99693, accuracy 0.25, precision [0.0, 0.3333333333333333, nan, 0.5, 0.25, nan, 0.0, 0.5, 0.0], recall [0.0, 0.5, 0.0, 0.14285714285714285, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:38.052589: step 970, loss 1.76164, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8, 0.0, 0.0, 0.0, 0.5, nan], recall [0.0, 0.5, nan, 0.4444444444444444, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:38.202301: step 971, loss 2.08801, accuracy 0.1875, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.5, 0.0, 0.0, 0.2222222222222222, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:38.347443: step 972, loss 1.66394, accuracy 0.4375, precision [0.0, 0.6666666666666666, 0.0, 0.6666666666666666, nan, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.4, nan, 0.5714285714285714, nan, nan, nan, 0.5, nan]
2019-02-19T19:45:38.502549: step 973, loss 1.93607, accuracy 0.4375, precision [nan, 0.4, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.45454545454545453, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:38.652259: step 974, loss 1.64777, accuracy 0.5, precision [0.0, 1.0, nan, 0.7142857142857143, 0.3333333333333333, 0.0, 0.0, 1.0, nan], recall [nan, 0.25, nan, 0.5, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:38.805302: step 975, loss 1.26064, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.6, nan, nan, 0.75, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.6, nan, nan, 1.0, nan]
2019-02-19T19:45:38.952773: step 976, loss 2.00308, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, nan, 0.0, nan, 1.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.3333333333333333, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:39.100134: step 977, loss 1.64904, accuracy 0.5, precision [0.5, 1.0, nan, 0.7142857142857143, 0.5, 0.0, nan, 0.0, 0.0], recall [0.5, 0.5, 0.0, 0.625, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:39.248825: step 978, loss 1.61622, accuracy 0.5, precision [nan, 0.25, 0.0, 1.0, 0.0, nan, nan, 1.0, nan], recall [nan, 0.5, nan, 0.4166666666666667, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:39.400103: step 979, loss 1.79753, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:39.551465: step 980, loss 1.54334, accuracy 0.5, precision [nan, 0.5, 0.25, 0.8333333333333334, 0.5, nan, 0.0, nan, 0.0], recall [nan, 1.0, 0.3333333333333333, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:39.704776: step 981, loss 1.62197, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 0.8, 0.3333333333333333, 0.0, nan, nan, nan], recall [1.0, nan, 0.0, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:39.852755: step 982, loss 2.23847, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.2, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.1, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:40.006336: step 983, loss 1.51494, accuracy 0.4375, precision [0.0, 0.25, nan, 0.8, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.4444444444444444, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:45:40.154279: step 984, loss 1.51268, accuracy 0.625, precision [0.0, 0.5, 0.0, 1.0, nan, 0.0, nan, nan, nan], recall [nan, 1.0, 0.0, 0.6363636363636364, nan, nan, nan, nan, nan]
2019-02-19T19:45:40.304322: step 985, loss 1.7416, accuracy 0.3125, precision [0.0, 0.3333333333333333, nan, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:45:40.456195: step 986, loss 1.71769, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, nan, 0.0, nan, 0.5, nan], recall [nan, 0.0, nan, 0.4166666666666667, nan, nan, nan, 0.5, nan]
2019-02-19T19:45:40.609570: step 987, loss 1.94603, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.5, 1.0, 0.0, nan, 0.5, 0.0], recall [nan, 0.0, nan, 0.375, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:45:40.760447: step 988, loss 1.57423, accuracy 0.375, precision [0.5, 0.0, nan, 0.75, 0.0, 0.0, 0.0, 0.6666666666666666, nan], recall [0.3333333333333333, 0.0, nan, 0.3, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:40.916039: step 989, loss 1.57974, accuracy 0.5625, precision [0.0, 0.5, nan, 1.0, 1.0, 0.0, nan, 0.3333333333333333, nan], recall [nan, 0.6666666666666666, nan, 0.45454545454545453, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:41.066074: step 990, loss 1.96785, accuracy 0.4375, precision [0.0, 0.5, 1.0, 1.0, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [0.0, 1.0, 1.0, 0.3, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:41.218583: step 991, loss 1.7477, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.8, 0.6, 0.0, 0.0, 1.0, nan], recall [nan, 0.0, nan, 0.4444444444444444, 0.75, nan, nan, 0.5, nan]
2019-02-19T19:45:41.364110: step 992, loss 1.55668, accuracy 0.6875, precision [0.0, 0.0, nan, 1.0, 0.8333333333333334, 0.0, nan, nan, 0.0], recall [nan, 0.0, nan, 0.6, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:41.517422: step 993, loss 1.62433, accuracy 0.375, precision [nan, 0.0, nan, 0.25, 1.0, 0.0, nan, 0.5, nan], recall [0.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:45:41.667082: step 994, loss 1.81144, accuracy 0.625, precision [0.5, 0.3333333333333333, 0.0, 0.8, 0.75, nan, nan, 1.0, nan], recall [0.3333333333333333, 0.5, nan, 0.6666666666666666, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:45:41.815820: step 995, loss 1.76481, accuracy 0.5, precision [0.6, 0.0, 0.0, 0.3333333333333333, 1.0, nan, nan, nan, 0.0], recall [1.0, 0.0, nan, 0.2, 0.5714285714285714, nan, nan, nan, nan]
2019-02-19T19:45:41.964220: step 996, loss 1.56428, accuracy 0.4375, precision [1.0, 0.5, nan, 0.16666666666666666, 1.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 1.0, nan, 0.3333333333333333, 0.2, nan, nan, 0.0, nan]
2019-02-19T19:45:42.113634: step 997, loss 1.52883, accuracy 0.5, precision [1.0, 0.5, 0.0, 0.6, 1.0, nan, 0.0, 0.0, nan], recall [0.6666666666666666, 1.0, 0.0, 0.42857142857142855, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:42.261604: step 998, loss 1.81751, accuracy 0.3125, precision [0.0, 0.0, nan, 0.6, 0.4, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, 0.2857142857142857, nan, nan, nan, nan]
2019-02-19T19:45:42.412871: step 999, loss 1.6745, accuracy 0.375, precision [0.4, nan, 0.0, 0.75, 0.5, 0.0, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.3, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:42.562200: step 1000, loss 2.16044, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.6666666666666666, nan], recall [1.0, nan, nan, 0.14285714285714285, 0.2, nan, nan, 0.6666666666666666, nan]

Evaluation:
[[ 41   2   0  39   4   0   0   0   0]
 [ 32  38   0  83   8   0   0   3   0]
 [  4   0   0  60  27   0   0   0   0]
 [ 15   4   0 238  45   0   0   2   0]
 [  5   0   0  61 105   0   0   0   0]
 [  2   0   0  37  10   0   0   2   0]
 [  1   0   0  15   9   0   0   1   0]
 [ 12   1   0  58   8   0   0  28   0]
 [  1   1   0  14   9   0   0   0   0]]
2019-02-19T19:45:44.995070: step 1000, loss 1.65592, accuracy 0.439024, precision [0.47674418604651164, 0.23170731707317074, 0.0, 0.7828947368421053, 0.6140350877192983, 0.0, 0.0, 0.2616822429906542, 0.0], recall [0.36283185840707965, 0.8260869565217391, nan, 0.3933884297520661, 0.4666666666666667, nan, nan, 0.7777777777777778, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550605378/checkpoints/model-1000

2019-02-19T19:45:45.222642: step 1001, loss 2.20175, accuracy 0.375, precision [0.5, 1.0, 0.0, 0.6, 0.5, 0.0, nan, 0.0, 0.0], recall [0.5, 1.0, 0.0, 0.375, 0.25, nan, nan, nan, nan]
2019-02-19T19:45:45.372424: step 1002, loss 1.69406, accuracy 0.5, precision [nan, 0.0, 0.0, 0.7142857142857143, 1.0, nan, nan, 0.25, 0.0], recall [nan, nan, nan, 0.5, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:45:45.525261: step 1003, loss 1.68909, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.5, 1.0, nan, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.375, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:45.672974: step 1004, loss 1.60213, accuracy 0.375, precision [0.0, 0.5, nan, 0.5714285714285714, 0.5, nan, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.5714285714285714, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:45:45.821237: step 1005, loss 1.65252, accuracy 0.4375, precision [0.3333333333333333, 0.0, 0.0, 0.6, 0.6666666666666666, 0.0, nan, 1.0, 0.0], recall [0.25, 0.0, 0.0, 0.5, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:45:45.971098: step 1006, loss 1.78474, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.6666666666666666, 0.0], recall [0.5, nan, nan, 0.3333333333333333, 0.0, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:45:46.121807: step 1007, loss 1.25256, accuracy 0.625, precision [0.8, 0.3333333333333333, 0.0, 0.75, 0.5, nan, nan, 1.0, nan], recall [0.5714285714285714, 1.0, nan, 0.75, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:45:46.272499: step 1008, loss 1.83303, accuracy 0.375, precision [1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.6, 1.0, nan, 0.2222222222222222, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:46.421335: step 1009, loss 1.83262, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.8, 0.75, nan, nan, 0.0, 0.0], recall [0.5, nan, 0.0, 0.5, 0.6, nan, nan, nan, nan]
2019-02-19T19:45:46.575762: step 1010, loss 1.59066, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.5, nan, 0.0, 0.6666666666666666, nan], recall [1.0, nan, nan, 0.36363636363636365, 1.0, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:45:46.729391: step 1011, loss 1.77376, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6, 0.2, nan, nan, 0.5, nan], recall [0.0, nan, nan, 0.2727272727272727, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:46.878587: step 1012, loss 1.66459, accuracy 0.5, precision [1.0, 0.0, 0.25, 1.0, 0.5, 0.0, nan, 0.5, nan], recall [0.5, nan, 1.0, 0.3333333333333333, 1.0, nan, 0.0, 1.0, nan]
2019-02-19T19:45:47.031665: step 1013, loss 1.43411, accuracy 0.5625, precision [1.0, 0.5, nan, 0.8333333333333334, 0.0, nan, 0.0, 1.0, nan], recall [1.0, 1.0, nan, 0.45454545454545453, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:47.183169: step 1014, loss 2.36014, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [0.0, nan, nan, 0.16666666666666666, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:47.329443: step 1015, loss 1.84378, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.75, 0.5, nan, 0.0, 1.0, 0.0], recall [0.0, nan, nan, 0.3333333333333333, 0.6666666666666666, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:45:47.477529: step 1016, loss 1.55239, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, nan, nan, 0.6666666666666666, nan], recall [0.0, 0.0, nan, 0.5, 0.6666666666666666, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:45:47.627759: step 1017, loss 1.89144, accuracy 0.375, precision [0.5, nan, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.375, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:45:47.776529: step 1018, loss 2.14238, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.3333333333333333, 0.0], recall [nan, 0.0, nan, 0.18181818181818182, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:47.926942: step 1019, loss 1.47344, accuracy 0.5, precision [1.0, 0.0, nan, 0.6666666666666666, 1.0, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.4, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:45:48.075443: step 1020, loss 1.69829, accuracy 0.625, precision [0.0, 0.0, nan, 1.0, 0.5, 0.0, 0.0, 1.0, nan], recall [0.0, nan, nan, 0.6153846153846154, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:48.226402: step 1021, loss 2.09296, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.4, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:48.374548: step 1022, loss 1.59601, accuracy 0.625, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.5, nan], recall [nan, 0.0, nan, 0.6153846153846154, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:48.519312: step 1023, loss 1.73386, accuracy 0.25, precision [0.0, 0.0, nan, 0.5, 1.0, 0.0, nan, 0.0, nan], recall [0.0, nan, 0.0, 0.2, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:45:48.670636: step 1024, loss 1.92258, accuracy 0.5, precision [0.0, 0.6666666666666666, nan, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:45:48.823572: step 1025, loss 1.50799, accuracy 0.4375, precision [0.75, 0.5, 0.0, 0.5, 0.5, 0.0, nan, 0.0, nan], recall [0.6, 0.5, nan, 0.16666666666666666, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:45:48.972292: step 1026, loss 1.29543, accuracy 0.4375, precision [0.75, 0.25, 0.25, 1.0, 0.0, nan, 0.0, nan, nan], recall [1.0, 0.5, 1.0, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:45:49.122982: step 1027, loss 1.8133, accuracy 0.375, precision [0.0, 0.25, 0.0, 0.8, 0.5, nan, 0.0, 0.0, nan], recall [nan, 1.0, 0.0, 0.4, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:49.269785: step 1028, loss 1.5036, accuracy 0.4375, precision [0.0, 1.0, nan, 0.6, 0.3333333333333333, nan, 0.0, 1.0, nan], recall [nan, 0.6666666666666666, 0.0, 0.375, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:49.414861: step 1029, loss 1.81154, accuracy 0.25, precision [nan, 0.0, 0.0, 0.75, 0.0, nan, nan, 0.5, nan], recall [0.0, nan, nan, 0.21428571428571427, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:49.563779: step 1030, loss 1.34174, accuracy 0.6875, precision [0.5, 1.0, 0.0, 0.8, 1.0, nan, nan, 1.0, 0.0], recall [1.0, 1.0, nan, 0.5, 1.0, nan, 0.0, 1.0, nan]
2019-02-19T19:45:49.715429: step 1031, loss 1.35615, accuracy 0.5625, precision [0.0, 0.6666666666666666, nan, 0.8, 0.75, nan, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.5, 0.75, nan, nan, nan, nan]
2019-02-19T19:45:49.865627: step 1032, loss 1.74016, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 0.75, 1.0, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.3, 0.4, nan, nan, nan, nan]
2019-02-19T19:45:50.015004: step 1033, loss 1.38564, accuracy 0.5, precision [0.0, 1.0, 0.0, 0.25, 0.5, 0.0, nan, 1.0, nan], recall [0.0, 0.5714285714285714, nan, 0.25, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:50.164092: step 1034, loss 1.36118, accuracy 0.6875, precision [0.0, 0.3333333333333333, 0.5, 1.0, 1.0, nan, nan, 0.6666666666666666, nan], recall [0.0, 0.5, 1.0, 0.8333333333333334, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:50.310701: step 1035, loss 1.88599, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.75, nan, nan, 1.0, 0.0], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.6, nan, 0.0, 0.5, nan]
2019-02-19T19:45:50.461351: step 1036, loss 1.70627, accuracy 0.375, precision [0.3333333333333333, nan, nan, 0.6666666666666666, nan, 0.0, nan, 0.2, nan], recall [0.25, 0.0, nan, 0.5, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:50.608766: step 1037, loss 1.19624, accuracy 0.6875, precision [0.3333333333333333, 0.5, nan, 1.0, 0.8, nan, nan, 0.5, nan], recall [0.5, 0.3333333333333333, nan, 0.6666666666666666, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:50.757247: step 1038, loss 1.79749, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [nan, 0.5, nan, 0.38461538461538464, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:50.909638: step 1039, loss 1.80839, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.75, 0.0, nan, 0.3333333333333333, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:45:51.064828: step 1040, loss 1.89727, accuracy 0.375, precision [0.5, 0.0, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:51.212994: step 1041, loss 2.05962, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 0.4, nan, 0.25, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:51.361435: step 1042, loss 1.96601, accuracy 0.375, precision [0.0, 0.5, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 1.0, 0.2222222222222222, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:51.516117: step 1043, loss 1.46289, accuracy 0.625, precision [0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, nan], recall [nan, 0.5, nan, 0.6, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:45:51.663836: step 1044, loss 1.56349, accuracy 0.5625, precision [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, nan], recall [0.5, nan, nan, 0.5555555555555556, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:45:51.811271: step 1045, loss 2.04956, accuracy 0.3125, precision [0.0, 0.25, nan, 0.6666666666666666, nan, 0.0, nan, 0.4, nan], recall [nan, 0.5, nan, 0.2222222222222222, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:51.958492: step 1046, loss 1.31059, accuracy 0.5, precision [nan, 0.5, 0.0, 0.6666666666666666, 0.3333333333333333, nan, nan, 0.5, nan], recall [nan, 1.0, nan, 0.4444444444444444, 0.25, nan, nan, 1.0, nan]
2019-02-19T19:45:52.113533: step 1047, loss 2.2984, accuracy 0.375, precision [0.5, 0.0, nan, 0.6, 0.5, 0.0, 0.0, 0.5, nan], recall [0.5, 0.0, nan, 0.3333333333333333, 1.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:45:52.262608: step 1048, loss 1.66332, accuracy 0.375, precision [0.75, 0.0, nan, 0.42857142857142855, nan, nan, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.375, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:52.412612: step 1049, loss 1.30503, accuracy 0.625, precision [nan, 1.0, 0.0, 0.6666666666666666, 0.6, nan, nan, 1.0, 0.0], recall [nan, 0.5, nan, 0.5714285714285714, 0.6, nan, nan, 1.0, nan]
2019-02-19T19:45:52.567255: step 1050, loss 1.32823, accuracy 0.5, precision [0.3333333333333333, 0.5, 0.0, 0.8, 0.5, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.4444444444444444, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:52.719119: step 1051, loss 1.06657, accuracy 0.625, precision [0.6666666666666666, nan, 0.0, 0.7, 0.5, nan, nan, nan, nan], recall [1.0, 0.0, nan, 0.7, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:45:52.870008: step 1052, loss 1.58901, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.5714285714285714, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:53.020122: step 1053, loss 2.00296, accuracy 0.375, precision [0.6666666666666666, 0.0, 0.0, 1.0, nan, 0.0, nan, 0.5, 0.0], recall [0.6666666666666666, nan, nan, 0.2727272727272727, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:53.170598: step 1054, loss 1.98899, accuracy 0.25, precision [nan, 0.0, nan, 0.5714285714285714, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:53.317560: step 1055, loss 1.64519, accuracy 0.375, precision [0.5, 0.5, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.3333333333333333, nan, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:53.462051: step 1056, loss 1.83508, accuracy 0.4375, precision [0.0, nan, nan, 0.8333333333333334, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:53.609717: step 1057, loss 1.99251, accuracy 0.375, precision [0.5, 0.0, 0.0, 0.8, 0.0, 0.0, nan, 0.5, 0.0], recall [1.0, 0.0, 0.0, 0.3333333333333333, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:53.763029: step 1058, loss 1.70263, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [1.0, nan, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:45:53.912953: step 1059, loss 1.90473, accuracy 0.4375, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 1.0, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:45:54.065365: step 1060, loss 1.71345, accuracy 0.5, precision [nan, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [nan, 1.0, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:54.215750: step 1061, loss 1.59383, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 1.0, nan], recall [nan, nan, 0.0, 0.38461538461538464, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:54.366397: step 1062, loss 1.83763, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, nan], recall [0.0, 1.0, nan, 0.4, nan, nan, nan, 0.5, nan]
2019-02-19T19:45:54.517920: step 1063, loss 1.55439, accuracy 0.5625, precision [nan, 0.5, nan, 0.7777777777777778, 0.0, 0.0, nan, 0.5, nan], recall [0.0, 0.5, nan, 0.6363636363636364, nan, nan, nan, 0.5, nan]
2019-02-19T19:45:54.667660: step 1064, loss 1.93811, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.8, 0.5, 0.0, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.36363636363636365, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:54.817405: step 1065, loss 1.63439, accuracy 0.375, precision [0.0, 0.25, nan, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.45454545454545453, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:54.961145: step 1066, loss 1.92195, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.6, 0.2, nan, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.3, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:55.109261: step 1067, loss 2.04268, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.2, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:55.257708: step 1068, loss 1.83481, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.6, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, 0.25, nan, 0.375, 0.25, nan, nan, nan, nan]
2019-02-19T19:45:55.403089: step 1069, loss 1.82649, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.8333333333333334, nan, 0.0, 0.0, 0.5, nan], recall [nan, 0.3333333333333333, nan, 0.5, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:45:55.552684: step 1070, loss 1.63734, accuracy 0.4375, precision [nan, 0.0, 0.3333333333333333, 0.5714285714285714, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 1.0, 0.4444444444444444, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:55.705760: step 1071, loss 1.36296, accuracy 0.5625, precision [0.3333333333333333, nan, nan, 1.0, 0.75, 0.0, nan, 0.5, nan], recall [1.0, 0.0, nan, 0.4444444444444444, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:45:55.852389: step 1072, loss 1.71027, accuracy 0.4375, precision [0.5, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.4, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:56.003268: step 1073, loss 1.58954, accuracy 0.4375, precision [nan, 0.3333333333333333, 1.0, 0.7142857142857143, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.3333333333333333, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:45:56.156386: step 1074, loss 2.24005, accuracy 0.25, precision [0.5, 0.16666666666666666, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.5, 0.0, 0.18181818181818182, nan, nan, nan, nan, nan]
2019-02-19T19:45:56.306064: step 1075, loss 1.67604, accuracy 0.4375, precision [0.5, 0.3333333333333333, nan, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.3333333333333333, nan, 0.4444444444444444, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:56.453452: step 1076, loss 1.6441, accuracy 0.3125, precision [0.0, 0.16666666666666666, 0.0, 1.0, 0.75, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.1, 0.75, nan, nan, nan, nan]
2019-02-19T19:45:56.601906: step 1077, loss 2.12611, accuracy 0.25, precision [0.0, 0.0, nan, 0.75, 0.25, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.21428571428571427, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:56.752357: step 1078, loss 1.71615, accuracy 0.5, precision [0.5, 0.6666666666666666, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.6666666666666666, nan, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:56.906584: step 1079, loss 1.91217, accuracy 0.375, precision [0.0, 0.25, nan, 0.75, 0.5, 0.0, nan, 0.3333333333333333, 0.0], recall [0.0, 0.25, nan, 0.375, 1.0, 0.0, nan, 1.0, nan]
2019-02-19T19:45:57.057930: step 1080, loss 2.72914, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [nan, 0.1, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:45:57.213081: step 1081, loss 1.72595, accuracy 0.4375, precision [0.5, 0.3333333333333333, 0.0, 0.75, nan, nan, 0.0, 1.0, 0.0], recall [0.5, 0.16666666666666666, nan, 0.6, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:57.367875: step 1082, loss 1.35868, accuracy 0.5625, precision [0.3333333333333333, 0.0, nan, 1.0, 1.0, 0.0, nan, 1.0, nan], recall [0.5, 0.0, nan, 0.6666666666666666, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:45:57.515805: step 1083, loss 1.81584, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.625, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.45454545454545453, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:57.665362: step 1084, loss 1.71124, accuracy 0.375, precision [nan, 0.4, 0.0, 0.75, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 1.0, nan, 0.375, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:45:57.814670: step 1085, loss 2.05521, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.09090909090909091, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:57.964027: step 1086, loss 1.85571, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, nan], recall [nan, 1.0, nan, 0.21428571428571427, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:58.111144: step 1087, loss 2.06525, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.5, 0.25, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.16666666666666666, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:45:58.258353: step 1088, loss 1.70657, accuracy 0.5, precision [0.0, nan, 0.0, 0.8888888888888888, nan, 0.0, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.6153846153846154, nan, nan, nan, 0.0, nan]
2019-02-19T19:45:58.406911: step 1089, loss 1.98716, accuracy 0.25, precision [nan, nan, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, nan], recall [nan, 0.0, nan, 0.15384615384615385, nan, nan, nan, 1.0, nan]
2019-02-19T19:45:58.557017: step 1090, loss 1.60456, accuracy 0.375, precision [1.0, 0.5, 0.0, 0.6, 0.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.5, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:45:58.703012: step 1091, loss 1.97522, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.5, nan, 0.0, 1.0, 0.0], recall [1.0, nan, nan, 0.45454545454545453, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:45:58.852483: step 1092, loss 1.72222, accuracy 0.4375, precision [1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0], recall [0.5, 1.0, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:45:59.000180: step 1093, loss 2.10476, accuracy 0.3125, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [1.0, nan, nan, 0.3076923076923077, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:45:59.147363: step 1094, loss 1.83809, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.25, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:45:59.299325: step 1095, loss 1.66386, accuracy 0.5, precision [nan, nan, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:45:59.452522: step 1096, loss 1.56138, accuracy 0.375, precision [nan, 0.3333333333333333, 0.2, 0.6666666666666666, 0.6666666666666666, nan, 0.0, 0.0, nan], recall [0.0, 0.5, 1.0, 0.2222222222222222, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:45:59.604894: step 1097, loss 1.75253, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.4166666666666667, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:45:59.758744: step 1098, loss 1.65936, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, 0.75, 0.0, 0.0, nan, nan], recall [nan, nan, nan, 0.4166666666666667, 0.75, nan, nan, nan, nan]
2019-02-19T19:45:59.906636: step 1099, loss 1.60884, accuracy 0.5625, precision [0.6666666666666666, nan, nan, 1.0, 0.75, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.4, 0.75, nan, nan, nan, nan]
2019-02-19T19:46:00.052885: step 1100, loss 1.54554, accuracy 0.375, precision [0.0, 0.0, nan, 0.6, 0.5, nan, 0.0, 0.5, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, nan, nan, 0.5, nan]
2019-02-19T19:46:00.202424: step 1101, loss 1.79218, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, nan, 0.0, nan], recall [1.0, nan, 0.0, 0.2, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:46:00.351963: step 1102, loss 1.89155, accuracy 0.375, precision [0.5, 0.5, 0.0, 0.8, 0.0, 0.0, nan, 0.0, 0.0], recall [0.5, 1.0, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:00.497915: step 1103, loss 1.4444, accuracy 0.625, precision [0.0, 1.0, nan, 0.7777777777777778, 1.0, nan, 0.0, 0.3333333333333333, nan], recall [nan, 1.0, 0.0, 0.7, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:46:00.652589: step 1104, loss 1.71382, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.8, 1.0, nan, 0.0, 1.0, 0.0], recall [1.0, nan, nan, 0.4, 0.3333333333333333, nan, nan, 0.5, nan]
2019-02-19T19:46:00.803926: step 1105, loss 1.36428, accuracy 0.625, precision [1.0, nan, 0.25, 0.875, 0.5, nan, nan, 0.0, nan], recall [0.5, nan, 1.0, 0.6363636363636364, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:00.956888: step 1106, loss 2.03938, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.6666666666666666, 0.0], recall [1.0, 0.0, 0.0, 0.2857142857142857, nan, nan, nan, 0.4, nan]
2019-02-19T19:46:01.102979: step 1107, loss 1.61326, accuracy 0.5, precision [nan, 0.0, nan, 0.7777777777777778, 0.25, 0.0, nan, nan, nan], recall [0.0, nan, nan, 0.5384615384615384, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:01.252533: step 1108, loss 1.29124, accuracy 0.5625, precision [0.0, nan, nan, 0.7142857142857143, 0.6, nan, nan, 0.3333333333333333, nan], recall [nan, 0.0, 0.0, 0.625, 0.75, nan, nan, 0.5, nan]
2019-02-19T19:46:01.398180: step 1109, loss 1.78306, accuracy 0.375, precision [nan, 0.0, 0.0, 1.0, 0.5, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:01.544490: step 1110, loss 1.93277, accuracy 0.4375, precision [nan, nan, 0.0, 0.8, 0.6666666666666666, nan, 0.0, 1.0, 0.0], recall [nan, nan, nan, 0.4, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:46:01.689367: step 1111, loss 1.42908, accuracy 0.4375, precision [nan, 0.14285714285714285, nan, 1.0, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:01.842722: step 1112, loss 1.78604, accuracy 0.5625, precision [0.0, nan, 0.5, 0.875, 1.0, 0.0, nan, nan, nan], recall [0.0, nan, 1.0, 0.5384615384615384, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:01.998104: step 1113, loss 2.49118, accuracy 0.0625, precision [0.0, 0.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.0], recall [nan, nan, nan, 0.0, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:02.148693: step 1114, loss 1.98916, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8, nan, 0.0, 0.0, 1.0, nan], recall [nan, nan, nan, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:02.298330: step 1115, loss 1.70953, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 0.75, 0.0, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:02.445867: step 1116, loss 1.75801, accuracy 0.4375, precision [1.0, 0.0, nan, 1.0, 0.75, 0.0, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.375, 0.6, nan, nan, nan, nan]
2019-02-19T19:46:02.596220: step 1117, loss 1.81473, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.5, 0.75, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.2857142857142857, 0.42857142857142855, nan, nan, 0.0, nan]
2019-02-19T19:46:02.746494: step 1118, loss 1.65603, accuracy 0.4375, precision [1.0, 0.5, 0.0, 0.6666666666666666, 1.0, nan, nan, 0.0, 0.0], recall [0.6666666666666666, 1.0, nan, 0.3333333333333333, 0.2, nan, nan, nan, nan]
2019-02-19T19:46:02.898613: step 1119, loss 1.82105, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.14285714285714285, 0.2857142857142857, nan, nan, nan, nan]
2019-02-19T19:46:03.049795: step 1120, loss 2.07618, accuracy 0.375, precision [0.5, 0.0, 0.0, 0.7142857142857143, nan, 0.0, 0.0, 0.0, 0.0], recall [0.5, nan, nan, 0.5555555555555556, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:03.201262: step 1121, loss 1.59195, accuracy 0.4375, precision [0.0, 0.3333333333333333, 1.0, 0.7142857142857143, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 1.0, 1.0, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:03.351832: step 1122, loss 1.33601, accuracy 0.625, precision [0.5, 0.6, 0.0, 0.8, 1.0, nan, nan, 1.0, 0.0], recall [1.0, 0.75, nan, 0.5, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:03.503183: step 1123, loss 1.83974, accuracy 0.5, precision [nan, 0.5, 0.0, 1.0, 0.6666666666666666, nan, 0.0, nan, 0.0], recall [nan, 0.3333333333333333, nan, 0.5555555555555556, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:03.648023: step 1124, loss 1.70233, accuracy 0.5625, precision [0.0, 1.0, 0.25, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, 1.0, 0.4166666666666667, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:03.796837: step 1125, loss 1.56739, accuracy 0.5, precision [nan, 0.75, 0.0, 0.75, 0.5, 0.0, nan, 0.5, 0.0], recall [0.0, 0.75, nan, 0.375, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:03.945623: step 1126, loss 1.69779, accuracy 0.5, precision [1.0, 0.5, nan, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.4, nan, 0.5555555555555556, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:04.096705: step 1127, loss 1.8062, accuracy 0.375, precision [1.0, 0.5, 0.0, 0.75, 0.5, 0.0, 0.0, 0.0, nan], recall [0.5, 0.5, nan, 0.3333333333333333, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:04.246272: step 1128, loss 1.57921, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.6, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.4, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:04.398070: step 1129, loss 1.49194, accuracy 0.5625, precision [1.0, 0.5, 0.0, 0.875, nan, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.6363636363636364, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:04.546485: step 1130, loss 1.7806, accuracy 0.4375, precision [0.0, 0.5, 0.5, 0.8, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.3333333333333333, 1.0, 0.4444444444444444, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:04.689920: step 1131, loss 1.74584, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.36363636363636365, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:04.837910: step 1132, loss 1.76566, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.8, 0.25, 0.0, nan, 0.0, nan], recall [0.0, 0.25, nan, 0.4444444444444444, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:04.990495: step 1133, loss 1.98903, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 1.0, nan, 0.0, nan, 0.25, nan], recall [nan, 0.25, nan, 0.09090909090909091, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:05.140962: step 1134, loss 1.38818, accuracy 0.75, precision [nan, 0.5, 0.0, 1.0, 0.5, nan, nan, 0.6666666666666666, nan], recall [0.0, 1.0, nan, 0.7272727272727273, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:05.296173: step 1135, loss 1.43867, accuracy 0.5625, precision [0.0, 0.6666666666666666, 0.5, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 1.0, 1.0, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:46:05.443898: step 1136, loss 1.2827, accuracy 0.625, precision [1.0, 0.75, 0.5, 1.0, 0.5, nan, nan, 0.0, nan], recall [0.5, 0.75, 1.0, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:05.593653: step 1137, loss 1.88946, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.4166666666666667, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:05.743961: step 1138, loss 2.29116, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan, 0.0], recall [0.0, 0.0, nan, 0.2, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:05.888685: step 1139, loss 1.38202, accuracy 0.625, precision [0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, nan, 0.0, 0.0, nan], recall [nan, nan, 1.0, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:06.044503: step 1140, loss 1.70105, accuracy 0.5625, precision [0.0, nan, 0.0, 0.9, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.6923076923076923, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:06.196849: step 1141, loss 1.65736, accuracy 0.375, precision [1.0, 0.14285714285714285, 0.3333333333333333, 1.0, 0.0, nan, 0.0, nan, nan], recall [0.5, 1.0, 1.0, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:46:06.345785: step 1142, loss 1.72442, accuracy 0.5625, precision [nan, 0.5, 0.0, 1.0, 1.0, nan, 0.0, 0.6666666666666666, 0.0], recall [nan, 1.0, 0.0, 0.25, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:06.493208: step 1143, loss 1.93385, accuracy 0.375, precision [nan, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.2857142857142857, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:06.641880: step 1144, loss 1.46359, accuracy 0.625, precision [0.6666666666666666, 0.6666666666666666, nan, 0.75, 1.0, 0.0, nan, 0.5, nan], recall [1.0, 1.0, nan, 0.375, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:06.788869: step 1145, loss 1.94751, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.23076923076923078, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:06.939211: step 1146, loss 1.53782, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.75, 0.5, 0.0, 0.0, 1.0, nan], recall [nan, 0.5, nan, 0.3333333333333333, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:46:07.086887: step 1147, loss 1.81707, accuracy 0.5, precision [0.0, 0.25, nan, 0.6666666666666666, 0.75, nan, nan, 0.6666666666666666, 0.0], recall [nan, 1.0, 0.0, 0.2857142857142857, 0.75, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:46:07.240115: step 1148, loss 1.60193, accuracy 0.4375, precision [0.5, 0.0, 0.0, 0.6, 0.75, 0.0, nan, 0.0, nan], recall [1.0, nan, nan, 0.42857142857142855, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:07.390533: step 1149, loss 1.95112, accuracy 0.375, precision [0.0, 1.0, nan, 0.8, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.5714285714285714, 0.14285714285714285, nan, nan, 0.0, nan]
2019-02-19T19:46:07.541372: step 1150, loss 1.42893, accuracy 0.5625, precision [0.5, 0.5, 0.5, 0.5714285714285714, 1.0, nan, nan, 0.5, nan], recall [1.0, 1.0, 0.5, 0.5714285714285714, 0.25, nan, nan, 1.0, nan]
2019-02-19T19:46:07.691615: step 1151, loss 1.95075, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.0, 0.5, 0.0, nan, 0.3333333333333333, nan], recall [nan, 0.0, 0.0, 0.0, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:46:07.846108: step 1152, loss 1.74188, accuracy 0.375, precision [0.0, 0.4, nan, 0.6666666666666666, 0.5, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, nan, 0.2857142857142857, 0.16666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:07.996489: step 1153, loss 1.72773, accuracy 0.375, precision [0.25, 0.0, nan, 0.5714285714285714, 0.5, nan, nan, nan, nan], recall [0.3333333333333333, 0.0, nan, 0.5714285714285714, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:46:08.100746: step 1154, loss 1.64534, accuracy 0.3, precision [nan, 0.0, 0.0, 0.5, nan, nan, nan, 0.5, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:08.253881: step 1155, loss 1.71353, accuracy 0.5, precision [0.3333333333333333, 0.75, 0.0, 0.8, 0.0, 0.0, nan, nan, 0.0], recall [1.0, 0.75, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:08.404751: step 1156, loss 1.75038, accuracy 0.5, precision [nan, 1.0, nan, 0.7142857142857143, 0.4, 0.0, 0.0, nan, 0.0], recall [0.0, 0.5, nan, 0.5555555555555556, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:08.557204: step 1157, loss 1.51319, accuracy 0.4375, precision [1.0, 0.5, 0.0, 1.0, 0.4, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.2, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:08.704945: step 1158, loss 1.68086, accuracy 0.25, precision [nan, nan, 0.0, 0.4444444444444444, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.4444444444444444, nan, nan, nan, 0.0, nan]
2019-02-19T19:46:08.852790: step 1159, loss 1.31274, accuracy 0.4375, precision [0.6666666666666666, 0.0, nan, 0.6666666666666666, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.6666666666666666, 0.0, nan, 0.4444444444444444, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:09.001941: step 1160, loss 1.39117, accuracy 0.625, precision [0.0, 1.0, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.6153846153846154, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:09.152171: step 1161, loss 1.78948, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.38461538461538464, nan, nan, nan, 0.0, nan]
2019-02-19T19:46:09.302204: step 1162, loss 1.42249, accuracy 0.625, precision [0.5, 0.3333333333333333, 0.0, 0.8571428571428571, 1.0, nan, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.6, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:09.448424: step 1163, loss 1.96036, accuracy 0.25, precision [nan, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.3076923076923077, nan, nan, nan, 0.0, nan]
2019-02-19T19:46:09.601882: step 1164, loss 2.16028, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:09.753829: step 1165, loss 1.70891, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.4, 0.0, nan, 1.0, nan], recall [nan, nan, nan, 0.15384615384615385, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:09.903241: step 1166, loss 1.69982, accuracy 0.5, precision [1.0, nan, 0.0, 1.0, 0.0, nan, nan, 1.0, 0.0], recall [0.5, nan, nan, 0.4166666666666667, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:10.049562: step 1167, loss 1.28864, accuracy 0.5625, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:46:10.198902: step 1168, loss 2.17617, accuracy 0.125, precision [0.0, 0.0, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.13333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:10.349393: step 1169, loss 2.22031, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0], recall [nan, nan, nan, 0.42857142857142855, nan, nan, nan, 0.5, nan]
2019-02-19T19:46:10.506383: step 1170, loss 1.39822, accuracy 0.4375, precision [nan, 0.5, 0.0, 0.6666666666666666, 0.5714285714285714, nan, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.2222222222222222, 0.8, nan, nan, nan, nan]
2019-02-19T19:46:10.654302: step 1171, loss 1.94756, accuracy 0.25, precision [1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:10.807042: step 1172, loss 2.4334, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, nan, 0.0, 0.2222222222222222, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:10.957961: step 1173, loss 1.84777, accuracy 0.3125, precision [1.0, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.5, 1.0, nan, 0.2222222222222222, 0.25, nan, nan, nan, nan]
2019-02-19T19:46:11.108489: step 1174, loss 2.20828, accuracy 0.25, precision [0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.18181818181818182, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:11.257690: step 1175, loss 1.35532, accuracy 0.5625, precision [nan, 0.0, 0.0, 0.6666666666666666, 1.0, nan, nan, 0.3333333333333333, nan], recall [0.0, nan, 0.0, 0.8571428571428571, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:46:11.407933: step 1176, loss 1.7088, accuracy 0.5625, precision [0.0, nan, nan, 0.7777777777777778, 0.6666666666666666, 0.0, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.6363636363636364, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:11.559322: step 1177, loss 1.73546, accuracy 0.375, precision [0.0, 0.25, 0.0, 0.5, 0.6, nan, nan, 0.5, nan], recall [0.0, 1.0, nan, 0.14285714285714285, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:11.708726: step 1178, loss 1.48067, accuracy 0.375, precision [0.5, nan, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.3333333333333333, nan], recall [0.5, 0.0, nan, 0.0, 0.8, nan, nan, 0.5, nan]
2019-02-19T19:46:11.860848: step 1179, loss 1.76076, accuracy 0.3125, precision [0.3333333333333333, 0.0, 0.0, 0.75, 0.16666666666666666, nan, 0.0, nan, nan], recall [1.0, nan, 0.0, 0.3, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:12.004948: step 1180, loss 1.8498, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6, 0.3333333333333333, nan, 0.0, 0.5, nan], recall [0.0, nan, nan, 0.3, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:46:12.153200: step 1181, loss 1.86694, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.16666666666666666, 0.3333333333333333, nan, nan, nan, 0.0], recall [nan, 0.0, nan, 0.16666666666666666, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:12.304434: step 1182, loss 1.45363, accuracy 0.5, precision [nan, 0.6666666666666666, 1.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, 1.0, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:12.454129: step 1183, loss 1.28025, accuracy 0.6875, precision [1.0, 0.5, 1.0, 0.875, nan, 0.0, nan, 0.0, nan], recall [1.0, 1.0, 0.6666666666666666, 0.7, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:12.602682: step 1184, loss 1.77102, accuracy 0.25, precision [nan, 1.0, 0.0, 0.6666666666666666, 0.2, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.18181818181818182, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:12.749555: step 1185, loss 1.58548, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, nan, 1.0, nan], recall [nan, 0.0, nan, 0.5, 1.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:46:12.900599: step 1186, loss 1.58473, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.6666666666666666, nan], recall [nan, 0.0, nan, 0.45454545454545453, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:13.052716: step 1187, loss 1.53799, accuracy 0.4375, precision [0.5, 0.0, 0.3333333333333333, 1.0, nan, nan, nan, 0.25, 0.0], recall [1.0, 0.0, 1.0, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:13.202955: step 1188, loss 1.77151, accuracy 0.3125, precision [0.0, 0.0, nan, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:13.352438: step 1189, loss 1.32928, accuracy 0.625, precision [0.0, nan, 0.0, 0.875, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5833333333333334, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:13.502254: step 1190, loss 1.62026, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.7, 0.0, nan, 0.0, nan, 0.0], recall [nan, nan, 0.0, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:13.656846: step 1191, loss 1.59714, accuracy 0.5, precision [1.0, 0.3333333333333333, nan, 0.6666666666666666, 0.75, 0.0, 0.0, 0.5, nan], recall [1.0, 1.0, nan, 0.25, 0.6, nan, nan, 1.0, nan]
2019-02-19T19:46:13.806296: step 1192, loss 1.31531, accuracy 0.5625, precision [0.0, 1.0, nan, 0.75, 0.8, nan, 0.0, 0.25, nan], recall [0.0, 1.0, nan, 0.42857142857142855, 0.8, nan, nan, 0.5, nan]
2019-02-19T19:46:13.950424: step 1193, loss 1.5205, accuracy 0.5625, precision [0.0, nan, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:14.095973: step 1194, loss 1.82805, accuracy 0.375, precision [0.0, nan, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.4, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:46:14.244668: step 1195, loss 1.91475, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.0, 1.0, 1.0, nan, 0.0, 0.5, 0.0], recall [nan, 1.0, nan, 0.4166666666666667, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:46:14.391558: step 1196, loss 1.18573, accuracy 0.6875, precision [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, nan, 0.0, 0.5, nan], recall [1.0, nan, nan, 0.5454545454545454, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:14.541371: step 1197, loss 1.4059, accuracy 0.5625, precision [0.25, nan, 0.0, 0.875, 0.3333333333333333, nan, nan, nan, nan], recall [1.0, nan, 0.0, 0.6363636363636364, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:14.695940: step 1198, loss 1.58955, accuracy 0.4375, precision [0.0, 0.2, 0.0, 1.0, 1.0, nan, nan, 0.5, nan], recall [nan, 0.5, nan, 0.2727272727272727, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:14.842992: step 1199, loss 1.64672, accuracy 0.4375, precision [0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.4, 0.25, nan, nan, nan, nan]
2019-02-19T19:46:14.987408: step 1200, loss 1.93661, accuracy 0.375, precision [0.0, nan, 0.0, 0.75, 0.6, nan, 0.0, nan, nan], recall [0.0, nan, nan, 0.3, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:15.137548: step 1201, loss 1.4946, accuracy 0.5, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, nan, 1.0, nan], recall [nan, nan, nan, 0.4666666666666667, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:15.291808: step 1202, loss 1.73476, accuracy 0.375, precision [0.5, 0.5, 0.0, 1.0, 0.25, nan, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:15.440345: step 1203, loss 1.19205, accuracy 0.6875, precision [0.0, 0.3333333333333333, 1.0, 0.8571428571428571, 1.0, nan, nan, 1.0, 0.0], recall [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:46:15.587259: step 1204, loss 1.69695, accuracy 0.4375, precision [nan, 0.5, nan, 0.6, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:15.740674: step 1205, loss 1.60583, accuracy 0.375, precision [1.0, 0.2, 0.5, 0.6666666666666666, 0.5, 0.0, nan, 0.0, 0.0], recall [0.5, 1.0, 1.0, 0.2, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:15.889891: step 1206, loss 1.64269, accuracy 0.4375, precision [0.0, 0.0, nan, 1.0, 0.4, 0.0, nan, 0.5, nan], recall [0.0, nan, nan, 0.4, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:16.038111: step 1207, loss 1.58706, accuracy 0.5625, precision [0.0, 0.6666666666666666, 0.0, 0.7142857142857143, 0.6666666666666666, 0.0, nan, nan, nan], recall [0.0, 1.0, nan, 0.625, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:16.185588: step 1208, loss 1.69802, accuracy 0.625, precision [1.0, 0.0, nan, 1.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.6363636363636364, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:16.338676: step 1209, loss 1.69318, accuracy 0.5, precision [nan, 0.4, 0.0, 1.0, 0.25, nan, 0.0, nan, nan], recall [nan, 1.0, nan, 0.4166666666666667, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:16.489971: step 1210, loss 2.11918, accuracy 0.375, precision [0.3333333333333333, nan, 0.0, 0.8, 1.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.36363636363636365, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:16.643158: step 1211, loss 1.85935, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:16.795135: step 1212, loss 1.79027, accuracy 0.3125, precision [0.0, 0.0, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:46:16.942836: step 1213, loss 2.08703, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.25, nan], recall [nan, 0.0, nan, 0.2727272727272727, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:46:17.088204: step 1214, loss 1.6672, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6, 0.0, nan, 0.0, 0.6666666666666666, nan], recall [0.0, 0.0, 0.0, 0.3, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:17.235726: step 1215, loss 1.47986, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 1.0, nan], recall [nan, nan, nan, 0.35714285714285715, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:17.387060: step 1216, loss 1.64301, accuracy 0.25, precision [0.0, 0.25, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:17.534084: step 1217, loss 1.59065, accuracy 0.375, precision [0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, 0.6666666666666666, 0.375, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:17.682526: step 1218, loss 1.50882, accuracy 0.375, precision [nan, 0.6666666666666666, 0.0, 0.75, 0.0, 0.0, nan, 0.5, nan], recall [nan, 0.6666666666666666, 0.0, 0.2727272727272727, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:17.833608: step 1219, loss 1.55382, accuracy 0.4375, precision [0.4, 0.5, 0.0, 0.75, 0.3333333333333333, 0.0, nan, nan, nan], recall [1.0, 0.2, nan, 0.375, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:17.980963: step 1220, loss 1.9699, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 0.3333333333333333, 0.0, nan, nan, 0.0], recall [nan, nan, nan, 0.23076923076923078, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:18.131787: step 1221, loss 1.36328, accuracy 0.6875, precision [nan, 1.0, 0.5, 1.0, 0.6, nan, nan, 1.0, 0.0], recall [nan, 1.0, 1.0, 0.4444444444444444, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:18.280560: step 1222, loss 2.00303, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 1.0, nan], recall [0.0, nan, nan, 0.16666666666666666, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:18.438634: step 1223, loss 1.46067, accuracy 0.5, precision [0.5, 0.5, 0.0, 0.75, 0.5, 0.0, nan, nan, nan], recall [0.5, 1.0, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:18.583605: step 1224, loss 1.80965, accuracy 0.4375, precision [0.0, 0.0, nan, 0.75, 0.6, 0.0, 0.0, 1.0, nan], recall [nan, 0.0, nan, 0.3, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:46:18.731362: step 1225, loss 1.456, accuracy 0.625, precision [0.5, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.5, 0.0], recall [1.0, 0.0, nan, 0.6363636363636364, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:18.877787: step 1226, loss 1.77289, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.25, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:46:19.029844: step 1227, loss 1.88118, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8, 0.6666666666666666, nan, 0.0, 0.0, nan], recall [nan, nan, 0.0, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:19.183774: step 1228, loss 1.63859, accuracy 0.4375, precision [0.0, 0.6, 0.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.75, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:19.337747: step 1229, loss 1.82302, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.3333333333333333, 0.0, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:19.487336: step 1230, loss 1.76077, accuracy 0.4375, precision [0.5, nan, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, nan], recall [1.0, 0.0, nan, 0.25, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:46:19.632199: step 1231, loss 1.71896, accuracy 0.1875, precision [0.5, 0.0, 0.0, 0.25, 0.5, nan, nan, 0.0, nan], recall [0.3333333333333333, nan, nan, 0.1, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:19.776928: step 1232, loss 2.55556, accuracy 0.1875, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.18181818181818182, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:19.930388: step 1233, loss 1.37542, accuracy 0.4375, precision [1.0, 0.5, 0.5, 0.42857142857142855, 0.0, 0.0, nan, 0.5, nan], recall [1.0, 1.0, 0.3333333333333333, 0.42857142857142855, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:20.079701: step 1234, loss 1.42762, accuracy 0.625, precision [1.0, 0.25, 1.0, 0.7142857142857143, 1.0, 0.0, nan, nan, nan], recall [0.5, 1.0, 1.0, 0.7142857142857143, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:20.226991: step 1235, loss 1.82751, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, nan, 0.0], recall [0.0, nan, nan, 0.5, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:20.382590: step 1236, loss 1.64319, accuracy 0.5, precision [0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, nan], recall [nan, 1.0, nan, 0.36363636363636365, 1.0, nan, 0.0, 1.0, nan]
2019-02-19T19:46:20.531040: step 1237, loss 1.56599, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.25, 0.0, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:20.686035: step 1238, loss 1.47306, accuracy 0.5, precision [0.5, 0.25, nan, 1.0, 0.8, nan, nan, 0.0, 0.0], recall [0.5, 0.5, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:20.833219: step 1239, loss 1.48079, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 0.75, 0.0, 0.0, nan, 0.6666666666666666, nan], recall [1.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:20.980091: step 1240, loss 1.34361, accuracy 0.4375, precision [0.3333333333333333, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, nan, 0.0, nan], recall [0.5, 1.0, 1.0, 0.25, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:21.129001: step 1241, loss 1.61541, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8, 0.25, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:21.275871: step 1242, loss 1.55683, accuracy 0.4375, precision [0.0, 0.6666666666666666, nan, 0.5714285714285714, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.6666666666666666, 0.2, nan, nan, nan, nan]
2019-02-19T19:46:21.418893: step 1243, loss 1.88889, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 0.5, nan, 0.0, 0.25, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:21.570543: step 1244, loss 1.3624, accuracy 0.5, precision [1.0, 0.2, nan, 0.6666666666666666, 0.6666666666666666, nan, nan, 0.0, nan], recall [0.5, 0.5, nan, 0.5714285714285714, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:21.720593: step 1245, loss 1.59288, accuracy 0.4375, precision [0.5, 0.25, nan, 0.6, 1.0, 0.0, 0.0, 0.5, nan], recall [0.3333333333333333, 0.5, nan, 0.375, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:46:21.873914: step 1246, loss 1.42187, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 1.0, nan, 0.0, 0.5, nan], recall [nan, 0.0, 0.0, 0.45454545454545453, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:22.024100: step 1247, loss 1.86931, accuracy 0.375, precision [0.3333333333333333, 0.3333333333333333, 0.0, 0.6, 1.0, nan, 0.0, 0.0, nan], recall [0.5, 0.5, 0.0, 0.3, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:22.174191: step 1248, loss 1.85593, accuracy 0.5, precision [0.0, 0.0, nan, 1.0, 0.4, nan, nan, nan, 0.0], recall [nan, nan, nan, 0.5, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:46:22.324861: step 1249, loss 1.59712, accuracy 0.375, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.18181818181818182, nan, nan, nan, 0.0, nan]
2019-02-19T19:46:22.482441: step 1250, loss 1.41835, accuracy 0.5, precision [1.0, 0.3333333333333333, nan, 0.7142857142857143, nan, nan, nan, 0.0, 0.0], recall [0.5, 0.5, nan, 0.625, 0.0, nan, nan, 0.0, nan]

Evaluation:
[[ 19  26   0  41   0   0   0   0   0]
 [  6  70   0  83   2   0   0   3   0]
 [  0   3   0  87   1   0   0   0   0]
 [  1  29   0 271   2   0   0   1   0]
 [  1   1   0 113  56   0   0   0   0]
 [  0   3   0  45   2   0   0   1   0]
 [  0   1   0  24   0   0   0   1   0]
 [  3   5   0  62   1   0   0  36   0]
 [  1   3   0  20   1   0   0   0   0]]
2019-02-19T19:46:24.909893: step 1250, loss 1.61792, accuracy 0.440976, precision [0.22093023255813954, 0.4268292682926829, 0.0, 0.8914473684210527, 0.32748538011695905, 0.0, 0.0, 0.3364485981308411, 0.0], recall [0.6129032258064516, 0.49645390070921985, nan, 0.3632707774798928, 0.8615384615384616, nan, nan, 0.8571428571428571, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550605378/checkpoints/model-1250

2019-02-19T19:46:25.142254: step 1251, loss 1.42537, accuracy 0.4375, precision [0.5, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, nan, nan, 1.0, 0.0], recall [1.0, 0.5, nan, 0.2222222222222222, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:25.293222: step 1252, loss 1.19638, accuracy 0.5625, precision [nan, 0.5714285714285714, 0.0, 1.0, 0.0, nan, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.5555555555555556, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:25.442595: step 1253, loss 1.5546, accuracy 0.4375, precision [nan, 0.3333333333333333, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:46:25.586439: step 1254, loss 1.45345, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.625, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:25.734417: step 1255, loss 1.86033, accuracy 0.3125, precision [0.5, 0.3333333333333333, 0.0, 0.75, 0.0, nan, nan, 0.0, 0.0], recall [1.0, 0.3333333333333333, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:46:25.885883: step 1256, loss 1.4776, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.75, 0.5, 0.0, nan, 1.0, nan], recall [nan, 0.0, nan, 0.6, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:26.032694: step 1257, loss 2.04246, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.0, 0.0, nan, nan, nan], recall [nan, 0.3333333333333333, nan, 0.2727272727272727, nan, nan, nan, 0.0, nan]
2019-02-19T19:46:26.182477: step 1258, loss 1.80143, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.45454545454545453, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:26.329359: step 1259, loss 1.55586, accuracy 0.5625, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [1.0, nan, nan, 0.5384615384615384, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:26.475298: step 1260, loss 1.8189, accuracy 0.5625, precision [nan, 0.5, 0.0, 0.8888888888888888, 0.0, 0.0, nan, nan, 0.0], recall [nan, 0.3333333333333333, nan, 0.8, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:26.626662: step 1261, loss 1.51563, accuracy 0.4375, precision [0.0, 1.0, nan, 0.8, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.4, 0.0, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:26.772206: step 1262, loss 1.66824, accuracy 0.4375, precision [0.0, 0.3333333333333333, nan, 0.75, 0.3333333333333333, nan, nan, 0.6666666666666666, nan], recall [nan, 1.0, nan, 0.3, 0.5, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:46:26.923628: step 1263, loss 1.69451, accuracy 0.375, precision [nan, 0.4, 0.0, 1.0, 0.5, nan, nan, 0.0, 0.0], recall [nan, 1.0, 0.0, 0.3, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:27.076474: step 1264, loss 1.2491, accuracy 0.625, precision [nan, 0.6666666666666666, nan, 0.7142857142857143, 0.5, nan, nan, 0.5, nan], recall [0.0, 1.0, nan, 0.5555555555555556, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:27.225737: step 1265, loss 1.69557, accuracy 0.3125, precision [0.0, 1.0, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.5, 0.0, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:27.377150: step 1266, loss 1.65967, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:27.525633: step 1267, loss 1.95117, accuracy 0.3125, precision [0.3333333333333333, 0.0, nan, 0.6, nan, nan, 0.0, 0.0, 0.0], recall [0.6666666666666666, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:27.675706: step 1268, loss 2.17161, accuracy 0.375, precision [0.0, nan, 0.0, 1.0, 0.6666666666666666, 0.0, nan, 0.5, 0.0], recall [0.0, 0.0, nan, 0.3, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:27.826954: step 1269, loss 1.35918, accuracy 0.5625, precision [0.3333333333333333, 0.5, nan, 0.75, 1.0, 0.0, 0.0, 0.6666666666666666, nan], recall [1.0, 0.3333333333333333, 0.0, 0.6, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:27.975825: step 1270, loss 2.01986, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.6, 0.6666666666666666, nan, nan, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.375, 0.4, nan, nan, nan, nan]
2019-02-19T19:46:28.122893: step 1271, loss 1.7755, accuracy 0.25, precision [0.5, 0.0, nan, 0.6, 0.0, 0.0, nan, 0.0, 0.0], recall [0.25, nan, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:28.274633: step 1272, loss 1.84238, accuracy 0.3125, precision [0.0, 0.0, 0.5, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [nan, nan, 1.0, 0.4444444444444444, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:46:28.421876: step 1273, loss 1.85172, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.625, nan, nan, nan, 1.0, nan], recall [nan, nan, 0.0, 0.5555555555555556, 0.0, 0.0, nan, 0.5, nan]
2019-02-19T19:46:28.570318: step 1274, loss 1.43731, accuracy 0.5, precision [nan, 1.0, 1.0, 0.7142857142857143, 0.0, nan, 0.0, 0.0, nan], recall [nan, 1.0, 0.5, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:28.719782: step 1275, loss 1.87643, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, 0.4, nan, 0.0, 1.0, nan], recall [1.0, nan, nan, 0.16666666666666666, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:28.870459: step 1276, loss 1.5748, accuracy 0.5, precision [1.0, 0.2857142857142857, 0.0, 1.0, nan, 0.0, 0.0, nan, 1.0], recall [1.0, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, 1.0]
2019-02-19T19:46:29.021215: step 1277, loss 1.52977, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.3333333333333333, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [nan, 1.0, 1.0, 0.4166666666666667, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:29.167069: step 1278, loss 1.96289, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.2857142857142857, nan, nan, 0.5714285714285714, 1.0, 0.0, nan, nan, nan]
2019-02-19T19:46:29.319448: step 1279, loss 1.39238, accuracy 0.5, precision [1.0, 0.0, nan, 0.5714285714285714, 1.0, 0.0, 0.0, nan, nan], recall [0.2, nan, 0.0, 0.6666666666666666, 1.0, nan, nan, nan, 0.0]
2019-02-19T19:46:29.467227: step 1280, loss 1.54222, accuracy 0.5, precision [0.5, 0.0, nan, 1.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0], recall [0.6666666666666666, nan, nan, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:29.612769: step 1281, loss 1.57709, accuracy 0.5, precision [0.3333333333333333, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.5, nan, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:29.760123: step 1282, loss 1.79718, accuracy 0.5, precision [0.0, 1.0, 0.0, 0.8333333333333334, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:29.909376: step 1283, loss 1.27261, accuracy 0.625, precision [0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [0.5, 0.6666666666666666, nan, 0.6, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:30.059704: step 1284, loss 1.64938, accuracy 0.3125, precision [0.0, 0.5, 0.3333333333333333, 1.0, 0.25, nan, nan, 0.0, nan], recall [nan, 1.0, 1.0, 0.08333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:30.218121: step 1285, loss 1.86586, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.6, nan, 0.1111111111111111, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:30.367918: step 1286, loss 1.29024, accuracy 0.625, precision [0.6666666666666666, nan, 0.5, 0.75, 0.6666666666666666, 0.0, nan, 0.6666666666666666, nan], recall [1.0, 0.0, 1.0, 0.42857142857142855, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:30.514382: step 1287, loss 1.71236, accuracy 0.5, precision [0.0, 0.6666666666666666, nan, 0.7142857142857143, 0.5, 0.0, 0.0, nan, nan], recall [nan, 1.0, nan, 0.5555555555555556, 0.2, nan, nan, nan, nan]
2019-02-19T19:46:30.659929: step 1288, loss 1.57325, accuracy 0.4375, precision [1.0, 1.0, 0.0, 0.5, 0.5, 0.0, nan, 0.3333333333333333, nan], recall [0.5, 0.5, nan, 0.375, 1.0, nan, nan, 1.0, 0.0]
2019-02-19T19:46:30.808229: step 1289, loss 1.4782, accuracy 0.5625, precision [0.25, 1.0, 0.0, 0.6666666666666666, 0.8, nan, nan, 0.5, nan], recall [1.0, 0.5, nan, 0.3333333333333333, 0.8, nan, nan, 0.5, nan]
2019-02-19T19:46:30.957839: step 1290, loss 2.1939, accuracy 0.4375, precision [1.0, 0.0, nan, 0.8, 0.5, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.0, 0.0, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:31.106751: step 1291, loss 1.95183, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, nan, 0.0, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:31.255903: step 1292, loss 1.66858, accuracy 0.5625, precision [0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:31.408372: step 1293, loss 1.60969, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 1.0, nan, 0.0, 1.0, nan], recall [nan, nan, nan, 0.38461538461538464, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:31.555852: step 1294, loss 1.58424, accuracy 0.375, precision [nan, 0.25, nan, 0.6, 0.5, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.3, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:46:31.705218: step 1295, loss 1.98515, accuracy 0.4375, precision [nan, 0.5, 0.0, 0.7142857142857143, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [nan, 0.5, nan, 0.4166666666666667, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:31.853509: step 1296, loss 1.66101, accuracy 0.375, precision [nan, 0.0, 0.2, 0.75, 0.5, nan, 0.0, nan, nan], recall [0.0, 0.0, 1.0, 0.375, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:32.000889: step 1297, loss 1.44964, accuracy 0.5625, precision [nan, 0.5, 0.0, 0.8, 0.6666666666666666, nan, 0.0, 0.5, nan], recall [nan, 1.0, nan, 0.4444444444444444, 0.6666666666666666, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:46:32.154341: step 1298, loss 1.74082, accuracy 0.4375, precision [0.5, 0.6666666666666666, 0.0, 0.6, nan, 0.0, 0.0, 0.5, nan], recall [1.0, 0.6666666666666666, nan, 0.42857142857142855, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:46:32.303166: step 1299, loss 1.83006, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.5714285714285714, nan, 0.0, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:32.449345: step 1300, loss 1.81245, accuracy 0.4375, precision [nan, 0.3333333333333333, 0.0, 0.8333333333333334, 0.0, 0.0, nan, 0.5, nan], recall [0.0, 1.0, nan, 0.4166666666666667, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:32.601509: step 1301, loss 1.91386, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, nan, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.5, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:46:32.748559: step 1302, loss 1.31544, accuracy 0.5625, precision [1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, nan, 0.0, nan, nan], recall [1.0, 1.0, nan, 0.25, 0.8, nan, nan, nan, nan]
2019-02-19T19:46:32.896920: step 1303, loss 1.9113, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:46:33.048998: step 1304, loss 1.65383, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, nan, nan], recall [nan, 0.0, nan, 0.36363636363636365, 0.75, nan, nan, nan, nan]
2019-02-19T19:46:33.194520: step 1305, loss 1.56759, accuracy 0.5, precision [0.5, 1.0, nan, 0.6, nan, 0.0, nan, nan, nan], recall [1.0, 1.0, 0.0, 0.6666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:33.345133: step 1306, loss 1.90107, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.5, nan], recall [nan, nan, nan, 0.21428571428571427, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:33.495148: step 1307, loss 1.70027, accuracy 0.5, precision [0.3333333333333333, 1.0, 0.0, 0.8333333333333334, nan, 0.0, 0.0, nan, nan], recall [1.0, 0.6666666666666666, nan, 0.625, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:46:33.641078: step 1308, loss 1.85816, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, nan, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.45454545454545453, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:33.789208: step 1309, loss 1.54612, accuracy 0.375, precision [nan, 0.5, 0.0, 0.5, 0.6666666666666666, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 1.0, 0.0, 0.125, 1.0, nan, 0.0, 1.0, nan]
2019-02-19T19:46:33.937364: step 1310, loss 1.58974, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.46153846153846156, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:34.085398: step 1311, loss 1.76326, accuracy 0.25, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.5, nan, 0.18181818181818182, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:34.230950: step 1312, loss 1.68978, accuracy 0.5625, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0], recall [nan, nan, nan, 0.45454545454545453, 0.5, nan, 1.0, 1.0, nan]
2019-02-19T19:46:34.382298: step 1313, loss 1.48591, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.6, 1.0, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.3333333333333333, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:34.531635: step 1314, loss 1.56263, accuracy 0.5, precision [1.0, 0.0, nan, 0.8571428571428571, 0.16666666666666666, 0.0, nan, nan, nan], recall [1.0, 0.0, nan, 0.5454545454545454, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:34.680833: step 1315, loss 1.70712, accuracy 0.375, precision [nan, 0.0, 0.0, 0.7142857142857143, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5, 1.0, nan, 0.0, nan, nan]
2019-02-19T19:46:34.831220: step 1316, loss 1.52164, accuracy 0.625, precision [0.75, 1.0, nan, 1.0, 0.25, nan, nan, 0.0, 0.0], recall [0.75, 1.0, nan, 0.5555555555555556, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:34.978192: step 1317, loss 1.93325, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.25, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:35.127362: step 1318, loss 1.82564, accuracy 0.25, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.4, 0.0], recall [nan, nan, nan, 0.18181818181818182, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:35.273358: step 1319, loss 2.15749, accuracy 0.3125, precision [0.25, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, nan, 0.0], recall [1.0, 0.0, nan, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:35.423023: step 1320, loss 1.51358, accuracy 0.5625, precision [0.3333333333333333, nan, 0.25, 0.8571428571428571, 1.0, 0.0, nan, nan, nan], recall [1.0, nan, 1.0, 0.5454545454545454, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:35.575627: step 1321, loss 1.47062, accuracy 0.625, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, nan], recall [1.0, nan, nan, 0.5833333333333334, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:35.723682: step 1322, loss 1.21265, accuracy 0.5625, precision [1.0, 0.3333333333333333, 0.0, 0.8, 0.5, nan, nan, 0.5, nan], recall [1.0, 1.0, nan, 0.5, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:46:35.871086: step 1323, loss 1.37066, accuracy 0.5625, precision [0.0, 1.0, 0.0, 0.875, 0.5, nan, nan, nan, 0.0], recall [nan, 0.5, nan, 0.5833333333333334, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:36.022947: step 1324, loss 1.46003, accuracy 0.5625, precision [0.3333333333333333, 0.5, 0.0, 0.8571428571428571, 0.5, 0.0, nan, nan, nan], recall [0.3333333333333333, 1.0, nan, 0.6666666666666666, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:36.171216: step 1325, loss 1.88803, accuracy 0.375, precision [0.0, nan, 0.0, 0.8, 0.3333333333333333, nan, 0.0, 0.3333333333333333, nan], recall [nan, 0.0, nan, 0.36363636363636365, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:46:36.320319: step 1326, loss 1.83832, accuracy 0.5, precision [nan, 1.0, 0.0, 0.8571428571428571, 0.3333333333333333, nan, 0.0, nan, 0.0], recall [nan, 1.0, nan, 0.46153846153846156, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:36.465665: step 1327, loss 2.05946, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.5, nan], recall [0.0, nan, nan, 0.25, 0.0, nan, nan, 0.2, nan]
2019-02-19T19:46:36.611792: step 1328, loss 1.73357, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.5, nan], recall [nan, nan, nan, 0.4166666666666667, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:46:36.757789: step 1329, loss 1.34024, accuracy 0.4375, precision [1.0, 0.0, 0.0, 1.0, 0.25, nan, 0.0, 0.5, nan], recall [0.5, nan, nan, 0.3, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:36.907017: step 1330, loss 1.49148, accuracy 0.4375, precision [0.25, nan, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, nan, nan], recall [1.0, 0.0, nan, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:37.053991: step 1331, loss 1.95208, accuracy 0.3125, precision [0.0, 0.0, nan, 0.5714285714285714, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.36363636363636365, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:37.200096: step 1332, loss 1.74496, accuracy 0.4375, precision [0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [1.0, nan, 1.0, 0.2, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:37.349005: step 1333, loss 1.58891, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.6, 1.0, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.3333333333333333, 0.4, nan, 0.0, nan, nan]
2019-02-19T19:46:37.501483: step 1334, loss 1.87415, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.5, 0.3333333333333333, nan, 0.0, 0.5, nan], recall [nan, nan, nan, 0.3, 0.25, nan, nan, 0.5, nan]
2019-02-19T19:46:37.650724: step 1335, loss 1.84788, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 0.8333333333333334, 0.5, 0.0, nan, 0.0, 0.0], recall [0.0, 1.0, 0.0, 0.5555555555555556, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:37.798185: step 1336, loss 1.97013, accuracy 0.3125, precision [0.5, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [1.0, 1.0, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:37.947794: step 1337, loss 1.77963, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.35714285714285715, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:38.098375: step 1338, loss 1.9, accuracy 0.3125, precision [0.5, 0.0, nan, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, nan], recall [0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:38.249899: step 1339, loss 1.7683, accuracy 0.5, precision [1.0, 0.6, 0.0, 0.6666666666666666, 0.5, 0.0, nan, nan, nan], recall [0.6666666666666666, 1.0, nan, 0.2857142857142857, 0.5, 0.0, nan, nan, nan]
2019-02-19T19:46:38.396783: step 1340, loss 1.67394, accuracy 0.375, precision [0.5, 0.5, nan, 0.5, 0.0, 0.0, nan, 1.0, 0.0], recall [0.6666666666666666, 0.5, 0.0, 0.2222222222222222, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:38.544822: step 1341, loss 1.84765, accuracy 0.375, precision [0.0, 0.5, 0.5, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, 1.0, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:38.693789: step 1342, loss 1.55617, accuracy 0.375, precision [0.25, 0.0, 0.0, 0.8, 0.5, nan, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:38.844752: step 1343, loss 1.78746, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, nan, 0.0, nan, nan], recall [nan, 0.0, nan, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:38.990718: step 1344, loss 1.63435, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, nan], recall [nan, 0.3333333333333333, nan, 0.2222222222222222, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:39.135248: step 1345, loss 2.0929, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, nan, 0.0, nan, 1.0, 0.0], recall [0.0, nan, 0.0, 0.5, nan, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:46:39.284722: step 1346, loss 1.41839, accuracy 0.625, precision [1.0, 0.8, 0.0, 0.8, 0.0, nan, nan, 0.3333333333333333, nan], recall [0.5, 0.8, nan, 0.5, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:39.433962: step 1347, loss 1.65949, accuracy 0.5, precision [nan, 0.5, 0.0, 0.8, 0.75, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.4, 0.75, nan, nan, nan, nan]
2019-02-19T19:46:39.582410: step 1348, loss 1.5808, accuracy 0.5625, precision [0.0, 0.5, 1.0, 0.5, 0.6666666666666666, nan, nan, 0.5, 0.0], recall [0.0, 1.0, 1.0, 0.4, 0.6666666666666666, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:46:39.728671: step 1349, loss 1.7552, accuracy 0.3125, precision [nan, nan, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, nan], recall [0.0, 0.0, 0.0, 0.4444444444444444, 0.0, nan, 1.0, nan, nan]
2019-02-19T19:46:39.877610: step 1350, loss 1.83467, accuracy 0.375, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:40.027974: step 1351, loss 1.69839, accuracy 0.375, precision [nan, 0.4, nan, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 1.0, 0.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:46:40.177946: step 1352, loss 1.68564, accuracy 0.3125, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.0, nan, 0.0, nan, nan], recall [0.0, 1.0, nan, 0.36363636363636365, nan, nan, nan, 0.0, nan]
2019-02-19T19:46:40.327198: step 1353, loss 1.5682, accuracy 0.5625, precision [nan, 0.5, 0.0, 0.8571428571428571, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.5454545454545454, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:40.478954: step 1354, loss 1.58127, accuracy 0.5, precision [1.0, 0.2, nan, 1.0, 1.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.2222222222222222, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:40.630089: step 1355, loss 1.15239, accuracy 0.625, precision [1.0, 0.5, 0.0, 0.8, 0.3333333333333333, nan, nan, 1.0, nan], recall [1.0, 1.0, nan, 0.5, 0.5, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:46:40.779307: step 1356, loss 2.19472, accuracy 0.5, precision [nan, nan, 0.0, 1.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.0], recall [0.0, 0.0, 0.0, 0.5454545454545454, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:40.930443: step 1357, loss 2.18961, accuracy 0.25, precision [0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.6666666666666666, nan, 0.125, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:41.081134: step 1358, loss 1.67444, accuracy 0.4375, precision [0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, 0.0]
2019-02-19T19:46:41.235655: step 1359, loss 1.03508, accuracy 0.75, precision [0.0, 1.0, 0.5, 0.8333333333333334, 1.0, nan, nan, nan, nan], recall [nan, 0.5, 1.0, 0.625, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:41.382288: step 1360, loss 1.47633, accuracy 0.5, precision [0.0, 0.5, 0.5, 1.0, 0.75, 0.0, nan, 1.0, nan], recall [nan, 0.6666666666666666, 1.0, 0.125, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:41.536045: step 1361, loss 1.9989, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:41.685251: step 1362, loss 2.01801, accuracy 0.3125, precision [nan, 0.16666666666666666, 0.0, 0.6, 0.5, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3, 0.2, nan, nan, nan, nan]
2019-02-19T19:46:41.842982: step 1363, loss 1.5178, accuracy 0.5, precision [0.0, 1.0, 0.0, 0.8, 1.0, 0.0, nan, 0.3333333333333333, nan], recall [nan, 0.5, nan, 0.5, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:46:41.999022: step 1364, loss 1.97439, accuracy 0.4375, precision [nan, 0.5, 0.25, 1.0, 0.5, 0.0, nan, nan, 0.0], recall [nan, 0.5, 1.0, 0.36363636363636365, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:42.154348: step 1365, loss 1.43717, accuracy 0.5, precision [0.0, 0.6, 0.0, 0.8, 0.0, nan, 0.0, 0.5, nan], recall [0.0, 0.75, nan, 0.4444444444444444, nan, nan, nan, 0.5, nan]
2019-02-19T19:46:42.308793: step 1366, loss 1.55185, accuracy 0.5, precision [0.0, 0.5, nan, 0.8, 0.6, nan, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.4444444444444444, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:42.459579: step 1367, loss 1.68335, accuracy 0.3125, precision [1.0, 0.5, 0.0, 0.4, 0.0, nan, nan, 1.0, 0.0], recall [1.0, 1.0, nan, 0.2, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:46:42.607996: step 1368, loss 1.90395, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.21428571428571427, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:42.756273: step 1369, loss 1.7467, accuracy 0.5625, precision [0.5, 1.0, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.5, nan], recall [1.0, 0.5, nan, 0.5555555555555556, 1.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:46:42.906201: step 1370, loss 1.33963, accuracy 0.625, precision [0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.5454545454545454, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:43.057729: step 1371, loss 1.89172, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.3333333333333333, 0.0], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:46:43.204361: step 1372, loss 1.62647, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.5, nan], recall [nan, nan, nan, 0.42857142857142855, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:43.354850: step 1373, loss 1.37259, accuracy 0.4375, precision [0.75, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, nan, 1.0, nan], recall [1.0, nan, 1.0, 0.2222222222222222, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:46:43.502829: step 1374, loss 1.72298, accuracy 0.5, precision [0.0, nan, 0.5, 0.7777777777777778, nan, nan, nan, 0.0, nan], recall [nan, 0.0, 1.0, 0.7777777777777778, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:43.656650: step 1375, loss 2.03317, accuracy 0.375, precision [0.0, 0.5, nan, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.25, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:43.804265: step 1376, loss 1.64631, accuracy 0.625, precision [1.0, 0.5, 0.0, 1.0, 1.0, nan, nan, 0.5, 0.0], recall [1.0, 1.0, 0.0, 0.5, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:43.953354: step 1377, loss 1.53713, accuracy 0.625, precision [0.0, 0.0, nan, 1.0, 0.5, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.6, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:44.107941: step 1378, loss 1.72683, accuracy 0.3125, precision [0.5, 0.6666666666666666, 0.0, 0.5, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.1, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:44.261329: step 1379, loss 2.03921, accuracy 0.375, precision [0.5, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:44.407801: step 1380, loss 1.66424, accuracy 0.4375, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 1.0, nan], recall [nan, 1.0, nan, 0.35714285714285715, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:44.555283: step 1381, loss 1.415, accuracy 0.625, precision [1.0, 0.0, nan, 1.0, 0.3333333333333333, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.5714285714285714, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:44.708041: step 1382, loss 1.03211, accuracy 0.625, precision [0.0, 0.5, 0.0, 1.0, 0.8, nan, nan, nan, nan], recall [nan, 1.0, nan, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:44.856674: step 1383, loss 1.55171, accuracy 0.5625, precision [0.5, 0.5, 0.0, 1.0, nan, nan, 0.0, 0.0, nan], recall [0.5, 0.6666666666666666, nan, 0.5454545454545454, nan, nan, nan, nan, nan]
2019-02-19T19:46:45.012196: step 1384, loss 1.49642, accuracy 0.5, precision [0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.5, nan], recall [1.0, nan, nan, 0.36363636363636365, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:45.160666: step 1385, loss 1.85487, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.2, nan], recall [nan, nan, nan, 0.14285714285714285, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:45.309977: step 1386, loss 1.34303, accuracy 0.5, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.75, nan], recall [nan, nan, nan, 0.45454545454545453, 0.0, nan, nan, 0.75, nan]
2019-02-19T19:46:45.464153: step 1387, loss 2.01222, accuracy 0.3125, precision [0.5, 0.2, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0], recall [0.5, 1.0, nan, 0.2, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:45.616083: step 1388, loss 1.36973, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.875, 0.0, nan, nan, nan, 0.0], recall [0.2, nan, nan, 0.6363636363636364, nan, nan, nan, nan, nan]
2019-02-19T19:46:45.764733: step 1389, loss 1.71208, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, nan], recall [0.0, 1.0, nan, 0.375, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:46:45.915496: step 1390, loss 1.71344, accuracy 0.4375, precision [0.0, 0.5, 0.3333333333333333, 0.75, 0.3333333333333333, nan, 0.0, 1.0, 0.0], recall [0.0, 1.0, 1.0, 0.3, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:46:46.065915: step 1391, loss 1.66217, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.2727272727272727, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:46.212037: step 1392, loss 1.70081, accuracy 0.5, precision [0.5, 0.5, 0.0, 0.7142857142857143, 0.5, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.3333333333333333, nan, 0.625, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:46.361184: step 1393, loss 1.54044, accuracy 0.5625, precision [0.0, 0.5, nan, 0.75, 1.0, nan, 0.0, 0.5, 0.0], recall [nan, 0.5, 0.0, 0.6666666666666666, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:46:46.511064: step 1394, loss 1.76582, accuracy 0.4375, precision [0.5, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.5, nan], recall [0.3333333333333333, 1.0, nan, 0.4, nan, nan, nan, 0.5, nan]
2019-02-19T19:46:46.657983: step 1395, loss 1.76274, accuracy 0.375, precision [nan, 0.14285714285714285, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.5, nan, nan, nan, 0.0, nan]
2019-02-19T19:46:46.804638: step 1396, loss 1.80886, accuracy 0.375, precision [nan, 0.4, 0.3333333333333333, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.5, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:46.953938: step 1397, loss 1.32451, accuracy 0.5, precision [nan, 0.5, 0.0, 0.625, 0.5, 0.0, nan, 0.5, nan], recall [nan, 1.0, 0.0, 0.5555555555555556, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:47.101268: step 1398, loss 2.16607, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, nan, 0.0, 0.06666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:46:47.253210: step 1399, loss 1.46563, accuracy 0.5, precision [nan, 0.5, 0.0, 0.75, 0.0, nan, 0.0, 1.0, nan], recall [nan, 0.75, 0.0, 0.42857142857142855, nan, nan, nan, 0.5, nan]
2019-02-19T19:46:47.404128: step 1400, loss 2.11656, accuracy 0.1875, precision [0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.0, 1.0, 0.2, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:47.551825: step 1401, loss 1.46677, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.6, 0.5, nan, nan, 0.5, nan], recall [0.5, 0.0, 0.0, 0.375, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:47.701500: step 1402, loss 1.53282, accuracy 0.5625, precision [0.6666666666666666, nan, 0.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.0, nan, 0.6666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:47.849798: step 1403, loss 1.25768, accuracy 0.625, precision [0.0, 0.8, 0.0, 1.0, nan, nan, 0.0, 0.3333333333333333, nan], recall [nan, 0.8, nan, 0.5555555555555556, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:48.000208: step 1404, loss 2.03452, accuracy 0.4375, precision [0.0, 0.0, nan, 0.7142857142857143, nan, 0.0, nan, 0.5, 0.0], recall [nan, 0.0, nan, 0.5, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:48.152370: step 1405, loss 2.46477, accuracy 0.3125, precision [0.5, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, nan, 0.0, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:48.299088: step 1406, loss 1.61801, accuracy 0.625, precision [0.0, nan, 0.3333333333333333, 0.8888888888888888, 1.0, 0.0, nan, nan, nan], recall [nan, nan, 0.5, 0.6666666666666666, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:48.449210: step 1407, loss 1.14546, accuracy 0.625, precision [0.0, 1.0, 0.0, 0.75, 1.0, nan, 0.0, nan, nan], recall [nan, 0.8, nan, 0.6, 0.75, nan, 0.0, 0.0, nan]
2019-02-19T19:46:48.596867: step 1408, loss 1.60612, accuracy 0.4375, precision [0.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [nan, 0.6666666666666666, nan, 0.4, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:48.747625: step 1409, loss 1.57794, accuracy 0.5, precision [1.0, 0.25, nan, 1.0, 0.25, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:48.898291: step 1410, loss 1.67743, accuracy 0.5, precision [0.5, 0.5, 0.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.45454545454545453, 1.0, nan, nan, 0.0, 0.0]
2019-02-19T19:46:49.050297: step 1411, loss 1.46997, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.5, nan, nan, 0.36363636363636365, nan, nan, nan, 0.0, 0.0]
2019-02-19T19:46:49.199350: step 1412, loss 1.5891, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.6, 0.5, nan, 0.0, 1.0, nan], recall [nan, 0.25, nan, 0.375, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:49.348571: step 1413, loss 1.59555, accuracy 0.5, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.5454545454545454, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:49.498153: step 1414, loss 1.23404, accuracy 0.5, precision [0.3333333333333333, 1.0, 0.0, 0.5, 0.75, nan, nan, nan, 0.0], recall [1.0, 0.25, nan, 0.5, 0.75, nan, nan, 0.0, nan]
2019-02-19T19:46:49.648861: step 1415, loss 1.5847, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.5555555555555556, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:49.797966: step 1416, loss 1.75279, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.0, nan, nan], recall [nan, 0.75, nan, 0.125, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:49.949794: step 1417, loss 1.3646, accuracy 0.5625, precision [0.0, 0.0, nan, 1.0, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.45454545454545453, 0.8, nan, nan, nan, nan]
2019-02-19T19:46:50.101547: step 1418, loss 1.69092, accuracy 0.25, precision [nan, 0.25, 0.0, 0.5, 0.5, 0.0, 0.0, 0.5, nan], recall [nan, 1.0, nan, 0.09090909090909091, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:46:50.249882: step 1419, loss 1.80089, accuracy 0.375, precision [nan, 0.5, nan, 0.75, 0.2, 0.0, nan, 0.25, nan], recall [nan, 0.25, nan, 0.375, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:46:50.398432: step 1420, loss 1.37619, accuracy 0.5625, precision [nan, 0.25, 0.25, 1.0, 1.0, 1.0, nan, 0.0, nan], recall [nan, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, nan, nan, nan]
2019-02-19T19:46:50.546245: step 1421, loss 1.82737, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 0.75, 0.0, 0.0, 1.0, nan], recall [nan, 1.0, nan, 0.14285714285714285, 0.75, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:46:50.697039: step 1422, loss 1.44734, accuracy 0.5, precision [0.5, 0.0, nan, 1.0, 0.6, 0.0, 0.0, nan, nan], recall [0.6666666666666666, nan, nan, 0.3, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:50.845868: step 1423, loss 1.7398, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.4444444444444444, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:50.995493: step 1424, loss 1.75362, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.6, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.25, nan, 0.42857142857142855, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:46:51.148580: step 1425, loss 1.47532, accuracy 0.4375, precision [0.0, 0.4, nan, 0.5714285714285714, nan, 0.0, nan, 1.0, nan], recall [0.0, 0.6666666666666666, 0.0, 0.5714285714285714, 0.0, nan, 0.0, 0.5, nan]
2019-02-19T19:46:51.298577: step 1426, loss 1.22408, accuracy 0.5625, precision [0.0, 0.5, nan, 0.8333333333333334, nan, nan, 0.5, 0.5, nan], recall [0.0, 0.6666666666666666, nan, 0.7142857142857143, 0.0, nan, 1.0, 0.5, nan]
2019-02-19T19:46:51.452703: step 1427, loss 1.59545, accuracy 0.5625, precision [nan, 0.5, 0.0, 0.7142857142857143, 1.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.5, nan, 0.5555555555555556, 0.75, 0.0, nan, nan, nan]
2019-02-19T19:46:51.607030: step 1428, loss 1.47232, accuracy 0.625, precision [0.0, 0.5, 0.0, 1.0, 1.0, nan, nan, 0.6666666666666666, 0.0], recall [nan, 0.5, nan, 0.5714285714285714, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:51.756123: step 1429, loss 1.60001, accuracy 0.5, precision [1.0, 0.0, nan, 0.8333333333333334, 0.5, 0.0, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:51.902893: step 1430, loss 1.55873, accuracy 0.5, precision [0.0, 0.5, 0.0, 0.8333333333333334, 0.5, nan, 0.0, nan, nan], recall [nan, 0.6666666666666666, 0.0, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:52.053352: step 1431, loss 1.64739, accuracy 0.5625, precision [1.0, 0.0, 0.0, 0.8333333333333334, 0.6666666666666666, nan, 0.3333333333333333, nan, nan], recall [1.0, nan, 0.0, 0.5555555555555556, 0.5, nan, 1.0, nan, nan]
2019-02-19T19:46:52.201166: step 1432, loss 1.7696, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, nan, 0.5, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.2857142857142857, nan, nan, 1.0, nan]
2019-02-19T19:46:52.351261: step 1433, loss 1.52072, accuracy 0.375, precision [0.0, 0.5, nan, 0.6666666666666666, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.4, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:46:52.500650: step 1434, loss 1.74293, accuracy 0.5, precision [0.5, 0.3333333333333333, 0.0, 1.0, nan, nan, nan, 0.0, 0.0], recall [1.0, 1.0, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:52.651244: step 1435, loss 1.7157, accuracy 0.5625, precision [0.0, 0.0, nan, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [nan, nan, nan, 0.5625, nan, nan, nan, nan, nan]
2019-02-19T19:46:52.801000: step 1436, loss 1.53572, accuracy 0.625, precision [nan, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, nan, nan], recall [nan, 1.0, nan, 0.5454545454545454, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:52.953723: step 1437, loss 1.67515, accuracy 0.5, precision [0.0, 0.5714285714285714, 0.0, 0.75, 0.0, nan, nan, 0.5, nan], recall [nan, 0.8, nan, 0.3333333333333333, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:46:53.103008: step 1438, loss 1.61347, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.75, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.6, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:53.255096: step 1439, loss 1.42166, accuracy 0.5625, precision [nan, 1.0, 0.0, 0.875, 0.5, 0.0, nan, nan, 0.0], recall [nan, 1.0, nan, 0.5384615384615384, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:53.401454: step 1440, loss 1.20283, accuracy 0.6875, precision [0.0, 0.75, 1.0, 1.0, 0.0, nan, 0.0, 1.0, nan], recall [nan, 1.0, 1.0, 0.5, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:53.552658: step 1441, loss 1.61734, accuracy 0.4375, precision [0.6666666666666666, 0.5, nan, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [0.6666666666666666, 0.5, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:46:53.701892: step 1442, loss 2.0434, accuracy 0.3125, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.25, 0.0], recall [0.0, 0.2, nan, 0.3333333333333333, nan, nan, nan, 1.0, nan]
2019-02-19T19:46:53.849420: step 1443, loss 1.62397, accuracy 0.4375, precision [0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, nan, 0.3333333333333333, nan], recall [nan, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:46:53.997671: step 1444, loss 1.52361, accuracy 0.5, precision [0.0, 0.0, 0.6666666666666666, 1.0, 1.0, nan, 0.0, 0.25, nan], recall [nan, nan, 1.0, 0.4, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:46:54.145040: step 1445, loss 1.29421, accuracy 0.625, precision [nan, 1.0, 0.0, 0.75, 1.0, nan, 0.0, 0.5, nan], recall [nan, 0.3333333333333333, nan, 0.6, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:54.295542: step 1446, loss 1.71871, accuracy 0.4375, precision [0.5, 0.0, 0.0, 1.0, 0.4, 0.0, nan, 0.0, nan], recall [1.0, nan, nan, 0.2727272727272727, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:54.450539: step 1447, loss 2.55547, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, 0.0], recall [0.0, 1.0, nan, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:54.597120: step 1448, loss 1.89214, accuracy 0.4375, precision [1.0, 0.5, nan, 0.8, 0.0, 0.0, nan, 0.5, nan], recall [1.0, 0.5, nan, 0.4444444444444444, nan, nan, nan, 0.25, nan]
2019-02-19T19:46:54.751869: step 1449, loss 1.8497, accuracy 0.25, precision [1.0, 0.0, nan, 0.25, 0.25, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.1111111111111111, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:46:54.898863: step 1450, loss 1.79884, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, 0.0, 0.25, 0.25, nan, nan, nan, nan]
2019-02-19T19:46:55.048022: step 1451, loss 1.62469, accuracy 0.3125, precision [0.3333333333333333, 0.5, 0.0, 0.6666666666666666, 0.25, 0.0, 0.0, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.2, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:55.197371: step 1452, loss 1.68382, accuracy 0.375, precision [0.0, nan, 0.0, 0.625, 1.0, 0.0, 0.0, nan, 0.0], recall [0.0, 0.0, 0.0, 0.625, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:55.353958: step 1453, loss 1.33786, accuracy 0.625, precision [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, nan, nan, 0.0], recall [1.0, 1.0, 0.5, 0.8, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:46:55.500320: step 1454, loss 1.19436, accuracy 0.6875, precision [1.0, 0.6666666666666666, 0.0, 0.8, 0.75, nan, nan, 1.0, nan], recall [1.0, 1.0, nan, 0.5714285714285714, 0.75, nan, nan, 0.5, nan]
2019-02-19T19:46:55.646732: step 1455, loss 1.66022, accuracy 0.4375, precision [nan, 0.0, 1.0, 0.8, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, nan, 0.3333333333333333, 0.36363636363636365, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:55.797771: step 1456, loss 1.45352, accuracy 0.5625, precision [0.0, 0.5, 0.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.5454545454545454, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:55.951381: step 1457, loss 1.52628, accuracy 0.4375, precision [0.0, 0.0, nan, 0.75, 0.8, 0.0, nan, nan, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:46:56.098123: step 1458, loss 1.40466, accuracy 0.625, precision [0.0, nan, 0.0, 1.0, 1.0, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.6666666666666666, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:46:56.253782: step 1459, loss 1.49383, accuracy 0.5625, precision [0.75, 0.0, 0.0, 1.0, 0.7142857142857143, 0.0, nan, nan, nan], recall [1.0, nan, nan, 0.14285714285714285, 0.8333333333333334, nan, nan, nan, nan]
2019-02-19T19:46:56.408876: step 1460, loss 1.59106, accuracy 0.3125, precision [0.0, 0.5, nan, 0.0, 0.5, 0.0, 0.0, 1.0, nan], recall [0.0, 0.6666666666666666, 0.0, 0.0, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:46:56.561442: step 1461, loss 1.83331, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6, 0.5, 0.0, nan, 1.0, 0.0], recall [0.0, 0.0, nan, 0.3333333333333333, 0.25, nan, nan, 1.0, nan]
2019-02-19T19:46:56.707809: step 1462, loss 1.557, accuracy 0.5625, precision [0.0, 0.5, nan, 0.7142857142857143, 1.0, nan, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.625, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:56.855806: step 1463, loss 1.78002, accuracy 0.5, precision [1.0, 1.0, 0.0, 0.5, 1.0, nan, nan, nan, nan], recall [0.6666666666666666, 1.0, nan, 0.2857142857142857, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:57.003568: step 1464, loss 1.36629, accuracy 0.625, precision [0.5, 0.6666666666666666, nan, 0.7142857142857143, 0.6666666666666666, nan, 0.0, nan, nan], recall [0.5, 0.6666666666666666, nan, 0.7142857142857143, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:46:57.150054: step 1465, loss 1.28351, accuracy 0.625, precision [0.6666666666666666, 0.3333333333333333, nan, 1.0, 0.6666666666666666, 0.0, nan, 1.0, 0.0], recall [1.0, 1.0, nan, 0.375, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:46:57.296281: step 1466, loss 2.52427, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.42857142857142855, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:57.446434: step 1467, loss 1.93813, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:46:57.599020: step 1468, loss 1.5542, accuracy 0.5625, precision [0.0, 0.25, 1.0, 0.8333333333333334, 1.0, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.5, 1.0, 0.5, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:57.751438: step 1469, loss 1.57188, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.4, nan, nan, 0.0, 0.0], recall [0.5, nan, 0.0, 0.2, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:57.900128: step 1470, loss 2.30281, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:46:58.047773: step 1471, loss 1.65894, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.5, 0.3333333333333333, nan, nan, nan, nan], recall [0.0, 0.6666666666666666, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:46:58.192695: step 1472, loss 1.99741, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.25, nan, nan, 1.0, nan], recall [0.0, 0.3333333333333333, nan, 0.2222222222222222, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:46:58.340371: step 1473, loss 2.01239, accuracy 0.3125, precision [0.0, nan, 0.0, 1.0, 0.4, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:58.485676: step 1474, loss 1.93794, accuracy 0.375, precision [0.0, 1.0, nan, 0.6, 0.25, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.2, nan, 0.375, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:46:58.635542: step 1475, loss 1.5213, accuracy 0.5625, precision [1.0, 0.8, nan, 0.75, 0.2, 0.0, nan, nan, nan], recall [1.0, 0.8, nan, 0.375, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:58.786996: step 1476, loss 1.57185, accuracy 0.4375, precision [nan, 0.0, 0.25, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.5, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:58.941720: step 1477, loss 0.974688, accuracy 0.6875, precision [nan, 1.0, 0.0, 0.6666666666666666, 0.75, nan, nan, 0.6666666666666666, nan], recall [nan, 1.0, nan, 0.5714285714285714, 0.6, nan, nan, 1.0, nan]
2019-02-19T19:46:59.092054: step 1478, loss 1.20296, accuracy 0.5625, precision [1.0, 1.0, 0.0, 0.5, 1.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.14285714285714285, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:46:59.243550: step 1479, loss 1.86069, accuracy 0.3125, precision [0.5, nan, 0.0, 0.75, 0.2, 0.0, 0.0, nan, nan], recall [0.5, 0.0, 0.0, 0.375, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:46:59.390301: step 1480, loss 1.82092, accuracy 0.4375, precision [0.5, 0.2, 1.0, 0.6666666666666666, 0.0, 0.0, nan, nan, nan], recall [1.0, 1.0, 1.0, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:46:59.541726: step 1481, loss 1.4556, accuracy 0.5625, precision [nan, nan, 0.3333333333333333, 0.8333333333333334, 1.0, nan, 0.0, 0.0, 0.0], recall [nan, nan, 1.0, 0.5555555555555556, 0.5, nan, nan, nan, nan]
2019-02-19T19:46:59.691310: step 1482, loss 1.79845, accuracy 0.375, precision [nan, 1.0, 0.0, 0.75, 0.5, nan, 0.0, nan, 0.0], recall [nan, 1.0, nan, 0.375, 0.2, nan, nan, 0.0, nan]
2019-02-19T19:46:59.836657: step 1483, loss 1.88584, accuracy 0.375, precision [0.0, 0.5, nan, 0.8, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.4, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:46:59.984579: step 1484, loss 1.31134, accuracy 0.6875, precision [0.5, 1.0, 1.0, 1.0, 1.0, nan, nan, 0.3333333333333333, nan], recall [1.0, 1.0, 1.0, 0.42857142857142855, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:47:00.140335: step 1485, loss 1.49448, accuracy 0.4375, precision [0.5, 0.3333333333333333, 0.2, 1.0, 1.0, 0.0, 0.0, nan, nan], recall [1.0, 1.0, 1.0, 0.1111111111111111, 0.75, nan, nan, nan, nan]
2019-02-19T19:47:00.288085: step 1486, loss 1.50488, accuracy 0.375, precision [nan, 0.0, 0.0, 0.5, 0.3333333333333333, nan, nan, 0.5, nan], recall [nan, 0.0, nan, 0.3, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:47:00.436347: step 1487, loss 1.46204, accuracy 0.5625, precision [0.0, 0.0, 0.5, 0.8333333333333334, 1.0, 0.0, 0.0, nan, nan], recall [0.0, nan, 0.6666666666666666, 0.5555555555555556, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:47:00.585482: step 1488, loss 1.74714, accuracy 0.375, precision [1.0, 0.5, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0], recall [0.5, 1.0, nan, 0.2, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:47:00.739779: step 1489, loss 1.56714, accuracy 0.5, precision [nan, 0.0, 0.3333333333333333, 1.0, 0.5, nan, nan, 0.0, nan], recall [0.0, nan, 1.0, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:47:00.891630: step 1490, loss 1.99013, accuracy 0.375, precision [nan, 0.0, 0.3333333333333333, 1.0, nan, 0.0, 0.0, 0.0, nan], recall [nan, nan, 1.0, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:47:01.041711: step 1491, loss 1.47138, accuracy 0.5, precision [nan, 0.3333333333333333, 1.0, 0.75, 0.6, 0.0, nan, 0.0, nan], recall [nan, 1.0, 1.0, 0.3333333333333333, 0.6, nan, nan, nan, nan]
2019-02-19T19:47:01.190254: step 1492, loss 1.34436, accuracy 0.625, precision [1.0, 1.0, 0.5, 0.8333333333333334, 0.3333333333333333, 0.0, nan, nan, nan], recall [1.0, 1.0, 0.6666666666666666, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:47:01.344861: step 1493, loss 1.81157, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.16666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:47:01.494476: step 1494, loss 1.63586, accuracy 0.5, precision [0.0, 0.5, nan, 0.75, 1.0, nan, nan, 0.25, nan], recall [0.0, 1.0, nan, 0.375, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:47:01.647770: step 1495, loss 1.49217, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, 0.6, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.4166666666666667, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:47:01.802201: step 1496, loss 1.75674, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, nan, nan, 0.0, 1.0, nan], recall [0.0, 0.0, 0.0, 0.4, nan, nan, nan, 0.5, nan]
2019-02-19T19:47:01.957430: step 1497, loss 1.23299, accuracy 0.5625, precision [0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:47:02.109628: step 1498, loss 1.70013, accuracy 0.5, precision [nan, 0.0, 1.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [0.0, nan, 1.0, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:47:02.258367: step 1499, loss 2.16106, accuracy 0.3125, precision [0.0, 0.0, nan, 0.8, 0.5, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:47:02.409978: step 1500, loss 1.62695, accuracy 0.5, precision [0.5, 1.0, 0.0, 1.0, 0.5, 0.0, nan, 0.3333333333333333, nan], recall [0.5, 0.5, nan, 0.3333333333333333, 1.0, nan, nan, 1.0, nan]

Evaluation:
[[ 23  32   0  29   1   0   0   1   0]
 [  8  72   0  78   3   0   0   3   0]
 [  0   5  10  75   1   0   0   0   0]
 [  3  28  14 254   4   0   0   1   0]
 [  1   3   0 106  60   0   0   1   0]
 [  1   3   2  40   4   0   0   1   0]
 [  1   0   0  20   4   0   0   1   0]
 [  6   5   1  60   2   0   0  33   0]
 [  3   1   0  19   1   0   0   1   0]]
2019-02-19T19:47:04.862722: step 1500, loss 1.58143, accuracy 0.440976, precision [0.26744186046511625, 0.43902439024390244, 0.10989010989010989, 0.8355263157894737, 0.3508771929824561, 0.0, 0.0, 0.308411214953271, 0.0], recall [0.5, 0.48322147651006714, 0.37037037037037035, 0.37298091042584436, 0.75, nan, nan, 0.7857142857142857, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550605378/checkpoints/model-1500

