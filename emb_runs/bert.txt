hello
768
Loading data...
{'sympathy_and_emotional_support': 0, 'not_related_or_irrelevant': 1, 'infrastructure_and_utilities_damage': 2, 'other_useful_information': 3, 'injured_or_dead_people': 4, 'caution_and_advice': 5, 'displaced_people_and_evacuations': 6, 'donation_needs_or_offers_or_volunteering_services': 7, 'missing_trapped_or_found_people': 8}
Max Document length: 2407
Vocabulary Size: 1
Train/Dev split: 9226/1025
Writing to /home/ubuntu/Project/runs/1550603912

2019-02-19T19:18:48.252889: step 1, loss 17.5413, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.125, 1.0, 0.0, 0.0, nan, nan], recall [0.0, nan, 0.0, 1.0, 0.1111111111111111, 0.0, nan, nan, nan]
2019-02-19T19:18:52.368886: step 2, loss 19.962, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, 0.0, 0.125, nan, 0.0, nan, nan, 0.0]
2019-02-19T19:18:56.391658: step 3, loss 25.7267, accuracy 0.0625, precision [0.0, 0.0, nan, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.07692307692307693, nan, 0.0, nan, nan, nan]
2019-02-19T19:19:00.390273: step 4, loss 17.6787, accuracy 0.0625, precision [nan, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.3333333333333333, 0.0, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:19:04.384187: step 5, loss 10.6303, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 0.25, 0.0, 0.5, nan, 0.0, nan], recall [nan, 0.16666666666666666, nan, 0.16666666666666666, 0.0, 0.3333333333333333, nan, nan, nan]
2019-02-19T19:19:08.393891: step 6, loss 13.7415, accuracy 0.125, precision [0.0, 0.5, 0.0, 0.25, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.2, 0.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:19:12.398764: step 7, loss 20.03, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan], recall [0.5, 0.0, 0.0, nan, 0.6666666666666666, 0.0, 0.0, 0.0, nan]
2019-02-19T19:19:16.406374: step 8, loss 11.0521, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.3333333333333333, 0.2, nan, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0]
2019-02-19T19:19:20.400894: step 9, loss 11.6178, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.3333333333333333, 0.5, nan, nan, 0.5, nan], recall [nan, 1.0, 0.0, 1.0, 0.4, nan, 0.0, 0.5, 0.0]
2019-02-19T19:19:24.398201: step 10, loss 16.7506, accuracy 0, precision [0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.0, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:19:28.396777: step 11, loss 16.568, accuracy 0.0625, precision [0.0, 0.0, nan, 0.25, 0.0, 0.0, nan, nan, 0.0], recall [0.0, nan, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:19:32.398184: step 12, loss 13.5936, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 0.0, 0.5, nan, nan, 0.3333333333333333, nan], recall [0.25, nan, 0.0, 0.0, 0.5, nan, 0.0, 0.5, nan]
2019-02-19T19:19:36.402469: step 13, loss 13.4569, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.4, 0.25, 0.0, 0.0, 0.0, 0.0], recall [0.0, nan, 0.0, 0.4, 0.3333333333333333, nan, nan, 0.0, 0.0]
2019-02-19T19:19:40.495188: step 14, loss 18.3479, accuracy 0.125, precision [0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0, nan, 0.0, nan], recall [0.3333333333333333, nan, 0.0, 0.0, 0.0, nan, 0.0, nan, 0.0]
2019-02-19T19:19:44.508164: step 15, loss 20.9487, accuracy 0.125, precision [0.0, 0.3333333333333333, 0.0, 0.0, 0.5, nan, 0.0, 0.0, 0.0], recall [0.0, 1.0, nan, 0.0, 0.2, nan, 0.0, 0.0, nan]
2019-02-19T19:19:48.514509: step 16, loss 13.2761, accuracy 0.125, precision [1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, nan], recall [0.2, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, nan, nan, 0.0]
2019-02-19T19:19:52.523034: step 17, loss 11.7283, accuracy 0.4375, precision [nan, 0.5, nan, 0.5714285714285714, 0.5, nan, nan, 0.0, 0.0], recall [0.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, nan, 0.0]
2019-02-19T19:19:56.525189: step 18, loss 12.6301, accuracy 0.3125, precision [1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, nan], recall [0.5, 0.2, nan, 0.5, 0.3333333333333333, nan, nan, 0.0, 0.0]
2019-02-19T19:20:00.533859: step 19, loss 14.5692, accuracy 0.0625, precision [0.0, 0.25, 0.0, 0.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.125, nan, 0.0, 0.0, 0.0, nan, nan, 0.0]
2019-02-19T19:20:04.531202: step 20, loss 16.1806, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.5, 0.0, nan, nan, nan, nan], recall [0.0, 0.0, 0.0, 0.375, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:20:08.543044: step 21, loss 10.6982, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.5, 0.0, nan, nan, nan, nan], recall [nan, 0.25, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:20:12.538909: step 22, loss 9.73742, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.5, nan, 0.0, nan, nan, nan]
2019-02-19T19:20:16.536916: step 23, loss 11.4059, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.3, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:20:20.532474: step 24, loss 12.4988, accuracy 0.1875, precision [0.0, nan, 0.5, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, 1.0, 0.18181818181818182, nan, nan, nan, 0.0, nan]
2019-02-19T19:20:24.545908: step 25, loss 14.4423, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, nan, nan]
2019-02-19T19:20:28.605918: step 26, loss 12.4012, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, 0.5, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.75, 1.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:20:32.612083: step 27, loss 6.95157, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan, 0.5, 0.0], recall [nan, 0.8, 0.0, 0.0, 0.0, 0.0, nan, 0.5, 0.0]
2019-02-19T19:20:36.609052: step 28, loss 8.5077, accuracy 0.25, precision [nan, 0.0, 0.0, 0.0, 0.25, 1.0, nan, 1.0, 1.0], recall [nan, nan, 0.0, 0.0, 0.5, 1.0, nan, 0.125, 0.3333333333333333]
2019-02-19T19:20:40.616804: step 29, loss 11.8901, accuracy 0.25, precision [0.6666666666666666, 1.0, 0.0, 0.2, 0.0, nan, nan, 0.0, nan], recall [1.0, 1.0, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0]
2019-02-19T19:20:44.627037: step 30, loss 11.8948, accuracy 0.4375, precision [0.3333333333333333, 0.0, 0.5, 0.0, 0.8333333333333334, nan, 0.0, nan, nan], recall [1.0, 0.0, 1.0, nan, 0.5, 0.0, nan, 0.0, nan]
2019-02-19T19:20:48.627044: step 31, loss 5.32699, accuracy 0.375, precision [0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [0.0, 0.6666666666666666, 0.0, 0.0, 0.8, nan, 0.0, 0.0, nan]
2019-02-19T19:20:52.634428: step 32, loss 9.68551, accuracy 0.3125, precision [0.5, 0.0, 1.0, 0.0, 0.5, 0.0, nan, 1.0, nan], recall [0.25, 0.0, 0.6666666666666666, nan, 0.2, 0.0, 0.0, 1.0, nan]
2019-02-19T19:20:56.634993: step 33, loss 14.7845, accuracy 0.125, precision [0.5, 0.0, 0.5, 0.0, 0.0, nan, 0.0, 0.0, nan], recall [0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan, nan]
2019-02-19T19:21:00.642746: step 34, loss 12.4373, accuracy 0.0625, precision [nan, 0.3333333333333333, 0.0, 0.0, nan, nan, 0.0, 0.0, nan], recall [0.0, 0.5, 0.0, 0.0, 0.0, nan, nan, nan, nan]
2019-02-19T19:21:04.644362: step 35, loss 8.48774, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.6, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, 0.5, 1.0, nan, 0.0, 0.0, nan]
2019-02-19T19:21:08.643290: step 36, loss 13.0688, accuracy 0.1875, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.125, 0.0, nan, nan, nan, nan]
2019-02-19T19:21:12.646005: step 37, loss 12.7271, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.375, nan, nan, 0.0, nan, nan]
2019-02-19T19:21:16.671493: step 38, loss 6.62085, accuracy 0.3125, precision [0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, 1.0, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:21:20.676085: step 39, loss 10.7928, accuracy 0.1875, precision [0.0, 0.0, nan, 0.6, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.2727272727272727, nan, 0.0, 0.0, nan, nan]
2019-02-19T19:21:24.671877: step 40, loss 20.6725, accuracy 0.0625, precision [nan, 0.0, nan, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.16666666666666666, nan, nan, 0.0, nan, nan]
2019-02-19T19:21:28.671596: step 41, loss 17.5427, accuracy 0, precision [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, nan]
2019-02-19T19:21:32.680228: step 42, loss 10.9789, accuracy 0.0625, precision [nan, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.16666666666666666, 0.0, 0.0, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:21:36.682936: step 43, loss 9.82195, accuracy 0.0625, precision [1.0, 0.0, 0.0, 0.0, 0.0, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.0, nan, nan, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:21:40.680999: step 44, loss 9.35831, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.0, 0.42857142857142855, nan, 0.0, nan, nan], recall [0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, nan]
2019-02-19T19:21:44.686504: step 45, loss 7.73632, accuracy 0.125, precision [nan, 0.0, 0.0, 0.2, 0.0, 1.0, nan, 0.0, 0.0], recall [nan, nan, 0.0, 0.5, 0.0, 0.25, nan, 0.0, nan]
2019-02-19T19:21:48.689366: step 46, loss 14.6721, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.0, 0.6666666666666666, nan, nan, nan, 0.0], recall [0.0, 0.0, 0.0, 0.0, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:21:52.685299: step 47, loss 12.1851, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, nan], recall [nan, 0.0, 0.0, 0.0, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:21:56.686846: step 48, loss 16.4077, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.2, 0.125, nan, nan, nan, nan]
2019-02-19T19:22:00.704052: step 49, loss 9.40184, accuracy 0.3125, precision [0.0, 0.0, nan, 0.5714285714285714, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.8, 0.16666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:22:04.815741: step 50, loss 12.8972, accuracy 0.3125, precision [0.0, 0.5, 0.3333333333333333, 0.3333333333333333, 0.5, nan, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, 0.5, 0.14285714285714285, 0.6666666666666666, nan, 0.0, nan, nan]
2019-02-19T19:22:08.830418: step 51, loss 7.09715, accuracy 0.25, precision [0.0, 0.0, 0.5, 0.4, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.5, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:22:12.826523: step 52, loss 11.0321, accuracy 0.1875, precision [0.0, 0.0, nan, 0.5, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, nan, 0.0, 0.08333333333333333, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:22:16.836320: step 53, loss 6.79116, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.0, 0.0, 0.4444444444444444, nan, nan, 0.0, 0.0, nan]
2019-02-19T19:22:20.844204: step 54, loss 5.90864, accuracy 0.3125, precision [nan, 0.0, 0.25, 0.5, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [nan, 0.0, 0.25, 0.2857142857142857, 1.0, nan, nan, 0.0, 0.0]
2019-02-19T19:22:24.842619: step 55, loss 11.9766, accuracy 0.1875, precision [0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.2857142857142857, 0.25, 0.0, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:22:28.854394: step 56, loss 5.4933, accuracy 0.25, precision [0.0, 1.0, 1.0, 0.16666666666666666, 0.3333333333333333, nan, nan, nan, nan], recall [0.0, 0.5, 1.0, 0.3333333333333333, 0.5, 0.0, nan, 0.0, 0.0]
2019-02-19T19:22:32.865281: step 57, loss 10.3396, accuracy 0.0625, precision [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.25, 0.0, 0.0, 0.0, 0.0, nan, nan, 0.0]
2019-02-19T19:22:36.870946: step 58, loss 8.03911, accuracy 0.125, precision [nan, 0.0, nan, 0.0, 0.5, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0]
2019-02-19T19:22:40.893669: step 59, loss 7.69163, accuracy 0.25, precision [1.0, 0.4, 0.0, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [0.25, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:22:44.901238: step 60, loss 8.26525, accuracy 0.1875, precision [nan, 0.3333333333333333, 0.0, 0.2222222222222222, nan, nan, nan, 0.0, nan], recall [0.0, 0.5, 0.0, 0.2857142857142857, 0.0, nan, 0.0, nan, nan]
2019-02-19T19:22:48.897219: step 61, loss 8.60552, accuracy 0.1875, precision [0.0, 0.0, 0.3333333333333333, 0.5, nan, nan, nan, 0.0, 0.0], recall [0.0, nan, 1.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0, 0.0]
2019-02-19T19:22:53.008930: step 62, loss 8.56279, accuracy 0.125, precision [nan, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.2, 0.5, 0.0, nan, nan, nan]
2019-02-19T19:22:57.024305: step 63, loss 9.86997, accuracy 0.125, precision [0.0, 0.5, 0.0, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.2, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:23:01.021528: step 64, loss 6.12873, accuracy 0.25, precision [nan, 0.25, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:23:05.027666: step 65, loss 8.77159, accuracy 0.25, precision [0.0, 0.0, 1.0, 0.42857142857142855, nan, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.5, 0.6, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:23:09.025479: step 66, loss 6.0289, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.4, 1.0, nan, 0.0, 0.3333333333333333, 0.0], recall [nan, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, nan, 1.0, nan]
2019-02-19T19:23:13.033664: step 67, loss 3.60207, accuracy 0.5625, precision [0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, nan, 0.0, 0.5, nan], recall [0.0, 0.6666666666666666, 0.0, 0.5714285714285714, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:23:17.031962: step 68, loss 4.6544, accuracy 0.4375, precision [0.3333333333333333, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.5714285714285714, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:23:21.041382: step 69, loss 3.58185, accuracy 0.5625, precision [nan, 0.6666666666666666, 0.0, 0.6666666666666666, 0.75, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5714285714285714, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:23:25.045739: step 70, loss 6.67768, accuracy 0.125, precision [1.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.5, nan], recall [0.25, 0.0, nan, 0.0, nan, nan, nan, 0.14285714285714285, nan]
2019-02-19T19:23:29.047962: step 71, loss 7.9832, accuracy 0.1875, precision [0.0, 0.0, nan, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.3333333333333333, nan, nan, 0.0, nan, 0.0]
2019-02-19T19:23:33.055266: step 72, loss 9.11868, accuracy 0.1875, precision [0.0, nan, 0.0, 0.75, 0.0, 0.0, nan, nan, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:23:37.055808: step 73, loss 6.05912, accuracy 0.375, precision [0.0, 0.3333333333333333, nan, 0.4, 1.0, nan, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.4, 0.5, nan, 0.0, nan, nan]
2019-02-19T19:23:41.100546: step 74, loss 5.58733, accuracy 0.4375, precision [0.5, 0.3333333333333333, 1.0, 0.5, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 0.3333333333333333, 1.0, 0.2857142857142857, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:23:45.101313: step 75, loss 4.37156, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.5, 0.75, 0.0, 1.0, 0.5, 0.0], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.6, nan, 0.3333333333333333, 0.3333333333333333, nan]
2019-02-19T19:23:49.109660: step 76, loss 5.04403, accuracy 0.3125, precision [nan, 0.25, nan, 0.16666666666666666, 0.75, 0.0, nan, 0.0, nan], recall [nan, 0.25, 0.0, 0.25, 0.6, nan, nan, nan, nan]
2019-02-19T19:23:53.126809: step 77, loss 7.46253, accuracy 0.1875, precision [0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan, 0.3333333333333333, 0.0], recall [0.0, 0.5, 0.0, 0.0, 0.0, nan, 0.0, 0.5, nan]
2019-02-19T19:23:57.134390: step 78, loss 5.68954, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, nan, 1.0, nan], recall [0.0, 0.0, nan, 0.0, 1.0, nan, nan, 0.16666666666666666, 0.0]
2019-02-19T19:24:01.152510: step 79, loss 6.04897, accuracy 0.125, precision [nan, 0.0, 0.0, 0.14285714285714285, 0.2, nan, nan, nan, 0.0], recall [0.0, 0.0, 0.0, 0.5, 0.25, 0.0, nan, 0.0, nan]
2019-02-19T19:24:05.157506: step 80, loss 8.00728, accuracy 0.0625, precision [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.125, 0.0, 0.0, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:24:09.160525: step 81, loss 5.3738, accuracy 0.25, precision [0.5, 0.14285714285714285, nan, 0.0, 0.5, nan, 1.0, 0.0, nan], recall [0.3333333333333333, 1.0, 0.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0]
2019-02-19T19:24:13.155828: step 82, loss 3.82176, accuracy 0.6875, precision [nan, 0.0, 0.6666666666666666, 0.8571428571428571, 0.6666666666666666, 0.0, nan, 1.0, nan], recall [0.0, nan, 1.0, 0.75, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:24:17.160452: step 83, loss 6.02063, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, nan, nan, nan], recall [0.0, 0.25, nan, 0.5, 0.5, 0.0, nan, 0.0, nan]
2019-02-19T19:24:21.167842: step 84, loss 4.21709, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.4, nan, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.5, 0.4, nan, nan, nan, nan]
2019-02-19T19:24:25.167326: step 85, loss 5.6524, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.5714285714285714, 0.75, 0.0, 0.0, nan, 0.0], recall [nan, nan, nan, 0.5, 0.6, 0.0, nan, nan, nan]
2019-02-19T19:24:29.277385: step 86, loss 5.08891, accuracy 0.5, precision [0.0, 0.6, nan, 0.7142857142857143, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.5555555555555556, 0.0, nan, nan, nan, nan]
2019-02-19T19:24:33.301623: step 87, loss 7.15918, accuracy 0.3125, precision [nan, 0.6666666666666666, nan, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.6666666666666666, nan, 0.5, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:24:37.297271: step 88, loss 6.21704, accuracy 0.375, precision [0.0, 0.0, 0.5, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.6666666666666666, 0.42857142857142855, 0.5, nan, 0.0, 0.0, nan]
2019-02-19T19:24:41.301861: step 89, loss 6.40129, accuracy 0.125, precision [0.0, 0.0, 0.25, 0.0, 1.0, nan, nan, 0.0, 0.0], recall [0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, nan, nan]
2019-02-19T19:24:45.310144: step 90, loss 8.13606, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, nan, 0.0, 0.0]
2019-02-19T19:24:49.316588: step 91, loss 4.95292, accuracy 0.375, precision [0.0, 0.5, nan, 0.5, 0.25, 1.0, 0.0, nan, 1.0], recall [nan, 0.5, 0.0, 0.3333333333333333, 1.0, 0.3333333333333333, nan, nan, 1.0]
2019-02-19T19:24:53.310220: step 92, loss 6.62951, accuracy 0.3125, precision [1.0, 0.6666666666666666, 0.5, nan, 0.0, 1.0, 0.0, 0.0, nan], recall [1.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, nan, nan, 0.0]
2019-02-19T19:24:57.307695: step 93, loss 7.14228, accuracy 0.1875, precision [0.0, nan, 1.0, 0.0, 0.2857142857142857, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, nan, 0.0, nan]
2019-02-19T19:25:01.309846: step 94, loss 4.02353, accuracy 0.375, precision [0.0, 0.25, nan, 0.2, 1.0, nan, nan, 0.5, nan], recall [0.0, 0.3333333333333333, 0.0, 1.0, 0.5, 0.0, nan, 0.5, nan]
2019-02-19T19:25:05.306970: step 95, loss 4.66458, accuracy 0.3125, precision [0.0, 0.5, nan, 0.5, 0.3333333333333333, 0.0, nan, 0.25, nan], recall [0.0, 0.2, nan, 0.5, 0.5, nan, nan, 0.5, 0.0]
2019-02-19T19:25:09.302256: step 96, loss 4.41517, accuracy 0.4375, precision [0.0, 0.2, nan, 0.75, nan, 0.0, 1.0, 0.6666666666666666, 0.0], recall [nan, 1.0, 0.0, 0.5, 0.0, nan, 1.0, 0.5, 0.0]
2019-02-19T19:25:13.304161: step 97, loss 3.15863, accuracy 0.375, precision [nan, 0.0, nan, 0.5, 1.0, nan, nan, 0.3333333333333333, 0.0], recall [nan, 0.0, nan, 0.6666666666666666, 0.3333333333333333, 0.0, nan, 0.2, nan]
2019-02-19T19:25:17.401595: step 98, loss 5.56169, accuracy 0.125, precision [nan, 0.0, nan, 0.1111111111111111, 0.5, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5, 0.25, nan, 0.0, 0.0, nan]
2019-02-19T19:25:21.425014: step 99, loss 6.59081, accuracy 0.1875, precision [0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [0.0, 0.0, 0.5, 0.0, 0.5, nan, nan, 0.0, 0.0]
2019-02-19T19:25:25.426197: step 100, loss 5.35856, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.2857142857142857, 0.0, nan, nan, 0.0, nan], recall [nan, 0.2, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:25:29.444538: step 101, loss 5.17978, accuracy 0.125, precision [nan, 0.0, 0.0, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.18181818181818182, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:25:33.458160: step 102, loss 2.61516, accuracy 0.4375, precision [0.5, 1.0, nan, 0.6666666666666666, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, 0.6666666666666666, nan, 0.3333333333333333, 1.0, nan, 0.0, 0.0, nan]
2019-02-19T19:25:37.467634: step 103, loss 3.86373, accuracy 0.5, precision [0.5, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, nan], recall [0.5, 1.0, nan, 0.4444444444444444, 0.5, nan, 0.5, nan, nan]
2019-02-19T19:25:41.481498: step 104, loss 5.22767, accuracy 0.3125, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.3333333333333333, 0.5, nan, 0.0, 0.0, nan]
2019-02-19T19:25:45.476642: step 105, loss 4.60317, accuracy 0.375, precision [0.0, 0.6666666666666666, nan, 0.42857142857142855, nan, nan, nan, 0.5, 0.0], recall [0.0, 0.5, nan, 0.75, 0.0, nan, nan, 0.25, nan]
2019-02-19T19:25:49.472490: step 106, loss 6.39627, accuracy 0.3125, precision [0.5, nan, 1.0, 0.16666666666666666, 0.6666666666666666, 0.0, nan, 0.0, 0.0], recall [1.0, 0.0, 1.0, 0.3333333333333333, 0.4, 0.0, 0.0, 0.0, nan]
2019-02-19T19:25:53.473612: step 107, loss 3.42025, accuracy 0.375, precision [1.0, 0.5, nan, 0.42857142857142855, 0.5, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.3333333333333333, nan, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:25:57.470449: step 108, loss 5.04641, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 1.0, nan], recall [0.5, nan, nan, 0.5, 0.2, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:26:01.474008: step 109, loss 4.21874, accuracy 0.3125, precision [0.0, 0.25, nan, 0.375, 1.0, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.6, 0.25, nan, 0.0, 0.0, nan]
2019-02-19T19:26:05.509191: step 110, loss 3.13177, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.5, nan], recall [0.0, nan, 0.0, 0.625, nan, nan, nan, 0.2, nan]
2019-02-19T19:26:09.503227: step 111, loss 2.47142, accuracy 0.5625, precision [0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, nan, 0.5, nan], recall [0.0, 1.0, 1.0, 0.3333333333333333, 0.75, nan, nan, 0.5, nan]
2019-02-19T19:26:13.510664: step 112, loss 4.22687, accuracy 0.4375, precision [nan, 1.0, 0.0, 0.5454545454545454, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.75, nan, nan, 0.0, nan, nan]
2019-02-19T19:26:17.506513: step 113, loss 4.70206, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.4444444444444444, nan, 0.0, nan, 0.0, 0.0]
2019-02-19T19:26:21.502848: step 114, loss 4.21747, accuracy 0.4375, precision [0.3333333333333333, 0.0, 0.5, 0.6666666666666666, 0.5, 1.0, nan, nan, 0.0], recall [0.5, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, nan, nan, nan]
2019-02-19T19:26:25.510791: step 115, loss 6.39538, accuracy 0.125, precision [0.0, 0.0, nan, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.25, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:26:29.515592: step 116, loss 4.05321, accuracy 0.1875, precision [0.0, 0.3333333333333333, nan, 0.14285714285714285, 0.3333333333333333, 0.0, nan, nan, nan], recall [0.0, 0.3333333333333333, 0.0, 0.25, 1.0, nan, nan, 0.0, 0.0]
2019-02-19T19:26:33.518114: step 117, loss 5.63471, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.5, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.16666666666666666, 0.0, 0.4, nan, 0.0, nan, nan, nan]
2019-02-19T19:26:37.516144: step 118, loss 4.25715, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 0.4, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:26:41.517604: step 119, loss 4.21105, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.5, 0.75, 0.0, 0.0, nan, 0.0], recall [0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:26:45.518058: step 120, loss 4.39277, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.5, nan, nan, 0.25, nan], recall [0.0, 0.3333333333333333, 0.0, 0.25, 0.3333333333333333, 0.0, nan, 0.5, nan]
2019-02-19T19:26:49.523709: step 121, loss 2.87284, accuracy 0.375, precision [0.25, 1.0, 0.5, 0.5, 1.0, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.3333333333333333, 0.5, 0.6666666666666666, 0.3333333333333333, 0.0, nan, nan, 0.0]
2019-02-19T19:26:53.648319: step 122, loss 2.94887, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.4, 0.75, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.6666666666666666, 0.75, 0.0, nan, 0.0, 0.0]
2019-02-19T19:26:57.661812: step 123, loss 4.02787, accuracy 0.3125, precision [nan, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.5, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:27:01.663544: step 124, loss 4.06978, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6, 0.5, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.3333333333333333, 0.0, 0.75, 0.25, 0.0, nan, 0.0, 0.0]
2019-02-19T19:27:05.660882: step 125, loss 4.21035, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.14285714285714285, 1.0, nan, 0.0, 0.5, nan], recall [nan, nan, nan, 1.0, 0.07692307692307693, nan, nan, 0.5, nan]
2019-02-19T19:27:09.661450: step 126, loss 3.45387, accuracy 0.375, precision [1.0, nan, nan, 0.5, 0.25, 0.0, 0.0, 0.3333333333333333, nan], recall [0.5, 0.0, 0.0, 0.5, 1.0, nan, nan, 0.25, nan]
2019-02-19T19:27:13.668993: step 127, loss 2.0688, accuracy 0.5625, precision [1.0, 0.0, nan, 0.625, 0.0, nan, nan, 1.0, nan], recall [1.0, nan, nan, 0.7142857142857143, nan, nan, nan, 0.2857142857142857, nan]
2019-02-19T19:27:17.684907: step 128, loss 4.28868, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.6, 0.25, nan, nan, 0.0, 0.0], recall [0.5, nan, 0.0, 0.3, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:27:21.693830: step 129, loss 2.94122, accuracy 0.4375, precision [0.3333333333333333, 0.5, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.6666666666666666, 0.0], recall [0.5, 0.25, 0.0, 0.4, 1.0, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:27:25.696565: step 130, loss 3.21422, accuracy 0.3125, precision [0.0, 0.0, nan, 0.5, 0.25, nan, 0.0, 0.6666666666666666, nan], recall [0.0, 0.0, 0.0, 0.25, 1.0, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:27:29.705859: step 131, loss 2.99706, accuracy 0.375, precision [0.0, nan, 0.0, 1.0, 0.2, nan, 0.5, 0.5, 0.5], recall [nan, nan, 0.0, 0.4, 0.3333333333333333, 0.0, 1.0, 0.25, 1.0]
2019-02-19T19:27:33.705219: step 132, loss 3.99817, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.4444444444444444, 1.0, nan, nan, 0.0, 0.0]
2019-02-19T19:27:37.707294: step 133, loss 4.24127, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.25, 0.75, 0.0, nan, nan, 1.0], recall [0.0, 0.0, 0.0, 0.16666666666666666, 0.75, nan, nan, 0.0, 1.0]
2019-02-19T19:27:41.808410: step 134, loss 4.4893, accuracy 0.125, precision [nan, 0.0, 0.0, 0.2, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.25, 0.3333333333333333, nan, 0.0, nan, 0.0]
2019-02-19T19:27:45.836809: step 135, loss 3.82483, accuracy 0.3125, precision [0.5, nan, nan, 0.3333333333333333, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, nan, nan, 0.42857142857142855, 0.25, nan, 0.0, 0.0, 0.0]
2019-02-19T19:27:49.842495: step 136, loss 3.58599, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.16666666666666666, 1.0, 0.0, 1.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.16666666666666666, 1.0, 0.0, 1.0, nan, nan]
2019-02-19T19:27:53.838937: step 137, loss 4.74994, accuracy 0.25, precision [0.0, 0.25, nan, 0.3333333333333333, 0.5, nan, nan, 0.2, 0.0], recall [nan, 1.0, 0.0, 0.14285714285714285, 0.3333333333333333, nan, 0.0, 0.5, nan]
2019-02-19T19:27:57.839967: step 138, loss 5.21612, accuracy 0.25, precision [0.0, 0.0, nan, 0.0, 0.6666666666666666, nan, 0.5, nan, 0.25], recall [0.0, 0.0, nan, 0.0, 0.4, nan, 1.0, nan, 0.25]
2019-02-19T19:28:01.839477: step 139, loss 3.06661, accuracy 0.5, precision [0.0, nan, 0.6666666666666666, 0.5, 0.75, 0.3333333333333333, nan, nan, 0.0], recall [0.0, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, nan, 0.0, 0.0]
2019-02-19T19:28:05.841732: step 140, loss 3.6272, accuracy 0.375, precision [1.0, 1.0, 0.5, 0.3333333333333333, 0.0, nan, 0.0, 0.5, nan], recall [0.5, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, nan, 1.0, 0.0]
2019-02-19T19:28:09.840256: step 141, loss 3.81925, accuracy 0.1875, precision [nan, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 1.0, 0.0, 0.5, 0.0, nan, nan, 0.0]
2019-02-19T19:28:13.841668: step 142, loss 1.66653, accuracy 0.3125, precision [1.0, 0.16666666666666666, nan, 0.0, 1.0, 0.0, nan, 0.0, nan], recall [0.5, 1.0, 0.0, 0.0, 0.75, 0.0, nan, nan, 0.0]
2019-02-19T19:28:17.852208: step 143, loss 4.83625, accuracy 0.25, precision [nan, 0.0, 1.0, 0.2857142857142857, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.3333333333333333, 0.5, 0.5, 0.0, nan, 0.0, nan]
2019-02-19T19:28:21.850302: step 144, loss 3.67393, accuracy 0.3125, precision [0.0, 0.0, 1.0, 0.5, nan, 0.0, nan, 0.0, nan], recall [nan, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan, 0.0]
2019-02-19T19:28:25.849888: step 145, loss 2.09063, accuracy 0.4375, precision [nan, 0.5, 0.0, 0.5, 0.5, nan, nan, 0.0, nan], recall [nan, 0.4, 0.0, 0.8, 0.5, 0.0, nan, 0.0, nan]
2019-02-19T19:28:29.972125: step 146, loss 2.62022, accuracy 0.25, precision [0.3333333333333333, nan, 0.5, 0.2, nan, 0.0, 0.5, 0.0, 0.0], recall [1.0, 0.0, 0.5, 0.25, 0.0, 0.0, 1.0, nan, 0.0]
2019-02-19T19:28:33.982449: step 147, loss 1.23736, accuracy 0.5, precision [nan, 1.0, 0.0, 0.5555555555555556, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.5, nan, 0.625, nan, nan, nan, nan, nan]
2019-02-19T19:28:37.981051: step 148, loss 3.94267, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.6, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.2222222222222222, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:28:41.977721: step 149, loss 2.36398, accuracy 0.4375, precision [1.0, 0.0, nan, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.5, 0.5, nan, nan, nan, 0.0]
2019-02-19T19:28:45.990403: step 150, loss 2.25894, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.5714285714285714, 0.0, nan, nan, 0.5, nan], recall [nan, 0.0, 0.0, 0.36363636363636365, nan, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:28:50.004514: step 151, loss 2.36749, accuracy 0.4375, precision [nan, 0.3333333333333333, 0.0, 0.8333333333333334, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.5555555555555556, 0.3333333333333333, nan, 0.0, nan, nan]
2019-02-19T19:28:54.016458: step 152, loss 2.21733, accuracy 0.375, precision [nan, 0.0, 1.0, 0.3333333333333333, 0.5, nan, 0.5, 0.0, 0.0], recall [nan, 0.0, 0.25, 0.6666666666666666, 0.5, nan, 1.0, 0.0, nan]
2019-02-19T19:28:58.031065: step 153, loss 2.22266, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, nan, nan, nan], recall [nan, nan, nan, 0.5454545454545454, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:29:02.052969: step 154, loss 3.35752, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.4, 1.0, 0.5, 0.0, nan, nan], recall [nan, 1.0, 0.0, 0.25, 0.5, 1.0, nan, 0.0, nan]
2019-02-19T19:29:06.051034: step 155, loss 2.36424, accuracy 0.5625, precision [0.0, 0.25, nan, 1.0, 0.6666666666666666, nan, nan, 1.0, nan], recall [nan, 0.3333333333333333, nan, 0.5714285714285714, 1.0, 0.0, nan, 0.6666666666666666, nan]
2019-02-19T19:29:10.067176: step 156, loss 3.52988, accuracy 0.25, precision [1.0, 0.0, 0.0, 0.5, nan, 0.0, 0.0, 0.0, 0.0], recall [1.0, nan, 0.0, 0.42857142857142855, 0.0, nan, 0.0, 0.0, 0.0]
2019-02-19T19:29:14.197919: step 157, loss 3.65809, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.4, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, nan, nan]
2019-02-19T19:29:18.219160: step 158, loss 3.96537, accuracy 0.125, precision [0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, nan, nan, nan, 0.0], recall [0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.0, nan, 0.0, nan]
2019-02-19T19:29:22.219070: step 159, loss 2.70897, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 0.0, nan], recall [1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5, nan, 0.0]
2019-02-19T19:29:26.230620: step 160, loss 3.02403, accuracy 0.4375, precision [0.0, 0.0, 0.5, 0.4, 0.8, 0.0, nan, nan, nan], recall [nan, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, nan]
2019-02-19T19:29:30.232626: step 161, loss 2.25168, accuracy 0.25, precision [nan, nan, 0.0, 0.14285714285714285, 1.0, nan, nan, 0.0, nan], recall [0.0, nan, 0.0, 0.3333333333333333, 0.5, nan, 0.0, nan, 0.0]
2019-02-19T19:29:34.239383: step 162, loss 3.30263, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 0.2222222222222222, 0.0, nan, nan, nan, nan], recall [1.0, 0.0, 0.0, 0.4, 0.0, nan, 0.0, 0.0, 0.0]
2019-02-19T19:29:38.247962: step 163, loss 3.10024, accuracy 0.375, precision [0.0, 0.3333333333333333, nan, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, nan], recall [0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.6666666666666666, nan, 1.0, nan, nan]
2019-02-19T19:29:42.267321: step 164, loss 3.93601, accuracy 0.1875, precision [0.0, 0.5, 0.3333333333333333, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.25, 0.5, 0.25, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:29:46.269960: step 165, loss 3.22949, accuracy 0.375, precision [1.0, nan, 0.3333333333333333, 0.3333333333333333, 0.5, nan, 0.0, nan, 0.0], recall [1.0, 0.0, 1.0, 0.6666666666666666, 0.4, nan, nan, 0.0, nan]
2019-02-19T19:29:50.277256: step 166, loss 1.59934, accuracy 0.5, precision [nan, 0.5, 0.5, 0.75, 0.3333333333333333, nan, 1.0, 0.0, 0.0], recall [0.0, 0.25, 0.6666666666666666, 0.6, 1.0, nan, 1.0, 0.0, nan]
2019-02-19T19:29:54.278674: step 167, loss 1.99345, accuracy 0.5, precision [0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 0.42857142857142855, nan, nan, nan, nan], recall [1.0, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, nan, nan, nan]
2019-02-19T19:29:58.286803: step 168, loss 2.50213, accuracy 0.4375, precision [0.3333333333333333, nan, 1.0, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, 0.5, 0.7142857142857143, 0.0, nan, nan, 0.0, 0.0]
2019-02-19T19:30:02.384012: step 169, loss 2.76919, accuracy 0.375, precision [0.0, 0.0, 1.0, 0.75, 0.6666666666666666, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, 0.5, 0.42857142857142855, 0.6666666666666666, nan, nan, 0.0, 0.0]
2019-02-19T19:30:06.412192: step 170, loss 2.36765, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.2, 1.0, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:30:10.407023: step 171, loss 3.21245, accuracy 0.25, precision [nan, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 1.0, nan], recall [0.0, 0.0, 0.6666666666666666, 0.16666666666666666, 0.0, nan, nan, 0.5, 0.0]
2019-02-19T19:30:14.402213: step 172, loss 2.08649, accuracy 0.5, precision [0.0, 1.0, 1.0, 0.6, 0.5, 0.0, 0.0, 1.0, nan], recall [0.0, 0.5, 0.5, 0.75, 0.6666666666666666, nan, nan, 0.3333333333333333, 0.0]
2019-02-19T19:30:18.400676: step 173, loss 3.16741, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.4, 0.6666666666666666, nan, 0.0, 0.5, nan], recall [nan, nan, 0.0, 0.5, 0.4, 0.0, 0.0, 0.5, nan]
2019-02-19T19:30:22.395754: step 174, loss 1.69845, accuracy 0.4375, precision [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.5, 0.42857142857142855, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:30:26.400101: step 175, loss 3.1888, accuracy 0.3125, precision [nan, 0.0, 1.0, 0.4, 0.5, nan, 0.0, 0.3333333333333333, nan], recall [0.0, nan, 1.0, 0.4, 0.2, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:30:30.394411: step 176, loss 3.43124, accuracy 0.25, precision [0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [0.0, 1.0, 0.0, 0.4, 0.25, nan, nan, nan, 0.0]
2019-02-19T19:30:34.393274: step 177, loss 2.50506, accuracy 0.375, precision [0.0, 0.25, 1.0, 0.25, 1.0, nan, 0.0, 0.5, 0.0], recall [0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.0]
2019-02-19T19:30:38.392274: step 178, loss 2.78969, accuracy 0.25, precision [nan, 0.0, 0.0, 0.5714285714285714, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.6666666666666666, nan, 0.0, 0.0, 0.0, 0.0]
2019-02-19T19:30:42.398106: step 179, loss 2.76306, accuracy 0.125, precision [0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.3333333333333333, 0.2, nan, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:30:46.395974: step 180, loss 2.20986, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.6666666666666666, 0.2, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, 0.4, 0.5, 0.6666666666666666, 0.0, nan, 0.0, nan]
2019-02-19T19:30:50.526374: step 181, loss 3.74577, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0], recall [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0]
2019-02-19T19:30:54.541107: step 182, loss 2.99895, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6, 0.0, 0.0, 0.5, nan, nan], recall [0.0, 0.5, nan, 0.6, 0.0, 0.0, 1.0, 0.0, 0.0]
2019-02-19T19:30:58.543847: step 183, loss 2.25025, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.42857142857142855, nan, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.42857142857142855, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:31:02.555094: step 184, loss 3.42235, accuracy 0.1875, precision [0.0, 0.3333333333333333, nan, 0.3333333333333333, nan, nan, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.4, 0.0, 0.0, 0.0, nan, 0.0]
2019-02-19T19:31:06.566036: step 185, loss 3.0473, accuracy 0.25, precision [0.0, nan, nan, 0.5, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5, 1.0, nan, 0.0, nan, nan]
2019-02-19T19:31:10.569640: step 186, loss 3.4268, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.25, 0.0, 0.2857142857142857, nan, nan, nan, 0.0, nan]
2019-02-19T19:31:14.579209: step 187, loss 3.12609, accuracy 0.3125, precision [nan, 1.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.5714285714285714, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:31:18.586507: step 188, loss 2.34946, accuracy 0.4375, precision [0.0, 0.0, 1.0, 0.8333333333333334, 0.0, 0.0, 0.0, 1.0, nan], recall [0.0, 0.0, 1.0, 0.45454545454545453, nan, nan, nan, 0.5, nan]
2019-02-19T19:31:22.598702: step 189, loss 1.78836, accuracy 0.4375, precision [0.0, 0.6666666666666666, nan, 0.625, nan, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.45454545454545453, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:31:26.606258: step 190, loss 3.65335, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.09090909090909091, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:31:30.604482: step 191, loss 2.75039, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.5, nan], recall [nan, 0.0, nan, 0.25, 1.0, 0.0, nan, 1.0, nan]
2019-02-19T19:31:34.606103: step 192, loss 2.22506, accuracy 0.375, precision [nan, 0.0, 0.0, 1.0, 0.5, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.375, 0.75, nan, nan, nan, 0.0]
2019-02-19T19:31:38.708143: step 193, loss 2.81904, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.5, 0.0, nan, 0.5, 0.5, 0.0], recall [nan, 0.5, nan, 0.2222222222222222, nan, 0.0, 1.0, 1.0, nan]
2019-02-19T19:31:42.728401: step 194, loss 1.89889, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [0.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.0, nan, 0.0, nan]
2019-02-19T19:31:46.737019: step 195, loss 2.23569, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.0, 0.5, nan, nan, 0.3333333333333333, 0.0], recall [0.0, 0.0, 0.0, 0.0, 0.6666666666666666, nan, nan, 0.2, nan]
2019-02-19T19:31:50.733809: step 196, loss 2.46743, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.4, 1.0, 1.0, nan, 0.0, nan, nan], recall [nan, 1.0, 1.0, 0.4, 0.75, nan, 0.0, 0.0, nan]
2019-02-19T19:31:54.739062: step 197, loss 2.0002, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0], recall [nan, 0.0, nan, 0.5, 0.4, 0.0, nan, 0.25, nan]
2019-02-19T19:31:58.737283: step 198, loss 2.3236, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.4, 0.25, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.5, 0.3333333333333333, nan, 0.0, 0.0, nan]
2019-02-19T19:32:02.735254: step 199, loss 2.84627, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0], recall [0.0, 1.0, nan, 0.0, 0.2857142857142857, nan, nan, 0.0, nan]
2019-02-19T19:32:06.725258: step 200, loss 3.03424, accuracy 0.25, precision [0.0, 1.0, nan, 0.2, 0.0, 0.0, nan, 0.4, 0.0], recall [nan, 0.5, 0.0, 0.5, 0.0, nan, nan, 0.4, nan]
2019-02-19T19:32:10.736636: step 201, loss 2.664, accuracy 0.3125, precision [0.0, 0.0, nan, 0.16666666666666666, 1.0, 0.0, 0.0, nan, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.8, 0.0, nan, 0.0, nan]
2019-02-19T19:32:14.746425: step 202, loss 2.622, accuracy 0.1875, precision [nan, 0.5, 0.0, 0.3333333333333333, 0.25, 0.0, 0.0, nan, nan], recall [0.0, 0.2, nan, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, nan]
2019-02-19T19:32:18.749647: step 203, loss 1.9462, accuracy 0.4375, precision [0.3333333333333333, 0.3333333333333333, 0.0, 0.42857142857142855, 1.0, nan, nan, nan, nan], recall [1.0, 1.0, 0.0, 0.6, 0.4, 0.0, nan, 0.0, nan]
2019-02-19T19:32:22.747208: step 204, loss 2.17747, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan], recall [0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0]
2019-02-19T19:32:26.872020: step 205, loss 2.48101, accuracy 0.3125, precision [nan, 0.5, 0.25, 0.2857142857142857, 1.0, nan, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, 0.5, 0.6666666666666666, 0.5, 0.0, nan, 0.0, nan]
2019-02-19T19:32:30.885060: step 206, loss 2.29545, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.5, 0.75, 0.0, 1.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 1.0, nan, 1.0, 0.0, 0.0]
2019-02-19T19:32:34.881280: step 207, loss 1.55026, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.625, 0.6666666666666666, nan, nan, 0.0, 0.0], recall [0.0, nan, 0.0, 0.7142857142857143, 0.4, 0.0, nan, nan, nan]
2019-02-19T19:32:38.880285: step 208, loss 1.64942, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.5, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5714285714285714, 0.4, nan, nan, nan, nan]
2019-02-19T19:32:42.882212: step 209, loss 2.33947, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, 1.0, nan, nan, 0.0, nan], recall [nan, nan, 0.0, 0.3, 0.25, nan, 0.0, nan, nan]
2019-02-19T19:32:46.883827: step 210, loss 2.26223, accuracy 0.25, precision [nan, 0.0, 1.0, 0.6, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, 0.3333333333333333, 0.5, 0.0, 0.0, nan, nan, 0.0]
2019-02-19T19:32:50.900215: step 211, loss 1.41046, accuracy 0.4375, precision [0.0, 0.0, nan, 0.625, 0.6666666666666666, nan, nan, nan, nan], recall [0.0, nan, 0.0, 0.625, 1.0, 0.0, nan, nan, nan]
2019-02-19T19:32:54.905340: step 212, loss 1.77365, accuracy 0.4375, precision [0.0, 0.0, nan, 0.6666666666666666, 0.75, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.5, 0.6, nan, nan, 0.0, nan]
2019-02-19T19:32:58.905932: step 213, loss 2.13131, accuracy 0.3125, precision [0.25, 0.0, 1.0, 0.6, 0.0, nan, nan, nan, nan], recall [0.2, 0.0, 0.5, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:33:02.908338: step 214, loss 1.92404, accuracy 0.375, precision [0.0, 0.4, 0.3333333333333333, 1.0, 0.5, nan, 0.0, nan, 0.0], recall [0.0, 0.5, 1.0, 0.14285714285714285, 1.0, 0.0, nan, nan, nan]
2019-02-19T19:33:06.903286: step 215, loss 2.02264, accuracy 0.375, precision [nan, 0.75, 0.0, 0.375, nan, 0.0, nan, nan, 0.0], recall [0.0, 0.5, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:33:11.011470: step 216, loss 1.7538, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.6, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.25, nan, 0.375, 0.5, nan, nan, nan, nan]
2019-02-19T19:33:15.038713: step 217, loss 2.58011, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 1.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:33:19.036841: step 218, loss 1.66607, accuracy 0.375, precision [nan, 1.0, 0.0, 0.5555555555555556, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.2, nan, 0.625, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:33:23.041820: step 219, loss 2.66562, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.125, 1.0, nan, nan, nan, nan]
2019-02-19T19:33:27.037679: step 220, loss 2.59836, accuracy 0.1875, precision [nan, 0.0, nan, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:33:31.033114: step 221, loss 2.64437, accuracy 0.25, precision [0.0, 0.25, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:33:35.033476: step 222, loss 2.3083, accuracy 0.5, precision [0.0, 0.0, 0.5, 0.875, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.5, 0.5833333333333334, nan, nan, nan, nan, nan]
2019-02-19T19:33:39.028180: step 223, loss 2.79617, accuracy 0.375, precision [0.25, 1.0, nan, 0.6666666666666666, 0.3333333333333333, 0.0, 1.0, 0.0, nan], recall [1.0, 0.5, nan, 0.2222222222222222, 0.3333333333333333, nan, 1.0, nan, nan]
2019-02-19T19:33:43.043437: step 224, loss 2.0339, accuracy 0.25, precision [0.0, 0.0, nan, 0.5714285714285714, nan, nan, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:33:47.049439: step 225, loss 1.49259, accuracy 0.625, precision [0.0, 0.3333333333333333, 0.5, 0.8, 0.75, nan, nan, 1.0, nan], recall [nan, 1.0, 0.3333333333333333, 0.5714285714285714, 1.0, nan, 0.0, 1.0, nan]
2019-02-19T19:33:51.061926: step 226, loss 1.85702, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.6, 0.0, 1.0, nan, 0.5, 0.0], recall [nan, 0.3333333333333333, nan, 0.3, 0.0, 1.0, nan, 1.0, nan]
2019-02-19T19:33:55.067115: step 227, loss 2.74825, accuracy 0.375, precision [0.0, 0.75, 0.0, 0.6, 0.0, nan, 0.0, nan, nan], recall [nan, 1.0, nan, 0.42857142857142855, 0.0, 0.0, nan, nan, 0.0]
2019-02-19T19:33:59.190658: step 228, loss 2.04541, accuracy 0.3125, precision [nan, nan, 0.0, 0.5714285714285714, 0.5, nan, nan, 0.0, nan], recall [nan, nan, 0.0, 0.5714285714285714, 0.14285714285714285, 0.0, nan, nan, nan]
2019-02-19T19:34:03.209312: step 229, loss 2.19095, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.4, 0.3333333333333333, nan, nan, nan, nan], recall [0.0, 0.3333333333333333, nan, 0.4, 0.3333333333333333, nan, nan, 0.0, 0.0]
2019-02-19T19:34:07.202156: step 230, loss 2.27008, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.5, 0.0], recall [0.0, 1.0, nan, 0.5, 0.0, 0.0, nan, 0.2, nan]
2019-02-19T19:34:11.199733: step 231, loss 2.00339, accuracy 0.4375, precision [0.0, 0.0, 0.5, 0.6666666666666666, nan, nan, nan, 0.0, nan], recall [0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:34:15.199197: step 232, loss 2.24017, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.6666666666666666, 0.25, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, 1.0, 0.2, 0.0, nan, nan, 0.0, 0.0]
2019-02-19T19:34:19.208101: step 233, loss 2.09246, accuracy 0.375, precision [0.0, 1.0, 0.5, 0.6, 0.0, nan, 0.0, 0.5, nan], recall [nan, 0.3333333333333333, 1.0, 0.6, 0.0, nan, nan, 0.25, 0.0]
2019-02-19T19:34:23.213558: step 234, loss 1.84069, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.5, nan, 0.0, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:34:27.215875: step 235, loss 2.53546, accuracy 0.1875, precision [0.0, 0.0, nan, 0.5, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.0, 0.0, 0.6, nan, nan, nan, 0.0, nan]
2019-02-19T19:34:31.208773: step 236, loss 1.84197, accuracy 0.3125, precision [0.0, 0.0, 0.5, 0.6, 0.5, nan, 0.0, nan, 0.0], recall [0.0, 0.0, 0.3333333333333333, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:34:35.218404: step 237, loss 1.7058, accuracy 0.4375, precision [1.0, 0.6666666666666666, 0.5, 0.5, 0.5, 0.0, nan, 0.0, 0.0], recall [0.3333333333333333, 1.0, 0.25, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:34:39.215743: step 238, loss 1.84063, accuracy 0.25, precision [nan, 0.0, 0.3333333333333333, 0.2857142857142857, 0.0, nan, 1.0, 0.0, nan], recall [0.0, 0.0, 0.5, 0.3333333333333333, 0.0, nan, 0.5, nan, 0.0]
2019-02-19T19:34:43.229415: step 239, loss 2.24904, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.25, nan, nan, nan, 0.0, nan]
2019-02-19T19:34:47.331220: step 240, loss 2.05908, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.4, 0.6, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:34:51.355231: step 241, loss 1.62793, accuracy 0.5, precision [0.0, 0.0, 1.0, 0.75, 0.75, nan, 1.0, 0.0, nan], recall [0.0, 0.0, 0.3333333333333333, 0.6, 0.75, nan, 1.0, 0.0, nan]
2019-02-19T19:34:55.359468: step 242, loss 2.3087, accuracy 0.375, precision [0.3333333333333333, nan, 0.3333333333333333, 0.4, nan, nan, nan, 0.5, 0.0], recall [0.25, 0.0, 1.0, 0.2857142857142857, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:34:59.357265: step 243, loss 1.57626, accuracy 0.5, precision [0.0, 0.0, 0.5, 1.0, 0.4, 0.0, nan, nan, nan], recall [0.0, nan, 0.6666666666666666, 0.4444444444444444, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:35:03.351346: step 244, loss 1.92114, accuracy 0.375, precision [nan, 1.0, 0.0, 0.3333333333333333, 0.75, 0.0, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.4, 1.0, nan, 0.0, 0.0, nan]
2019-02-19T19:35:07.344845: step 245, loss 1.4867, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 0.375, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:35:11.351299: step 246, loss 2.1849, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.5, 0.5, nan, 0.0, 0.0, 0.0], recall [0.5, 0.0, 0.0, 0.4, 0.25, nan, nan, nan, nan]
2019-02-19T19:35:15.351404: step 247, loss 1.94154, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.25, 1.0, 0.0, nan, nan, nan], recall [nan, 0.0, nan, 0.125, 0.75, nan, nan, 0.0, 0.0]
2019-02-19T19:35:19.364106: step 248, loss 2.35763, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 0.25, nan], recall [0.0, nan, 0.0, 0.375, 0.3333333333333333, nan, 0.0, 1.0, nan]
2019-02-19T19:35:23.365594: step 249, loss 2.09458, accuracy 0.375, precision [0.0, 0.0, 1.0, 0.75, 0.25, 0.0, nan, 0.0, nan], recall [nan, 0.0, 1.0, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:35:27.366912: step 250, loss 1.65177, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.5714285714285714, 0.3333333333333333, 0.0, nan, 0.0, 1.0], recall [nan, nan, 0.0, 0.4444444444444444, 0.5, 0.0, nan, 0.0, 1.0]

Evaluation:
