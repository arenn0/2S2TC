Loading data...
{'sympathy_and_emotional_support': 0, 'not_related_or_irrelevant': 1, 'infrastructure_and_utilities_damage': 2, 'other_useful_information': 3, 'injured_or_dead_people': 4, 'caution_and_advice': 5, 'displaced_people_and_evacuations': 6, 'donation_needs_or_offers_or_volunteering_services': 7, 'missing_trapped_or_found_people': 8}
Max Document length: 2407
Vocabulary Size: 1
Train/Dev split: 9226/1025
Writing to /home/ubuntu/Project/runs/1550606084

2019-02-19T19:54:46.199786: step 1, loss 10.4362, accuracy 0.125, precision [0.0, 1.0, 0.0, 0.1111111111111111, nan, 0.0, nan, 0.0, nan], recall [nan, 0.25, nan, 0.3333333333333333, 0.0, nan, 0.0, 0.0, 0.0]
2019-02-19T19:54:46.387202: step 2, loss 10.9628, accuracy 0.1875, precision [0.0, 0.0, nan, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.21428571428571427, nan, nan, 0.0, 0.0, nan]
2019-02-19T19:54:46.547861: step 3, loss 8.34201, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.0, 0.0, nan, 0.5, nan], recall [nan, 1.0, nan, 0.2727272727272727, nan, nan, nan, 0.3333333333333333, 0.0]
2019-02-19T19:54:46.707756: step 4, loss 10.7183, accuracy 0.125, precision [0.0, 0.0, 0.3333333333333333, 1.0, 0.0, nan, 0.0, nan, nan], recall [nan, nan, 0.3333333333333333, 0.125, nan, 0.0, 0.0, 0.0, nan]
2019-02-19T19:54:46.861827: step 5, loss 10.4568, accuracy 0.0625, precision [nan, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.125, nan, nan, nan, nan, nan, 0.0, nan]
2019-02-19T19:54:47.019038: step 6, loss 11.0616, accuracy 0.0625, precision [0.0, 0.5, 0.0, 0.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0, 0.0, nan, nan]
2019-02-19T19:54:47.173361: step 7, loss 12.5539, accuracy 0, precision [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:54:47.327503: step 8, loss 9.55242, accuracy 0.1875, precision [0.0, 0.25, 0.3333333333333333, 0.0, 1.0, nan, nan, 0.0, nan], recall [0.0, 0.25, 0.3333333333333333, nan, 0.16666666666666666, 0.0, nan, nan, nan]
2019-02-19T19:54:47.485449: step 9, loss 7.7727, accuracy 0.125, precision [nan, 1.0, nan, 0.0, 0.25, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, nan, 0.2, nan, 0.0, nan, nan]
2019-02-19T19:54:47.634966: step 10, loss 5.63348, accuracy 0.375, precision [nan, 0.5, nan, 0.6, 0.2857142857142857, 0.0, nan, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, 0.6, 0.6666666666666666, 0.0, 0.0, nan, nan]
2019-02-19T19:54:47.785289: step 11, loss 8.84479, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.25, 1.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.16666666666666666, 0.2857142857142857, 0.0, nan, nan, nan]
2019-02-19T19:54:47.936197: step 12, loss 9.83882, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.75, 1.0, nan, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.375, 0.16666666666666666, 0.0, nan, nan, nan]
2019-02-19T19:54:48.092628: step 13, loss 6.74554, accuracy 0.3125, precision [0.0, 0.25, 0.0, 0.75, 0.0, 0.0, 0.0, 1.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.2, nan]
2019-02-19T19:54:48.246621: step 14, loss 7.43406, accuracy 0.25, precision [nan, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.5, nan], recall [nan, 0.0, nan, 0.375, 0.0, nan, nan, 0.25, nan]
2019-02-19T19:54:48.406407: step 15, loss 9.16629, accuracy 0.25, precision [0.0, 0.0, 0.2, 0.5, nan, nan, 0.0, 1.0, nan], recall [nan, nan, 1.0, 0.25, 0.0, nan, nan, 0.16666666666666666, nan]
2019-02-19T19:54:48.560125: step 16, loss 3.43768, accuracy 0.4375, precision [nan, 1.0, 0.0, 0.36363636363636365, 0.5, nan, nan, nan, nan], recall [nan, 0.3333333333333333, nan, 1.0, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:54:48.708647: step 17, loss 7.24123, accuracy 0.125, precision [nan, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.2222222222222222, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:54:48.858366: step 18, loss 7.81062, accuracy 0.1875, precision [0.0, nan, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.42857142857142855, nan, 0.0, 0.0, 0.0, nan]
2019-02-19T19:54:49.012155: step 19, loss 7.60433, accuracy 0.25, precision [nan, 0.16666666666666666, nan, 0.6, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, 0.0, 0.6, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:54:49.168187: step 20, loss 7.53281, accuracy 0.125, precision [0.0, 0.0, nan, 0.4, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:54:49.321887: step 21, loss 5.96371, accuracy 0.25, precision [0.0, 0.75, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan], recall [nan, 0.2727272727272727, 0.0, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:54:49.476392: step 22, loss 7.4023, accuracy 0, precision [0.0, 0.0, 0.0, 0.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.0, nan, 0.0, nan, nan, nan]
2019-02-19T19:54:49.629606: step 23, loss 7.95153, accuracy 0.0625, precision [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan, 0.0], recall [0.0, 0.0, 0.14285714285714285, 0.0, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:54:49.785327: step 24, loss 7.23738, accuracy 0.0625, precision [0.25, 0.0, nan, 0.0, 0.0, nan, nan, 0.0, 0.0], recall [0.25, 0.0, 0.0, 0.0, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:54:49.937038: step 25, loss 6.36197, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, nan, nan], recall [0.0, nan, 0.0, 0.4, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:54:50.096273: step 26, loss 4.1873, accuracy 0.375, precision [0.5, 0.3333333333333333, nan, 0.3333333333333333, 1.0, 0.0, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.3333333333333333, 0.5, 0.0, nan, 0.0, nan]
2019-02-19T19:54:50.249372: step 27, loss 6.17835, accuracy 0.25, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, nan, 0.0, 0.0], recall [0.25, nan, nan, 0.2222222222222222, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:54:50.404513: step 28, loss 5.88584, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.6666666666666666, 0.14285714285714285, 0.0, nan, 0.0, nan]
2019-02-19T19:54:50.559486: step 29, loss 5.14078, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.16666666666666666, 0.14285714285714285, nan, nan, nan, nan], recall [0.0, nan, nan, 0.16666666666666666, 0.16666666666666666, 0.0, nan, 0.0, nan]
2019-02-19T19:54:50.712047: step 30, loss 4.64485, accuracy 0.125, precision [0.0, 0.25, nan, 0.2, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.25, nan, 0.2, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:54:50.866943: step 31, loss 6.46918, accuracy 0.125, precision [0.5, 0.0, 0.0, 0.25, nan, nan, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.16666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:54:51.017614: step 32, loss 4.25606, accuracy 0.1875, precision [nan, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.25, nan, 0.2857142857142857, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:54:51.173401: step 33, loss 4.6566, accuracy 0.25, precision [0.3333333333333333, nan, nan, 0.4, 0.2, 0.0, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.6666666666666666, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:54:51.333173: step 34, loss 5.11521, accuracy 0.25, precision [0.5, nan, 0.0, 0.4, 0.0, 0.0, 0.0, 0.5, nan], recall [0.16666666666666666, 0.0, nan, 0.4, nan, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:54:51.484762: step 35, loss 4.89814, accuracy 0.0625, precision [0.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.16666666666666666, nan, 0.0, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:54:51.638995: step 36, loss 4.93923, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.4, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.4, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:54:51.793315: step 37, loss 4.66565, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.2857142857142857, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:54:51.945706: step 38, loss 5.76577, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.25, nan, nan, 0.0, nan], recall [nan, nan, 0.0, 0.14285714285714285, 0.25, 0.0, 0.0, nan, 0.0]
2019-02-19T19:54:52.099852: step 39, loss 6.4079, accuracy 0.0625, precision [nan, 0.0, nan, 0.16666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:54:52.250567: step 40, loss 5.60502, accuracy 0.1875, precision [0.0, 0.0, 0.5, 0.4, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, nan, 0.5, 0.4, 0.0, 0.0, nan, 0.0, 0.0]
2019-02-19T19:54:52.401545: step 41, loss 3.83147, accuracy 0.125, precision [0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, nan, nan, nan], recall [nan, 1.0, 0.0, 0.25, 0.0, nan, nan, 0.0, 0.0]
2019-02-19T19:54:52.554452: step 42, loss 4.87013, accuracy 0.1875, precision [0.0, 0.0, nan, 0.2, 0.5, 0.0, 0.0, 0.5, 0.0], recall [0.0, 0.0, 0.0, 0.25, 0.3333333333333333, 0.0, nan, 0.5, nan]
2019-02-19T19:54:52.704504: step 43, loss 3.51783, accuracy 0.125, precision [nan, 0.0, 0.25, 0.16666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, 0.5, 0.16666666666666666, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:54:52.860405: step 44, loss 3.62561, accuracy 0.25, precision [0.0, 0.0, 0.5, 0.0, 0.75, nan, nan, 0.0, nan], recall [nan, 0.0, 0.3333333333333333, 0.0, 0.5, nan, 0.0, 0.0, nan]
2019-02-19T19:54:53.012795: step 45, loss 3.4136, accuracy 0.1875, precision [nan, 0.3333333333333333, nan, 0.0, 0.25, 0.0, 0.0, 0.5, nan], recall [0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, nan, 0.0, 1.0, nan]
2019-02-19T19:54:53.166965: step 46, loss 3.42473, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 0.2222222222222222, 0.3333333333333333, nan, nan, nan, nan], recall [0.0, 0.3333333333333333, nan, 0.5, 0.2, nan, 0.0, nan, 0.0]
2019-02-19T19:54:53.322949: step 47, loss 4.52785, accuracy 0.125, precision [0.0, 1.0, 0.0, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, 0.0, 0.2, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:54:53.476024: step 48, loss 4.14164, accuracy 0.1875, precision [nan, 0.25, 0.0, 0.5, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.2, 0.0, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:54:53.632600: step 49, loss 2.40401, accuracy 0.4375, precision [0.0, 0.0, nan, 0.7142857142857143, 0.25, nan, nan, 0.5, nan], recall [nan, 0.0, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:54:53.793459: step 50, loss 3.79114, accuracy 0.4375, precision [0.25, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, nan, 0.0, 0.0], recall [1.0, 0.42857142857142855, 0.0, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:54:53.944812: step 51, loss 3.99825, accuracy 0.125, precision [0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.4, 0.0, 0.0, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:54:54.096925: step 52, loss 3.90191, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.0, 0.2, 0.0, nan, 1.0, 0.0], recall [nan, 0.2, 0.0, 0.0, 0.5, nan, nan, 0.2, nan]
2019-02-19T19:54:54.248257: step 53, loss 5.99231, accuracy 0.0625, precision [0.0, 0.0, nan, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.0, 0.125, nan, nan, 0.0, 0.0]
2019-02-19T19:54:54.403982: step 54, loss 5.09468, accuracy 0.0625, precision [nan, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2019-02-19T19:54:54.559296: step 55, loss 4.57517, accuracy 0, precision [0.0, 0.0, 0.0, 0.0, nan, 0.0, nan, nan, nan], recall [0.0, nan, 0.0, nan, 0.0, 0.0, nan, 0.0, 0.0]
2019-02-19T19:54:54.715697: step 56, loss 3.98023, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, nan], recall [0.0, nan, 0.0, nan, 0.0, 1.0, 0.0, 0.0, 0.0]
2019-02-19T19:54:54.864763: step 57, loss 3.22646, accuracy 0.25, precision [nan, 0.0, nan, 0.42857142857142855, 0.0, 0.0, nan, 1.0, 0.0], recall [0.0, nan, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0]
2019-02-19T19:54:55.019997: step 58, loss 3.74537, accuracy 0.3125, precision [nan, nan, 0.0, 0.5, 0.2, nan, nan, 0.25, nan], recall [nan, 0.0, nan, 0.3333333333333333, 1.0, 0.0, nan, 1.0, 0.0]
2019-02-19T19:54:55.174490: step 59, loss 5.29366, accuracy 0, precision [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.0, 0.0, 0.0, 0.0, nan, 0.0]
2019-02-19T19:54:55.329102: step 60, loss 5.4403, accuracy 0.125, precision [0.0, 0.0, nan, 0.3333333333333333, 0.25, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.1111111111111111, 0.3333333333333333, 0.0, nan, nan, 0.0]
2019-02-19T19:54:55.481524: step 61, loss 3.06608, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2222222222222222, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:54:55.635767: step 62, loss 2.8993, accuracy 0.375, precision [0.75, 0.25, 0.0, 0.5, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.5, 0.0, 0.4, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:54:55.790860: step 63, loss 2.6652, accuracy 0.375, precision [0.5, 1.0, 0.0, 0.4444444444444444, 0.0, nan, 0.0, nan, nan], recall [0.5, 0.5, nan, 0.5714285714285714, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:54:55.944937: step 64, loss 4.00132, accuracy 0.1875, precision [0.5, 0.5, nan, 0.0, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:54:56.097555: step 65, loss 3.11387, accuracy 0.3125, precision [1.0, 0.5, 0.0, 0.25, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.25, 0.5, 0.0, 0.5, 0.25, nan, nan, nan, nan]
2019-02-19T19:54:56.250472: step 66, loss 2.87918, accuracy 0.25, precision [0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, nan], recall [0.5, 0.25, nan, 0.0, 0.0, 0.0, nan, 0.25, nan]
2019-02-19T19:54:56.403885: step 67, loss 3.0554, accuracy 0.1875, precision [nan, 0.5, 0.0, 0.16666666666666666, 0.25, 0.0, nan, nan, 0.0], recall [0.0, 0.2, nan, 0.25, 0.3333333333333333, 0.0, nan, 0.0, nan]
2019-02-19T19:54:56.559632: step 68, loss 2.76046, accuracy 0.25, precision [0.0, 0.0, 0.5, 0.2, 0.5, nan, nan, nan, 0.0], recall [nan, 0.0, 1.0, 0.25, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:54:56.713954: step 69, loss 2.15564, accuracy 0.375, precision [nan, 0.0, nan, 0.5, 0.5, 0.0, nan, 0.5, 0.0], recall [0.0, 0.0, 0.0, 0.75, 1.0, nan, nan, 0.25, nan]
2019-02-19T19:54:56.869704: step 70, loss 2.15618, accuracy 0.3125, precision [0.0, 0.0, nan, 0.8333333333333334, 0.0, nan, nan, nan, nan], recall [0.0, 0.0, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:54:57.022193: step 71, loss 4.58189, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.75, 0.0, 0.0, nan, nan, nan], recall [nan, 0.0, nan, 0.25, nan, nan, nan, 0.0, nan]
2019-02-19T19:54:57.177126: step 72, loss 2.15347, accuracy 0.25, precision [0.3333333333333333, 0.0, nan, 0.5, 0.25, nan, 0.0, 0.0, nan], recall [0.3333333333333333, nan, nan, 0.25, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:54:57.328428: step 73, loss 2.26591, accuracy 0.375, precision [0.3333333333333333, 0.3333333333333333, 0.0, 0.6, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 0.16666666666666666, nan, 0.5, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:54:57.476878: step 74, loss 3.43161, accuracy 0.125, precision [nan, 0.25, nan, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:54:57.631538: step 75, loss 2.81276, accuracy 0.3125, precision [1.0, 0.0, nan, 0.5, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.3333333333333333, 0.2857142857142857, nan, nan, nan, nan]
2019-02-19T19:54:57.786464: step 76, loss 3.21225, accuracy 0.1875, precision [0.0, nan, 0.0, 0.0, 0.6, 0.0, nan, nan, nan], recall [nan, 0.0, 0.0, 0.0, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:54:57.939524: step 77, loss 3.38454, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0], recall [0.0, 0.5, 0.0, nan, 0.1111111111111111, nan, 0.5, nan, nan]
2019-02-19T19:54:58.097919: step 78, loss 2.97224, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, nan, 0.3333333333333333, nan, 0.0, nan, 0.0]
2019-02-19T19:54:58.248230: step 79, loss 2.50296, accuracy 0.25, precision [1.0, 0.5, 0.0, 0.2, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [0.5, 0.3333333333333333, nan, 1.0, 0.14285714285714285, 0.0, 0.0, 0.0, nan]
2019-02-19T19:54:58.406561: step 80, loss 3.94922, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.2, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2, 0.0, 0.0, 0.0, 0.0, nan]
2019-02-19T19:54:58.560670: step 81, loss 2.13686, accuracy 0.125, precision [0.5, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.2, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:54:58.714382: step 82, loss 2.74049, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.5, nan, nan, nan, 0.5, nan], recall [0.0, 1.0, nan, 0.5, 0.0, 0.0, nan, 0.2, nan]
2019-02-19T19:54:58.866041: step 83, loss 3.26154, accuracy 0.125, precision [0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.14285714285714285, nan, nan, 0.0, 0.0, nan]
2019-02-19T19:54:59.017692: step 84, loss 1.91804, accuracy 0.4375, precision [0.0, 0.3333333333333333, 1.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, 0.3333333333333333, 0.45454545454545453, nan, nan, nan, nan, nan]
2019-02-19T19:54:59.171632: step 85, loss 3.29947, accuracy 0.125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.2, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:54:59.323338: step 86, loss 2.89852, accuracy 0, precision [nan, 0.0, nan, 0.0, 0.0, nan, nan, 0.0, nan], recall [nan, nan, 0.0, 0.0, nan, nan, nan, 0.0, nan]
2019-02-19T19:54:59.477154: step 87, loss 2.70463, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.5, 0.0], recall [nan, 0.0, 0.0, 0.3333333333333333, nan, nan, nan, 0.5, nan]
2019-02-19T19:54:59.629227: step 88, loss 2.65506, accuracy 0.125, precision [0.0, 0.5, nan, 0.2, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.5, 0.0, 0.2, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:54:59.783710: step 89, loss 2.20327, accuracy 0.25, precision [1.0, 0.5, 1.0, 0.0, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.25, 0.3333333333333333, 0.0, nan, nan, nan, 0.0, nan]
2019-02-19T19:54:59.932647: step 90, loss 2.42561, accuracy 0.3125, precision [0.0, 0.6666666666666666, 1.0, 0.25, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.4, 0.4, 1.0, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:00.086781: step 91, loss 1.84006, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.0, 0.4, nan, nan, 0.0, nan], recall [0.0, 0.25, 0.0, 0.0, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:00.242769: step 92, loss 1.95374, accuracy 0.25, precision [0.0, 1.0, nan, 0.4, 0.2, nan, nan, 0.0, nan], recall [nan, 0.125, 0.0, 0.6666666666666666, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:00.399650: step 93, loss 2.78503, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.25, nan, nan, 0.0, 0.0, 0.0], recall [0.0, 0.4, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:00.555621: step 94, loss 3.11047, accuracy 0.125, precision [0.0, 0.3333333333333333, 0.0, 0.0, 0.5, nan, 0.0, 0.0, 0.0], recall [0.0, 1.0, nan, 0.0, 0.125, nan, nan, nan, nan]
2019-02-19T19:55:00.710722: step 95, loss 1.96137, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.2857142857142857, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, nan, nan, 0.4, 0.1111111111111111, nan, nan, nan, nan]
2019-02-19T19:55:00.865823: step 96, loss 2.9968, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.16666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:01.019899: step 97, loss 2.52805, accuracy 0.1875, precision [0.0, 0.0, nan, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:01.170231: step 98, loss 2.67639, accuracy 0.1875, precision [nan, 0.2, 0.0, 1.0, 0.2, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.1, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:55:01.326779: step 99, loss 2.1436, accuracy 0.3125, precision [0.25, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.25, 0.0, nan, 0.4444444444444444, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:01.478667: step 100, loss 2.46378, accuracy 0.25, precision [0.0, nan, 0.0, 0.5, 0.0, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.2727272727272727, nan, nan, nan, 0.5, nan]
2019-02-19T19:55:01.626181: step 101, loss 2.49092, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.625, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.625, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:01.780727: step 102, loss 2.15566, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, nan], recall [0.0, nan, nan, 0.2857142857142857, nan, nan, nan, 0.25, nan]
2019-02-19T19:55:01.933556: step 103, loss 2.14889, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.25, 1.0, nan, nan, 0.0, 0.0], recall [0.0, nan, 0.0, 0.5, 0.4, nan, nan, 0.0, nan]
2019-02-19T19:55:02.086245: step 104, loss 2.02632, accuracy 0.3125, precision [0.5, 0.0, nan, 0.6, nan, nan, 0.0, 0.3333333333333333, nan], recall [1.0, 0.0, 0.0, 0.6, nan, nan, nan, 0.2, nan]
2019-02-19T19:55:02.240187: step 105, loss 2.11885, accuracy 0.25, precision [0.0, 0.0, nan, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.3076923076923077, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:55:02.394689: step 106, loss 2.65006, accuracy 0.125, precision [1.0, 0.0, nan, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [1.0, nan, 0.0, 0.09090909090909091, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:02.547974: step 107, loss 2.32669, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.3333333333333333, 0.0, 0.25, nan, nan, 0.0, 0.0, nan]
2019-02-19T19:55:02.701508: step 108, loss 2.54711, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.4, 0.5, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:55:02.851336: step 109, loss 1.72507, accuracy 0.375, precision [0.5, nan, 0.0, 0.5, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [1.0, 0.0, 0.0, 0.6666666666666666, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:55:03.007949: step 110, loss 1.8525, accuracy 0.5, precision [0.5, 0.5, 0.0, 0.75, 0.0, 0.0, nan, nan, nan], recall [0.5, 0.25, 0.0, 0.6666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:55:03.164412: step 111, loss 1.8134, accuracy 0.4375, precision [0.0, 1.0, 0.5, 0.625, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 0.25, 0.5, 0.8333333333333334, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:55:03.317071: step 112, loss 2.50864, accuracy 0.0625, precision [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.25, nan, 0.0, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:03.471339: step 113, loss 2.84243, accuracy 0.0625, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.07142857142857142, nan, nan, nan, nan, nan]
2019-02-19T19:55:03.620625: step 114, loss 1.96718, accuracy 0.3125, precision [0.0, 0.0, nan, 0.7142857142857143, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.45454545454545453, nan, 0.0, nan, nan, nan]
2019-02-19T19:55:03.779111: step 115, loss 1.81086, accuracy 0.4375, precision [0.0, 0.0, nan, 0.875, 0.0, nan, nan, nan, 0.0], recall [nan, 0.0, nan, 0.5384615384615384, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:55:03.933763: step 116, loss 2.518, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.3333333333333333, 0.0, 0.0, 0.0, nan, nan]
2019-02-19T19:55:04.085209: step 117, loss 2.17731, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.7142857142857143, 0.5, nan, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5555555555555556, 0.5, nan, nan, nan, 0.0]
2019-02-19T19:55:04.236949: step 118, loss 2.51998, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.6, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3, 0.0, nan, nan, nan, 0.0]
2019-02-19T19:55:04.389076: step 119, loss 1.93393, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.75, 0.25, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.3333333333333333, 0.5, 0.0, nan, 0.0, 0.0]
2019-02-19T19:55:04.537134: step 120, loss 1.70923, accuracy 0.4375, precision [0.0, 0.3333333333333333, nan, 0.8333333333333334, 0.25, nan, 0.0, nan, nan], recall [nan, 1.0, 0.0, 0.4166666666666667, 1.0, 0.0, nan, nan, nan]
2019-02-19T19:55:04.685777: step 121, loss 1.88177, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.75, 0.6, 0.0, 0.0, nan, nan], recall [0.0, 0.3333333333333333, 0.0, 0.5, 0.6, nan, nan, nan, nan]
2019-02-19T19:55:04.840979: step 122, loss 2.43958, accuracy 0.0625, precision [0.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.25, nan, 0.0, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:04.991698: step 123, loss 2.4095, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5714285714285714, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:05.149002: step 124, loss 1.73146, accuracy 0.375, precision [0.0, 1.0, nan, 0.3, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 0.25, nan, 0.6, 0.2857142857142857, nan, nan, nan, nan]
2019-02-19T19:55:05.303647: step 125, loss 2.25627, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.375, 0.14285714285714285, nan, nan, nan, nan]
2019-02-19T19:55:05.456741: step 126, loss 2.13643, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:05.605346: step 127, loss 2.33956, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.4, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:05.757463: step 128, loss 1.87487, accuracy 0.375, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, nan, nan, nan, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:55:05.910967: step 129, loss 2.11149, accuracy 0.25, precision [0.0, 0.2, 0.0, 0.6, nan, 0.0, nan, 0.0, nan], recall [nan, 0.25, nan, 0.42857142857142855, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:06.058126: step 130, loss 1.67054, accuracy 0.5625, precision [0.0, 1.0, nan, 0.875, 0.5, nan, 0.0, nan, nan], recall [nan, 0.25, nan, 0.6363636363636364, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:06.210906: step 131, loss 2.03166, accuracy 0.25, precision [nan, nan, 0.0, 0.5714285714285714, 0.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.0, 0.0, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:06.362315: step 132, loss 1.92223, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.875, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.5384615384615384, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:06.517610: step 133, loss 1.90135, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:55:06.672323: step 134, loss 1.85368, accuracy 0.375, precision [0.0, 0.5, nan, 0.7142857142857143, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:55:06.825585: step 135, loss 2.90088, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:06.975117: step 136, loss 2.03497, accuracy 0.25, precision [0.0, 0.5, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.23076923076923078, nan, nan, 0.0, nan, nan]
2019-02-19T19:55:07.126894: step 137, loss 2.32286, accuracy 0.1875, precision [nan, 0.2, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.18181818181818182, nan, nan, 0.0, nan, nan]
2019-02-19T19:55:07.279101: step 138, loss 1.74011, accuracy 0.25, precision [0.3333333333333333, 0.0, nan, 0.3333333333333333, nan, nan, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.42857142857142855, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:55:07.428026: step 139, loss 1.66496, accuracy 0.375, precision [nan, 0.0, nan, 0.4, 0.5, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.8, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:55:07.581570: step 140, loss 1.94881, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:55:07.729094: step 141, loss 1.81285, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.0, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.46153846153846156, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:55:07.887084: step 142, loss 2.38657, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [0.0, nan, nan, 0.38461538461538464, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:08.042843: step 143, loss 1.64761, accuracy 0.25, precision [0.0, 0.0, nan, 0.8, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.36363636363636365, nan, nan, 0.0, 0.0, nan]
2019-02-19T19:55:08.198561: step 144, loss 1.72858, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3, 1.0, 0.0, nan, 0.0, nan]
2019-02-19T19:55:08.348690: step 145, loss 1.88264, accuracy 0.4375, precision [0.0, 0.0, nan, 0.7142857142857143, 0.5, 0.0, 0.0, 1.0, nan], recall [0.0, nan, nan, 0.4166666666666667, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:55:08.502094: step 146, loss 2.34898, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.75, 0.0, nan, nan, nan, 0.0], recall [nan, nan, nan, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:08.653381: step 147, loss 1.59094, accuracy 0.5, precision [nan, 0.25, 0.0, 0.7142857142857143, 0.5, nan, nan, 1.0, nan], recall [0.0, 0.5, nan, 0.45454545454545453, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:55:08.810293: step 148, loss 2.00456, accuracy 0.25, precision [1.0, 0.0, 0.0, 0.42857142857142855, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:08.966201: step 149, loss 2.09228, accuracy 0.3125, precision [0.3333333333333333, 0.0, nan, 1.0, 0.4, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.2222222222222222, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:55:09.121802: step 150, loss 1.90178, accuracy 0.375, precision [0.3333333333333333, 0.25, 0.0, 0.5714285714285714, 0.0, nan, nan, nan, nan], recall [1.0, 0.3333333333333333, nan, 0.6666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:09.270834: step 151, loss 2.12596, accuracy 0.125, precision [0.3333333333333333, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.3333333333333333, 0.0, nan, 0.14285714285714285, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:09.424665: step 152, loss 1.99282, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.3333333333333333, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:55:09.576131: step 153, loss 1.90968, accuracy 0.3125, precision [0.3333333333333333, 1.0, 0.0, 0.75, 0.0, 0.0, nan, nan, nan], recall [1.0, 0.5, nan, 0.3, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:55:09.726564: step 154, loss 1.92624, accuracy 0.375, precision [0.5, 0.5, nan, 0.5714285714285714, 0.0, 0.0, nan, 0.0, nan], recall [0.3333333333333333, 0.5, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:09.879934: step 155, loss 2.49926, accuracy 0.25, precision [nan, 0.0, nan, 0.6, 0.0, 0.0, nan, 0.2, 0.0], recall [0.0, 0.0, nan, 0.42857142857142855, nan, 0.0, nan, 0.5, nan]
2019-02-19T19:55:10.034353: step 156, loss 1.87142, accuracy 0.3125, precision [0.5, 0.25, nan, 0.6, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.5, nan, 0.3, nan, 0.0, nan, nan, nan]
2019-02-19T19:55:10.185545: step 157, loss 2.6077, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.5, 0.5, nan, 0.0, 0.5, 0.0], recall [0.0, 0.0, nan, 0.125, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:55:10.331429: step 158, loss 1.88098, accuracy 0.25, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:10.488608: step 159, loss 2.09845, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.375, 0.0, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:10.639089: step 160, loss 2.06221, accuracy 0.3125, precision [0.0, 0.3333333333333333, nan, 0.75, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.375, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:55:10.786017: step 161, loss 1.82214, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.5, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.5, nan, 0.375, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:10.939540: step 162, loss 2.15177, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.25, 0.0, 0.0, nan, 0.0, nan]
2019-02-19T19:55:11.091971: step 163, loss 1.82239, accuracy 0.4375, precision [1.0, 0.5, 0.0, 0.6, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.3333333333333333, 0.5, nan, 0.375, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:55:11.246635: step 164, loss 1.97242, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.42857142857142855, 0.5, nan, 0.0, 1.0, nan], recall [nan, 1.0, nan, 0.375, 0.5, nan, nan, 0.2, nan]
2019-02-19T19:55:11.405861: step 165, loss 1.80921, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, nan, 1.0, nan], recall [0.0, nan, nan, 0.36363636363636365, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:55:11.557984: step 166, loss 1.88591, accuracy 0.3125, precision [0.0, 0.4, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.16666666666666666, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:11.712200: step 167, loss 2.14606, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:11.861932: step 168, loss 2.18591, accuracy 0.3125, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.3076923076923077, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:12.017554: step 169, loss 1.94755, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.26666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:12.170445: step 170, loss 2.10623, accuracy 0.5625, precision [1.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [0.3333333333333333, 0.6666666666666666, nan, 0.625, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:12.326930: step 171, loss 2.31316, accuracy 0.125, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:55:12.476435: step 172, loss 2.12967, accuracy 0.125, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.16666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:12.627513: step 173, loss 2.11464, accuracy 0.375, precision [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, 0.5, 0.36363636363636365, nan, 0.0, nan, 0.0, nan]
2019-02-19T19:55:12.783377: step 174, loss 1.92244, accuracy 0.25, precision [nan, 0.25, 0.0, 0.42857142857142855, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:12.936096: step 175, loss 2.11048, accuracy 0.375, precision [0.0, 0.6666666666666666, nan, 0.6, 0.5, 0.0, 0.0, nan, 0.0], recall [nan, 0.4, nan, 0.5, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:55:13.090616: step 176, loss 1.99875, accuracy 0.25, precision [1.0, 0.5, 0.0, 0.5, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.3333333333333333, 0.3333333333333333, nan, 0.14285714285714285, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:13.243179: step 177, loss 2.04476, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.4, 1.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, 0.0, 0.4, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:55:13.400313: step 178, loss 1.85305, accuracy 0.3125, precision [0.0, 0.6, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.6, 0.0, 0.2857142857142857, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:13.555881: step 179, loss 1.95872, accuracy 0.375, precision [0.0, 1.0, nan, 0.2, 0.5, 0.0, 0.0, nan, nan], recall [nan, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, nan, 0.0, nan]
2019-02-19T19:55:13.705728: step 180, loss 2.05127, accuracy 0.25, precision [0.0, 0.6, 0.0, 0.0, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.0, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:13.855103: step 181, loss 2.16601, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 0.5, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, 0.16666666666666666, 0.0, 0.2, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:14.008786: step 182, loss 2.22227, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.25, 1.0, nan, nan, 0.0, 0.0], recall [nan, 0.14285714285714285, 0.0, 0.2, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:14.163957: step 183, loss 2.03306, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.42857142857142855, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, 0.14285714285714285, 0.0, 0.6, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:14.317384: step 184, loss 2.0978, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.2, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.125, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:55:14.467576: step 185, loss 1.89568, accuracy 0.1875, precision [nan, 0.5, 0.0, 0.0, 0.6666666666666666, nan, nan, 0.0, nan], recall [0.0, 0.16666666666666666, 0.0, 0.0, 0.4, nan, nan, nan, nan]
2019-02-19T19:55:14.620173: step 186, loss 1.90936, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.16666666666666666, nan, 0.5555555555555556, nan, nan, nan, nan, nan]
2019-02-19T19:55:14.770209: step 187, loss 1.98622, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.6666666666666666, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.0, 0.0, 0.2857142857142857, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:14.923818: step 188, loss 2.29152, accuracy 0.25, precision [nan, nan, 0.0, 0.8, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:15.072792: step 189, loss 2.30041, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0], recall [0.0, 0.0, nan, 0.7142857142857143, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:55:15.230238: step 190, loss 2.04228, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:15.385990: step 191, loss 2.04855, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, nan, nan, 0.2, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:15.537831: step 192, loss 2.09254, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, nan], recall [0.0, nan, 0.0, 0.125, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:55:15.686996: step 193, loss 2.01144, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.5, nan, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:15.836913: step 194, loss 1.87953, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.46153846153846156, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:15.984538: step 195, loss 1.83552, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.5, nan, nan, nan, 0.0], recall [0.5, nan, nan, 0.45454545454545453, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:55:16.131483: step 196, loss 2.19438, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.16666666666666666, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:55:16.284690: step 197, loss 2.33251, accuracy 0.1875, precision [nan, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:16.435102: step 198, loss 1.98543, accuracy 0.1875, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:16.586632: step 199, loss 1.91377, accuracy 0.3125, precision [0.0, nan, 0.0, 0.8, 0.0, 0.0, nan, 0.25, nan], recall [nan, nan, nan, 0.3076923076923077, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:55:16.737156: step 200, loss 2.60078, accuracy 0.0625, precision [0.0, 0.0, nan, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.1, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:16.886684: step 201, loss 2.05013, accuracy 0.25, precision [0.25, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.5, nan], recall [1.0, nan, nan, 0.2, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:55:17.038862: step 202, loss 1.89978, accuracy 0.25, precision [0.5, 0.5, 0.3333333333333333, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.5, 0.5, 0.16666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:17.184068: step 203, loss 2.13212, accuracy 0.3125, precision [nan, 0.0, nan, 0.2, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0], recall [nan, nan, nan, 0.16666666666666666, 0.2857142857142857, nan, nan, 0.6666666666666666, nan]
2019-02-19T19:55:17.335054: step 204, loss 1.96997, accuracy 0.25, precision [0.3333333333333333, 0.0, 0.5, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.0, 1.0, 1.0, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:17.489135: step 205, loss 1.84312, accuracy 0.375, precision [0.5, 0.3333333333333333, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, nan, nan, nan], recall [0.6666666666666666, 1.0, 0.0, 0.25, 0.4, nan, nan, 0.0, nan]
2019-02-19T19:55:17.642106: step 206, loss 2.15976, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.2, 1.0, 0.0, 0.0, nan, nan], recall [nan, 0.0, 0.0, 0.25, 0.42857142857142855, nan, nan, 0.0, nan]
2019-02-19T19:55:17.796929: step 207, loss 2.11152, accuracy 0.25, precision [nan, 0.0, 0.0, 0.42857142857142855, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5, 0.3333333333333333, 0.0, nan, 0.0, nan]
2019-02-19T19:55:17.950102: step 208, loss 1.93566, accuracy 0.375, precision [0.0, 1.0, nan, 0.42857142857142855, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.2, nan, 1.0, 0.6666666666666666, 0.0, nan, 0.0, nan]
2019-02-19T19:55:18.100517: step 209, loss 2.07341, accuracy 0.1875, precision [0.3333333333333333, nan, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:18.251527: step 210, loss 1.8458, accuracy 0.375, precision [0.0, 0.5, nan, 0.42857142857142855, 0.6666666666666666, nan, 0.0, nan, 0.0], recall [0.0, 0.5, nan, 0.6, 0.2857142857142857, 0.0, nan, nan, nan]
2019-02-19T19:55:18.401922: step 211, loss 2.03538, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:18.550524: step 212, loss 2.06718, accuracy 0.25, precision [nan, 0.0, 0.0, 0.5714285714285714, nan, nan, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, nan, nan]
2019-02-19T19:55:18.699384: step 213, loss 2.17318, accuracy 0.3125, precision [0.0, 0.0, nan, 0.6666666666666666, 0.16666666666666666, nan, nan, nan, nan], recall [0.0, nan, 0.0, 0.36363636363636365, 1.0, 0.0, nan, nan, nan]
2019-02-19T19:55:18.852794: step 214, loss 2.05478, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.3333333333333333, nan, 0.0, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:19.007570: step 215, loss 1.94295, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.75, 0.0, 0.0, nan, nan, 0.0], recall [0.5, nan, 0.0, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:55:19.163053: step 216, loss 1.81904, accuracy 0.3125, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.0, 0.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:55:19.314057: step 217, loss 1.90355, accuracy 0.4375, precision [1.0, 0.25, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.5, 1.0, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:55:19.463809: step 218, loss 1.87039, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:19.613254: step 219, loss 1.93612, accuracy 0.3125, precision [nan, 0.2, 0.0, 0.6666666666666666, 0.0, nan, nan, nan, nan], recall [nan, 1.0, 0.0, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:19.766572: step 220, loss 2.33703, accuracy 0.125, precision [0.25, 0.0, 0.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0, nan], recall [0.3333333333333333, 0.0, nan, 0.1111111111111111, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:19.922087: step 221, loss 1.99828, accuracy 0.4375, precision [0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.4166666666666667, 0.5, 0.0, nan, nan, nan]
2019-02-19T19:55:20.073848: step 222, loss 1.93865, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:20.227147: step 223, loss 1.72532, accuracy 0.4375, precision [0.0, 0.25, 0.0, 1.0, 0.5, nan, 0.0, nan, nan], recall [nan, 1.0, 0.0, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:20.378910: step 224, loss 2.1408, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:20.528732: step 225, loss 2.32772, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, 1.0, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:20.675731: step 226, loss 2.09836, accuracy 0.375, precision [0.0, nan, nan, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:20.829941: step 227, loss 2.32662, accuracy 0.1875, precision [nan, nan, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:20.976497: step 228, loss 2.19468, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.18181818181818182, 0.2, nan, nan, nan, nan]
2019-02-19T19:55:21.128501: step 229, loss 1.86129, accuracy 0.3125, precision [nan, nan, 0.0, 0.7142857142857143, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:21.276109: step 230, loss 1.82302, accuracy 0.4375, precision [nan, 1.0, nan, 0.6666666666666666, 0.5, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.4444444444444444, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:55:21.424071: step 231, loss 1.67861, accuracy 0.5625, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.6666666666666666, nan, nan, 0.3333333333333333, nan], recall [nan, 0.0, nan, 0.5454545454545454, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:55:21.577084: step 232, loss 1.77663, accuracy 0.25, precision [0.0, nan, nan, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:21.727079: step 233, loss 1.76208, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, nan], recall [nan, 0.5, nan, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:55:21.876489: step 234, loss 2.06113, accuracy 0.25, precision [0.0, nan, 0.0, 1.0, 0.16666666666666666, 0.0, 0.0, nan, nan], recall [nan, 0.0, nan, 0.23076923076923078, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:22.026898: step 235, loss 1.91393, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:22.178702: step 236, loss 1.97644, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:22.326557: step 237, loss 1.89932, accuracy 0.375, precision [0.0, 0.75, 0.0, 0.5, 0.0, 0.0, nan, 1.0, nan], recall [0.0, 0.75, nan, 0.25, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:55:22.480513: step 238, loss 2.27185, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, nan, 0.0, 0.35714285714285715, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:22.636387: step 239, loss 2.29014, accuracy 0.25, precision [0.0, 0.0, nan, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:22.784827: step 240, loss 1.79525, accuracy 0.3125, precision [nan, nan, 0.0, 0.8, 0.25, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:22.936575: step 241, loss 1.96175, accuracy 0.25, precision [0.0, nan, 0.0, 0.75, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:23.085798: step 242, loss 2.13453, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.09090909090909091, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:23.239689: step 243, loss 1.62191, accuracy 0.375, precision [nan, 1.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.5, 0.0], recall [nan, 0.5, nan, 0.2222222222222222, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:55:23.390020: step 244, loss 1.81771, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.75, 1.0, nan, nan, 0.5, 0.0], recall [nan, nan, nan, 0.42857142857142855, 0.42857142857142855, nan, nan, 0.5, nan]
2019-02-19T19:55:23.541970: step 245, loss 1.92677, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.75, 0.3333333333333333, 0.0, 0.0, 0.25, nan], recall [nan, nan, 0.0, 0.375, 0.2, nan, nan, 0.5, nan]
2019-02-19T19:55:23.694308: step 246, loss 1.90612, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.75, nan, nan, 0.3333333333333333, 0.0], recall [0.0, 0.3333333333333333, nan, 0.3333333333333333, 0.75, nan, nan, 1.0, nan]
2019-02-19T19:55:23.846342: step 247, loss 1.68202, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.5, nan], recall [nan, nan, nan, 0.4444444444444444, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:55:23.993259: step 248, loss 1.89033, accuracy 0.1875, precision [0.0, nan, 0.0, 0.4, 0.25, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.25, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:55:24.141886: step 249, loss 1.99963, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, nan, nan, nan], recall [nan, 0.0, 0.0, 0.5, 0.2857142857142857, nan, nan, 0.0, nan]
2019-02-19T19:55:24.290512: step 250, loss 2.05316, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6, 1.0, nan, 0.0, nan, nan], recall [nan, nan, nan, 0.2727272727272727, 0.3333333333333333, nan, nan, 0.0, nan]

Evaluation:
[[  5  10   0  53   6   0   0  15   0]
 [  0  21   0 101   4   0   0  18   0]
 [  0   0   0  69  12   0   0   6   0]
 [  1   8   0 292  30   0   0  14   0]
 [  0   2   0 114  42   0   0   6   0]
 [  1   1   0  47   2   0   0   2   0]
 [  0   0   0  13   9   0   0   0   0]
 [  0   2   0  73  11   0   0  17   0]
 [  0   0   0  14   2   0   0   2   0]]
2019-02-19T19:55:30.113354: step 250, loss 1.82617, accuracy 0.367805, precision [0.056179775280898875, 0.14583333333333334, 0.0, 0.8463768115942029, 0.25609756097560976, 0.0, 0.0, 0.1650485436893204, 0.0], recall [0.7142857142857143, 0.4772727272727273, nan, 0.37628865979381443, 0.3559322033898305, nan, nan, 0.2125, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550606084/checkpoints/model-250

2019-02-19T19:55:30.357678: step 251, loss 1.95244, accuracy 0.1875, precision [0.0, 0.2, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:30.504328: step 252, loss 1.77653, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 1.0, 0.0, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:30.655847: step 253, loss 1.94528, accuracy 0.3125, precision [0.0, 0.0, nan, 0.5714285714285714, 0.25, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.4, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:55:30.815386: step 254, loss 1.5266, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.42857142857142855, 0.5, nan, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:55:30.964314: step 255, loss 2.08777, accuracy 0.1875, precision [nan, 0.3333333333333333, nan, 0.5, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:31.120118: step 256, loss 1.65808, accuracy 0.5, precision [1.0, 0.3333333333333333, nan, 0.7142857142857143, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, 1.0, 0.0, 0.625, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:55:31.270908: step 257, loss 2.00659, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.6, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, 0.0, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:31.422893: step 258, loss 1.9415, accuracy 0.25, precision [nan, 0.2, 0.0, 0.5, 0.5, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.09090909090909091, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:55:31.571076: step 259, loss 2.37414, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, nan, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:31.725204: step 260, loss 1.68457, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.0, 0.7777777777777778, 0.0, nan, nan, nan, nan], recall [0.0, 0.25, nan, 0.6363636363636364, nan, nan, nan, nan, nan]
2019-02-19T19:55:31.875496: step 261, loss 1.83069, accuracy 0.375, precision [nan, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, 0.0, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:55:32.034025: step 262, loss 1.9585, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:32.186840: step 263, loss 1.96226, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:32.336017: step 264, loss 2.108, accuracy 0.25, precision [0.0, 0.75, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.42857142857142855, nan, 0.14285714285714285, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:32.487948: step 265, loss 1.81081, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.5, 0.0, 0.0, nan, nan, nan], recall [nan, 0.16666666666666666, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:32.645733: step 266, loss 1.81401, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.2, 0.0, 0.0, nan, nan], recall [0.0, 0.5, 0.0, 0.3, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:32.803834: step 267, loss 2.08625, accuracy 0.3125, precision [nan, 0.6666666666666666, nan, 0.5, 0.2, 0.0, 0.0, nan, nan], recall [0.0, 1.0, nan, 0.25, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:55:32.953253: step 268, loss 2.12756, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, nan, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:33.101986: step 269, loss 2.0014, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.42857142857142855, 0.5, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:33.249735: step 270, loss 1.93047, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.25, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:33.401135: step 271, loss 1.91664, accuracy 0.125, precision [0.0, 0.0, nan, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.4, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:33.555747: step 272, loss 1.67399, accuracy 0.375, precision [0.0, 0.8, nan, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:33.711124: step 273, loss 1.83835, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:33.860453: step 274, loss 2.13766, accuracy 0.0625, precision [0.0, nan, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.06666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:55:34.014842: step 275, loss 1.94227, accuracy 0.4375, precision [0.0, nan, 0.0, 0.7777777777777778, 0.0, nan, nan, nan, nan], recall [nan, 0.0, nan, 0.6363636363636364, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:34.162708: step 276, loss 1.58449, accuracy 0.5, precision [0.0, nan, 0.5, 0.8571428571428571, 0.2, nan, 0.0, nan, nan], recall [0.0, nan, 1.0, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:34.309436: step 277, loss 2.16855, accuracy 0.125, precision [0.25, 0.0, 0.0, 0.5, 0.0, nan, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.08333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:34.463129: step 278, loss 1.90635, accuracy 0.5, precision [0.5, nan, nan, 0.8571428571428571, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.5454545454545454, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:34.613237: step 279, loss 1.8871, accuracy 0.375, precision [0.0, 0.0, nan, 0.8333333333333334, 0.2, nan, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.4166666666666667, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:55:34.761310: step 280, loss 1.83737, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.0, 0.0, 0.4, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:55:34.912184: step 281, loss 1.97372, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.75, nan, 0.0, nan, nan, nan], recall [nan, nan, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:35.061615: step 282, loss 2.13072, accuracy 0.1875, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:35.211209: step 283, loss 2.05372, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [nan, nan, 0.0, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:35.361116: step 284, loss 1.84823, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5, nan], recall [0.0, 1.0, 0.0, 0.375, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:55:35.514034: step 285, loss 2.08064, accuracy 0.25, precision [1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], recall [0.5, nan, 0.0, 0.18181818181818182, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:35.665096: step 286, loss 2.06234, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8, 0.3333333333333333, nan, 0.0, nan, nan], recall [0.0, nan, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:35.816521: step 287, loss 1.81144, accuracy 0.375, precision [0.0, 0.25, 0.5, 0.8, nan, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.5, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:55:35.972043: step 288, loss 1.83181, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 0.5, 0.0, 0.0, nan, nan, nan], recall [0.3333333333333333, 0.0, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:36.126104: step 289, loss 2.00284, accuracy 0.125, precision [0.0, 0.0, 0.5, 0.2, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, 0.2, 0.14285714285714285, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:36.278577: step 290, loss 2.18972, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.75, 0.0, nan, 0.0, nan], recall [nan, nan, 0.0, 0.3333333333333333, 0.75, nan, nan, 0.0, nan]
2019-02-19T19:55:36.433145: step 291, loss 2.00379, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:36.589892: step 292, loss 2.14501, accuracy 0.25, precision [1.0, 0.0, 0.0, 0.5, 0.3333333333333333, nan, 0.0, 0.0, 0.0], recall [0.3333333333333333, nan, 0.0, 0.2857142857142857, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:55:36.734474: step 293, loss 1.98691, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.5, nan, nan, nan, 0.0], recall [0.0, 0.0, nan, 0.6666666666666666, 0.16666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:55:36.887538: step 294, loss 1.77039, accuracy 0.375, precision [0.0, 0.25, 0.0, 0.5714285714285714, nan, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.5, 0.0, 0.5714285714285714, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:55:37.038272: step 295, loss 2.1081, accuracy 0.25, precision [nan, 0.0, 0.0, 0.8, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:37.185537: step 296, loss 1.83277, accuracy 0.1875, precision [nan, 0.25, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:37.335988: step 297, loss 1.93832, accuracy 0.375, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 0.0, 0.0, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:37.488396: step 298, loss 1.86753, accuracy 0.375, precision [0.0, 0.25, 0.0, 1.0, 0.3333333333333333, nan, 0.0, nan, 0.0], recall [0.0, 0.5, nan, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:55:37.642483: step 299, loss 1.70456, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.2857142857142857, 0.3333333333333333, nan, nan, nan, nan], recall [0.0, 0.0, 0.0, 0.3333333333333333, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:37.791651: step 300, loss 2.02108, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.75, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:37.939179: step 301, loss 1.57249, accuracy 0.5625, precision [0.0, 0.8, 0.0, 0.75, 0.5, 0.0, nan, nan, nan], recall [nan, 0.8, 0.0, 0.5, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:55:38.090989: step 302, loss 1.85313, accuracy 0.25, precision [0.0, 0.0, nan, 0.8, 0.0, 0.0, nan, nan, nan], recall [0.0, nan, nan, 0.3076923076923077, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:38.239851: step 303, loss 1.71122, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [nan, 0.4, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:38.391484: step 304, loss 1.3863, accuracy 0.5625, precision [1.0, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 1.0, 0.0, 0.5833333333333334, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:38.539822: step 305, loss 1.43805, accuracy 0.5625, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, nan, nan], recall [nan, 1.0, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:55:38.687599: step 306, loss 2.1737, accuracy 0.3125, precision [0.0, nan, 0.2, 1.0, 0.6666666666666666, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, 1.0, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:38.838666: step 307, loss 1.9941, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:38.987306: step 308, loss 2.04065, accuracy 0.1875, precision [0.0, 0.0, nan, 0.5, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:39.136359: step 309, loss 2.22011, accuracy 0.125, precision [0.0, 0.3333333333333333, 0.0, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.2, nan, 0.09090909090909091, nan, nan, nan, nan, nan]
2019-02-19T19:55:39.290049: step 310, loss 1.91825, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 0.7142857142857143, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.2, nan, 0.45454545454545453, nan, nan, nan, nan, nan]
2019-02-19T19:55:39.444679: step 311, loss 1.93609, accuracy 0.4375, precision [0.0, 0.25, 0.0, 0.8571428571428571, nan, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, 0.0, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:55:39.600643: step 312, loss 2.47752, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.16666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:39.754069: step 313, loss 1.73319, accuracy 0.1875, precision [0.0, nan, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:39.908720: step 314, loss 1.98626, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, nan, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:55:40.058139: step 315, loss 2.03973, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.16666666666666666, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:40.208318: step 316, loss 1.7113, accuracy 0.5, precision [0.0, 0.5, 0.0, 1.0, 0.5, 0.0, nan, nan, nan], recall [nan, 0.4, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:40.354590: step 317, loss 1.62304, accuracy 0.3125, precision [nan, 1.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.75, nan, 0.2222222222222222, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:40.507585: step 318, loss 1.87636, accuracy 0.4375, precision [1.0, 0.5, 0.0, 0.5, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.2, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:40.656511: step 319, loss 2.06966, accuracy 0.4375, precision [0.3333333333333333, 0.6666666666666666, nan, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [0.5, 0.2857142857142857, 0.0, 0.6666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:55:40.804228: step 320, loss 2.00569, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:40.953117: step 321, loss 1.80194, accuracy 0.3125, precision [0.3333333333333333, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.3333333333333333, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:41.103333: step 322, loss 1.89518, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8571428571428571, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, nan, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:41.259080: step 323, loss 1.99179, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:41.409093: step 324, loss 2.08404, accuracy 0.25, precision [nan, 0.4, 0.0, 0.4, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.6666666666666666, 0.0, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:55:41.566972: step 325, loss 1.66502, accuracy 0.4375, precision [0.6666666666666666, 0.5, 0.0, 1.0, 0.3333333333333333, 0.0, nan, nan, nan], recall [0.6666666666666666, 1.0, 0.0, 0.3, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:41.717355: step 326, loss 2.23804, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, nan, 0.0], recall [0.3333333333333333, 0.0, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:41.870254: step 327, loss 2.31981, accuracy 0.1875, precision [0.0, 0.0, nan, 1.0, 0.2, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.2, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:42.022046: step 328, loss 1.88832, accuracy 0.375, precision [0.0, 0.75, 0.0, 1.0, 0.16666666666666666, 0.0, nan, nan, 0.0], recall [nan, 0.6, 0.0, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:42.172436: step 329, loss 1.68967, accuracy 0.4375, precision [0.0, 0.3333333333333333, nan, 0.8, 0.25, 0.0, nan, 0.5, nan], recall [0.0, 0.5, nan, 0.4, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:55:42.323924: step 330, loss 1.6609, accuracy 0.375, precision [nan, 0.0, nan, 0.75, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.0, 0.0, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:42.470800: step 331, loss 1.97259, accuracy 0.375, precision [0.5, 0.0, 0.0, 0.8, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [1.0, nan, nan, 0.3333333333333333, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:42.620772: step 332, loss 2.41579, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:42.773053: step 333, loss 2.11689, accuracy 0.3125, precision [0.5, 0.0, 0.2, 0.6, nan, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.0, 1.0, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:42.920703: step 334, loss 2.17455, accuracy 0.25, precision [nan, nan, 0.0, 0.3333333333333333, 0.4, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2857142857142857, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:43.072963: step 335, loss 1.69994, accuracy 0.5, precision [0.5, 1.0, 0.0, 0.625, 0.5, nan, nan, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.7142857142857143, 0.2, nan, nan, nan, nan]
2019-02-19T19:55:43.225747: step 336, loss 2.11727, accuracy 0.4375, precision [0.0, 0.5, nan, 0.625, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 0.6666666666666666, nan, 0.625, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:43.376262: step 337, loss 1.7262, accuracy 0.25, precision [nan, 0.5, 0.0, 0.2857142857142857, 0.5, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.4, 0.1111111111111111, nan, nan, nan, nan]
2019-02-19T19:55:43.530442: step 338, loss 1.96831, accuracy 0.25, precision [0.5, 0.0, nan, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.0, nan, 0.375, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:43.678533: step 339, loss 2.21392, accuracy 0.125, precision [0.0, 0.5, 0.0, 0.25, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.1111111111111111, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:43.829062: step 340, loss 1.97645, accuracy 0.375, precision [0.5, 1.0, nan, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.5, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:55:43.982382: step 341, loss 1.89879, accuracy 0.25, precision [nan, 0.2, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.5, nan], recall [nan, 1.0, nan, 0.16666666666666666, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:55:44.130542: step 342, loss 1.61202, accuracy 0.5, precision [nan, 0.0, nan, 0.875, 0.2, nan, 0.0, nan, nan], recall [nan, nan, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:44.282102: step 343, loss 1.65924, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:44.434138: step 344, loss 1.69906, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.5454545454545454, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:44.587975: step 345, loss 1.68851, accuracy 0.5625, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.6923076923076923, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:44.737118: step 346, loss 2.23435, accuracy 0.0625, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.06666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:55:44.883458: step 347, loss 1.81076, accuracy 0.25, precision [nan, 0.0, 0.0, 0.75, 0.25, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.23076923076923078, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:45.037511: step 348, loss 2.43463, accuracy 0.25, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.25, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:45.192898: step 349, loss 2.05108, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.23076923076923078, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:45.341535: step 350, loss 1.88415, accuracy 0.25, precision [nan, 0.25, 0.0, 0.6, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:45.493372: step 351, loss 2.12797, accuracy 0.1875, precision [0.0, nan, 0.0, 0.75, 0.0, 0.0, nan, nan, nan], recall [nan, nan, 0.0, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:45.649418: step 352, loss 1.83216, accuracy 0.375, precision [0.0, 0.0, nan, 0.7142857142857143, 0.0, 0.0, 0.0, 1.0, nan], recall [nan, 0.0, nan, 0.38461538461538464, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:55:45.799780: step 353, loss 2.07846, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.5, 1.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0], recall [nan, 0.6666666666666666, 1.0, 0.09090909090909091, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:45.953845: step 354, loss 2.01206, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.2222222222222222, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:46.104109: step 355, loss 1.96948, accuracy 0.375, precision [0.5, 0.4, 0.0, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [1.0, 1.0, 0.0, 0.2, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:46.256997: step 356, loss 1.75097, accuracy 0.5, precision [nan, nan, 0.0, 0.6, 0.75, 0.0, 0.0, 0.5, nan], recall [0.0, 0.0, nan, 0.5, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:55:46.409233: step 357, loss 1.94058, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6, 0.5, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.5, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:55:46.561603: step 358, loss 1.9352, accuracy 0.25, precision [nan, 0.0, 0.25, 0.2857142857142857, 0.5, 0.0, nan, nan, 0.0], recall [0.0, 0.0, 0.5, 0.5, 0.14285714285714285, nan, nan, nan, nan]
2019-02-19T19:55:46.714703: step 359, loss 2.00102, accuracy 0.3125, precision [nan, 0.5714285714285714, 0.0, 0.5, 0.0, 0.0, 0.0, nan, nan], recall [nan, 1.0, 0.0, 0.1111111111111111, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:46.866969: step 360, loss 1.92057, accuracy 0.3125, precision [0.0, 1.0, nan, 0.25, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 0.4, nan, 0.5, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:55:47.015696: step 361, loss 1.50276, accuracy 0.5, precision [nan, 0.6666666666666666, 0.5, 0.5, 0.5, nan, nan, 0.0, nan], recall [nan, 0.6666666666666666, 0.5, 0.42857142857142855, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:55:47.165871: step 362, loss 2.15838, accuracy 0.1875, precision [0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0, 0.3333333333333333, nan], recall [nan, 0.5, 0.0, 0.0, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:55:47.311539: step 363, loss 2.03213, accuracy 0.1875, precision [0.0, nan, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, nan, nan], recall [nan, 0.0, 0.0, 0.14285714285714285, 0.2857142857142857, nan, nan, nan, nan]
2019-02-19T19:55:47.459949: step 364, loss 2.08068, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.3333333333333333, nan, 0.0, nan, nan], recall [nan, 0.0, nan, 0.125, 0.16666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:55:47.608388: step 365, loss 1.99647, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.3333333333333333, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.16666666666666666, 0.125, nan, nan, 0.0, nan]
2019-02-19T19:55:47.754812: step 366, loss 1.95062, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, nan, 0.42857142857142855, 0.0, nan, 0.0, nan], recall [0.0, 0.25, nan, 0.0, 0.75, nan, nan, 0.0, nan]
2019-02-19T19:55:47.905199: step 367, loss 1.73087, accuracy 0.1875, precision [nan, 0.6666666666666666, 0.0, 0.25, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.14285714285714285, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:48.058345: step 368, loss 1.76195, accuracy 0.4375, precision [nan, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, nan, nan, nan], recall [0.0, 0.6666666666666666, nan, 0.5, 0.375, nan, nan, nan, nan]
2019-02-19T19:55:48.210606: step 369, loss 1.88134, accuracy 0.3125, precision [1.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, nan, 0.0, nan], recall [0.3333333333333333, nan, 0.25, 0.5, 0.2, nan, nan, nan, nan]
2019-02-19T19:55:48.362555: step 370, loss 1.78415, accuracy 0.5, precision [nan, 0.3333333333333333, 0.0, 0.625, 0.6666666666666666, 0.0, nan, nan, nan], recall [0.0, 1.0, 0.0, 0.7142857142857143, 0.6666666666666666, 0.0, nan, 0.0, nan]
2019-02-19T19:55:48.515993: step 371, loss 1.65425, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.2, 0.5, nan, nan, nan, 0.0], recall [0.0, 0.4, 0.0, 0.25, 0.4, nan, nan, nan, nan]
2019-02-19T19:55:48.663800: step 372, loss 1.7673, accuracy 0.3125, precision [0.0, nan, 0.0, 0.5, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 1.0, 0.2, nan, nan, 0.0, nan]
2019-02-19T19:55:48.813247: step 373, loss 1.8337, accuracy 0.25, precision [0.0, 0.0, 0.5, 0.375, nan, nan, nan, 0.0, nan], recall [nan, 0.0, 0.5, 0.375, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:48.966154: step 374, loss 1.67168, accuracy 0.4375, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.6, nan, nan, 0.0, nan, nan]
2019-02-19T19:55:49.114371: step 375, loss 1.64241, accuracy 0.4375, precision [0.5, 0.0, 0.0, 0.8571428571428571, 0.0, nan, nan, 0.0, nan], recall [0.3333333333333333, 0.0, 0.0, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:49.262368: step 376, loss 2.25176, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, nan, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:49.417305: step 377, loss 1.8994, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.16666666666666666, 1.0, 0.0, nan, nan, 0.0], recall [0.0, 0.4, 0.0, 0.14285714285714285, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:49.571320: step 378, loss 1.76406, accuracy 0.375, precision [0.0, 0.8, nan, 0.5, 0.0, nan, nan, nan, 0.0], recall [0.0, 0.5714285714285714, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:49.720438: step 379, loss 2.13416, accuracy 0.125, precision [nan, 0.0, 0.0, 0.4, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, 0.0, 0.16666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:49.873739: step 380, loss 2.04384, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, nan], recall [nan, 0.0, nan, 0.3333333333333333, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:55:50.024181: step 381, loss 1.98324, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8, 0.25, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:55:50.181238: step 382, loss 1.91018, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.26666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:50.330199: step 383, loss 2.1494, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, nan, 0.0, 0.2727272727272727, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:50.483694: step 384, loss 1.95241, accuracy 0.3125, precision [nan, 0.0, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:50.633978: step 385, loss 1.88472, accuracy 0.5, precision [0.0, 1.0, nan, 0.8571428571428571, nan, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.6666666666666666, nan, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:50.782047: step 386, loss 1.76723, accuracy 0.375, precision [0.0, nan, 0.0, 0.7142857142857143, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.45454545454545453, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:50.932031: step 387, loss 1.64913, accuracy 0.5625, precision [1.0, 0.5, 0.0, 0.8571428571428571, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:51.079746: step 388, loss 2.01814, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.15384615384615385, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:51.233421: step 389, loss 1.78595, accuracy 0.25, precision [0.0, 0.0, nan, 0.8, nan, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:51.390702: step 390, loss 1.5099, accuracy 0.4375, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:55:51.539265: step 391, loss 1.82974, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, nan, nan], recall [nan, nan, nan, 0.35714285714285715, 0.0, nan, 0.0, nan, nan]
2019-02-19T19:55:51.683734: step 392, loss 2.17103, accuracy 0.3125, precision [0.0, nan, 0.3333333333333333, 0.8, 0.0, 0.0, 0.0, nan, 0.0], recall [nan, nan, 1.0, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:51.834145: step 393, loss 1.92479, accuracy 0.3125, precision [0.0, 0.0, nan, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.45454545454545453, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:51.984879: step 394, loss 2.17893, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.1, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:52.136017: step 395, loss 1.88251, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.2, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:55:52.288077: step 396, loss 1.63285, accuracy 0.5, precision [nan, 0.5, nan, 0.8571428571428571, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.4, nan, 0.6, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:52.437734: step 397, loss 1.92171, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.4166666666666667, 0.0, nan, 0.0, 0.0, nan]
2019-02-19T19:55:52.588109: step 398, loss 2.03495, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.6, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.16666666666666666, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:52.737969: step 399, loss 1.77146, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:52.887354: step 400, loss 1.7739, accuracy 0.375, precision [0.0, 0.8, 0.0, 0.4, nan, nan, nan, 0.0, 0.0], recall [nan, 0.4, nan, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:53.045206: step 401, loss 1.77401, accuracy 0.5, precision [0.5, 0.6666666666666666, 0.0, 1.0, 0.0, nan, 0.0, 0.5, 0.0], recall [1.0, 0.4, nan, 0.4444444444444444, nan, nan, nan, 1.0, nan]
2019-02-19T19:55:53.195058: step 402, loss 1.96767, accuracy 0.4375, precision [0.0, 0.5, nan, 0.8333333333333334, 0.0, 0.0, nan, 1.0, nan], recall [nan, 0.25, nan, 0.5, nan, nan, nan, 0.5, nan]
2019-02-19T19:55:53.347681: step 403, loss 1.94626, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.6666666666666666, nan, 0.0, 0.0, nan, nan], recall [0.0, 0.2, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:53.498454: step 404, loss 2.02692, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, nan, 1.0, nan], recall [0.0, 0.0, nan, 0.4, nan, nan, nan, 1.0, nan]
2019-02-19T19:55:53.652115: step 405, loss 1.73008, accuracy 0.375, precision [0.5, 0.6666666666666666, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.4, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:53.804536: step 406, loss 1.79848, accuracy 0.5, precision [0.0, 0.5, 0.0, 0.875, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.25, nan, 0.7, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:53.953552: step 407, loss 1.65267, accuracy 0.4375, precision [nan, 0.25, nan, 0.6666666666666666, 0.0, nan, nan, nan, 0.0], recall [nan, 0.2, nan, 0.6666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:54.103328: step 408, loss 1.55016, accuracy 0.625, precision [0.0, 0.0, 0.0, 1.0, nan, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.7692307692307693, nan, nan, nan, nan, nan]
2019-02-19T19:55:54.250438: step 409, loss 1.87549, accuracy 0.125, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.14285714285714285, nan, nan, nan, nan, nan]
2019-02-19T19:55:54.397201: step 410, loss 1.90135, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:54.547936: step 411, loss 2.13669, accuracy 0.375, precision [0.0, 0.0, nan, 0.8571428571428571, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:55:54.701636: step 412, loss 2.02095, accuracy 0.25, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.18181818181818182, nan, nan, nan, nan, nan]
2019-02-19T19:55:54.855583: step 413, loss 2.14636, accuracy 0.25, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:55:55.009861: step 414, loss 2.45609, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:55:55.159500: step 415, loss 2.28883, accuracy 0.25, precision [0.0, 0.5, nan, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.21428571428571427, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:55.313469: step 416, loss 2.01006, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, nan, nan, 0.0], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:55.465917: step 417, loss 2.44166, accuracy 0.1875, precision [0.3333333333333333, nan, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.0, 0.0, 0.15384615384615385, nan, nan, nan, nan, nan]
2019-02-19T19:55:55.619375: step 418, loss 2.15864, accuracy 0.25, precision [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.5, nan, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:55:55.773459: step 419, loss 1.66408, accuracy 0.4375, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.4666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:55:55.925956: step 420, loss 1.97859, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:56.073990: step 421, loss 2.05915, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 1.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4166666666666667, 0.5, nan, nan, nan, nan]
2019-02-19T19:55:56.228872: step 422, loss 1.75779, accuracy 0.5, precision [0.0, 0.0, nan, 0.8571428571428571, 0.25, nan, 0.0, 0.5, nan], recall [0.0, 0.0, nan, 0.5454545454545454, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:55:56.386267: step 423, loss 1.90859, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.6, nan, nan, nan, nan, nan]
2019-02-19T19:55:56.537753: step 424, loss 2.31737, accuracy 0.25, precision [0.0, 0.5, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], recall [0.0, 1.0, nan, 0.18181818181818182, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:55:56.683896: step 425, loss 1.69439, accuracy 0.375, precision [1.0, 0.5, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [0.2, 1.0, nan, 0.375, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:56.830436: step 426, loss 1.88462, accuracy 0.5, precision [0.3333333333333333, 0.0, 0.0, 0.875, 0.0, nan, 0.0, nan, 0.0], recall [0.3333333333333333, 0.0, nan, 0.7777777777777778, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:56.984310: step 427, loss 2.02083, accuracy 0.375, precision [0.0, 0.4, 0.0, 0.75, 0.5, 0.0, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.375, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:55:57.130732: step 428, loss 2.0605, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:57.281276: step 429, loss 1.90385, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, nan, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:55:57.432471: step 430, loss 1.95048, accuracy 0.3125, precision [0.0, 0.0, nan, 0.6, 0.6666666666666666, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.42857142857142855, 0.4, nan, nan, nan, nan]
2019-02-19T19:55:57.579917: step 431, loss 2.13901, accuracy 0.25, precision [0.5, 0.0, nan, 0.6666666666666666, 0.0, nan, nan, 0.0, 0.0], recall [1.0, nan, nan, 0.15384615384615385, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:57.729668: step 432, loss 1.76155, accuracy 0.1875, precision [0.0, 0.0, nan, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:57.877882: step 433, loss 1.99743, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:55:58.028915: step 434, loss 1.58819, accuracy 0.4375, precision [0.6, nan, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:58.176513: step 435, loss 1.87011, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, nan], recall [0.6666666666666666, 0.0, nan, 0.375, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:55:58.322720: step 436, loss 1.7002, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.8, 0.3333333333333333, nan, nan, nan, nan], recall [1.0, nan, nan, 0.36363636363636365, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:58.472803: step 437, loss 1.84034, accuracy 0.25, precision [nan, 0.25, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:55:58.621456: step 438, loss 2.12545, accuracy 0.25, precision [1.0, 0.3333333333333333, nan, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:58.767376: step 439, loss 1.75483, accuracy 0.4375, precision [nan, 0.3333333333333333, 0.0, 0.8333333333333334, 0.5, nan, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.5555555555555556, 0.25, nan, nan, nan, nan]
2019-02-19T19:55:58.922091: step 440, loss 2.47826, accuracy 0.125, precision [0.2, 0.0, 0.0, 0.5, nan, nan, 0.0, 0.0, 0.0], recall [0.5, 0.0, nan, 0.08333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:59.072746: step 441, loss 1.66959, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.8571428571428571, 0.5, nan, nan, 0.0, nan], recall [0.5, nan, nan, 0.5454545454545454, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:59.222676: step 442, loss 1.85911, accuracy 0.375, precision [nan, 0.25, 0.0, 0.5714285714285714, 0.5, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:55:59.375106: step 443, loss 1.87183, accuracy 0.3125, precision [0.25, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.25, 0.0, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:59.525041: step 444, loss 1.89622, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.25, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.23076923076923078, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:59.673622: step 445, loss 1.78609, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:55:59.830457: step 446, loss 2.22837, accuracy 0.1875, precision [0.0, 0.25, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.07692307692307693, 1.0, nan, nan, nan, nan]
2019-02-19T19:55:59.981679: step 447, loss 1.76008, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:00.132671: step 448, loss 1.91399, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.5, 0.0, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:00.285025: step 449, loss 1.79154, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:00.436346: step 450, loss 1.54185, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.3333333333333333, nan, nan, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:00.587889: step 451, loss 2.07932, accuracy 0.25, precision [0.3333333333333333, 0.0, 0.0, 0.6, nan, nan, 0.0, nan, 0.0], recall [0.5, nan, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:00.735747: step 452, loss 2.09031, accuracy 0.3125, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:56:00.883313: step 453, loss 1.86004, accuracy 0.4375, precision [0.0, nan, nan, 1.0, 0.2, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.46153846153846156, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:01.038121: step 454, loss 1.72851, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.4, nan, nan, nan, nan], recall [nan, 0.0, nan, 0.36363636363636365, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:01.186447: step 455, loss 1.78199, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.75, 0.25, nan, 0.0, 0.5, nan], recall [0.0, 0.0, nan, 0.3, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:56:01.333274: step 456, loss 2.03951, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:01.484306: step 457, loss 1.76686, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, 0.0, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:01.641449: step 458, loss 2.16305, accuracy 0.1875, precision [1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [0.5, nan, nan, 0.18181818181818182, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:01.792485: step 459, loss 2.41943, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, nan, 0.0, 0.0, nan, 0.0], recall [nan, nan, nan, 0.26666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:01.945302: step 460, loss 2.07842, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.4, nan, 0.0, 1.0, nan], recall [0.0, 0.0, nan, 0.25, 0.6666666666666666, nan, nan, 1.0, nan]
2019-02-19T19:56:02.100963: step 461, loss 1.86431, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.45454545454545453, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:02.250245: step 462, loss 1.49852, accuracy 0.4375, precision [0.5, 0.3333333333333333, 0.0, 0.5714285714285714, 0.5, nan, nan, nan, nan], recall [0.5, 1.0, 0.0, 0.4444444444444444, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:56:02.400968: step 463, loss 1.74241, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.5, 0.0, nan, 0.45454545454545453, nan, nan, nan, nan, nan]
2019-02-19T19:56:02.549873: step 464, loss 1.73841, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, nan, nan], recall [0.0, nan, nan, 0.3076923076923077, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:02.697299: step 465, loss 1.76369, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.8571428571428571, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.6, 0.2, nan, nan, nan, nan]
2019-02-19T19:56:02.853272: step 466, loss 2.09135, accuracy 0.1875, precision [0.3333333333333333, 0.0, nan, 0.3333333333333333, 0.0, nan, 0.0, 0.0, 0.0], recall [0.3333333333333333, 0.0, nan, 0.2222222222222222, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:03.001810: step 467, loss 1.90166, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:03.151756: step 468, loss 1.62, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:03.303416: step 469, loss 1.86272, accuracy 0.375, precision [0.0, 0.16666666666666666, 0.0, 0.8333333333333334, nan, nan, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:03.454500: step 470, loss 1.8368, accuracy 0.25, precision [0.0, 0.2, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:03.608071: step 471, loss 1.50639, accuracy 0.5, precision [0.0, 0.8, nan, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:56:03.763627: step 472, loss 1.86329, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.25, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:03.914225: step 473, loss 1.92624, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:04.065310: step 474, loss 1.72392, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:04.212756: step 475, loss 2.16151, accuracy 0.1875, precision [nan, 0.25, 0.0, 0.25, 1.0, 0.0, 0.0, nan, nan], recall [0.0, 0.5, nan, 0.125, 0.3333333333333333, nan, nan, 0.0, nan]
2019-02-19T19:56:04.365071: step 476, loss 1.88674, accuracy 0.375, precision [nan, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.25, nan, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:56:04.519022: step 477, loss 2.62818, accuracy 0.125, precision [0.0, 0.5, nan, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.0, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:04.671165: step 478, loss 1.85998, accuracy 0.3125, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.4, nan, 0.3, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:04.820734: step 479, loss 2.42914, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.5, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.2, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:04.970400: step 480, loss 2.00187, accuracy 0.1875, precision [nan, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.2, nan, 0.18181818181818182, nan, nan, nan, nan, nan]
2019-02-19T19:56:05.118219: step 481, loss 1.50942, accuracy 0.5625, precision [1.0, nan, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.6666666666666666, nan], recall [1.0, 0.0, nan, 0.5454545454545454, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:56:05.271243: step 482, loss 2.14874, accuracy 0.125, precision [0.0, nan, 0.0, 0.4, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.2222222222222222, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:05.425813: step 483, loss 2.11471, accuracy 0.3125, precision [0.5, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.25, nan], recall [1.0, 0.3333333333333333, nan, 0.2, nan, nan, nan, 0.5, nan]
2019-02-19T19:56:05.575588: step 484, loss 2.229, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.2, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:05.726647: step 485, loss 2.06913, accuracy 0.125, precision [0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.1111111111111111, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:05.872389: step 486, loss 1.93371, accuracy 0.1875, precision [nan, 0.25, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.13333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:06.024455: step 487, loss 1.87111, accuracy 0.4375, precision [0.0, 0.6666666666666666, nan, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.45454545454545453, nan, nan, nan, nan, nan]
2019-02-19T19:56:06.178347: step 488, loss 1.65199, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.6, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.3, nan, nan, nan, nan, nan]
2019-02-19T19:56:06.329137: step 489, loss 1.45709, accuracy 0.5625, precision [0.0, 0.3333333333333333, nan, 0.8888888888888888, 0.0, nan, 0.0, nan, nan], recall [nan, 0.5, nan, 0.6153846153846154, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:06.479021: step 490, loss 2.03965, accuracy 0.375, precision [nan, 1.0, 0.0, 0.625, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:06.631816: step 491, loss 2.2629, accuracy 0.0625, precision [0.0, 1.0, 0.0, 0.0, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.2, nan, 0.0, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:06.782102: step 492, loss 2.17701, accuracy 0.375, precision [nan, 1.0, 0.0, 0.5, 0.2, nan, 0.0, nan, nan], recall [nan, 0.4, nan, 0.375, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:56:06.936468: step 493, loss 1.73513, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.8, nan, nan, nan, nan, 0.0], recall [nan, 0.0, nan, 0.6666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:07.086445: step 494, loss 2.03706, accuracy 0.25, precision [0.0, 0.25, 0.0, 1.0, 0.2, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.18181818181818182, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:07.238986: step 495, loss 1.79069, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:07.395869: step 496, loss 2.17942, accuracy 0.125, precision [nan, 0.2, 0.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.1111111111111111, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:07.551557: step 497, loss 2.15677, accuracy 0.125, precision [0.0, 0.0, nan, 0.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.0], recall [nan, 0.0, nan, 0.0, 0.25, nan, nan, 0.5, nan]
2019-02-19T19:56:07.698315: step 498, loss 1.71625, accuracy 0.5, precision [0.0, 0.5, nan, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:56:07.843012: step 499, loss 1.97339, accuracy 0.25, precision [0.0, nan, nan, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3076923076923077, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:07.990765: step 500, loss 1.9368, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.25, nan, 0.3, 0.0, nan, nan, nan, nan]

Evaluation:
[[  6  28   0  47   4   0   0   4   0]
 [  0  46   0  94   3   0   0   1   0]
 [  0   1   0  84   2   0   0   0   0]
 [  1  25   0 312   7   0   0   0   0]
 [  0   6   0 138  20   0   0   0   0]
 [  0   2   0  48   3   0   0   0   0]
 [  0   0   0  22   0   0   0   0   0]
 [  1  15   0  81   3   0   0   3   0]
 [  0   3   0  15   0   0   0   0   0]]
2019-02-19T19:56:10.425527: step 500, loss 1.81216, accuracy 0.377561, precision [0.06741573033707865, 0.3194444444444444, 0.0, 0.9043478260869565, 0.12195121951219512, 0.0, 0.0, 0.02912621359223301, 0.0], recall [0.75, 0.36507936507936506, nan, 0.37098692033293695, 0.47619047619047616, nan, nan, 0.375, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550606084/checkpoints/model-500

2019-02-19T19:56:10.649215: step 501, loss 2.00066, accuracy 0.1875, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.15384615384615385, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:10.796202: step 502, loss 2.23759, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.6666666666666666, nan, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.16666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:10.945652: step 503, loss 2.10597, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.0, 0.6, nan, nan, nan, nan], recall [nan, 0.0, nan, 0.0, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:56:11.107171: step 504, loss 1.84094, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan, nan], recall [nan, 0.625, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:11.256906: step 505, loss 1.89396, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.5, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:11.409156: step 506, loss 1.6835, accuracy 0.5, precision [0.5, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:11.555374: step 507, loss 1.65029, accuracy 0.4375, precision [0.0, 0.75, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:56:11.707386: step 508, loss 1.59104, accuracy 0.5625, precision [1.0, 1.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:56:11.857240: step 509, loss 1.6326, accuracy 0.3125, precision [0.0, 0.75, 0.0, 0.4, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.6, nan, 0.2222222222222222, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:12.010643: step 510, loss 1.75041, accuracy 0.3125, precision [0.5, 0.5, 0.0, 0.75, 0.0, 0.0, nan, nan, 0.0], recall [0.5, 0.5, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:12.158157: step 511, loss 2.26871, accuracy 0.125, precision [0.0, nan, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.16666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:12.311793: step 512, loss 2.04505, accuracy 0.375, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan, 0.0], recall [1.0, 0.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:56:12.458894: step 513, loss 2.04597, accuracy 0.3125, precision [0.0, nan, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:12.612570: step 514, loss 1.96919, accuracy 0.25, precision [1.0, 0.5, 0.0, 0.3333333333333333, 0.5, 0.0, 0.0, 0.0, nan], recall [0.5, 1.0, nan, 0.1, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:56:12.762880: step 515, loss 1.75359, accuracy 0.4375, precision [0.3333333333333333, 1.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.6666666666666666, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:12.919532: step 516, loss 1.77501, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:13.069499: step 517, loss 1.68336, accuracy 0.5, precision [0.6666666666666666, 1.0, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [0.6666666666666666, 0.6, nan, 0.42857142857142855, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:13.216973: step 518, loss 1.80535, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:13.364846: step 519, loss 1.70129, accuracy 0.4375, precision [0.5, 0.6666666666666666, nan, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.6666666666666666, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:13.517565: step 520, loss 1.77433, accuracy 0.375, precision [1.0, 1.0, 0.5, 0.4, 0.0, nan, 0.0, 0.0, nan], recall [0.6666666666666666, 0.25, 1.0, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:13.671945: step 521, loss 1.96571, accuracy 0.3125, precision [1.0, 0.5, nan, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [0.3333333333333333, 0.4, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:56:13.826400: step 522, loss 1.83147, accuracy 0.3125, precision [0.6666666666666666, 0.4, 0.0, 0.5, 0.0, nan, nan, 0.0, 0.0], recall [1.0, 0.5, nan, 0.16666666666666666, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:13.983418: step 523, loss 1.96897, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.5, 0.5, nan, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.1, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:56:14.131332: step 524, loss 1.57467, accuracy 0.4375, precision [nan, 0.75, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.6, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:14.286575: step 525, loss 1.77059, accuracy 0.4375, precision [1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, 1.0, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:56:14.440807: step 526, loss 1.66316, accuracy 0.375, precision [nan, 0.0, nan, 0.7142857142857143, 0.25, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.625, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:14.591078: step 527, loss 1.82624, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, nan, 0.0], recall [nan, nan, nan, 0.4375, nan, nan, nan, nan, nan]
2019-02-19T19:56:14.747053: step 528, loss 1.96695, accuracy 0.3125, precision [1.0, 0.5, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.2, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:14.894575: step 529, loss 1.74011, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:56:15.044671: step 530, loss 1.81789, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.0, nan, nan, nan, nan], recall [0.0, 0.0, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:15.197624: step 531, loss 1.79644, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, nan, nan], recall [nan, 0.0, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:15.349913: step 532, loss 1.90904, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.25, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:15.502904: step 533, loss 1.95047, accuracy 0.375, precision [0.0, 0.2, 0.0, 1.0, nan, nan, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:56:15.655930: step 534, loss 1.81409, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [1.0, nan, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:15.807835: step 535, loss 1.46468, accuracy 0.5, precision [0.5, nan, 0.0, 1.0, 0.16666666666666666, 0.0, nan, nan, nan], recall [1.0, 0.0, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:56:15.964559: step 536, loss 1.82062, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 0.8333333333333334, nan, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.38461538461538464, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:16.117284: step 537, loss 2.34733, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, nan, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:56:16.269431: step 538, loss 1.90092, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.26666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:56:16.414043: step 539, loss 1.9758, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [nan, nan, nan, 0.4375, nan, nan, nan, nan, nan]
2019-02-19T19:56:16.564703: step 540, loss 1.57084, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.875, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.4666666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:16.715338: step 541, loss 2.12964, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.25, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.21428571428571427, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:56:16.871639: step 542, loss 2.26654, accuracy 0.25, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:56:17.021618: step 543, loss 1.56181, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:17.177611: step 544, loss 2.05478, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.25, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:17.326064: step 545, loss 1.8462, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.25, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:17.479507: step 546, loss 1.43721, accuracy 0.625, precision [0.3333333333333333, 0.6666666666666666, nan, 1.0, 0.0, nan, nan, nan, nan], recall [1.0, 0.6666666666666666, nan, 0.5833333333333334, nan, nan, nan, nan, nan]
2019-02-19T19:56:17.629919: step 547, loss 1.98339, accuracy 0.1875, precision [0.0, 0.0, nan, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.15384615384615385, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:56:17.777212: step 548, loss 2.20539, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, nan, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:17.926957: step 549, loss 1.59499, accuracy 0.4375, precision [1.0, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:18.073752: step 550, loss 1.74847, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, nan, nan, 0.0], recall [nan, 0.0, nan, 0.38461538461538464, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:18.222032: step 551, loss 1.7441, accuracy 0.3125, precision [0.0, nan, 0.0, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:18.366593: step 552, loss 1.76876, accuracy 0.25, precision [0.0, 0.0, nan, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:18.521339: step 553, loss 1.78373, accuracy 0.375, precision [1.0, nan, 0.0, 0.5, 0.6666666666666666, nan, nan, 0.25, 0.0], recall [1.0, 0.0, nan, 0.25, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:56:18.676526: step 554, loss 1.5085, accuracy 0.5, precision [0.0, 0.25, nan, 0.6666666666666666, 0.7142857142857143, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.4, 0.7142857142857143, nan, nan, 0.0, nan]
2019-02-19T19:56:18.826224: step 555, loss 2.08434, accuracy 0.1875, precision [1.0, 0.0, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.16666666666666666, 0.2, nan, nan, 0.0, nan]
2019-02-19T19:56:18.977531: step 556, loss 1.97506, accuracy 0.375, precision [0.0, nan, 0.5, 0.75, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 1.0, 0.375, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:19.126086: step 557, loss 2.08109, accuracy 0.25, precision [0.3333333333333333, nan, 0.0, 0.5, 0.0, nan, nan, 0.0, 0.0], recall [1.0, nan, nan, 0.42857142857142855, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:19.270780: step 558, loss 1.84744, accuracy 0.375, precision [nan, 0.0, 0.0, 0.5, 0.8, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.2857142857142857, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:19.422454: step 559, loss 1.89909, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.4, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:19.569447: step 560, loss 2.24758, accuracy 0.25, precision [nan, 0.6666666666666666, nan, 0.5, 0.0, 0.0, 0.0, 0.2, 0.0], recall [nan, 0.6666666666666666, nan, 0.125, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:56:19.720555: step 561, loss 1.70618, accuracy 0.5625, precision [0.0, 0.5, 0.0, 0.8571428571428571, 1.0, nan, 0.0, 1.0, nan], recall [0.0, 0.5, nan, 0.6666666666666666, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:56:19.869151: step 562, loss 1.97, accuracy 0.1875, precision [0.5, 0.0, nan, 0.2, 1.0, 0.0, 0.0, 0.0, nan], recall [0.25, nan, nan, 0.2, 0.14285714285714285, nan, nan, nan, nan]
2019-02-19T19:56:20.019985: step 563, loss 1.81947, accuracy 0.25, precision [nan, nan, 0.0, 0.4, 0.3333333333333333, 0.0, nan, 0.5, nan], recall [nan, 0.0, nan, 0.2222222222222222, 0.2, nan, nan, 1.0, nan]
2019-02-19T19:56:20.169163: step 564, loss 1.92904, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.75, 1.0, 0.0, nan, 0.0, 0.0], recall [nan, nan, nan, 0.2727272727272727, 0.75, nan, nan, 0.0, nan]
2019-02-19T19:56:20.319639: step 565, loss 1.99516, accuracy 0.375, precision [0.0, 0.0, nan, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, nan, nan, 0.42857142857142855, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:20.472136: step 566, loss 1.83046, accuracy 0.4375, precision [nan, 1.0, 0.0, 0.6666666666666666, nan, 0.0, nan, 0.25, 0.0], recall [0.0, 0.5, nan, 0.5, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:56:20.620208: step 567, loss 1.84314, accuracy 0.3125, precision [0.0, 0.25, nan, 0.75, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.375, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:56:20.774682: step 568, loss 2.37958, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 1.0, 0.0], recall [nan, 0.0, nan, 0.1, 0.25, nan, nan, 1.0, nan]
2019-02-19T19:56:20.926543: step 569, loss 2.25274, accuracy 0.3125, precision [0.0, 0.2, nan, 1.0, nan, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:21.076797: step 570, loss 1.9336, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.4, 0.0, nan, nan, nan], recall [nan, 0.0, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:56:21.229226: step 571, loss 2.35359, accuracy 0.1875, precision [0.0, nan, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:21.381514: step 572, loss 2.00094, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.3333333333333333, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:21.540371: step 573, loss 1.59245, accuracy 0.4375, precision [0.5, 0.25, 0.0, 0.8, 0.3333333333333333, nan, 0.0, nan, nan], recall [1.0, 0.5, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:21.691684: step 574, loss 1.88739, accuracy 0.3125, precision [1.0, 0.2, nan, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.25, 1.0, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:56:21.846121: step 575, loss 1.88806, accuracy 0.25, precision [0.0, 0.0, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:21.992447: step 576, loss 2.0297, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.35714285714285715, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:22.230893: step 577, loss 2.04382, accuracy 0.2, precision [0.0, 1.0, nan, 0.5, 0.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.5, nan, 0.14285714285714285, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:22.386375: step 578, loss 1.87307, accuracy 0.25, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, nan, nan, 0.15384615384615385, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:22.536700: step 579, loss 1.75276, accuracy 0.3125, precision [nan, 0.6666666666666666, 0.0, 0.5, 0.0, nan, 0.0, 0.5, nan], recall [nan, 0.6666666666666666, nan, 0.18181818181818182, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:56:22.685734: step 580, loss 1.87255, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.25, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:22.836518: step 581, loss 1.86525, accuracy 0.4375, precision [0.0, 0.6666666666666666, 0.0, 0.8, 0.5, nan, nan, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:23.000807: step 582, loss 1.80374, accuracy 0.5625, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.5, nan], recall [nan, 0.0, nan, 0.5833333333333334, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:56:23.149584: step 583, loss 1.78506, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:23.306780: step 584, loss 1.93718, accuracy 0.5, precision [nan, 0.25, 0.0, 0.875, 0.0, nan, 0.0, nan, 0.0], recall [nan, 0.5, nan, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:23.455981: step 585, loss 1.82233, accuracy 0.4375, precision [1.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:56:23.602193: step 586, loss 1.72569, accuracy 0.5625, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.6153846153846154, nan, nan, nan, nan, nan]
2019-02-19T19:56:23.762487: step 587, loss 1.67309, accuracy 0.625, precision [1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.5384615384615384, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:23.911997: step 588, loss 2.05231, accuracy 0.125, precision [0.0, 0.3333333333333333, 0.0, 0.5, 0.0, nan, 0.0, nan, nan], recall [nan, 0.25, nan, 0.08333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:24.065136: step 589, loss 1.97224, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:24.215488: step 590, loss 2.1857, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:24.366246: step 591, loss 1.89413, accuracy 0.3125, precision [1.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.08333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:24.514404: step 592, loss 2.07975, accuracy 0.25, precision [0.0, 0.0, nan, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.26666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:56:24.661962: step 593, loss 1.79234, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.38461538461538464, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:24.810009: step 594, loss 1.77301, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8, 0.25, 0.0, 0.0, nan, nan], recall [nan, 0.5, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:56:24.961192: step 595, loss 1.59625, accuracy 0.375, precision [nan, 0.4, nan, 0.5714285714285714, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:25.110187: step 596, loss 1.74107, accuracy 0.5625, precision [nan, 0.4, nan, 1.0, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:25.258646: step 597, loss 1.84662, accuracy 0.3125, precision [1.0, 0.0, nan, 0.6, 0.5, 0.0, 0.0, nan, nan], recall [0.5, 0.0, nan, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:25.411410: step 598, loss 1.83591, accuracy 0.3125, precision [0.25, 0.5, nan, 0.75, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.2727272727272727, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:25.561998: step 599, loss 1.74285, accuracy 0.375, precision [1.0, 0.25, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:25.717098: step 600, loss 1.74672, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:25.864078: step 601, loss 1.98225, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:26.012543: step 602, loss 1.8443, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:26.157984: step 603, loss 1.92447, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [nan, 0.25, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:26.311795: step 604, loss 1.6464, accuracy 0.5, precision [0.0, 0.6666666666666666, 0.0, 1.0, 0.5, nan, 0.0, 0.0, 0.0], recall [0.0, 0.6666666666666666, nan, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:26.460110: step 605, loss 1.67067, accuracy 0.5625, precision [nan, 0.3333333333333333, nan, 1.0, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [nan, 0.5, nan, 0.5384615384615384, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:26.609503: step 606, loss 1.78933, accuracy 0.375, precision [nan, 0.75, 0.0, 1.0, nan, nan, 0.0, 0.0, nan], recall [0.0, 0.75, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:56:26.761769: step 607, loss 1.71787, accuracy 0.5, precision [nan, 0.5714285714285714, 0.0, 0.8, 0.0, 0.0, nan, nan, nan], recall [nan, 1.0, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:26.917037: step 608, loss 1.9549, accuracy 0.3125, precision [0.0, 0.4, 0.0, 1.0, 0.0, 0.0, nan, nan, 0.0], recall [nan, 0.4, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:56:27.062619: step 609, loss 2.15127, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:27.212537: step 610, loss 2.05728, accuracy 0.25, precision [0.3333333333333333, nan, 0.0, 0.6666666666666666, nan, 0.0, nan, 0.25, 0.0], recall [1.0, 0.0, nan, 0.15384615384615385, nan, nan, nan, 1.0, nan]
2019-02-19T19:56:27.365944: step 611, loss 2.16137, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.42857142857142855, nan, 0.0, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:27.517658: step 612, loss 1.60498, accuracy 0.5, precision [0.0, 1.0, 0.0, 1.0, 0.0, nan, nan, 0.5, nan], recall [nan, 0.5714285714285714, nan, 0.375, nan, nan, nan, 1.0, nan]
2019-02-19T19:56:27.666139: step 613, loss 1.98356, accuracy 0.4375, precision [0.5, 0.5, 0.0, 0.8, 1.0, 0.0, nan, nan, nan], recall [0.5, 0.2, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:27.815314: step 614, loss 2.15239, accuracy 0.1875, precision [0.5, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, 0.0], recall [1.0, 0.14285714285714285, nan, 0.125, nan, nan, nan, nan, nan]
2019-02-19T19:56:27.968024: step 615, loss 1.96865, accuracy 0.25, precision [nan, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.2222222222222222, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:28.115778: step 616, loss 1.73853, accuracy 0.3125, precision [0.0, 0.75, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [nan, 0.42857142857142855, 0.0, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:56:28.269933: step 617, loss 1.69603, accuracy 0.4375, precision [0.25, 0.6, 0.0, 0.75, nan, nan, nan, nan, nan], recall [0.5, 0.5, nan, 0.375, nan, nan, nan, nan, nan]
2019-02-19T19:56:28.418519: step 618, loss 1.84683, accuracy 0.375, precision [0.0, 0.5, nan, 0.5, 0.0, nan, nan, nan, nan], recall [0.0, 0.5714285714285714, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:28.577514: step 619, loss 2.07433, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.14285714285714285, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:28.732868: step 620, loss 2.15199, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, 0.0], recall [0.5, 0.0, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:56:28.883868: step 621, loss 1.9552, accuracy 0.25, precision [0.2, 0.6666666666666666, 0.0, 0.25, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.2, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:56:29.036898: step 622, loss 1.61718, accuracy 0.5625, precision [1.0, 0.75, nan, 0.5, nan, nan, 0.0, nan, nan], recall [1.0, 0.3333333333333333, nan, 0.8333333333333334, nan, nan, nan, nan, nan]
2019-02-19T19:56:29.187885: step 623, loss 1.7435, accuracy 0.4375, precision [nan, 1.0, 0.0, 0.6, 0.4, nan, 0.0, 0.5, nan], recall [nan, 0.2, 0.0, 0.42857142857142855, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:56:29.339214: step 624, loss 1.97358, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:29.487991: step 625, loss 1.912, accuracy 0.1875, precision [nan, 1.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:56:29.635542: step 626, loss 1.92737, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5714285714285714, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:56:29.785693: step 627, loss 1.82309, accuracy 0.4375, precision [0.3333333333333333, 1.0, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.5, 0.0, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:56:29.937099: step 628, loss 1.92474, accuracy 0.25, precision [nan, 0.5, 0.0, 0.375, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.2, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:30.094599: step 629, loss 1.80541, accuracy 0.375, precision [nan, 0.0, nan, 0.8571428571428571, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.46153846153846156, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:30.245457: step 630, loss 2.09101, accuracy 0.125, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.09090909090909091, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:30.396134: step 631, loss 1.70017, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.4, nan, 0.3, nan, nan, nan, nan, nan]
2019-02-19T19:56:30.548734: step 632, loss 2.04898, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.3, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:56:30.701249: step 633, loss 1.93313, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, nan, 0.5, 0.0], recall [nan, 0.0, nan, 0.2727272727272727, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:56:30.846813: step 634, loss 1.76934, accuracy 0.3125, precision [nan, nan, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:30.999520: step 635, loss 1.61882, accuracy 0.5, precision [0.5, 0.25, 0.0, 1.0, 0.3333333333333333, nan, nan, nan, nan], recall [1.0, 1.0, nan, 0.4166666666666667, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:31.150673: step 636, loss 2.51703, accuracy 0.125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.14285714285714285, nan, nan, nan, nan, nan]
2019-02-19T19:56:31.304619: step 637, loss 1.98811, accuracy 0.25, precision [0.0, 0.16666666666666666, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, 0.0, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:31.455908: step 638, loss 1.69306, accuracy 0.3125, precision [0.0, 0.0, nan, 0.6666666666666666, 0.25, nan, nan, nan, nan], recall [nan, 0.0, 0.0, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:31.608409: step 639, loss 2.01735, accuracy 0.3125, precision [nan, 0.25, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:31.757164: step 640, loss 1.78547, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [1.0, 0.0, nan, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:56:31.916016: step 641, loss 1.77638, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 0.8333333333333334, 0.3333333333333333, nan, nan, nan, nan], recall [0.0, 0.5, nan, 0.45454545454545453, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:32.064846: step 642, loss 1.75441, accuracy 0.375, precision [nan, 0.25, 0.0, 0.8, 0.5, nan, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:32.217563: step 643, loss 1.88904, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, nan, nan, 0.4666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:56:32.363267: step 644, loss 1.84026, accuracy 0.3125, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:32.515151: step 645, loss 1.92711, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:32.661314: step 646, loss 1.81651, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.8571428571428571, 0.0, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.6, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:32.813378: step 647, loss 1.99238, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.8, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:32.969236: step 648, loss 1.98544, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:33.122770: step 649, loss 2.03484, accuracy 0.3125, precision [0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:33.275066: step 650, loss 1.80973, accuracy 0.4375, precision [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan, 0.0], recall [1.0, nan, nan, 0.38461538461538464, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:33.423984: step 651, loss 1.84312, accuracy 0.4375, precision [0.3333333333333333, nan, nan, 1.0, 0.2, nan, 0.0, 0.0, nan], recall [1.0, nan, 0.0, 0.4166666666666667, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:33.570246: step 652, loss 1.97601, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.2, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.15384615384615385, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:33.723499: step 653, loss 1.61925, accuracy 0.5, precision [0.0, 0.5, 0.0, 1.0, nan, nan, 0.0, 1.0, nan], recall [0.0, 0.75, nan, 0.5, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:56:33.876056: step 654, loss 1.90713, accuracy 0.4375, precision [nan, nan, nan, 0.875, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:34.033222: step 655, loss 2.13761, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.3333333333333333, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.2, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:34.185199: step 656, loss 1.73301, accuracy 0.5625, precision [0.0, 0.5, nan, 0.6666666666666666, 0.5, nan, nan, 0.5, nan], recall [0.0, 0.5, nan, 0.6666666666666666, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:56:34.335917: step 657, loss 1.79802, accuracy 0.4375, precision [0.0, 0.5, nan, 0.8, 0.0, 0.0, 0.0, 1.0, nan], recall [0.0, 1.0, nan, 0.4444444444444444, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:56:34.489177: step 658, loss 1.87719, accuracy 0.1875, precision [0.0, nan, nan, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, 0.0, 0.25, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:34.635106: step 659, loss 1.89188, accuracy 0.25, precision [nan, 0.25, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:34.789639: step 660, loss 2.12392, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 0.4, 0.0, nan, nan, 0.0, 0.0], recall [0.5, 0.0, nan, 0.2857142857142857, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:34.946217: step 661, loss 2.02388, accuracy 0.3125, precision [0.0, 0.25, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, 0.0, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:35.100770: step 662, loss 1.90189, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 1.0, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.4166666666666667, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:56:35.252949: step 663, loss 1.87253, accuracy 0.25, precision [nan, 0.0, 0.0, 0.5714285714285714, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:35.401118: step 664, loss 1.70223, accuracy 0.375, precision [0.5, 0.5, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [1.0, 0.5, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:35.555044: step 665, loss 1.82682, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.25, 0.0, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:35.701464: step 666, loss 1.73164, accuracy 0.625, precision [nan, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.5, 1.0, 0.5833333333333334, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:35.857035: step 667, loss 1.75956, accuracy 0.3125, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:56:36.008035: step 668, loss 1.92324, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:56:36.157285: step 669, loss 1.96645, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.23076923076923078, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:36.305011: step 670, loss 2.07237, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 0.0, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:56:36.454612: step 671, loss 2.0015, accuracy 0.3125, precision [0.5, 0.25, 0.0, 0.6666666666666666, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.18181818181818182, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:36.603138: step 672, loss 2.06843, accuracy 0.25, precision [nan, 0.0, 0.0, 0.8, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:36.752666: step 673, loss 1.53422, accuracy 0.5, precision [0.0, 1.0, nan, 0.8333333333333334, 0.0, 0.0, nan, 1.0, nan], recall [nan, 0.5, nan, 0.5, nan, nan, nan, 0.5, nan]
2019-02-19T19:56:36.903640: step 674, loss 1.6641, accuracy 0.25, precision [0.0, nan, nan, 0.8, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:56:37.051495: step 675, loss 2.23491, accuracy 0.125, precision [0.0, 0.16666666666666666, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.07142857142857142, nan, nan, nan, nan, nan]
2019-02-19T19:56:37.201759: step 676, loss 1.72256, accuracy 0.375, precision [0.5, 0.0, nan, 0.7142857142857143, 0.0, 0.0, nan, 0.0, nan], recall [0.3333333333333333, 0.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:56:37.354357: step 677, loss 2.01999, accuracy 0.1875, precision [0.0, nan, 0.0, 0.5, 0.14285714285714285, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:56:37.500323: step 678, loss 1.79688, accuracy 0.3125, precision [0.25, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, nan, nan, 0.26666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:56:37.655151: step 679, loss 1.8316, accuracy 0.3125, precision [1.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.5, nan], recall [1.0, nan, nan, 0.21428571428571427, nan, nan, nan, 1.0, nan]
2019-02-19T19:56:37.807762: step 680, loss 2.12563, accuracy 0.1875, precision [nan, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, nan, 0.0], recall [0.0, nan, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:56:37.954984: step 681, loss 1.55904, accuracy 0.5, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:56:38.101512: step 682, loss 2.10153, accuracy 0.25, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:56:38.253080: step 683, loss 1.71614, accuracy 0.5, precision [0.3333333333333333, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.45454545454545453, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:38.404019: step 684, loss 1.73699, accuracy 0.25, precision [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.14285714285714285, nan, nan, nan, nan, nan]
2019-02-19T19:56:38.551937: step 685, loss 1.84714, accuracy 0.5, precision [nan, nan, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.5833333333333334, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:56:38.705137: step 686, loss 1.94347, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:38.854417: step 687, loss 2.07614, accuracy 0.3125, precision [0.0, 0.0, nan, 0.8, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.4, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:56:39.004869: step 688, loss 1.96287, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.26666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:56:39.150077: step 689, loss 1.96507, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:56:39.297116: step 690, loss 1.97679, accuracy 0.125, precision [nan, 0.25, 0.0, 0.5, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.07692307692307693, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:39.445700: step 691, loss 1.70434, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.7777777777777778, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5384615384615384, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:39.594410: step 692, loss 1.67707, accuracy 0.5625, precision [0.0, 0.5, 0.0, 0.8888888888888888, 0.0, nan, nan, nan, nan], recall [0.0, 1.0, nan, 0.5714285714285714, nan, nan, nan, nan, nan]
2019-02-19T19:56:39.747549: step 693, loss 1.79142, accuracy 0.3125, precision [0.0, 0.16666666666666666, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:56:39.902311: step 694, loss 2.24051, accuracy 0.25, precision [0.0, nan, 0.0, 1.0, 0.2, 0.0, nan, 0.0, 0.0], recall [nan, nan, nan, 0.2, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:40.048296: step 695, loss 2.19575, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.14285714285714285, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:40.200348: step 696, loss 1.89484, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.5, nan], recall [0.0, nan, nan, 0.2857142857142857, nan, nan, nan, 1.0, nan]
2019-02-19T19:56:40.349240: step 697, loss 2.06698, accuracy 0.3125, precision [0.5, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [0.5, nan, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:56:40.494577: step 698, loss 1.64747, accuracy 0.375, precision [0.0, 1.0, nan, 1.0, 0.25, 0.0, nan, 0.0, nan], recall [nan, 0.2, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:40.639743: step 699, loss 1.76933, accuracy 0.375, precision [nan, 0.5, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.6666666666666666, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:40.791604: step 700, loss 1.89537, accuracy 0.25, precision [0.0, 0.25, 0.0, 1.0, 0.0, nan, 0.0, nan, 0.0], recall [nan, 0.2, nan, 0.3, nan, 0.0, nan, nan, nan]
2019-02-19T19:56:40.944253: step 701, loss 1.93954, accuracy 0.3125, precision [0.3333333333333333, 0.25, 0.0, 0.75, 0.0, 0.0, 0.0, nan, nan], recall [0.5, 0.3333333333333333, 0.0, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:41.089069: step 702, loss 2.05152, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.5, nan], recall [nan, 0.0, nan, 0.2857142857142857, nan, nan, nan, 1.0, nan]
2019-02-19T19:56:41.239121: step 703, loss 2.08934, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, nan, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.3, nan, nan, nan, nan, nan]
2019-02-19T19:56:41.387583: step 704, loss 2.06209, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.875, nan, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.6363636363636364, nan, nan, nan, nan, nan]
2019-02-19T19:56:41.537441: step 705, loss 1.82387, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, nan, nan, nan], recall [nan, 0.3333333333333333, 0.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:41.689103: step 706, loss 1.62507, accuracy 0.375, precision [nan, 0.5, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.75, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:41.845314: step 707, loss 1.72884, accuracy 0.4375, precision [0.5, 0.0, nan, 0.75, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:41.999089: step 708, loss 1.98443, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.5, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.25, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:42.149800: step 709, loss 1.74944, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.8, nan, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.6153846153846154, nan, nan, nan, nan, nan]
2019-02-19T19:56:42.303184: step 710, loss 1.61391, accuracy 0.5, precision [0.6666666666666666, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.0, nan, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:42.454040: step 711, loss 1.68443, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:56:42.598264: step 712, loss 1.84856, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:56:42.753850: step 713, loss 2.00713, accuracy 0.25, precision [0.3333333333333333, 0.0, 0.0, 0.75, 0.0, nan, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.25, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:42.905181: step 714, loss 2.05395, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [nan, nan, 0.0, 0.13333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:43.059719: step 715, loss 1.73167, accuracy 0.25, precision [nan, 0.5, 0.0, 0.6, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.25, nan, 0.3, nan, nan, nan, nan, nan]
2019-02-19T19:56:43.213133: step 716, loss 1.90887, accuracy 0.4375, precision [0.0, 0.0, nan, 0.8333333333333334, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:43.362989: step 717, loss 2.14367, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:56:43.516495: step 718, loss 2.0195, accuracy 0.25, precision [0.0, 0.0, nan, 1.0, 0.2, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.21428571428571427, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:43.668361: step 719, loss 1.55066, accuracy 0.5, precision [0.5, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:56:43.816867: step 720, loss 1.66219, accuracy 0.4375, precision [0.25, nan, 0.0, 0.8571428571428571, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.46153846153846156, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:43.969017: step 721, loss 1.99896, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.25, 0.0, 0.0, nan, nan], recall [0.0, 1.0, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:44.118350: step 722, loss 1.62519, accuracy 0.375, precision [0.0, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, nan, nan, 0.375, nan, nan, nan, nan, nan]
2019-02-19T19:56:44.270401: step 723, loss 1.69524, accuracy 0.375, precision [nan, 0.2857142857142857, 0.0, 0.8, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:44.422476: step 724, loss 1.97495, accuracy 0.375, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, nan, nan], recall [0.5, nan, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:44.571622: step 725, loss 1.53861, accuracy 0.5, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, nan, 0.42857142857142855, nan, nan, nan, 1.0, nan]
2019-02-19T19:56:44.716758: step 726, loss 1.68232, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.875, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5384615384615384, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:44.865196: step 727, loss 1.66364, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.5, nan], recall [0.3333333333333333, 0.0, nan, 0.5555555555555556, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:56:45.014489: step 728, loss 1.81271, accuracy 0.3125, precision [0.0, 0.0, nan, 0.5714285714285714, 0.0, nan, nan, 1.0, 0.0], recall [0.0, nan, nan, 0.4, nan, nan, nan, 0.2, nan]
2019-02-19T19:56:45.166103: step 729, loss 1.44869, accuracy 0.6875, precision [0.5, 0.0, nan, 0.875, 0.5, nan, nan, 1.0, nan], recall [1.0, nan, nan, 0.7, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:56:45.319057: step 730, loss 2.0699, accuracy 0.1875, precision [nan, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.18181818181818182, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:45.466282: step 731, loss 1.89819, accuracy 0.25, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, nan, nan, 0.0], recall [nan, 1.0, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:56:45.618490: step 732, loss 1.71006, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:56:45.768111: step 733, loss 1.4214, accuracy 0.5625, precision [nan, 0.0, 0.0, 0.75, 0.0, nan, nan, nan, nan], recall [0.0, nan, nan, 0.75, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:45.915856: step 734, loss 1.748, accuracy 0.4375, precision [nan, 1.0, 0.0, 0.8333333333333334, 0.0, 0.0, nan, 0.25, nan], recall [nan, 1.0, nan, 0.38461538461538464, nan, nan, nan, 0.5, nan]
2019-02-19T19:56:46.067661: step 735, loss 1.99877, accuracy 0.3125, precision [0.0, 0.3333333333333333, nan, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:56:46.227440: step 736, loss 1.83641, accuracy 0.375, precision [0.3333333333333333, 0.3333333333333333, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.36363636363636365, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:46.371963: step 737, loss 1.78975, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.4166666666666667, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:46.523902: step 738, loss 1.61587, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.25, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.35714285714285715, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:46.678846: step 739, loss 1.84766, accuracy 0.125, precision [0.0, 0.0, nan, 1.0, 0.5, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.07692307692307693, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:56:46.832238: step 740, loss 1.78925, accuracy 0.4375, precision [1.0, 0.75, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.3, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:46.984071: step 741, loss 1.60676, accuracy 0.375, precision [nan, 0.4, 0.0, 0.8, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:47.133008: step 742, loss 1.63069, accuracy 0.4375, precision [1.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, nan, 0.0], recall [0.6666666666666666, 1.0, nan, 0.3, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:47.285191: step 743, loss 2.05418, accuracy 0.25, precision [0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.15384615384615385, nan, nan, nan, nan, nan]
2019-02-19T19:56:47.439544: step 744, loss 1.39839, accuracy 0.5625, precision [0.3333333333333333, 1.0, 0.0, 1.0, 0.0, nan, nan, nan, nan], recall [1.0, 1.0, nan, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:56:47.590009: step 745, loss 1.69389, accuracy 0.5, precision [nan, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.5, nan], recall [nan, 1.0, nan, 0.38461538461538464, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:56:47.743174: step 746, loss 1.93125, accuracy 0.3125, precision [nan, 0.25, 0.0, 0.75, 0.25, 0.0, nan, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:47.897393: step 747, loss 1.96417, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.2, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:56:48.052568: step 748, loss 1.95952, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:56:48.198762: step 749, loss 1.9247, accuracy 0.1875, precision [0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, 1.0, 0.2, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:48.347930: step 750, loss 1.8946, accuracy 0.25, precision [nan, 0.5, 0.0, 0.5, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.25, nan, 0.3, 0.0, nan, nan, 0.0, nan]

Evaluation:
[[ 16  41   0  32   0   0   0   0   0]
 [  3  67   0  74   0   0   0   0   0]
 [  1   9   0  76   1   0   0   0   0]
 [  3  55   0 286   1   0   0   0   0]
 [  0  17   0 132  15   0   0   0   0]
 [  1   7   0  44   1   0   0   0   0]
 [  0   1   0  21   0   0   0   0   0]
 [  4  25   0  69   3   0   0   2   0]
 [  0   7   0  11   0   0   0   0   0]]
2019-02-19T19:56:50.760464: step 750, loss 1.78852, accuracy 0.376585, precision [0.1797752808988764, 0.4652777777777778, 0.0, 0.8289855072463768, 0.09146341463414634, 0.0, 0.0, 0.019417475728155338, 0.0], recall [0.5714285714285714, 0.2925764192139738, nan, 0.3838926174496644, 0.7142857142857143, nan, nan, 1.0, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550606084/checkpoints/model-750

2019-02-19T19:56:50.991067: step 751, loss 2.09493, accuracy 0.375, precision [nan, nan, nan, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:56:51.143021: step 752, loss 1.68045, accuracy 0.375, precision [0.0, 1.0, nan, 0.8, 0.25, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:51.291854: step 753, loss 1.6776, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 0.6666666666666666, 0.25, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:51.442146: step 754, loss 2.01169, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, nan, nan], recall [nan, 0.0, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:56:51.597506: step 755, loss 1.97051, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 1.0, nan], recall [0.0, 0.0, nan, 0.2222222222222222, nan, nan, nan, 0.5, nan]
2019-02-19T19:56:51.748076: step 756, loss 1.94828, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:56:51.894672: step 757, loss 1.62817, accuracy 0.4375, precision [nan, nan, 0.0, 0.875, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.4666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:56:52.043040: step 758, loss 2.48543, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.26666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:56:52.191951: step 759, loss 1.99849, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.5, 0.0, nan, nan, 0.0], recall [nan, nan, nan, 0.38461538461538464, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:56:52.346538: step 760, loss 1.63631, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:56:52.504005: step 761, loss 1.58862, accuracy 0.5, precision [0.0, 0.6, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.75, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:56:52.655843: step 762, loss 1.6597, accuracy 0.5625, precision [nan, 0.5, nan, 0.8888888888888888, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.5714285714285714, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:52.812927: step 763, loss 2.13972, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 1.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:52.964054: step 764, loss 2.23454, accuracy 0.25, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:56:53.112704: step 765, loss 2.17012, accuracy 0.25, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, nan, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:56:53.265409: step 766, loss 2.13064, accuracy 0.1875, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.25, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:56:53.413242: step 767, loss 1.53993, accuracy 0.625, precision [0.6666666666666666, 1.0, 0.0, 0.8, 0.5, nan, nan, 1.0, nan], recall [0.6666666666666666, 0.6666666666666666, nan, 0.5, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:56:53.566737: step 768, loss 1.75511, accuracy 0.25, precision [0.5, 0.5, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.4, 0.3333333333333333, nan, 0.125, nan, nan, nan, nan, nan]
2019-02-19T19:56:53.716142: step 769, loss 1.77079, accuracy 0.5, precision [nan, 1.0, nan, 0.75, 0.25, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.6, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:56:53.867855: step 770, loss 1.96236, accuracy 0.375, precision [nan, 0.3333333333333333, nan, 1.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:54.016854: step 771, loss 1.84348, accuracy 0.375, precision [0.5, 1.0, 0.0, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:54.168331: step 772, loss 1.96506, accuracy 0.25, precision [0.0, 0.4, 0.0, 1.0, 0.5, nan, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.1, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:54.322651: step 773, loss 1.97619, accuracy 0.1875, precision [1.0, 0.25, 0.0, 0.2, 0.0, 0.0, nan, 0.0, nan], recall [0.25, 0.5, nan, 0.1111111111111111, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:54.474756: step 774, loss 1.78583, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 0.5, 1.0, nan, nan, nan, nan], recall [0.0, 0.25, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:54.627189: step 775, loss 1.75464, accuracy 0.5625, precision [0.0, 1.0, nan, 1.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, nan], recall [0.0, 1.0, nan, 0.6, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:56:54.783900: step 776, loss 1.85948, accuracy 0.4375, precision [nan, 0.75, 0.0, 0.8, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.6, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:54.932913: step 777, loss 1.84032, accuracy 0.375, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.4, nan, nan, nan, 0.0], recall [0.0, 0.0, 0.0, 0.5714285714285714, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:56:55.079276: step 778, loss 1.7576, accuracy 0.375, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:55.231444: step 779, loss 1.69593, accuracy 0.375, precision [0.0, nan, nan, 0.625, 0.25, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.45454545454545453, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:55.381205: step 780, loss 2.11446, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.4166666666666667, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:56:55.528184: step 781, loss 2.26393, accuracy 0.25, precision [0.0, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:55.679987: step 782, loss 1.70338, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.38461538461538464, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:55.826610: step 783, loss 1.86247, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, nan, 0.0, nan, nan], recall [nan, 0.0, nan, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:55.975381: step 784, loss 1.8836, accuracy 0.25, precision [nan, 0.0, 0.0, 0.6, 0.0, nan, nan, 0.5, 0.0], recall [nan, 0.0, nan, 0.25, nan, nan, nan, 0.5, nan]
2019-02-19T19:56:56.131825: step 785, loss 1.92496, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.8, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 1.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:56:56.284469: step 786, loss 2.08935, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.09090909090909091, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:56:56.437783: step 787, loss 1.82812, accuracy 0.4375, precision [nan, 0.5, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, nan], recall [nan, 0.6666666666666666, nan, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:56:56.590682: step 788, loss 2.01582, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 1.0, nan, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:56.736444: step 789, loss 2.13107, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.1875, nan, nan, nan, nan, nan]
2019-02-19T19:56:56.889883: step 790, loss 1.67475, accuracy 0.4375, precision [0.0, 0.25, 0.0, 0.8571428571428571, 0.0, nan, nan, nan, nan], recall [nan, 0.5, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:56:57.041240: step 791, loss 1.70275, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 0.14285714285714285, nan, nan, nan, nan], recall [nan, nan, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:57.187398: step 792, loss 2.20797, accuracy 0.125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, nan, nan, 0.125, nan, nan, nan, nan, nan]
2019-02-19T19:56:57.332045: step 793, loss 2.04436, accuracy 0.125, precision [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.0, nan, nan, nan, nan, nan]
2019-02-19T19:56:57.484493: step 794, loss 2.0058, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:56:57.637190: step 795, loss 1.96557, accuracy 0.3125, precision [0.5, nan, 0.0, 0.75, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.5, nan, nan, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:57.786996: step 796, loss 2.19831, accuracy 0.25, precision [0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:56:57.934688: step 797, loss 1.86998, accuracy 0.4375, precision [nan, 0.25, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.46153846153846156, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:58.083956: step 798, loss 2.03351, accuracy 0.1875, precision [0.25, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0], recall [0.5, 0.0, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:56:58.240688: step 799, loss 1.82492, accuracy 0.3125, precision [0.0, 1.0, nan, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.21428571428571427, 1.0, nan, nan, nan, nan]
2019-02-19T19:56:58.393441: step 800, loss 2.21139, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.2222222222222222, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:58.543625: step 801, loss 1.90452, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:56:58.696779: step 802, loss 1.97265, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5454545454545454, 0.5, nan, nan, nan, nan]
2019-02-19T19:56:58.851671: step 803, loss 1.77854, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, 0.0, 0.36363636363636365, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:59.001346: step 804, loss 1.9423, accuracy 0.5, precision [0.3333333333333333, 1.0, 0.0, 1.0, nan, 0.0, nan, nan, 0.0], recall [1.0, 0.2, nan, 0.6, nan, nan, nan, nan, nan]
2019-02-19T19:56:59.145455: step 805, loss 2.02752, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, 0.0, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:56:59.297832: step 806, loss 2.02293, accuracy 0.3125, precision [0.0, 1.0, nan, 0.6666666666666666, 0.0, 0.0, nan, 0.2, 0.0], recall [nan, 0.5, nan, 0.18181818181818182, nan, nan, nan, 1.0, nan]
2019-02-19T19:56:59.450079: step 807, loss 1.83146, accuracy 0.4375, precision [0.5, 0.5, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 1.0, 0.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:56:59.601003: step 808, loss 1.9682, accuracy 0.1875, precision [0.5, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.15384615384615385, nan, nan, nan, nan, nan]
2019-02-19T19:56:59.751225: step 809, loss 2.10236, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:56:59.901566: step 810, loss 1.99988, accuracy 0.0625, precision [0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.0, nan, nan, nan, nan, nan]
2019-02-19T19:57:00.049407: step 811, loss 1.73267, accuracy 0.5, precision [0.0, 0.5, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:57:00.203005: step 812, loss 1.72372, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2, nan], recall [0.0, 1.0, nan, 0.3, nan, nan, nan, 0.5, nan]
2019-02-19T19:57:00.352551: step 813, loss 2.07221, accuracy 0.3125, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, 0.0, nan, 0.0], recall [0.0, 1.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:00.502116: step 814, loss 1.93699, accuracy 0.3125, precision [nan, 1.0, 0.0, 0.8, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.5, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:57:00.650497: step 815, loss 1.93078, accuracy 0.5, precision [nan, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5833333333333334, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:00.802457: step 816, loss 1.87337, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.25, nan, 0.2222222222222222, nan, nan, nan, nan, nan]
2019-02-19T19:57:00.950365: step 817, loss 1.81063, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:01.100977: step 818, loss 2.17362, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 1.0, 0.0], recall [0.0, 0.5, nan, 0.18181818181818182, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:01.250281: step 819, loss 1.86763, accuracy 0.5, precision [nan, 0.4, 0.5, 0.8, nan, nan, nan, 0.25, nan], recall [0.0, 0.6666666666666666, 1.0, 0.4, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:01.397418: step 820, loss 1.87937, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.3333333333333333, nan], recall [nan, nan, nan, 0.2, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:01.551638: step 821, loss 2.15964, accuracy 0.25, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:57:01.699769: step 822, loss 1.58589, accuracy 0.5625, precision [0.5, nan, 0.0, 0.7777777777777778, 0.25, nan, nan, nan, nan], recall [1.0, 0.0, nan, 0.7, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:01.852284: step 823, loss 1.66576, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan], recall [nan, 0.25, 0.0, 0.375, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:57:02.001845: step 824, loss 1.94024, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [nan, 0.3333333333333333, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:57:02.158955: step 825, loss 1.90558, accuracy 0.5, precision [nan, 1.0, nan, 0.75, 1.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.2, nan, 0.6666666666666666, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:02.317309: step 826, loss 1.64519, accuracy 0.5, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 1.0, nan, 0.5384615384615384, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:02.468202: step 827, loss 1.74602, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.5, 0.5, nan, nan, 0.0, nan], recall [nan, 1.0, 0.0, 0.3, 0.25, nan, nan, nan, nan]
2019-02-19T19:57:02.620192: step 828, loss 1.78794, accuracy 0.375, precision [0.0, 0.5, nan, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:02.775121: step 829, loss 1.77947, accuracy 0.375, precision [1.0, 0.0, nan, 0.8, 0.0, nan, nan, 0.5, 0.0], recall [1.0, 0.0, nan, 0.36363636363636365, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:57:02.931310: step 830, loss 1.48741, accuracy 0.375, precision [0.0, 0.75, 0.0, 1.0, 0.25, nan, nan, 0.3333333333333333, nan], recall [nan, 0.75, nan, 0.1, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:03.083797: step 831, loss 1.62672, accuracy 0.375, precision [0.0, 0.6, 0.0, 1.0, 0.0, nan, 0.0, nan, nan], recall [nan, 0.75, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:03.236220: step 832, loss 2.17041, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:03.387991: step 833, loss 1.74039, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.5, nan], recall [nan, 0.0, nan, 0.35714285714285715, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:03.537310: step 834, loss 2.0679, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:57:03.691080: step 835, loss 1.58972, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.25, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.42857142857142855, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:03.845318: step 836, loss 2.01908, accuracy 0.25, precision [0.0, 0.16666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 1.0, nan], recall [nan, 0.5, nan, 0.15384615384615385, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:03.998990: step 837, loss 2.17495, accuracy 0.4375, precision [0.25, 0.0, nan, 1.0, 0.5, 0.0, nan, 0.0, 0.0], recall [0.5, nan, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:04.149578: step 838, loss 1.60961, accuracy 0.4375, precision [nan, 0.5, 0.0, 0.8, 0.3333333333333333, 0.0, nan, nan, nan], recall [nan, 0.5, nan, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:04.303691: step 839, loss 1.91439, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.8, 0.0, nan, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.2857142857142857, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:04.452734: step 840, loss 1.75433, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:04.605630: step 841, loss 1.66627, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.75, 0.5, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.375, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:04.757923: step 842, loss 1.60822, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.0, nan, nan, 0.5, nan], recall [0.0, 1.0, nan, 0.2727272727272727, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:57:04.912935: step 843, loss 1.82624, accuracy 0.3125, precision [0.0, 0.0, 1.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.0, 1.0, 0.375, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:57:05.068722: step 844, loss 1.7333, accuracy 0.375, precision [0.3333333333333333, 0.3333333333333333, nan, 0.75, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.25, nan, 0.375, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:57:05.217720: step 845, loss 1.59718, accuracy 0.5625, precision [nan, nan, 0.0, 1.0, 0.5, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.6666666666666666, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:57:05.368966: step 846, loss 1.55472, accuracy 0.5625, precision [1.0, 0.5, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.0, 0.0], recall [1.0, 1.0, nan, 0.45454545454545453, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:05.520077: step 847, loss 2.3364, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.5, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [nan, 0.4, nan, 0.1111111111111111, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:05.670158: step 848, loss 1.96522, accuracy 0.3125, precision [0.3333333333333333, 0.25, nan, 0.6, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, 1.0, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:05.822445: step 849, loss 1.58705, accuracy 0.5625, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:05.972043: step 850, loss 1.9661, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.4, 0.0, nan, 0.0, nan], recall [nan, nan, 0.0, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:06.127613: step 851, loss 1.60328, accuracy 0.5625, precision [1.0, 0.4, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:06.279934: step 852, loss 1.65825, accuracy 0.4375, precision [0.0, nan, 0.0, 0.875, 0.0, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.4666666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:06.432755: step 853, loss 1.64746, accuracy 0.5625, precision [nan, 1.0, nan, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:57:06.584889: step 854, loss 1.80765, accuracy 0.625, precision [0.0, 0.75, nan, 0.8571428571428571, nan, 0.0, 0.0, 1.0, 0.0], recall [0.0, 1.0, nan, 0.75, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:57:06.734962: step 855, loss 1.88088, accuracy 0.375, precision [0.3333333333333333, nan, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, nan, nan], recall [1.0, nan, nan, 0.38461538461538464, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:06.886077: step 856, loss 2.35602, accuracy 0.125, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, nan, nan, 0.0], recall [0.0, 0.5, nan, 0.08333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:07.038276: step 857, loss 1.80361, accuracy 0.1875, precision [0.0, 0.25, nan, 1.0, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.25, nan, 0.2, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:07.190026: step 858, loss 1.61436, accuracy 0.5, precision [0.5, 0.5, 0.0, 0.8, 0.0, nan, nan, 0.3333333333333333, nan], recall [0.6666666666666666, 0.3333333333333333, nan, 0.4444444444444444, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:07.339743: step 859, loss 1.84637, accuracy 0.25, precision [0.0, 0.3333333333333333, nan, 0.75, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.25, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:07.488108: step 860, loss 2.01516, accuracy 0.375, precision [0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:57:07.636853: step 861, loss 2.12485, accuracy 0.1875, precision [0.0, 0.3333333333333333, nan, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.25, nan, 0.18181818181818182, nan, nan, nan, nan, nan]
2019-02-19T19:57:07.785159: step 862, loss 1.71396, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.5714285714285714, 0.0, nan, nan, 1.0, nan], recall [1.0, 0.0, nan, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:57:07.934529: step 863, loss 1.93134, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:08.082319: step 864, loss 2.11746, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.14285714285714285, nan, nan, nan, nan, nan]
2019-02-19T19:57:08.233710: step 865, loss 1.88569, accuracy 0.3125, precision [0.0, nan, nan, 0.625, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.45454545454545453, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:08.388882: step 866, loss 1.91371, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, nan, 0.0, 1.0, 0.0], recall [0.5, nan, nan, 0.16666666666666666, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:08.542589: step 867, loss 1.77349, accuracy 0.4375, precision [0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.36363636363636365, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:08.694331: step 868, loss 2.08899, accuracy 0.125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, nan], recall [nan, 0.0, nan, 0.07692307692307693, nan, nan, nan, 1.0, 0.0]
2019-02-19T19:57:08.842564: step 869, loss 1.7951, accuracy 0.5, precision [nan, 1.0, 0.0, 1.0, 0.2, 0.0, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:08.991841: step 870, loss 1.7892, accuracy 0.3125, precision [0.5, 0.0, 0.0, 0.5, nan, nan, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:09.144638: step 871, loss 1.72036, accuracy 0.5, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:57:09.294463: step 872, loss 1.86089, accuracy 0.375, precision [0.6666666666666666, 0.5, 0.0, 0.6, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, 0.5, nan, 0.2727272727272727, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:09.442744: step 873, loss 1.66988, accuracy 0.5, precision [nan, 0.5714285714285714, nan, 0.8, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 0.8, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:57:09.596075: step 874, loss 1.95738, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:57:09.748264: step 875, loss 1.82004, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan, 0.0], recall [nan, 0.0, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:09.894305: step 876, loss 1.75905, accuracy 0.375, precision [nan, nan, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.42857142857142855, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:10.047591: step 877, loss 1.79378, accuracy 0.25, precision [0.0, nan, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:10.198514: step 878, loss 2.05886, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:10.344853: step 879, loss 1.57638, accuracy 0.25, precision [0.0, 0.2, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:57:10.497917: step 880, loss 1.82375, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 0.5, nan, 0.0, 1.0, 0.0], recall [nan, nan, nan, 0.42857142857142855, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:10.652770: step 881, loss 1.94078, accuracy 0.3125, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, 0.0, nan, nan], recall [0.5, nan, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:10.803361: step 882, loss 1.62489, accuracy 0.5625, precision [0.5, nan, 0.0, 0.75, 0.3333333333333333, nan, 0.0, 1.0, nan], recall [1.0, 0.0, nan, 0.5454545454545454, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:10.952170: step 883, loss 1.81627, accuracy 0.5625, precision [1.0, nan, nan, 1.0, nan, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:11.106809: step 884, loss 1.98217, accuracy 0.25, precision [0.0, 0.3333333333333333, nan, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.5, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:11.251737: step 885, loss 1.95407, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [nan, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:11.403524: step 886, loss 2.09916, accuracy 0.375, precision [0.5, 0.4, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.6666666666666666, 0.0, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:11.554014: step 887, loss 2.13528, accuracy 0.3125, precision [0.0, 0.5, nan, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.6666666666666666, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:57:11.702687: step 888, loss 1.95215, accuracy 0.25, precision [nan, 0.0, nan, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:11.850499: step 889, loss 1.76259, accuracy 0.375, precision [0.75, 0.0, 0.0, 0.5, 0.3333333333333333, 0.0, nan, nan, 0.0], recall [1.0, 0.0, nan, 0.2222222222222222, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:12.003716: step 890, loss 1.5859, accuracy 0.5625, precision [1.0, 0.3333333333333333, 0.0, 0.875, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:12.149503: step 891, loss 1.61189, accuracy 0.5625, precision [0.5, 0.5, 0.0, 0.875, 0.0, nan, nan, nan, nan], recall [1.0, 1.0, nan, 0.5384615384615384, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:12.303744: step 892, loss 1.78116, accuracy 0.375, precision [0.75, 0.3333333333333333, nan, 0.5, 0.0, nan, 0.0, 0.0, 0.0], recall [0.75, 0.25, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:12.457480: step 893, loss 1.57854, accuracy 0.5, precision [1.0, 0.25, 0.0, 0.6666666666666666, 1.0, 0.0, nan, 0.5, nan], recall [0.3333333333333333, 0.5, nan, 0.5, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:57:12.610139: step 894, loss 1.74428, accuracy 0.5, precision [nan, 0.0, 0.0, 0.8888888888888888, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.5714285714285714, nan, nan, nan, nan, nan]
2019-02-19T19:57:12.763308: step 895, loss 1.73364, accuracy 0.375, precision [nan, 0.25, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:57:12.917745: step 896, loss 1.86753, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 0.0, nan, nan, 0.3333333333333333, nan], recall [nan, nan, nan, 0.3333333333333333, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:13.066766: step 897, loss 2.07525, accuracy 0.375, precision [nan, 1.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 1.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:57:13.216987: step 898, loss 1.40323, accuracy 0.4375, precision [0.0, 0.8333333333333334, 0.0, 0.3333333333333333, 0.25, nan, nan, nan, nan], recall [0.0, 0.7142857142857143, nan, 0.14285714285714285, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:13.366592: step 899, loss 1.61248, accuracy 0.5, precision [nan, nan, 0.0, 0.75, 0.0, nan, 0.0, 0.5, nan], recall [nan, 0.0, nan, 0.5454545454545454, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:13.522429: step 900, loss 1.90494, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, nan, nan], recall [1.0, 0.0, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:57:13.668040: step 901, loss 1.44067, accuracy 0.625, precision [nan, 0.3333333333333333, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.6, nan, nan, nan, nan, nan]
2019-02-19T19:57:13.812630: step 902, loss 1.80528, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, nan, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:13.960044: step 903, loss 1.9067, accuracy 0.4375, precision [0.0, nan, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.4666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:57:14.116085: step 904, loss 1.76158, accuracy 0.5, precision [0.0, 0.25, 0.0, 1.0, 1.0, 0.0, nan, nan, 0.0], recall [nan, 0.5, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:14.266464: step 905, loss 1.3765, accuracy 0.6875, precision [nan, 1.0, 0.0, 1.0, 0.0, nan, nan, nan, 0.0], recall [nan, 1.0, nan, 0.6428571428571429, nan, nan, nan, nan, nan]
2019-02-19T19:57:14.415310: step 906, loss 1.60142, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:14.563443: step 907, loss 1.63216, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:14.711755: step 908, loss 1.67631, accuracy 0.4375, precision [0.3333333333333333, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:57:14.861940: step 909, loss 1.87345, accuracy 0.375, precision [nan, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:15.016195: step 910, loss 2.00894, accuracy 0.25, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, nan, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:57:15.170009: step 911, loss 2.2171, accuracy 0.25, precision [1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.3333333333333333, 1.0, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:57:15.323022: step 912, loss 1.6421, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.8333333333333334, 0.0, nan, nan, nan, nan], recall [nan, 0.5, nan, 0.38461538461538464, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:15.473017: step 913, loss 1.72299, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [0.5, nan, nan, 0.3333333333333333, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:15.622929: step 914, loss 1.95541, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.2857142857142857, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:15.775721: step 915, loss 2.04169, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.2, 0.0, nan, 1.0, nan], recall [0.0, nan, nan, 0.3, 1.0, nan, nan, 0.25, nan]
2019-02-19T19:57:15.924424: step 916, loss 1.81483, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.35714285714285715, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:16.075405: step 917, loss 1.90967, accuracy 0.25, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, nan, 0.5, nan], recall [0.0, nan, nan, 0.25, nan, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:57:16.225091: step 918, loss 1.99037, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.8, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.2857142857142857, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:16.377647: step 919, loss 1.90665, accuracy 0.375, precision [0.0, 0.3333333333333333, nan, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:16.524981: step 920, loss 1.78836, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.75, 0.5, 0.0, nan, 0.0, 0.0], recall [0.6666666666666666, nan, nan, 0.2727272727272727, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:16.674627: step 921, loss 1.83427, accuracy 0.3125, precision [0.25, 0.0, nan, 0.6666666666666666, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.16666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:16.824692: step 922, loss 1.76361, accuracy 0.5, precision [0.0, 0.0, nan, 0.875, 0.0, 0.0, nan, 0.5, nan], recall [nan, nan, nan, 0.5833333333333334, nan, nan, nan, 0.25, nan]
2019-02-19T19:57:16.978662: step 923, loss 1.86617, accuracy 0.3125, precision [0.5, 0.25, 0.0, 0.75, 0.0, nan, 0.0, nan, nan], recall [1.0, 1.0, nan, 0.2727272727272727, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:17.125357: step 924, loss 1.67353, accuracy 0.375, precision [0.0, nan, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:17.277483: step 925, loss 1.83085, accuracy 0.4375, precision [0.3333333333333333, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [0.5, 1.0, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:17.423129: step 926, loss 1.71908, accuracy 0.5625, precision [nan, 0.0, nan, 0.8888888888888888, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.7272727272727273, 0.25, nan, nan, nan, nan]
2019-02-19T19:57:17.576673: step 927, loss 1.80779, accuracy 0.3125, precision [0.0, nan, 0.0, 0.625, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:17.725397: step 928, loss 1.64498, accuracy 0.375, precision [0.5, nan, nan, 0.8, 0.0, nan, nan, 0.2, nan], recall [0.3333333333333333, 0.0, nan, 0.4, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:57:17.879487: step 929, loss 1.60855, accuracy 0.5625, precision [nan, 0.0, 0.0, 1.0, 0.5, nan, nan, 0.5, nan], recall [0.0, nan, nan, 0.5384615384615384, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:18.032689: step 930, loss 2.40233, accuracy 0.3125, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.5, nan, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:18.185484: step 931, loss 2.302, accuracy 0.0625, precision [0.0, 0.0, nan, 0.25, 0.0, 0.0, 0.0, nan, nan], recall [0.0, nan, nan, 0.08333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:18.336121: step 932, loss 2.35829, accuracy 0.1875, precision [nan, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, nan, 0.0], recall [0.0, 0.3333333333333333, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:18.487953: step 933, loss 1.68593, accuracy 0.3125, precision [nan, 0.2, 0.0, 0.75, 0.25, 0.0, 0.0, nan, nan], recall [0.0, 0.5, nan, 0.2727272727272727, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:18.636751: step 934, loss 1.77857, accuracy 0.25, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:57:18.792225: step 935, loss 1.73795, accuracy 0.375, precision [nan, 0.5, 0.0, 0.5714285714285714, 0.5, nan, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.4444444444444444, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:57:18.944645: step 936, loss 2.02384, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:19.095780: step 937, loss 2.06146, accuracy 0.25, precision [1.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:19.240180: step 938, loss 2.04857, accuracy 0.125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.15384615384615385, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:19.392758: step 939, loss 1.62377, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:19.544760: step 940, loss 1.86791, accuracy 0.1875, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.15384615384615385, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:19.692779: step 941, loss 1.82004, accuracy 0.4375, precision [0.5, 1.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.5, 0.5, nan, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:57:19.843176: step 942, loss 1.85799, accuracy 0.4375, precision [0.0, 0.0, nan, 0.8333333333333334, 1.0, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.4166666666666667, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:20.001040: step 943, loss 1.57961, accuracy 0.25, precision [0.0, 0.3333333333333333, nan, 0.6666666666666666, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.0, 0.25, nan, 0.2222222222222222, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:20.146116: step 944, loss 1.96654, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, nan, nan, 0.0, 0.5, 0.0], recall [nan, nan, nan, 0.4666666666666667, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:20.296197: step 945, loss 1.98477, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 0.25, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:20.445130: step 946, loss 1.90847, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.8, 0.25, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.2857142857142857, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:20.589436: step 947, loss 1.64605, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.25, nan, 0.0, 0.5, nan], recall [nan, 0.0, nan, 0.45454545454545453, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:57:20.745765: step 948, loss 2.01337, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.42857142857142855, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.2, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:20.898589: step 949, loss 1.69594, accuracy 0.5, precision [0.3333333333333333, 1.0, nan, 0.8, 0.6666666666666666, nan, 0.0, 0.0, 0.0], recall [0.5, 0.5, nan, 0.5, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:57:21.045231: step 950, loss 1.70899, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, nan, nan], recall [nan, 1.0, nan, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:21.201451: step 951, loss 1.70902, accuracy 0.4375, precision [0.5, 0.25, nan, 0.8, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.36363636363636365, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:21.350693: step 952, loss 1.81203, accuracy 0.375, precision [nan, 0.3333333333333333, nan, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:21.500604: step 953, loss 2.07551, accuracy 0.25, precision [0.0, nan, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, nan], recall [nan, 0.0, nan, 0.1111111111111111, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:57:21.649193: step 954, loss 1.66369, accuracy 0.4375, precision [0.5, 0.2, 0.0, 1.0, 0.0, 0.0, nan, 0.5, nan], recall [0.5, 1.0, nan, 0.4, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:57:21.798377: step 955, loss 1.96107, accuracy 0.25, precision [0.16666666666666666, 1.0, 0.0, 0.3333333333333333, nan, 0.0, 0.0, 0.5, nan], recall [0.3333333333333333, 0.5, nan, 0.16666666666666666, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:57:21.950342: step 956, loss 1.74854, accuracy 0.4375, precision [0.25, 0.5, 0.0, 0.75, 1.0, 0.0, 0.0, nan, nan], recall [0.5, 0.5, nan, 0.375, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:22.099968: step 957, loss 2.2196, accuracy 0.125, precision [0.0, 0.5, 0.0, 0.25, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.1111111111111111, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:22.249192: step 958, loss 1.93393, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5384615384615384, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:22.400786: step 959, loss 1.81482, accuracy 0.4375, precision [0.3333333333333333, 1.0, nan, 0.6, 0.0, nan, 0.0, 0.5, 0.0], recall [0.5, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:57:22.550306: step 960, loss 2.20703, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.16666666666666666, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:22.695329: step 961, loss 1.87057, accuracy 0.25, precision [nan, 0.0, nan, 0.5714285714285714, 0.0, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:22.846599: step 962, loss 1.70394, accuracy 0.5, precision [0.5, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0], recall [1.0, 0.4, nan, 0.2857142857142857, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:23.002488: step 963, loss 1.93869, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, nan, nan], recall [0.3333333333333333, nan, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:57:23.157226: step 964, loss 1.75031, accuracy 0.5625, precision [0.5, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:57:23.309525: step 965, loss 1.67466, accuracy 0.4375, precision [0.0, nan, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:57:23.461126: step 966, loss 1.72879, accuracy 0.375, precision [1.0, 1.0, 0.0, 0.75, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:23.609979: step 967, loss 1.92841, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, nan], recall [0.3333333333333333, 0.0, nan, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:23.762630: step 968, loss 1.86238, accuracy 0.375, precision [0.0, 0.2, 0.0, 1.0, 0.0, nan, 0.0, nan, nan], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:57:23.916024: step 969, loss 2.00578, accuracy 0.1875, precision [0.0, nan, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:57:24.070505: step 970, loss 2.14883, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, nan], recall [0.0, 1.0, nan, 0.18181818181818182, nan, nan, nan, 0.5, nan]
2019-02-19T19:57:24.219684: step 971, loss 2.01585, accuracy 0.125, precision [nan, 0.0, nan, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:57:24.376398: step 972, loss 1.80106, accuracy 0.3125, precision [0.25, 0.0, 0.0, 0.8, 0.0, nan, nan, nan, nan], recall [0.5, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:24.531081: step 973, loss 1.76528, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:57:24.676009: step 974, loss 1.82704, accuracy 0.375, precision [1.0, 0.25, nan, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 1.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:24.831087: step 975, loss 1.8547, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.2, 0.0], recall [0.0, 1.0, nan, 0.2, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:57:24.981828: step 976, loss 1.76843, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.3333333333333333, nan], recall [nan, 1.0, nan, 0.35714285714285715, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:25.133958: step 977, loss 2.04594, accuracy 0.3125, precision [nan, 0.4, 0.0, 1.0, 0.0, 0.0, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:25.288648: step 978, loss 2.0764, accuracy 0.4375, precision [0.0, nan, 0.0, 0.875, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:57:25.447344: step 979, loss 1.79522, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, nan, 0.0, nan, nan], recall [0.0, 0.3333333333333333, nan, 0.5454545454545454, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:25.596763: step 980, loss 2.05357, accuracy 0.25, precision [0.0, nan, 0.0, 0.8, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:25.748633: step 981, loss 1.82671, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 1.0, nan, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:57:25.903031: step 982, loss 1.71988, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:26.051418: step 983, loss 1.65896, accuracy 0.4375, precision [0.0, 0.5, nan, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:57:26.201614: step 984, loss 1.69557, accuracy 0.375, precision [1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.5, 0.0], recall [1.0, 1.0, nan, 0.23076923076923078, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:26.355568: step 985, loss 1.83717, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.8888888888888888, nan, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.6153846153846154, nan, nan, nan, nan, nan]
2019-02-19T19:57:26.508296: step 986, loss 1.63003, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:57:26.664684: step 987, loss 1.77631, accuracy 0.375, precision [0.0, nan, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:26.810569: step 988, loss 2.01495, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:26.967722: step 989, loss 1.67626, accuracy 0.4375, precision [0.0, 0.25, 0.0, 0.8333333333333334, 0.0, nan, nan, 1.0, 0.0], recall [nan, 0.3333333333333333, nan, 0.45454545454545453, nan, nan, nan, 0.5, nan]
2019-02-19T19:57:27.122619: step 990, loss 2.0347, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, 0.0, nan, nan, nan, 0.0], recall [1.0, 0.0, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:57:27.275978: step 991, loss 1.63613, accuracy 0.5, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, nan, nan, nan], recall [nan, nan, nan, 0.5333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:27.428487: step 992, loss 2.0219, accuracy 0.1875, precision [0.0, 0.14285714285714285, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.15384615384615385, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:27.578508: step 993, loss 1.62354, accuracy 0.375, precision [0.0, nan, nan, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [nan, nan, nan, 0.375, nan, nan, nan, nan, nan]
2019-02-19T19:57:27.727614: step 994, loss 2.07527, accuracy 0.125, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.08333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:27.881991: step 995, loss 1.79671, accuracy 0.4375, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.46153846153846156, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:28.033839: step 996, loss 2.00427, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [nan, nan, nan, 0.21428571428571427, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:28.187890: step 997, loss 1.78513, accuracy 0.375, precision [0.25, 0.5, nan, 1.0, 0.2, 0.0, nan, nan, nan], recall [0.5, 1.0, nan, 0.2727272727272727, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:28.337679: step 998, loss 1.82746, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.45454545454545453, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:57:28.483149: step 999, loss 2.06642, accuracy 0.4375, precision [nan, 0.25, 0.0, 1.0, nan, 0.0, nan, nan, 0.0], recall [nan, 1.0, nan, 0.46153846153846156, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:28.639366: step 1000, loss 1.6738, accuracy 0.4375, precision [0.0, nan, nan, 0.875, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.5, nan, nan, nan, 0.0, nan]

Evaluation:
[[ 19  14   0  52   0   0   0   4   0]
 [  6  33   0 101   0   0   0   4   0]
 [  1   0   0  85   1   0   0   0   0]
 [  3  13   0 328   1   0   0   0   0]
 [  2   3   0 143  16   0   0   0   0]
 [  1   2   0  47   1   0   0   2   0]
 [  0   0   0  22   0   0   0   0   0]
 [  4   4   0  80   1   0   0  14   0]
 [  1   0   0  17   0   0   0   0   0]]
2019-02-19T19:57:31.066646: step 1000, loss 1.76312, accuracy 0.4, precision [0.21348314606741572, 0.22916666666666666, 0.0, 0.9507246376811594, 0.0975609756097561, 0.0, 0.0, 0.13592233009708737, 0.0], recall [0.5135135135135135, 0.4782608695652174, nan, 0.37485714285714283, 0.8, nan, nan, 0.5833333333333334, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550606084/checkpoints/model-1000

2019-02-19T19:57:31.296494: step 1001, loss 1.9547, accuracy 0.3125, precision [0.3333333333333333, 0.0, nan, 1.0, 0.0, nan, 0.0, 0.25, nan], recall [0.5, nan, nan, 0.23076923076923078, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:31.449444: step 1002, loss 1.78934, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:57:31.598022: step 1003, loss 1.94012, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, nan, nan], recall [nan, nan, nan, 0.3125, nan, nan, nan, nan, nan]
2019-02-19T19:57:31.751437: step 1004, loss 1.61139, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.0, 0.875, nan, 0.0, nan, nan, nan], recall [nan, 0.3333333333333333, nan, 0.5384615384615384, nan, nan, nan, nan, nan]
2019-02-19T19:57:31.902413: step 1005, loss 1.92126, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.6666666666666666, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:57:32.051277: step 1006, loss 1.84797, accuracy 0.3125, precision [0.0, 1.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:32.199440: step 1007, loss 1.53366, accuracy 0.375, precision [0.3333333333333333, nan, 0.0, 0.7142857142857143, 0.0, nan, nan, nan, nan], recall [1.0, 0.0, nan, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:57:32.348599: step 1008, loss 1.67579, accuracy 0.5, precision [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [1.0, 0.5, nan, 0.36363636363636365, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:32.501035: step 1009, loss 1.72574, accuracy 0.4375, precision [nan, 0.6666666666666666, 0.0, 0.8333333333333334, nan, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:57:32.654552: step 1010, loss 1.52541, accuracy 0.625, precision [0.5, 1.0, nan, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:57:32.802697: step 1011, loss 1.88401, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.46153846153846156, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:32.948629: step 1012, loss 2.0396, accuracy 0.25, precision [nan, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.5, 0.0], recall [nan, 0.0, nan, 0.16666666666666666, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:33.105722: step 1013, loss 2.02756, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, nan, 0.0, 0.0, nan, nan], recall [0.0, nan, nan, 0.3076923076923077, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:33.253719: step 1014, loss 1.59373, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.46153846153846156, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:33.403720: step 1015, loss 1.71898, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.2, nan, 0.6666666666666666, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:33.554825: step 1016, loss 2.0995, accuracy 0.3125, precision [nan, 0.5, nan, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, nan, 0.0], recall [nan, 0.5, nan, 0.2, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:33.705997: step 1017, loss 1.85438, accuracy 0.3125, precision [0.5, 0.5, 0.0, 0.6666666666666666, 0.3333333333333333, nan, 0.0, 0.0, 0.0], recall [1.0, 0.5, nan, 0.18181818181818182, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:33.851603: step 1018, loss 2.06266, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:34.004243: step 1019, loss 1.81364, accuracy 0.3125, precision [0.0, 0.4, nan, 0.6666666666666666, 0.0, nan, 0.0, 0.5, nan], recall [0.0, 0.4, nan, 0.2857142857142857, nan, nan, nan, 0.5, nan]
2019-02-19T19:57:34.153130: step 1020, loss 1.66978, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8571428571428571, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.46153846153846156, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:34.302018: step 1021, loss 1.66006, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.5714285714285714, 0.5, nan, nan, nan, 0.0], recall [nan, 0.25, nan, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:34.446988: step 1022, loss 1.76451, accuracy 0.4375, precision [0.5, 0.6, nan, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.75, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:34.598849: step 1023, loss 2.14512, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.5714285714285714, nan, 0.0, 0.0, nan, 0.0], recall [0.0, 0.3333333333333333, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:34.747523: step 1024, loss 1.95772, accuracy 0.375, precision [0.5, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.5, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:34.895957: step 1025, loss 1.91833, accuracy 0.25, precision [0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.16666666666666666, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:35.042478: step 1026, loss 1.59524, accuracy 0.5, precision [0.25, 1.0, nan, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:35.191069: step 1027, loss 1.7614, accuracy 0.5, precision [nan, nan, 0.0, 0.8888888888888888, 0.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.0, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:35.343758: step 1028, loss 1.97953, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:57:35.491348: step 1029, loss 1.73688, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:57:35.648463: step 1030, loss 1.78659, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:57:35.799001: step 1031, loss 1.81907, accuracy 0.125, precision [0.25, 0.25, 0.0, 0.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.0, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:35.952836: step 1032, loss 1.80379, accuracy 0.375, precision [0.5, 0.4, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.5, 0.6666666666666666, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:36.103972: step 1033, loss 1.74733, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [nan, nan, nan, 0.3125, nan, nan, nan, nan, nan]
2019-02-19T19:57:36.254831: step 1034, loss 1.72511, accuracy 0.5, precision [0.5, 1.0, nan, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.5454545454545454, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:36.406560: step 1035, loss 1.6017, accuracy 0.5, precision [0.0, 0.0, nan, 0.875, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.5384615384615384, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:36.560594: step 1036, loss 2.04566, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.2, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:36.710522: step 1037, loss 1.61708, accuracy 0.5, precision [0.0, 0.5, nan, 1.0, 0.25, nan, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:36.860298: step 1038, loss 1.9194, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:37.008827: step 1039, loss 1.644, accuracy 0.4375, precision [nan, 0.3333333333333333, 0.0, 1.0, nan, nan, nan, nan, nan], recall [nan, 1.0, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:57:37.160428: step 1040, loss 1.76236, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:37.305628: step 1041, loss 1.78422, accuracy 0.375, precision [0.5, 0.3333333333333333, 0.0, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:37.456602: step 1042, loss 2.25485, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:37.611333: step 1043, loss 1.69024, accuracy 0.5, precision [0.0, 0.75, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.75, nan, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:57:37.763931: step 1044, loss 1.65977, accuracy 0.3125, precision [0.0, 0.6666666666666666, nan, 0.75, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 0.6666666666666666, nan, 0.2727272727272727, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:37.910868: step 1045, loss 2.13246, accuracy 0.25, precision [0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.5, 0.5, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:57:38.057901: step 1046, loss 2.10702, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [nan, 0.2, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:38.212465: step 1047, loss 2.08596, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.5, nan, 0.15384615384615385, nan, nan, nan, nan, nan]
2019-02-19T19:57:38.363445: step 1048, loss 2.10846, accuracy 0.125, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.07692307692307693, nan, nan, nan, nan, nan]
2019-02-19T19:57:38.515028: step 1049, loss 1.61555, accuracy 0.4375, precision [0.25, 1.0, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.6666666666666666, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:57:38.666795: step 1050, loss 1.42499, accuracy 0.6875, precision [1.0, 1.0, 0.3333333333333333, 1.0, nan, nan, 0.0, 0.0, nan], recall [1.0, 1.0, 1.0, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:38.816420: step 1051, loss 1.92841, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.15384615384615385, nan, nan, nan, nan, nan]
2019-02-19T19:57:38.963402: step 1052, loss 1.82836, accuracy 0.5, precision [0.5, 0.0, 1.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [1.0, nan, 1.0, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:57:39.120908: step 1053, loss 1.94733, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.3, nan, nan, nan, nan, nan]
2019-02-19T19:57:39.267376: step 1054, loss 1.88597, accuracy 0.375, precision [0.5, 0.0, 0.3333333333333333, 1.0, 0.0, nan, 0.0, nan, nan], recall [1.0, 0.0, 1.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:39.416670: step 1055, loss 1.8865, accuracy 0.375, precision [0.5, 0.5, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.3333333333333333, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:39.566016: step 1056, loss 1.87076, accuracy 0.3125, precision [0.5, 0.6666666666666666, 0.0, 1.0, 0.0, nan, 0.0, nan, nan], recall [1.0, 0.5, 0.0, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:57:39.723387: step 1057, loss 2.12406, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:57:39.879642: step 1058, loss 1.89921, accuracy 0.3125, precision [0.25, nan, nan, 1.0, 0.0, 0.0, nan, nan, 0.0], recall [1.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:40.030150: step 1059, loss 1.68303, accuracy 0.5, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, 0.0, 0.5714285714285714, nan, nan, nan, nan, nan]
2019-02-19T19:57:40.176387: step 1060, loss 2.1955, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.5, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [1.0, nan, 1.0, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:40.326129: step 1061, loss 1.68504, accuracy 0.4375, precision [0.0, 0.5, nan, 0.8571428571428571, 0.0, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, 0.0, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:57:40.477163: step 1062, loss 1.90825, accuracy 0.1875, precision [0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [0.0, 0.5, 0.0, 0.0, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:40.631785: step 1063, loss 2.00793, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.8333333333333334, 0.5, 0.0, 0.0, 0.5, 0.0], recall [0.5, 0.0, nan, 0.5, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:40.781283: step 1064, loss 1.49152, accuracy 0.4375, precision [1.0, nan, 0.0, 0.75, 0.0, nan, nan, nan, nan], recall [1.0, 0.0, nan, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:57:40.927834: step 1065, loss 1.84201, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.6, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:41.077573: step 1066, loss 1.86005, accuracy 0.25, precision [0.3333333333333333, 0.25, 0.0, 0.5, 0.0, nan, nan, nan, 0.0], recall [0.5, 0.2, nan, 0.2222222222222222, nan, nan, nan, nan, nan]
2019-02-19T19:57:41.226737: step 1067, loss 1.85549, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:57:41.383177: step 1068, loss 2.15417, accuracy 0.375, precision [nan, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:41.533538: step 1069, loss 1.89364, accuracy 0.25, precision [1.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [1.0, nan, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:57:41.683903: step 1070, loss 1.7608, accuracy 0.375, precision [1.0, 0.25, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [1.0, 1.0, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:57:41.838159: step 1071, loss 1.76965, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.26666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:57:41.984733: step 1072, loss 2.18414, accuracy 0.125, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.15384615384615385, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:42.128628: step 1073, loss 1.85458, accuracy 0.5625, precision [nan, 0.0, nan, 0.9, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.6, nan, nan, nan, nan, nan]
2019-02-19T19:57:42.284667: step 1074, loss 2.42928, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:42.435230: step 1075, loss 1.92047, accuracy 0.375, precision [nan, 0.3333333333333333, nan, 0.8, 0.0, 0.0, 0.0, nan, nan], recall [0.0, 1.0, nan, 0.3333333333333333, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:42.587155: step 1076, loss 1.77718, accuracy 0.3125, precision [0.6666666666666666, 0.5, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.18181818181818182, nan, nan, nan, nan, nan]
2019-02-19T19:57:42.741872: step 1077, loss 1.88971, accuracy 0.375, precision [1.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:42.893013: step 1078, loss 1.85959, accuracy 0.3125, precision [nan, 0.4, nan, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.6666666666666666, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:43.045675: step 1079, loss 1.76678, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:43.193614: step 1080, loss 1.94336, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, nan, nan, 0.1875, nan, nan, nan, nan, nan]
2019-02-19T19:57:43.338362: step 1081, loss 2.01665, accuracy 0.25, precision [nan, 0.5, 0.0, 1.0, 0.2, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.14285714285714285, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:43.484992: step 1082, loss 1.9862, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:43.636205: step 1083, loss 1.70848, accuracy 0.375, precision [1.0, 0.25, 0.0, 0.75, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.5, 0.5, nan, 0.3, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:43.788493: step 1084, loss 1.72728, accuracy 0.375, precision [0.5, 0.0, nan, 0.5, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.0, 0.0, 0.625, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:43.937234: step 1085, loss 2.03368, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 0.25, 0.25, 0.0, nan, 0.0, nan], recall [0.0, 0.3333333333333333, nan, 0.125, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:57:44.089910: step 1086, loss 1.96012, accuracy 0.25, precision [nan, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.2727272727272727, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:44.237679: step 1087, loss 1.68805, accuracy 0.4375, precision [0.5, nan, 0.0, 0.8, 0.3333333333333333, 0.0, 0.0, nan, nan], recall [0.5, 0.0, 0.0, 0.5714285714285714, 0.4, nan, nan, nan, nan]
2019-02-19T19:57:44.390279: step 1088, loss 1.86897, accuracy 0.3125, precision [nan, 0.4, nan, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:44.539640: step 1089, loss 1.40446, accuracy 0.625, precision [1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.6666666666666666, 1.0, 0.5454545454545454, nan, nan, nan, nan, nan]
2019-02-19T19:57:44.692770: step 1090, loss 2.13463, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.75, 1.0, nan, nan, 0.0, nan], recall [nan, 0.0, 0.0, 0.2727272727272727, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:57:44.843122: step 1091, loss 1.71409, accuracy 0.4375, precision [1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:44.993910: step 1092, loss 2.22525, accuracy 0.25, precision [0.0, nan, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, nan, nan, 0.2857142857142857, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:45.150478: step 1093, loss 1.8295, accuracy 0.3125, precision [0.5, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.3333333333333333, nan], recall [0.5, nan, nan, 0.25, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:57:45.297026: step 1094, loss 1.8407, accuracy 0.4375, precision [0.2, nan, 0.0, 1.0, 1.0, 0.0, nan, 0.0, nan], recall [1.0, nan, nan, 0.35714285714285715, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:45.448725: step 1095, loss 1.71972, accuracy 0.375, precision [0.0, 0.5, 0.5, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.3333333333333333, 1.0, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:45.598438: step 1096, loss 1.64087, accuracy 0.5, precision [nan, 0.4, 0.0, 0.8571428571428571, 0.0, nan, nan, nan, nan], recall [nan, 1.0, nan, 0.46153846153846156, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:45.744620: step 1097, loss 2.19078, accuracy 0.1875, precision [0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, 1.0, 0.15384615384615385, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:45.894820: step 1098, loss 1.93103, accuracy 0.4375, precision [nan, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:57:46.043555: step 1099, loss 1.93721, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.23076923076923078, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:46.191772: step 1100, loss 1.83537, accuracy 0.5625, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, nan, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:46.342490: step 1101, loss 1.95456, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.5, nan], recall [nan, 0.5, nan, 0.23076923076923078, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:46.493115: step 1102, loss 1.87151, accuracy 0.375, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.3333333333333333, 0.0], recall [nan, nan, nan, 0.3333333333333333, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:46.647563: step 1103, loss 2.16382, accuracy 0.25, precision [0.0, 0.25, nan, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:57:46.797907: step 1104, loss 1.69152, accuracy 0.4375, precision [nan, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.42857142857142855, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:46.945053: step 1105, loss 1.78587, accuracy 0.375, precision [nan, 0.25, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:57:47.094160: step 1106, loss 1.54473, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 1.0, nan, nan, 0.3333333333333333, nan], recall [0.0, nan, nan, 0.5, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:57:47.244501: step 1107, loss 1.70754, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.4666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:57:47.393930: step 1108, loss 1.77133, accuracy 0.4375, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.6666666666666666, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:57:47.547415: step 1109, loss 2.18775, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [1.0, nan, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:47.700951: step 1110, loss 1.50553, accuracy 0.5, precision [nan, 0.5, 0.0, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.75, nan, 0.45454545454545453, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:47.850445: step 1111, loss 1.96433, accuracy 0.3125, precision [1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.2, nan, 0.3, nan, nan, nan, nan, nan]
2019-02-19T19:57:47.999158: step 1112, loss 1.90049, accuracy 0.25, precision [0.0, 0.25, 0.0, 0.75, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.5, 0.0, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:48.147391: step 1113, loss 1.36405, accuracy 0.625, precision [0.6666666666666666, 1.0, nan, 1.0, 0.2, nan, 0.0, nan, nan], recall [1.0, 0.5, nan, 0.5454545454545454, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:48.296802: step 1114, loss 2.03825, accuracy 0.375, precision [0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, nan], recall [1.0, 1.0, 0.0, 0.25, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:48.447173: step 1115, loss 1.73988, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 0.0, 0.0], recall [nan, nan, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:48.602247: step 1116, loss 2.04553, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 1.0, 0.0], recall [nan, nan, nan, 0.35714285714285715, nan, nan, nan, 0.5, nan]
2019-02-19T19:57:48.751754: step 1117, loss 1.63006, accuracy 0.4375, precision [0.4, 0.5, nan, 0.75, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [1.0, 0.5, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:48.899915: step 1118, loss 1.86331, accuracy 0.5, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:57:49.050401: step 1119, loss 2.0717, accuracy 0.25, precision [1.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.5, 0.5, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:57:49.198025: step 1120, loss 1.72248, accuracy 0.1875, precision [nan, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 1.0, 0.0, 0.15384615384615385, nan, nan, nan, nan, nan]
2019-02-19T19:57:49.350058: step 1121, loss 1.57847, accuracy 0.4375, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.4666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:57:49.504090: step 1122, loss 1.90782, accuracy 0.3125, precision [nan, nan, 0.0, 0.6666666666666666, 0.25, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:49.653318: step 1123, loss 1.77192, accuracy 0.3125, precision [0.5, nan, 0.0, 0.8, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:49.806433: step 1124, loss 1.54881, accuracy 0.5, precision [0.0, 1.0, 0.0, 1.0, 0.0, nan, nan, nan, nan], recall [nan, 0.3333333333333333, nan, 0.5384615384615384, nan, nan, nan, nan, nan]
2019-02-19T19:57:49.955194: step 1125, loss 2.02915, accuracy 0.375, precision [0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 1.0, nan, 0.0, nan, 0.0, nan], recall [1.0, 0.6666666666666666, 1.0, 0.18181818181818182, nan, nan, nan, nan, nan]
2019-02-19T19:57:50.115685: step 1126, loss 2.04247, accuracy 0.1875, precision [0.5, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.07142857142857142, nan, nan, nan, nan, nan]
2019-02-19T19:57:50.264383: step 1127, loss 1.9332, accuracy 0.25, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.2, nan, nan, nan, nan, nan]
2019-02-19T19:57:50.416595: step 1128, loss 1.9334, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [nan, nan, nan, 0.42857142857142855, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:50.572398: step 1129, loss 1.54207, accuracy 0.5625, precision [nan, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.6153846153846154, nan, nan, nan, nan, nan]
2019-02-19T19:57:50.725249: step 1130, loss 1.89254, accuracy 0.5, precision [nan, nan, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.5714285714285714, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:50.877674: step 1131, loss 1.64662, accuracy 0.5, precision [0.3333333333333333, 0.6, nan, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.36363636363636365, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:51.029659: step 1132, loss 1.69695, accuracy 0.375, precision [nan, 0.5, nan, 0.75, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 1.0, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:51.178360: step 1133, loss 1.68859, accuracy 0.375, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:51.327417: step 1134, loss 1.61109, accuracy 0.375, precision [0.5, 0.5, 0.0, 0.75, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:57:51.472241: step 1135, loss 1.60005, accuracy 0.5625, precision [nan, 0.4, 0.0, 1.0, 0.0, nan, nan, nan, nan], recall [nan, 1.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:57:51.626555: step 1136, loss 1.55476, accuracy 0.5625, precision [0.5, 1.0, 0.0, 1.0, 0.25, 0.0, nan, nan, nan], recall [1.0, 1.0, nan, 0.5, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:51.773419: step 1137, loss 1.81811, accuracy 0.4375, precision [nan, 0.3333333333333333, 0.0, 0.8571428571428571, 0.0, nan, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.46153846153846156, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:51.925433: step 1138, loss 1.88165, accuracy 0.25, precision [0.5, 0.0, 0.0, 1.0, 0.2, 0.0, nan, 0.0, 0.0], recall [0.5, 0.0, nan, 0.18181818181818182, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:52.077796: step 1139, loss 1.71314, accuracy 0.375, precision [0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 0.5, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:52.226648: step 1140, loss 1.7023, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.5, nan], recall [1.0, 0.0, nan, 0.3076923076923077, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:52.377569: step 1141, loss 1.89317, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:52.524369: step 1142, loss 1.7639, accuracy 0.4375, precision [0.0, 0.6666666666666666, nan, 1.0, 0.2, 0.0, 0.0, 0.0, nan], recall [nan, 0.4, nan, 0.4444444444444444, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:52.672396: step 1143, loss 1.69069, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.2, nan, 0.0, nan, nan], recall [nan, nan, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:52.829322: step 1144, loss 1.66341, accuracy 0.375, precision [nan, nan, 0.0, 0.7142857142857143, 0.25, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:52.981537: step 1145, loss 1.87618, accuracy 0.4375, precision [nan, 0.5, nan, 0.8571428571428571, 0.0, 0.0, nan, nan, 0.0], recall [nan, 0.25, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:57:53.126848: step 1146, loss 1.99843, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:53.274187: step 1147, loss 1.87677, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 0.5, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [nan, 0.6666666666666666, nan, 0.1111111111111111, 0.25, nan, nan, nan, nan]
2019-02-19T19:57:53.421793: step 1148, loss 1.75405, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:53.568721: step 1149, loss 1.61332, accuracy 0.5, precision [0.0, 0.6666666666666666, 0.0, 1.0, 0.5, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.2222222222222222, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:53.715706: step 1150, loss 1.97538, accuracy 0.1875, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.25, nan, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:53.866153: step 1151, loss 2.00012, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.18181818181818182, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:54.013682: step 1152, loss 1.88693, accuracy 0.25, precision [nan, 0.5, 0.0, 0.42857142857142855, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.2, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:54.167811: step 1153, loss 2.07247, accuracy 0.3125, precision [0.0, 1.0, 0.0, 0.6, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:54.275787: step 1154, loss 1.93095, accuracy 0.5, precision [0.0, 0.5, nan, 0.75, 1.0, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:54.422777: step 1155, loss 2.02316, accuracy 0.375, precision [nan, 0.0, nan, 0.8571428571428571, nan, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:54.574852: step 1156, loss 1.78035, accuracy 0.375, precision [0.25, 1.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 0.3333333333333333, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:54.728287: step 1157, loss 2.01464, accuracy 0.25, precision [0.0, 0.5, nan, 0.75, 0.0, nan, 0.0, nan, nan], recall [nan, 0.2, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:54.882215: step 1158, loss 1.50685, accuracy 0.5, precision [1.0, 0.6666666666666666, nan, 1.0, 0.2, nan, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.2, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:55.027858: step 1159, loss 2.23422, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, nan, 0.0], recall [0.0, nan, nan, 0.21428571428571427, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:55.171070: step 1160, loss 1.8173, accuracy 0.375, precision [nan, nan, 0.0, 1.0, 0.0, nan, 0.0, nan, 0.0], recall [nan, 0.0, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:57:55.325081: step 1161, loss 1.99207, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.5, 0.0, nan, nan, nan], recall [0.0, 0.0, nan, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:55.477401: step 1162, loss 1.6874, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, nan, nan], recall [nan, 0.0, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:55.624686: step 1163, loss 1.77672, accuracy 0.375, precision [0.0, 1.0, 0.0, 0.8, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.3333333333333333, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:55.779712: step 1164, loss 2.08934, accuracy 0.1875, precision [0.0, 0.2, 0.0, 1.0, 0.0, 0.0, nan, 0.5, nan], recall [0.0, 1.0, nan, 0.08333333333333333, nan, nan, nan, 1.0, nan]
2019-02-19T19:57:55.930880: step 1165, loss 1.80457, accuracy 0.5, precision [0.0, nan, 0.0, 0.8571428571428571, 0.3333333333333333, nan, nan, 0.5, 0.0], recall [0.0, 0.0, nan, 0.5454545454545454, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:57:56.076091: step 1166, loss 1.78341, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.75, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.25, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:57:56.231346: step 1167, loss 1.73023, accuracy 0.25, precision [nan, nan, nan, 0.5, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:56.379993: step 1168, loss 1.7478, accuracy 0.375, precision [nan, nan, 0.5, 0.42857142857142855, 0.5, 0.0, nan, nan, nan], recall [nan, 0.0, 0.16666666666666666, 0.75, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:57:56.530056: step 1169, loss 1.5819, accuracy 0.375, precision [0.0, 1.0, 0.25, 0.75, 0.0, 0.0, nan, 1.0, nan], recall [nan, 1.0, 0.5, 0.375, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:57:56.681186: step 1170, loss 1.86649, accuracy 0.3125, precision [0.5, 0.0, 0.5, 0.42857142857142855, 0.0, 0.0, 0.0, nan, nan], recall [1.0, nan, 0.16666666666666666, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:57:56.835106: step 1171, loss 2.04074, accuracy 0.4375, precision [0.16666666666666666, 1.0, nan, 0.75, 0.3333333333333333, nan, nan, 0.5, nan], recall [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:57:56.990559: step 1172, loss 1.71694, accuracy 0.5, precision [0.0, 1.0, 0.0, 0.875, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, 0.0, 0.5833333333333334, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:57.140475: step 1173, loss 1.86305, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.16666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:57.295241: step 1174, loss 1.81266, accuracy 0.4375, precision [nan, 0.0, nan, 0.8571428571428571, 0.25, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.42857142857142855, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:57.443714: step 1175, loss 2.02455, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.2, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.26666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:57.592566: step 1176, loss 1.8581, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [nan, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:57.739652: step 1177, loss 1.5251, accuracy 0.5, precision [0.25, nan, nan, 1.0, 0.2, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:57.889165: step 1178, loss 1.49355, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.0, nan, nan, nan, 0.0], recall [1.0, nan, nan, 0.4666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:57:58.039900: step 1179, loss 1.77637, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.25, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:57:58.188934: step 1180, loss 1.84203, accuracy 0.3125, precision [nan, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:57:58.337885: step 1181, loss 1.81654, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.13333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:57:58.481735: step 1182, loss 1.81725, accuracy 0.4375, precision [0.0, 1.0, 0.0, 1.0, 0.25, nan, 0.0, nan, nan], recall [nan, 0.5, nan, 0.4166666666666667, 0.5, nan, nan, nan, nan]
2019-02-19T19:57:58.631636: step 1183, loss 1.69151, accuracy 0.4375, precision [0.0, nan, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.42857142857142855, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:58.785429: step 1184, loss 1.58407, accuracy 0.4375, precision [0.6666666666666666, 0.6, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.2, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:58.939859: step 1185, loss 2.62287, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.5, nan, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:57:59.085827: step 1186, loss 1.99266, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, nan], recall [0.0, nan, nan, 0.38461538461538464, nan, nan, nan, 0.5, nan]
2019-02-19T19:57:59.240581: step 1187, loss 1.97624, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.23076923076923078, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:59.388416: step 1188, loss 2.06893, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.23076923076923078, 1.0, nan, nan, nan, nan]
2019-02-19T19:57:59.543837: step 1189, loss 1.84793, accuracy 0.3125, precision [0.6666666666666666, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.2727272727272727, nan, nan, nan, nan, nan]
2019-02-19T19:57:59.693179: step 1190, loss 1.84276, accuracy 0.4375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 1.0, nan, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:57:59.843514: step 1191, loss 1.48546, accuracy 0.5, precision [0.5, 0.25, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [0.3333333333333333, 0.3333333333333333, nan, 0.6666666666666666, nan, nan, nan, 0.0, nan]
2019-02-19T19:57:59.992692: step 1192, loss 2.00145, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.2, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:00.144432: step 1193, loss 2.01581, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.0, 0.0, 0.0, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:00.296731: step 1194, loss 1.74143, accuracy 0.625, precision [0.0, 0.6666666666666666, nan, 0.8333333333333334, 0.5, nan, 0.0, nan, nan], recall [nan, 0.6666666666666666, nan, 0.625, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:00.447610: step 1195, loss 2.25645, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 1.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.35714285714285715, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:00.596715: step 1196, loss 1.63697, accuracy 0.5, precision [1.0, 1.0, nan, 0.6666666666666666, 0.16666666666666666, nan, nan, 0.0, nan], recall [0.5, 0.6666666666666666, 0.0, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:00.753086: step 1197, loss 2.17546, accuracy 0.1875, precision [0.25, 0.0, 0.0, 0.5, 0.25, nan, 0.0, 0.0, 0.0], recall [0.5, 0.0, nan, 0.125, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:00.899360: step 1198, loss 1.71422, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.2, nan, 0.6, nan, nan, nan, nan, nan]
2019-02-19T19:58:01.049053: step 1199, loss 1.63432, accuracy 0.4375, precision [0.5, 0.75, 0.0, 0.75, 0.0, nan, 0.0, nan, nan], recall [1.0, 0.42857142857142855, nan, 0.375, nan, nan, nan, nan, nan]
2019-02-19T19:58:01.200066: step 1200, loss 1.88373, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.0, nan, nan, 0.0, nan], recall [nan, 0.3333333333333333, 0.0, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:01.343678: step 1201, loss 1.86263, accuracy 0.25, precision [0.0, 0.5, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.09090909090909091, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:01.490380: step 1202, loss 1.52712, accuracy 0.4375, precision [nan, 0.5, 0.0, 0.8, 0.0, nan, nan, 1.0, nan], recall [nan, 0.6666666666666666, nan, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:01.643612: step 1203, loss 1.61746, accuracy 0.5, precision [0.0, nan, 0.0, 0.6, 0.6, nan, 0.0, 0.6666666666666666, nan], recall [nan, 0.0, 0.0, 0.42857142857142855, 0.6, nan, nan, 1.0, nan]
2019-02-19T19:58:01.794798: step 1204, loss 1.91312, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.2857142857142857, 0.0, nan, nan, 0.5, 0.0], recall [0.5, nan, 0.0, 0.25, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:01.951414: step 1205, loss 1.757, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.4, 1.0, nan, nan, 0.3333333333333333, 0.0], recall [nan, 0.25, nan, 0.3333333333333333, 0.4, nan, nan, 1.0, nan]
2019-02-19T19:58:02.105679: step 1206, loss 1.95614, accuracy 0.25, precision [nan, 0.5, 0.0, 0.4, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [nan, 0.25, nan, 0.2222222222222222, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:58:02.259812: step 1207, loss 1.68828, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.5, 0.0, nan, 0.0, nan, nan], recall [nan, 0.5, nan, 0.5714285714285714, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:02.407909: step 1208, loss 1.54562, accuracy 0.6875, precision [nan, 1.0, nan, 0.8888888888888888, 0.0, 0.0, nan, nan, nan], recall [nan, 0.75, 0.0, 0.7272727272727273, nan, nan, nan, nan, nan]
2019-02-19T19:58:02.558986: step 1209, loss 1.53619, accuracy 0.5625, precision [1.0, 0.5, nan, 0.75, nan, nan, 0.0, 0.0, nan], recall [1.0, 0.6666666666666666, nan, 0.5454545454545454, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:02.704521: step 1210, loss 2.22211, accuracy 0.25, precision [0.0, 1.0, 0.0, 0.5, nan, 0.0, nan, nan, 0.0], recall [nan, 0.3333333333333333, nan, 0.2222222222222222, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:02.855733: step 1211, loss 1.8103, accuracy 0.3125, precision [nan, 1.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 1.0, nan, 0.18181818181818182, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:03.004709: step 1212, loss 2.06998, accuracy 0.1875, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.0, nan, nan, nan, 0.0], recall [0.0, 0.3333333333333333, nan, 0.16666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:58:03.154025: step 1213, loss 1.70337, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.6666666666666666, nan], recall [nan, 0.0, nan, 0.4166666666666667, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:03.306513: step 1214, loss 1.84528, accuracy 0.25, precision [nan, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:58:03.455419: step 1215, loss 1.61975, accuracy 0.5625, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.5714285714285714, nan, nan, nan, nan, nan]
2019-02-19T19:58:03.605967: step 1216, loss 2.12341, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [nan, 0.0, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:03.750212: step 1217, loss 1.70904, accuracy 0.3125, precision [0.0, 0.25, 0.0, 1.0, 0.16666666666666666, nan, nan, 1.0, nan], recall [0.0, 1.0, nan, 0.16666666666666666, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:03.905181: step 1218, loss 1.70024, accuracy 0.4375, precision [nan, 1.0, 0.0, 1.0, 0.25, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:04.060458: step 1219, loss 2.39061, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0], recall [nan, 0.5, nan, 0.15384615384615385, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:04.212251: step 1220, loss 1.8164, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.3333333333333333, nan], recall [1.0, 0.0, nan, 0.3333333333333333, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:04.361341: step 1221, loss 1.9991, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.75, 0.2, 0.0, 0.0, 1.0, 0.0], recall [nan, 0.0, nan, 0.2727272727272727, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:58:04.508842: step 1222, loss 1.60537, accuracy 0.5, precision [0.0, nan, 0.0, 0.7777777777777778, 0.3333333333333333, nan, 0.0, nan, nan], recall [nan, nan, nan, 0.5384615384615384, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:58:04.659281: step 1223, loss 1.78955, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.4, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:04.809154: step 1224, loss 1.50993, accuracy 0.4375, precision [0.0, 0.3333333333333333, nan, 0.8, 0.25, nan, nan, 0.3333333333333333, nan], recall [nan, 0.5, nan, 0.4, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:58:04.967735: step 1225, loss 1.86293, accuracy 0.4375, precision [0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [0.6666666666666666, nan, nan, 0.3, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:05.120779: step 1226, loss 1.56648, accuracy 0.5, precision [0.0, 0.0, nan, 0.75, 0.5, nan, nan, 1.0, nan], recall [0.0, 0.0, nan, 0.6, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:58:05.272593: step 1227, loss 1.86604, accuracy 0.375, precision [0.5, 0.3333333333333333, nan, 0.75, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [1.0, 0.5, nan, 0.3, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:58:05.428179: step 1228, loss 1.85049, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.5, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.3333333333333333, 0.25, nan, nan, nan, nan]
2019-02-19T19:58:05.575825: step 1229, loss 2.2246, accuracy 0.25, precision [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:05.727180: step 1230, loss 1.5989, accuracy 0.4375, precision [0.5, 0.0, 0.0, 0.8333333333333334, 0.3333333333333333, 0.0, nan, nan, nan], recall [1.0, 0.0, nan, 0.45454545454545453, 1.0, nan, 0.0, nan, nan]
2019-02-19T19:58:05.877227: step 1231, loss 2.1774, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.75, 0.0, nan, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:06.027519: step 1232, loss 1.75576, accuracy 0.5, precision [1.0, nan, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [0.5, nan, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:58:06.179477: step 1233, loss 1.99712, accuracy 0.0625, precision [0.0, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.08333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:06.328338: step 1234, loss 1.57701, accuracy 0.4375, precision [nan, 0.25, 0.0, 0.8333333333333334, nan, nan, 0.0, 0.3333333333333333, nan], recall [0.0, 0.5, nan, 0.4166666666666667, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:06.476574: step 1235, loss 1.80064, accuracy 0.25, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:58:06.628605: step 1236, loss 1.84308, accuracy 0.5625, precision [1.0, 1.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.6666666666666666, 0.2, nan, 0.75, nan, nan, nan, nan, nan]
2019-02-19T19:58:06.781743: step 1237, loss 1.71242, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.875, 0.5, nan, nan, nan, nan], recall [nan, 0.0, nan, 0.5384615384615384, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:06.928784: step 1238, loss 1.78467, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.75, 0.0, 0.0, nan, 0.5, nan], recall [0.5, nan, nan, 0.25, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:07.081827: step 1239, loss 2.04135, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.14285714285714285, nan, nan, nan, nan, nan]
2019-02-19T19:58:07.234292: step 1240, loss 1.95883, accuracy 0.375, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.375, nan, nan, nan, nan, nan]
2019-02-19T19:58:07.383235: step 1241, loss 1.73934, accuracy 0.3125, precision [1.0, 0.0, nan, 1.0, 0.16666666666666666, nan, nan, 0.5, nan], recall [0.3333333333333333, 0.0, nan, 0.2, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:07.532933: step 1242, loss 2.17781, accuracy 0.3125, precision [0.0, 0.0, nan, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:07.692161: step 1243, loss 1.79543, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, nan, 0.0, nan, 0.0, nan], recall [1.0, nan, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:07.841322: step 1244, loss 1.49275, accuracy 0.5625, precision [1.0, 0.4, 0.0, 0.8333333333333334, 0.3333333333333333, nan, nan, nan, nan], recall [1.0, 1.0, nan, 0.5, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:58:07.991069: step 1245, loss 1.71632, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.875, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.4666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:58:08.147413: step 1246, loss 1.59327, accuracy 0.5, precision [nan, 0.75, 0.0, 0.8333333333333334, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.75, nan, 0.5, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:08.294698: step 1247, loss 1.99003, accuracy 0.25, precision [0.5, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.3333333333333333, 0.0], recall [1.0, nan, nan, 0.18181818181818182, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:08.445172: step 1248, loss 1.54925, accuracy 0.5625, precision [nan, nan, 0.0, 0.875, 0.25, nan, nan, 0.5, nan], recall [nan, 0.0, nan, 0.6363636363636364, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:58:08.595306: step 1249, loss 1.94623, accuracy 0.375, precision [0.0, 0.5, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 0.5, nan, 0.36363636363636365, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:58:08.749549: step 1250, loss 1.83705, accuracy 0.4375, precision [0.6666666666666666, 0.5, 0.0, 1.0, nan, nan, 0.0, 0.0, nan], recall [1.0, 0.75, nan, 0.2, nan, nan, nan, nan, nan]

Evaluation:
[[ 29  10   0  46   0   0   0   4   0]
 [ 12  33   0  93   0   0   0   6   0]
 [  1   0   0  85   1   0   0   0   0]
 [  8  11   0 325   1   0   0   0   0]
 [  4   3   0 141  16   0   0   0   0]
 [  3   1   0  46   1   0   0   2   0]
 [  0   0   0  22   0   0   0   0   0]
 [  8   5   0  74   1   0   0  15   0]
 [  2   1   0  15   0   0   0   0   0]]
2019-02-19T19:58:11.180934: step 1250, loss 1.74906, accuracy 0.407805, precision [0.3258426966292135, 0.22916666666666666, 0.0, 0.9420289855072463, 0.0975609756097561, 0.0, 0.0, 0.14563106796116504, 0.0], recall [0.43283582089552236, 0.515625, nan, 0.3837072018890201, 0.8, nan, nan, 0.5555555555555556, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550606084/checkpoints/model-1250

2019-02-19T19:58:11.401645: step 1251, loss 2.08737, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, 0.0, 0.36363636363636365, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:11.557627: step 1252, loss 2.05256, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.2, 0.0, 0.0, 0.5, nan], recall [0.0, 0.0, nan, 0.1111111111111111, 1.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:58:11.706019: step 1253, loss 1.77028, accuracy 0.5, precision [0.0, 0.5, 1.0, 1.0, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.0, 0.5, 1.0, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:11.857495: step 1254, loss 2.11676, accuracy 0.1875, precision [0.0, 0.0, 0.0, 0.6, 0.0, nan, nan, nan, 0.0], recall [0.0, nan, nan, 0.2727272727272727, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:12.011390: step 1255, loss 1.83861, accuracy 0.25, precision [0.0, 0.2, nan, 1.0, 0.25, nan, 0.0, 0.25, nan], recall [nan, 1.0, nan, 0.08333333333333333, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:58:12.164662: step 1256, loss 1.64509, accuracy 0.5, precision [0.4, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 1.0, nan], recall [1.0, 0.0, nan, 0.45454545454545453, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:12.315656: step 1257, loss 1.56082, accuracy 0.4375, precision [0.5, nan, nan, 1.0, 0.2, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.2727272727272727, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:12.464954: step 1258, loss 1.96005, accuracy 0.375, precision [0.5, 0.6666666666666666, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.375, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:12.614349: step 1259, loss 1.64482, accuracy 0.5, precision [0.5, 1.0, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 1.0, nan, 0.5, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:12.760642: step 1260, loss 2.06315, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, nan, nan, 0.25, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:12.911726: step 1261, loss 2.07482, accuracy 0.375, precision [nan, 1.0, 0.0, 0.8, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.6666666666666666, nan, 0.4, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:13.059999: step 1262, loss 1.8072, accuracy 0.4375, precision [nan, 0.25, 0.0, 0.8, nan, nan, nan, 0.5, 0.0], recall [0.0, 1.0, nan, 0.4444444444444444, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:13.207480: step 1263, loss 1.83357, accuracy 0.375, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [1.0, 0.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:58:13.354147: step 1264, loss 1.97455, accuracy 0.25, precision [0.0, 0.25, 0.0, 0.75, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:58:13.503005: step 1265, loss 1.92823, accuracy 0.375, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:58:13.651797: step 1266, loss 1.56735, accuracy 0.4375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.5, nan], recall [0.0, 0.0, nan, 0.45454545454545453, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:13.804771: step 1267, loss 1.82966, accuracy 0.375, precision [1.0, 0.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.5, nan, 1.0, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:13.951496: step 1268, loss 1.93395, accuracy 0.25, precision [1.0, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.5, 0.0, 0.0, 0.3, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:14.101953: step 1269, loss 2.1635, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.5, 0.3333333333333333, 0.0, 0.0, nan, 0.0], recall [0.0, 0.25, nan, 0.42857142857142855, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:58:14.255080: step 1270, loss 1.42998, accuracy 0.5, precision [nan, 1.0, 0.3333333333333333, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.5, 0.3, nan, nan, nan, nan, nan]
2019-02-19T19:58:14.405727: step 1271, loss 1.73547, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 1.0, nan], recall [0.0, nan, 0.0, 0.4166666666666667, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:14.554943: step 1272, loss 2.14856, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, 0.0, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:58:14.707690: step 1273, loss 1.50233, accuracy 0.4375, precision [nan, 0.3333333333333333, nan, 0.75, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.5, 0.0, 0.6666666666666666, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:14.861226: step 1274, loss 1.90046, accuracy 0.4375, precision [0.5, 0.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, nan, nan], recall [0.5, 0.0, 0.0, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:15.010932: step 1275, loss 1.87577, accuracy 0.5, precision [1.0, 0.5, 1.0, 0.8, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [1.0, 1.0, 0.25, 0.4444444444444444, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:15.168125: step 1276, loss 1.59769, accuracy 0.5625, precision [0.6666666666666666, 0.3333333333333333, 0.0, 0.8333333333333334, nan, 0.0, nan, 1.0, nan], recall [1.0, 0.5, 0.0, 0.5555555555555556, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:15.320517: step 1277, loss 1.86743, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.0, 0.0, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:58:15.466212: step 1278, loss 1.89413, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.6, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.3333333333333333, 0.0, 0.25, nan, nan, nan, nan, nan]
2019-02-19T19:58:15.621804: step 1279, loss 1.44828, accuracy 0.5, precision [0.0, 0.3333333333333333, 0.5, 0.8571428571428571, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, 0.5, 0.46153846153846156, nan, nan, nan, nan, nan]
2019-02-19T19:58:15.774731: step 1280, loss 1.627, accuracy 0.5, precision [nan, 0.0, nan, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:58:15.922484: step 1281, loss 1.52338, accuracy 0.5625, precision [nan, 0.0, 0.0, 0.8888888888888888, nan, 0.0, 0.0, 0.5, nan], recall [nan, 0.0, nan, 0.5714285714285714, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:16.077089: step 1282, loss 1.80392, accuracy 0.4375, precision [0.0, 0.5, 0.0, 1.0, 0.5, nan, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.38461538461538464, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:16.227528: step 1283, loss 1.99117, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.3125, nan, nan, nan, nan, nan]
2019-02-19T19:58:16.377069: step 1284, loss 1.59568, accuracy 0.5, precision [0.3333333333333333, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.46153846153846156, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:16.528942: step 1285, loss 1.84511, accuracy 0.375, precision [nan, 0.5, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:58:16.675620: step 1286, loss 1.62314, accuracy 0.4375, precision [0.0, 0.0, nan, 1.0, 0.25, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:16.825297: step 1287, loss 1.92854, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [nan, nan, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:16.977131: step 1288, loss 2.37536, accuracy 0.125, precision [0.0, 0.0, nan, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.13333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:58:17.124436: step 1289, loss 1.97619, accuracy 0.5, precision [nan, 0.0, 0.0, 0.875, 0.3333333333333333, nan, nan, nan, 0.0], recall [nan, 0.0, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:17.278379: step 1290, loss 2.27846, accuracy 0.3125, precision [nan, 0.5, 0.0, 1.0, 0.0, nan, 0.0, 0.3333333333333333, 0.0], recall [0.0, 0.6666666666666666, nan, 0.18181818181818182, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:17.427549: step 1291, loss 2.00164, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5, 0.5, 0.0, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:17.575836: step 1292, loss 1.70968, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.35714285714285715, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:17.732056: step 1293, loss 1.45642, accuracy 0.4375, precision [0.5, 0.5, 0.0, 0.6666666666666666, 0.25, 0.0, nan, 1.0, nan], recall [1.0, 0.3333333333333333, nan, 0.2222222222222222, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:17.886779: step 1294, loss 1.77986, accuracy 0.375, precision [nan, 0.5, 0.0, 0.7142857142857143, 0.0, nan, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:58:18.032320: step 1295, loss 2.1362, accuracy 0.25, precision [0.0, 0.0, nan, 0.8, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.26666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:58:18.180580: step 1296, loss 2.00267, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.0, nan, 0.0, nan, nan], recall [nan, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:58:18.328266: step 1297, loss 1.71522, accuracy 0.625, precision [1.0, 0.0, 0.0, 1.0, 0.5, nan, 0.0, nan, 0.0], recall [0.5, nan, nan, 0.6153846153846154, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:18.481789: step 1298, loss 1.57403, accuracy 0.375, precision [0.0, nan, 0.0, 0.6666666666666666, 0.25, 0.0, nan, 1.0, nan], recall [nan, 0.0, nan, 0.36363636363636365, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:18.632319: step 1299, loss 2.00422, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.5, nan], recall [0.0, 0.5, nan, 0.4166666666666667, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:18.789888: step 1300, loss 1.91848, accuracy 0.375, precision [0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, nan], recall [0.5, 1.0, nan, 0.25, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:18.939852: step 1301, loss 1.66918, accuracy 0.375, precision [0.0, nan, 0.0, 0.8333333333333334, 0.16666666666666666, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.4166666666666667, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:58:19.087223: step 1302, loss 1.72184, accuracy 0.5, precision [0.5, 0.0, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:58:19.243503: step 1303, loss 1.96822, accuracy 0.3125, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.25, nan], recall [1.0, 0.0, nan, 0.18181818181818182, 0.5, nan, nan, 1.0, nan]
2019-02-19T19:58:19.388344: step 1304, loss 1.63998, accuracy 0.5, precision [nan, nan, 0.0, 0.8571428571428571, 0.3333333333333333, 0.0, nan, nan, nan], recall [nan, nan, nan, 0.46153846153846156, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:19.538905: step 1305, loss 1.52674, accuracy 0.4375, precision [0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 0.0, nan, nan, nan, nan], recall [0.5, 0.6666666666666666, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:58:19.692161: step 1306, loss 1.76345, accuracy 0.3125, precision [0.0, nan, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.4166666666666667, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:19.847489: step 1307, loss 1.84831, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.25, 0.0, nan, 0.3333333333333333, nan], recall [0.0, 0.0, nan, 0.25, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:20.001557: step 1308, loss 2.11165, accuracy 0.3125, precision [1.0, 0.0, 0.0, 0.8, nan, nan, 0.0, 0.0, nan], recall [0.5, nan, nan, 0.3333333333333333, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:20.150461: step 1309, loss 1.65903, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.38461538461538464, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:20.298440: step 1310, loss 1.74876, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.6, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.25, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:20.449645: step 1311, loss 1.91255, accuracy 0.5, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.42857142857142855, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:20.599250: step 1312, loss 1.58884, accuracy 0.625, precision [nan, 1.0, 0.0, 1.0, nan, 0.0, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.6153846153846154, nan, nan, nan, nan, nan]
2019-02-19T19:58:20.752265: step 1313, loss 1.45492, accuracy 0.5, precision [0.5, 0.0, 0.0, 0.875, 0.0, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.5, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:20.900635: step 1314, loss 1.63457, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.36363636363636365, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:21.052930: step 1315, loss 1.76559, accuracy 0.125, precision [0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.07692307692307693, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:21.206068: step 1316, loss 1.58972, accuracy 0.375, precision [1.0, 0.5, nan, 0.6, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.2727272727272727, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:21.355578: step 1317, loss 1.98027, accuracy 0.3125, precision [0.5, 0.2, nan, 1.0, 1.0, nan, 0.0, 0.0, 0.0], recall [1.0, 1.0, nan, 0.15384615384615385, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:21.506459: step 1318, loss 1.98247, accuracy 0.3125, precision [0.0, 0.6666666666666666, nan, 0.5, 0.3333333333333333, nan, 0.0, 0.0, nan], recall [0.0, 0.6666666666666666, nan, 0.2222222222222222, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:58:21.658339: step 1319, loss 1.97007, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.2857142857142857, nan, 0.0, nan, nan], recall [0.0, nan, nan, 0.16666666666666666, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:21.808761: step 1320, loss 1.66688, accuracy 0.375, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, nan, 0.0, 0.0], recall [0.6666666666666666, nan, nan, 0.25, 0.4, nan, nan, nan, nan]
2019-02-19T19:58:21.960755: step 1321, loss 1.87731, accuracy 0.25, precision [0.2, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, nan, 0.0, nan, nan], recall [0.5, nan, nan, 0.16666666666666666, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:22.111212: step 1322, loss 1.76352, accuracy 0.5, precision [0.0, 0.0, 0.0, 0.8888888888888888, 0.0, nan, 0.0, nan, 0.0], recall [0.0, 0.0, nan, 0.5714285714285714, nan, nan, nan, nan, nan]
2019-02-19T19:58:22.263406: step 1323, loss 1.6115, accuracy 0.375, precision [1.0, 1.0, 0.0, 0.5714285714285714, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.5, nan, 0.4444444444444444, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:22.416768: step 1324, loss 1.74632, accuracy 0.4375, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, nan, nan], recall [0.0, 0.0, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:22.567236: step 1325, loss 2.34997, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.15384615384615385, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:22.714384: step 1326, loss 1.81791, accuracy 0.25, precision [nan, 0.0, 0.0, 0.8, 0.0, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.2857142857142857, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:22.859105: step 1327, loss 1.95369, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.35714285714285715, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:23.007209: step 1328, loss 1.51001, accuracy 0.5625, precision [1.0, 0.6666666666666666, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.3333333333333333, nan], recall [0.5, 1.0, nan, 0.45454545454545453, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:23.157624: step 1329, loss 1.42018, accuracy 0.4375, precision [0.5, 0.0, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.45454545454545453, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:23.307911: step 1330, loss 1.85744, accuracy 0.3125, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [0.3333333333333333, nan, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:23.464322: step 1331, loss 2.0442, accuracy 0.4375, precision [0.0, 1.0, 0.0, 0.8, nan, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.75, nan, 0.4, nan, nan, nan, nan, nan]
2019-02-19T19:58:23.618319: step 1332, loss 1.95315, accuracy 0.5, precision [nan, 0.0, nan, 0.8888888888888888, nan, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.5333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:58:23.768320: step 1333, loss 1.91821, accuracy 0.25, precision [0.0, 0.2, nan, 0.75, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:58:23.917916: step 1334, loss 1.7981, accuracy 0.4375, precision [nan, 0.0, 0.0, 0.875, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.5, nan, nan, nan, nan, nan]
2019-02-19T19:58:24.071841: step 1335, loss 1.95105, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:58:24.226339: step 1336, loss 1.99502, accuracy 0.5, precision [0.3333333333333333, 0.0, 0.0, 1.0, 1.0, nan, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.42857142857142855, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:24.376255: step 1337, loss 1.8985, accuracy 0.4375, precision [1.0, 0.5, nan, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:58:24.530170: step 1338, loss 1.69672, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.8333333333333334, 0.25, nan, nan, nan, 0.0], recall [0.0, 1.0, nan, 0.4166666666666667, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:58:24.683569: step 1339, loss 1.67227, accuracy 0.25, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.07692307692307693, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:24.833180: step 1340, loss 1.74525, accuracy 0.3125, precision [0.3333333333333333, nan, 0.0, 0.8, 0.0, nan, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.3076923076923077, nan, nan, nan, nan, nan]
2019-02-19T19:58:24.980736: step 1341, loss 1.84584, accuracy 0.4375, precision [0.5, nan, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, nan, nan], recall [1.0, nan, nan, 0.42857142857142855, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:25.130372: step 1342, loss 1.57488, accuracy 0.4375, precision [nan, 1.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:25.279527: step 1343, loss 2.13506, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, 1.0, nan, 0.21428571428571427, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:25.428552: step 1344, loss 2.24558, accuracy 0.1875, precision [0.3333333333333333, 0.0, 0.0, 1.0, nan, 0.0, nan, 0.2, nan], recall [0.5, nan, nan, 0.08333333333333333, nan, nan, nan, 0.5, nan]
2019-02-19T19:58:25.582812: step 1345, loss 1.70223, accuracy 0.4375, precision [1.0, 0.6666666666666666, nan, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.6666666666666666, 1.0, nan, 0.3, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:25.734679: step 1346, loss 1.36375, accuracy 0.625, precision [nan, 0.4, 0.0, 1.0, 0.0, nan, nan, nan, nan], recall [nan, 1.0, nan, 0.5714285714285714, nan, nan, nan, nan, nan]
2019-02-19T19:58:25.882526: step 1347, loss 2.12464, accuracy 0.1875, precision [0.0, 0.5, 0.0, 0.3333333333333333, 0.0, nan, nan, nan, 0.0], recall [0.0, 0.6666666666666666, nan, 0.09090909090909091, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:26.032540: step 1348, loss 1.88285, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:58:26.177651: step 1349, loss 1.6051, accuracy 0.5625, precision [nan, 0.16666666666666666, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.5714285714285714, nan, nan, nan, nan, nan]
2019-02-19T19:58:26.325924: step 1350, loss 1.73256, accuracy 0.375, precision [0.25, 0.0, 0.0, 0.8333333333333334, nan, 0.0, 0.0, nan, nan], recall [1.0, 0.0, nan, 0.45454545454545453, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:26.472806: step 1351, loss 1.82716, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:58:26.621545: step 1352, loss 1.92222, accuracy 0.25, precision [0.0, 0.25, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [0.0, 1.0, nan, 0.21428571428571427, nan, nan, nan, nan, nan]
2019-02-19T19:58:26.771746: step 1353, loss 1.85747, accuracy 0.3125, precision [0.6666666666666666, 0.0, 0.0, 1.0, 0.25, nan, nan, 0.0, 0.0], recall [0.6666666666666666, nan, nan, 0.2, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:58:26.927167: step 1354, loss 2.10564, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:27.079209: step 1355, loss 1.98288, accuracy 0.25, precision [0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, nan, nan], recall [1.0, 0.5, nan, 0.09090909090909091, nan, nan, nan, nan, nan]
2019-02-19T19:58:27.233516: step 1356, loss 1.55698, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.8571428571428571, 0.3333333333333333, nan, nan, 0.0, nan], recall [0.5, 0.0, nan, 0.6, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:58:27.382284: step 1357, loss 1.75222, accuracy 0.3125, precision [nan, 0.3333333333333333, 0.0, 0.8, 0.0, nan, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:27.533160: step 1358, loss 1.66443, accuracy 0.375, precision [0.0, 0.0, nan, 0.7142857142857143, 0.2, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.4166666666666667, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:27.686959: step 1359, loss 2.16933, accuracy 0.125, precision [0.5, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.3333333333333333, 0.0, nan, 0.09090909090909091, nan, nan, nan, nan, nan]
2019-02-19T19:58:27.836462: step 1360, loss 1.9744, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.5, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.18181818181818182, nan, nan, nan, nan, nan]
2019-02-19T19:58:27.986127: step 1361, loss 1.79107, accuracy 0.3125, precision [0.0, nan, 0.0, 1.0, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:58:28.136622: step 1362, loss 1.77736, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.3, nan, nan, nan, nan, nan]
2019-02-19T19:58:28.288037: step 1363, loss 1.78052, accuracy 0.4375, precision [0.3333333333333333, nan, 0.0, 0.8333333333333334, 0.0, nan, nan, nan, nan], recall [1.0, 0.0, nan, 0.45454545454545453, nan, nan, nan, nan, nan]
2019-02-19T19:58:28.443859: step 1364, loss 1.6399, accuracy 0.4375, precision [1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [0.3333333333333333, 0.5, nan, 0.4444444444444444, nan, nan, nan, nan, nan]
2019-02-19T19:58:28.591984: step 1365, loss 1.77166, accuracy 0.5, precision [0.0, 0.5, 0.0, 0.875, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.5384615384615384, nan, nan, nan, nan, nan]
2019-02-19T19:58:28.739067: step 1366, loss 1.91322, accuracy 0.4375, precision [nan, nan, 0.0, 0.8571428571428571, 0.3333333333333333, 0.0, 0.0, 0.0, nan], recall [0.0, nan, nan, 0.46153846153846156, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:28.893136: step 1367, loss 1.61525, accuracy 0.375, precision [0.0, 0.25, nan, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.38461538461538464, nan, nan, nan, nan, nan]
2019-02-19T19:58:29.046136: step 1368, loss 1.64694, accuracy 0.3125, precision [0.5, 0.3333333333333333, nan, 1.0, 0.0, 0.0, nan, 0.2, nan], recall [0.3333333333333333, 0.3333333333333333, nan, 0.2222222222222222, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:29.198886: step 1369, loss 1.62072, accuracy 0.375, precision [0.2, 1.0, 0.0, 1.0, 0.0, 0.0, nan, nan, nan], recall [1.0, 0.3333333333333333, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:58:29.353148: step 1370, loss 1.76307, accuracy 0.3125, precision [1.0, 0.25, 0.0, 0.6, 0.0, nan, nan, 0.0, nan], recall [0.3333333333333333, 1.0, nan, 0.2727272727272727, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:29.507569: step 1371, loss 1.96894, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:29.660432: step 1372, loss 1.7603, accuracy 0.375, precision [0.5, 0.0, 0.0, 1.0, 0.25, nan, nan, nan, 0.0], recall [0.25, 0.0, nan, 0.4444444444444444, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:29.813050: step 1373, loss 1.73154, accuracy 0.375, precision [nan, 0.3333333333333333, 0.0, 0.7142857142857143, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.5, nan, 0.4166666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:58:29.968970: step 1374, loss 2.12056, accuracy 0.25, precision [0.0, nan, nan, 1.0, 0.16666666666666666, nan, nan, 0.25, 0.0], recall [0.0, 0.0, nan, 0.2, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:30.119452: step 1375, loss 1.904, accuracy 0.5625, precision [1.0, 0.0, 0.0, 1.0, 0.5, 0.0, nan, 0.0, 0.0], recall [1.0, nan, nan, 0.5384615384615384, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:30.269267: step 1376, loss 1.84769, accuracy 0.5625, precision [0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0], recall [1.0, 1.0, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:30.414409: step 1377, loss 2.2773, accuracy 0.25, precision [0.5, nan, 0.0, 0.75, 0.0, 0.0, nan, 0.0, 0.0], recall [0.5, 0.0, nan, 0.3, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:30.566517: step 1378, loss 1.44337, accuracy 0.5, precision [1.0, 0.0, 0.0, 0.8571428571428571, 0.25, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:30.718743: step 1379, loss 1.60743, accuracy 0.375, precision [nan, 0.5, 0.0, 0.8, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.5, nan, 0.36363636363636365, nan, nan, nan, nan, nan]
2019-02-19T19:58:30.871725: step 1380, loss 1.71367, accuracy 0.4375, precision [0.0, 0.5, 0.0, 0.8571428571428571, 0.0, nan, nan, 0.0, nan], recall [nan, 0.5, nan, 0.42857142857142855, nan, nan, nan, nan, nan]
2019-02-19T19:58:31.019598: step 1381, loss 1.66327, accuracy 0.625, precision [nan, 0.5, 0.0, 0.8888888888888888, 1.0, 0.0, nan, 0.0, nan], recall [0.0, 1.0, nan, 0.6666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:31.171638: step 1382, loss 1.80605, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.5, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.4166666666666667, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:31.321091: step 1383, loss 1.83965, accuracy 0.375, precision [1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.3076923076923077, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:31.474097: step 1384, loss 1.94708, accuracy 0.3125, precision [nan, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, nan], recall [0.0, 1.0, nan, 0.23076923076923078, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:31.619953: step 1385, loss 1.94569, accuracy 0.4375, precision [nan, 0.5, nan, 0.8333333333333334, 0.0, 0.0, 0.0, 0.5, nan], recall [nan, 1.0, nan, 0.4166666666666667, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:31.768318: step 1386, loss 1.61294, accuracy 0.5625, precision [0.5, 0.5, nan, 0.8571428571428571, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.5, nan, 0.5454545454545454, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:58:31.914822: step 1387, loss 1.57502, accuracy 0.5, precision [0.0, 0.5, nan, 0.8571428571428571, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [0.0, 0.5, nan, 0.5454545454545454, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:58:32.066017: step 1388, loss 1.77192, accuracy 0.4375, precision [0.0, 0.75, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, nan], recall [nan, 0.75, nan, 0.3, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:32.219851: step 1389, loss 1.95934, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, nan, 0.0, 0.0], recall [nan, 0.5, 0.0, 0.16666666666666666, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:32.370522: step 1390, loss 1.65011, accuracy 0.375, precision [nan, 0.0, 0.5, 0.6, 0.3333333333333333, nan, 0.0, nan, nan], recall [0.0, nan, 1.0, 0.3333333333333333, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:58:32.518648: step 1391, loss 1.95347, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 1.0, nan, nan, 0.0, 0.0], recall [nan, 0.0, nan, 0.2727272727272727, 0.25, nan, nan, nan, nan]
2019-02-19T19:58:32.670854: step 1392, loss 1.90296, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.5, nan], recall [0.0, 0.0, 0.0, 0.36363636363636365, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:32.819077: step 1393, loss 1.96933, accuracy 0.375, precision [1.0, 0.5, 0.0, 0.42857142857142855, nan, 0.0, nan, 0.3333333333333333, nan], recall [1.0, 1.0, 0.0, 0.42857142857142855, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:58:32.966245: step 1394, loss 2.10926, accuracy 0.5, precision [0.0, 1.0, nan, 0.8333333333333334, 1.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.5, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:33.120660: step 1395, loss 1.91237, accuracy 0.3125, precision [0.6666666666666666, 0.2, nan, 0.0, 1.0, 0.0, 0.0, 1.0, nan], recall [0.4, 0.5, nan, 0.0, 0.3333333333333333, nan, nan, 1.0, nan]
2019-02-19T19:58:33.268113: step 1396, loss 1.92712, accuracy 0.3125, precision [0.0, 0.5, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.5, nan], recall [0.0, 0.5, nan, 0.18181818181818182, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:33.418305: step 1397, loss 1.67241, accuracy 0.5, precision [0.5, 0.0, 0.0, 0.8333333333333334, 0.6666666666666666, nan, nan, nan, 0.0], recall [0.5, 0.0, nan, 0.5, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:33.566659: step 1398, loss 1.87564, accuracy 0.25, precision [nan, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, nan, 0.0, 0.5, nan], recall [0.0, 1.0, nan, 0.3333333333333333, 0.0, nan, nan, 0.3333333333333333, nan]
2019-02-19T19:58:33.721011: step 1399, loss 1.7335, accuracy 0.375, precision [0.5, 0.25, 0.0, 0.6666666666666666, 0.5, nan, nan, 0.0, nan], recall [1.0, 0.5, nan, 0.25, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:58:33.872617: step 1400, loss 1.4872, accuracy 0.625, precision [1.0, 1.0, 0.0, 1.0, 0.5, nan, nan, 0.0, 0.0], recall [1.0, 1.0, nan, 0.4, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:34.028333: step 1401, loss 1.67985, accuracy 0.375, precision [0.0, 0.2, 0.0, 0.8333333333333334, 0.0, 0.0, nan, nan, nan], recall [nan, 1.0, nan, 0.38461538461538464, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:34.176405: step 1402, loss 1.6898, accuracy 0.375, precision [nan, 0.0, 0.0, 0.6666666666666666, 0.5, nan, nan, nan, 0.0], recall [0.0, 0.0, 0.0, 0.4, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:58:34.328184: step 1403, loss 1.84642, accuracy 0.4375, precision [0.4, nan, nan, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.3, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:34.478353: step 1404, loss 1.49706, accuracy 0.625, precision [1.0, 0.4, 0.0, 1.0, 0.0, nan, nan, nan, nan], recall [1.0, 1.0, nan, 0.6363636363636364, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:34.629249: step 1405, loss 1.83356, accuracy 0.3125, precision [0.0, 0.5, 0.0, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.5, 0.0, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:34.784576: step 1406, loss 2.14221, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.5714285714285714, 0.0, 0.0, nan, 0.0, 0.0], recall [nan, nan, nan, 0.3076923076923077, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:34.934620: step 1407, loss 1.64931, accuracy 0.4375, precision [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [1.0, 1.0, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:35.081415: step 1408, loss 1.83587, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8333333333333334, 0.0, nan, 0.0, 0.0, nan], recall [nan, 0.3333333333333333, nan, 0.4166666666666667, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:35.232627: step 1409, loss 1.72088, accuracy 0.3125, precision [0.5, 0.0, nan, 1.0, 0.0, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:58:35.384167: step 1410, loss 1.82507, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.36363636363636365, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:58:35.537645: step 1411, loss 1.66941, accuracy 0.5, precision [0.5, 0.6666666666666666, 0.0, 1.0, 0.0, nan, 0.0, nan, 0.0], recall [1.0, 1.0, nan, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:35.685832: step 1412, loss 1.53802, accuracy 0.375, precision [0.0, 0.5, nan, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, 1.0, nan, 0.3333333333333333, nan, nan, nan, nan, nan]
2019-02-19T19:58:35.836825: step 1413, loss 2.61273, accuracy 0.1875, precision [0.0, 0.4, 0.0, 1.0, nan, nan, 0.0, 0.0, 0.0], recall [nan, 0.6666666666666666, nan, 0.07692307692307693, nan, nan, nan, nan, nan]
2019-02-19T19:58:35.988864: step 1414, loss 1.47794, accuracy 0.5625, precision [nan, 0.6666666666666666, 0.0, 0.8333333333333334, 0.5, nan, nan, 0.5, nan], recall [nan, 0.6666666666666666, nan, 0.45454545454545453, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:36.139791: step 1415, loss 1.83774, accuracy 0.25, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [0.0, nan, nan, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:58:36.291589: step 1416, loss 1.46213, accuracy 0.625, precision [nan, 0.75, 0.0, 1.0, nan, 0.0, nan, 0.3333333333333333, nan], recall [nan, 1.0, nan, 0.5, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:36.443773: step 1417, loss 1.8549, accuracy 0.3125, precision [nan, 0.0, 0.0, 0.7142857142857143, 0.0, 0.0, 0.0, nan, nan], recall [nan, 0.0, nan, 0.35714285714285715, nan, nan, nan, nan, nan]
2019-02-19T19:58:36.598206: step 1418, loss 1.76806, accuracy 0.25, precision [1.0, 0.0, 0.0, 0.6666666666666666, 0.5, nan, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.2, 0.3333333333333333, nan, nan, nan, nan]
2019-02-19T19:58:36.746894: step 1419, loss 1.81278, accuracy 0.5625, precision [0.0, 0.75, nan, 0.75, 0.5, 0.0, nan, nan, nan], recall [0.0, 0.75, nan, 0.375, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:36.905242: step 1420, loss 1.82097, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.25, 0.0, nan, nan, 0.0], recall [0.0, 0.0, nan, 0.4166666666666667, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:37.052184: step 1421, loss 1.82827, accuracy 0.375, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 1.0, nan], recall [nan, nan, nan, 0.35714285714285715, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:37.202431: step 1422, loss 2.061, accuracy 0.3125, precision [0.0, 0.25, nan, 0.6, 0.5, 0.0, 0.0, nan, 0.0], recall [0.0, 0.25, nan, 0.3333333333333333, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:37.355021: step 1423, loss 1.79517, accuracy 0.3125, precision [0.0, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.25, nan], recall [nan, 0.0, nan, 0.2857142857142857, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:37.505175: step 1424, loss 1.56231, accuracy 0.3125, precision [0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, nan, 0.25, nan], recall [nan, 0.5, nan, 0.18181818181818182, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:37.654980: step 1425, loss 1.56698, accuracy 0.375, precision [0.0, 1.0, 0.0, 1.0, nan, nan, nan, 0.16666666666666666, nan], recall [nan, 1.0, nan, 0.3, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:58:37.808179: step 1426, loss 2.02999, accuracy 0.25, precision [nan, 0.0, nan, 0.5, nan, nan, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.36363636363636365, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:37.958793: step 1427, loss 1.88218, accuracy 0.3125, precision [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, nan], recall [0.0, 0.5, nan, 0.2727272727272727, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:38.109086: step 1428, loss 2.25112, accuracy 0.25, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.3076923076923077, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:38.260837: step 1429, loss 1.47176, accuracy 0.4375, precision [1.0, 0.0, 0.0, 1.0, 0.25, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:38.412907: step 1430, loss 2.04551, accuracy 0.1875, precision [0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, 0.0, nan, 0.2, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:38.569510: step 1431, loss 1.96357, accuracy 0.3125, precision [0.5, 0.5, nan, 0.6666666666666666, 0.0, nan, 0.0, 0.0, nan], recall [1.0, 1.0, nan, 0.16666666666666666, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:38.719282: step 1432, loss 1.88141, accuracy 0.375, precision [0.0, 0.3333333333333333, nan, 0.75, 0.0, nan, nan, 0.5, 0.0], recall [nan, 1.0, nan, 0.25, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:38.875065: step 1433, loss 1.93782, accuracy 0.375, precision [0.0, 0.0, 0.0, 0.8333333333333334, 0.5, nan, 0.0, 0.0, nan], recall [0.0, 0.0, nan, 0.5555555555555556, 0.25, nan, nan, nan, nan]
2019-02-19T19:58:39.025134: step 1434, loss 1.60983, accuracy 0.5, precision [1.0, 0.3333333333333333, nan, 1.0, 0.0, nan, 0.0, 0.5, nan], recall [1.0, 1.0, 0.0, 0.4166666666666667, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:39.177369: step 1435, loss 1.95614, accuracy 0.25, precision [0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.0, nan, 0.0], recall [0.0, 1.0, nan, 0.2222222222222222, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:39.328553: step 1436, loss 2.10708, accuracy 0.3125, precision [nan, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, nan, 0.0], recall [0.0, nan, nan, 0.36363636363636365, 1.0, nan, nan, 0.0, nan]
2019-02-19T19:58:39.475384: step 1437, loss 2.00082, accuracy 0.1875, precision [nan, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.1875, nan, nan, nan, nan, nan]
2019-02-19T19:58:39.626557: step 1438, loss 1.95686, accuracy 0.375, precision [0.0, 0.0, nan, 1.0, 0.0, 0.0, nan, 0.0, nan], recall [nan, nan, nan, 0.4, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:39.775005: step 1439, loss 1.51704, accuracy 0.5625, precision [nan, 0.0, 0.0, 0.9, 0.0, nan, nan, nan, nan], recall [nan, nan, 0.0, 0.6, nan, nan, nan, nan, nan]
2019-02-19T19:58:39.924507: step 1440, loss 2.01429, accuracy 0.125, precision [0.0, nan, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.0, nan], recall [0.0, nan, nan, 0.18181818181818182, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:40.071317: step 1441, loss 1.69872, accuracy 0.5, precision [0.5, 0.0, 0.0, 0.8571428571428571, 0.5, 0.0, nan, nan, 0.0], recall [1.0, 0.0, nan, 0.5454545454545454, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:40.223043: step 1442, loss 2.28745, accuracy 0.1875, precision [0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.5, nan, 0.15384615384615385, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:40.371820: step 1443, loss 1.88354, accuracy 0.375, precision [nan, 1.0, nan, 1.0, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, nan, 0.25, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:40.527904: step 1444, loss 1.77008, accuracy 0.25, precision [0.0, 0.5, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 1.0, 0.0, 0.23076923076923078, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:40.680099: step 1445, loss 1.60048, accuracy 0.3125, precision [nan, 0.2, 0.0, 0.5, 0.4, nan, 0.0, nan, nan], recall [nan, 1.0, nan, 0.18181818181818182, 0.6666666666666666, nan, nan, 0.0, nan]
2019-02-19T19:58:40.825689: step 1446, loss 1.73365, accuracy 0.5625, precision [0.0, 0.0, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5714285714285714, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:40.979612: step 1447, loss 1.81737, accuracy 0.4375, precision [0.5, 1.0, nan, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.5, 1.0, nan, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:41.128331: step 1448, loss 1.63766, accuracy 0.5625, precision [nan, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, nan, 0.0, 1.0, nan], recall [nan, 0.5, nan, 0.5, 1.0, nan, nan, 1.0, nan]
2019-02-19T19:58:41.276157: step 1449, loss 1.70079, accuracy 0.375, precision [nan, 0.0, 0.5, 0.8, 0.25, nan, 0.0, 0.0, 0.0], recall [nan, nan, 1.0, 0.3333333333333333, 0.5, nan, nan, 0.0, nan]
2019-02-19T19:58:41.431256: step 1450, loss 2.06782, accuracy 0.4375, precision [1.0, 0.0, 0.5, 1.0, nan, 0.0, 0.0, 0.3333333333333333, nan], recall [1.0, 0.0, 1.0, 0.4, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:58:41.581711: step 1451, loss 1.95389, accuracy 0.1875, precision [nan, 0.0, 0.0, 1.0, 0.0, nan, 0.0, 0.0, 0.0], recall [0.0, 0.0, nan, 0.23076923076923078, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:41.733794: step 1452, loss 1.71683, accuracy 0.375, precision [nan, 0.5, nan, 0.6, 0.25, 0.0, 0.0, 1.0, nan], recall [nan, 1.0, nan, 0.3, 0.25, nan, nan, 1.0, nan]
2019-02-19T19:58:41.885060: step 1453, loss 2.13576, accuracy 0.3125, precision [0.0, 0.0, 0.0, 0.6666666666666666, 0.0, nan, nan, 0.5, 0.0], recall [0.0, nan, nan, 0.36363636363636365, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:58:42.040613: step 1454, loss 1.56953, accuracy 0.3125, precision [0.0, 0.5, 0.5, 0.3333333333333333, nan, nan, 0.0, 0.25, nan], recall [0.0, 0.25, 0.5, 0.4, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:42.195579: step 1455, loss 1.83964, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.3333333333333333, 0.5, nan, 0.0, 0.0, 0.0], recall [nan, 0.6666666666666666, nan, 0.3333333333333333, 0.2222222222222222, nan, nan, 0.0, nan]
2019-02-19T19:58:42.347822: step 1456, loss 2.07792, accuracy 0.125, precision [0.0, 0.0, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, nan], recall [nan, nan, nan, 0.16666666666666666, 0.16666666666666666, nan, nan, 0.0, 0.0]
2019-02-19T19:58:42.500182: step 1457, loss 1.83489, accuracy 0.375, precision [1.0, 0.6666666666666666, 0.0, 0.5, 0.5, nan, 0.0, 0.0, 0.0], recall [1.0, 1.0, nan, 0.25, 0.2, nan, nan, nan, nan]
2019-02-19T19:58:42.655531: step 1458, loss 1.91322, accuracy 0.25, precision [0.0, 0.25, 0.0, 0.6666666666666666, 0.3333333333333333, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.2, 0.25, nan, nan, 0.0, nan]
2019-02-19T19:58:42.812198: step 1459, loss 2.03161, accuracy 0.25, precision [0.0, 0.0, nan, 1.0, 0.3333333333333333, nan, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:42.968522: step 1460, loss 1.92717, accuracy 0.375, precision [0.5, nan, 0.0, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, 0.0], recall [1.0, 0.0, nan, 0.38461538461538464, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:43.121011: step 1461, loss 1.18007, accuracy 0.625, precision [1.0, 1.0, 0.0, 0.4, 0.6666666666666666, nan, 0.0, nan, nan], recall [1.0, 0.625, 0.0, 0.5, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:43.269978: step 1462, loss 1.90744, accuracy 0.25, precision [0.0, 0.5, nan, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [nan, 0.16666666666666666, nan, 0.3333333333333333, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:43.423303: step 1463, loss 2.05242, accuracy 0.0625, precision [nan, 0.0, 0.0, 0.5, 0.0, 0.0, nan, 0.0, nan], recall [0.0, 0.0, nan, 0.08333333333333333, nan, nan, nan, 0.0, 0.0]
2019-02-19T19:58:43.576409: step 1464, loss 1.8669, accuracy 0.3125, precision [0.0, 0.4, nan, 0.75, 0.0, nan, nan, 0.0, 0.0], recall [nan, 0.5, 0.0, 0.3, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:43.731112: step 1465, loss 2.12939, accuracy 0.125, precision [0.0, 0.25, nan, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], recall [0.0, 0.3333333333333333, nan, 0.09090909090909091, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:43.882225: step 1466, loss 1.74341, accuracy 0.375, precision [0.0, 0.6666666666666666, 0.0, 0.75, 0.5, 0.0, 0.0, nan, nan], recall [0.0, 0.6666666666666666, nan, 0.3, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:44.030849: step 1467, loss 1.66171, accuracy 0.5, precision [0.0, 0.0, 0.0, 1.0, 1.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.5, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:44.181488: step 1468, loss 1.4928, accuracy 0.375, precision [0.5, 0.3333333333333333, 0.0, 0.6, 0.3333333333333333, nan, nan, nan, nan], recall [1.0, 0.25, nan, 0.3, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:44.340138: step 1469, loss 1.37102, accuracy 0.75, precision [0.5, 0.6666666666666666, 0.0, 1.0, nan, 0.0, nan, nan, nan], recall [1.0, 0.6666666666666666, nan, 0.75, nan, nan, nan, nan, nan]
2019-02-19T19:58:44.489520: step 1470, loss 1.78082, accuracy 0.5, precision [0.75, 0.0, 0.0, 1.0, 0.0, nan, nan, 0.0, 0.0], recall [1.0, 0.0, nan, 0.5555555555555556, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:44.640199: step 1471, loss 1.93041, accuracy 0.3125, precision [nan, 0.5, 0.0, 0.6, 0.0, 0.0, 0.0, 1.0, nan], recall [nan, 0.3333333333333333, nan, 0.2727272727272727, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:44.791709: step 1472, loss 2.27262, accuracy 0.375, precision [0.0, nan, 0.0, 1.0, nan, nan, 0.0, 0.0, 0.0], recall [nan, 0.0, nan, 0.6, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:44.942492: step 1473, loss 1.58166, accuracy 0.375, precision [0.0, 0.3333333333333333, 0.0, 0.8, 0.0, nan, nan, 0.3333333333333333, nan], recall [nan, 0.2, nan, 0.4, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:45.092086: step 1474, loss 1.45446, accuracy 0.5, precision [nan, 0.3333333333333333, 0.0, 1.0, 0.3333333333333333, nan, nan, 0.3333333333333333, nan], recall [nan, 1.0, nan, 0.45454545454545453, 0.5, nan, nan, 0.5, nan]
2019-02-19T19:58:45.242004: step 1475, loss 2.19293, accuracy 0.25, precision [0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, nan], recall [0.5, 0.0, nan, 0.2222222222222222, 0.0, nan, nan, 1.0, nan]
2019-02-19T19:58:45.397669: step 1476, loss 1.48672, accuracy 0.5625, precision [0.0, 0.0, 0.0, 1.0, nan, nan, nan, 0.2, nan], recall [nan, 0.0, nan, 0.5714285714285714, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:45.547318: step 1477, loss 2.25613, accuracy 0, precision [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, nan], recall [nan, 0.0, nan, 0.0, nan, nan, nan, nan, nan]
2019-02-19T19:58:45.701926: step 1478, loss 1.70706, accuracy 0.5, precision [nan, 0.0, 0.0, 0.8888888888888888, 0.0, nan, 0.0, nan, nan], recall [nan, 0.0, nan, 0.6153846153846154, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:45.852265: step 1479, loss 1.86003, accuracy 0.1875, precision [0.0, 0.0, nan, 0.5, 0.0, nan, nan, 0.0, nan], recall [nan, nan, nan, 0.25, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:46.004492: step 1480, loss 1.55268, accuracy 0.5, precision [0.0, 0.0, nan, 0.8888888888888888, 0.0, nan, nan, 0.0, nan], recall [nan, 0.0, nan, 0.6153846153846154, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:46.160358: step 1481, loss 1.94883, accuracy 0.1875, precision [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, nan, 0.0, 0.0], recall [0.0, 0.0, nan, 0.23076923076923078, nan, nan, nan, nan, nan]
2019-02-19T19:58:46.314975: step 1482, loss 1.44935, accuracy 0.625, precision [0.5, 0.6666666666666666, 0.0, 1.0, 1.0, nan, 0.0, nan, nan], recall [1.0, 1.0, nan, 0.45454545454545453, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:46.461121: step 1483, loss 2.27117, accuracy 0.25, precision [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, nan, 0.0], recall [nan, 0.0, nan, 0.26666666666666666, nan, nan, nan, nan, nan]
2019-02-19T19:58:46.611023: step 1484, loss 1.75534, accuracy 0.375, precision [0.0, 0.5, 0.0, 0.8333333333333334, nan, 0.0, nan, 0.0, 0.0], recall [0.0, 0.5, nan, 0.4166666666666667, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:46.765155: step 1485, loss 1.73095, accuracy 0.25, precision [nan, 0.0, 0.0, 0.6, 0.16666666666666666, 0.0, nan, 0.0, nan], recall [nan, 0.0, nan, 0.25, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:46.917926: step 1486, loss 1.68484, accuracy 0.375, precision [0.5, 0.5, nan, 1.0, 0.3333333333333333, nan, 0.0, 0.25, 0.0], recall [1.0, 0.6666666666666666, nan, 0.1111111111111111, 1.0, nan, nan, 0.5, nan]
2019-02-19T19:58:47.069470: step 1487, loss 1.70577, accuracy 0.5, precision [1.0, 0.0, nan, 1.0, 0.0, 0.0, nan, 1.0, nan], recall [1.0, nan, nan, 0.42857142857142855, nan, nan, nan, 1.0, nan]
2019-02-19T19:58:47.213947: step 1488, loss 2.00199, accuracy 0.125, precision [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, nan], recall [1.0, nan, nan, 0.06666666666666667, nan, nan, nan, nan, nan]
2019-02-19T19:58:47.364613: step 1489, loss 1.67416, accuracy 0.625, precision [0.25, nan, nan, 0.9, 0.0, nan, 0.0, nan, nan], recall [0.5, 0.0, nan, 0.75, nan, nan, nan, nan, nan]
2019-02-19T19:58:47.515396: step 1490, loss 2.01832, accuracy 0.25, precision [nan, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0], recall [nan, 0.0, 0.0, 0.2857142857142857, nan, nan, nan, nan, nan]
2019-02-19T19:58:47.664879: step 1491, loss 1.58942, accuracy 0.5, precision [0.5, 0.3333333333333333, nan, 0.75, 0.0, nan, nan, nan, 0.0], recall [0.5, 0.5, nan, 0.6, 0.0, nan, nan, nan, nan]
2019-02-19T19:58:47.811692: step 1492, loss 1.46453, accuracy 0.5625, precision [0.6666666666666666, 0.0, nan, 1.0, 0.25, nan, nan, 0.0, nan], recall [1.0, nan, nan, 0.46153846153846156, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:47.956742: step 1493, loss 1.69451, accuracy 0.375, precision [nan, 0.25, 0.0, 0.8333333333333334, 0.0, nan, nan, 0.0, nan], recall [nan, 1.0, nan, 0.38461538461538464, 0.0, nan, nan, 0.0, nan]
2019-02-19T19:58:48.104301: step 1494, loss 1.73113, accuracy 0.4375, precision [0.5, nan, 0.0, 1.0, 0.6666666666666666, 0.0, nan, 0.0, nan], recall [1.0, nan, nan, 0.3076923076923077, 1.0, nan, nan, nan, nan]
2019-02-19T19:58:48.251246: step 1495, loss 1.7974, accuracy 0.375, precision [0.5, 0.0, 0.0, 0.75, 0.6666666666666666, nan, nan, 0.0, nan], recall [1.0, 0.0, nan, 0.2727272727272727, 0.6666666666666666, nan, nan, nan, nan]
2019-02-19T19:58:48.406465: step 1496, loss 1.44941, accuracy 0.4375, precision [0.25, 0.3333333333333333, 0.0, 1.0, 1.0, nan, nan, 0.3333333333333333, nan], recall [0.5, 0.25, nan, 0.6, 0.3333333333333333, nan, nan, 0.5, nan]
2019-02-19T19:58:48.560531: step 1497, loss 1.39832, accuracy 0.5625, precision [0.25, 1.0, 0.0, 0.7142857142857143, 0.0, nan, nan, 1.0, nan], recall [1.0, 0.6666666666666666, nan, 0.5555555555555556, 0.0, nan, nan, 0.5, nan]
2019-02-19T19:58:48.709394: step 1498, loss 2.09435, accuracy 0.3125, precision [0.0, 0.3333333333333333, 0.0, 0.75, 0.5, 0.0, nan, 0.0, nan], recall [nan, 0.5, nan, 0.25, 0.5, nan, nan, nan, nan]
2019-02-19T19:58:48.858502: step 1499, loss 1.54821, accuracy 0.5, precision [0.0, 0.25, 0.0, 0.875, 0.0, 0.0, nan, nan, nan], recall [0.0, 0.5, nan, 0.6363636363636364, nan, nan, nan, 0.0, nan]
2019-02-19T19:58:49.011825: step 1500, loss 1.59475, accuracy 0.5, precision [nan, 1.0, nan, 0.7142857142857143, 0.25, 0.0, nan, 0.0, nan], recall [nan, 0.6666666666666666, nan, 0.45454545454545453, 1.0, nan, nan, 0.0, nan]

Evaluation:
[[ 34  10   0  43   0   0   0   2   0]
 [ 12  38   0  90   0   0   0   4   0]
 [  2   1   0  83   1   0   0   0   0]
 [ 16  12   0 314   2   0   0   1   0]
 [  5   3   0 138  18   0   0   0   0]
 [  3   1   0  47   1   0   0   1   0]
 [  0   0   0  22   0   0   0   0   0]
 [ 10   8   0  75   1   0   0   9   0]
 [  2   1   0  15   0   0   0   0   0]]
2019-02-19T19:58:51.474028: step 1500, loss 1.72588, accuracy 0.402927, precision [0.38202247191011235, 0.2638888888888889, 0.0, 0.9101449275362319, 0.10975609756097561, 0.0, 0.0, 0.08737864077669903, 0.0], recall [0.40476190476190477, 0.5135135135135135, nan, 0.37968561064087064, 0.782608695652174, nan, nan, 0.5294117647058824, nan]

Saved model checkpoint to /home/ubuntu/Project/runs/1550606084/checkpoints/model-1500

